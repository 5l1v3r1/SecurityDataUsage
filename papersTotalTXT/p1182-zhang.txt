Decide Now or Decide Later? Quantifying the Tradeoff

between Prospective and Retrospective Access Decisions

Wen Zhang1, You Chen2, Thaddeus R. Cybulski4, Daniel Fabbri1,2, Carl A. Gunter3, Patrick Lawlor4

, David Liebovitz5, Bradley Malin1,2

1EECS Dept.

2Biomedical Informatics Dept.

Vanderbilt University
Nashville, TN, USA

3Dept. of Computer Science

University of Illinois

Urbana, IL, USA

4Dept. of PM&R
5Dept. of Medicine

Northwestern University

Chicago, IL, USA

{wen.zhang.1, b.malin, you.chen, daniel.fabbri}@vanderbilt.edu, cgunter@illinois.edu

{DavidL, cyb, patrick-lawlor}@northwestern.edu

ABSTRACT
One of the greatest challenges an organization faces is deter-
mining when an employee is permitted to utilize a certain
resource in a system. This “insider threat” can be addressed
through two strategies: i) prospective methods, such as ac-
cess control, that make a decision at the time of a request,
and ii) retrospective methods, such as post hoc auditing, that
make a decision in the light of the knowledge gathered af-
terwards. While it is recognized that each strategy has a
distinct set of beneﬁts and drawbacks, there has been lit-
tle investigation into how to provide system administrators
with practical guidance on when one or the other should
be applied. To address this problem, we introduce a frame-
work to compare these strategies on a common quantitative
scale. In doing so, we translate these strategies into classi-
ﬁcation problems using a context-based feature space that
assesses the likelihood that an access request is legitimate.
We then introduce a technique called bispective analysis to
compare the performance of the classiﬁcation models under
the situation of non-equivalent costs for false positive and
negative instances, a signiﬁcant extension on traditional cost
analysis techniques, such as analysis of the receiver operator
characteristic (ROC) curve. Using domain-speciﬁc cost es-
timates and access logs of several months from a large Elec-
tronic Medical Record (EMR) system, we demonstrate how
bispective analysis can support meaningful decisions about
the relative merits of prospective and retrospective decision
making for speciﬁc types of hospital personnel.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright 2014 ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660341 .

Categories and Subject Descriptors
D.4.6 [Operating Systems]: Security and Protection—Ac-
cess Control ; H.2.8 [Database Management]: Database
Applications—Data mining

General Terms
Security, Algorithms

Keywords
access control, audit, context, data mining, decision support

1.

INTRODUCTION

A fundamental tradeoﬀ in authorization pits making a de-
cision prospectively, before access is granted, against mak-
ing a decision retrospectively, when an audit is carried out.
Much of the work on access control has focused on the
prospective decision making, but it has often been pointed
out [15, 20] that retrospective decision making, in which
users beg for forgiveness rather than permission, has some
signiﬁcant advantages. In many applications: (1) it is diﬃ-
cult to determine what access a user requires in advance, (2)
denying access to a user with a legitimate need could result
in signiﬁcant inconvenience, expense, or loss, (3) most users
are responsible and can be trusted to access resources for le-
gitimate reasons, and (4) accountability (such as disciplinary
action) is eﬀective in deterring abuses. An iconic example
of such a situation is access to patient records in Electronic
Medical Record (EMR) systems, where (1) hospital work-
ﬂows are complex and commonly involve emergencies and
unexpected events, (2) lack of timely access could result in
the loss of a patient’s life, (3) most healthcare providers are
highly trained and ethical professionals, and (4) there are
strong penalties for abuse. These four criteria (and oth-
ers, such as the ability in certain cases to roll back an ille-
gitimate action) provide a good qualitative story for when
retrospective decision-making based on audit may be bet-
ter than prospective decision-making based on preventing
access to a resource. We see the phenomena in many non-
computer contexts already. For example, a red light tells a
driver not to cross an intersection, but it does not prevent
the driver from crossing it. On the other hand, there are in-

1182stances where retrospective techniques are inadequate or too
risky: the honor system may not be suﬃcient if the stakes
for abuse are too high and the eﬀectiveness of accountability
is too low.

Given the recognition that retrospective techniques will
have their place, we are led to ask:
is there any system-
atic way to determine when retrospective techniques are
better than prospective ones? Ideally this would be done
quantitatively by measuring the tradeoﬀ between the risks
of addressing an abuse at audit time versus denying access
to user when it is requested.
If we accept the idea that
the implementation of access control provides, in general,
only an approximation of the desired access rules, then we
may be able to quantify the rules with a Receiver Operating
Curve (ROC) that compares false positives to true positives
(a technique commonly used already for biometric authen-
tication systems [18]). Better decision making then means
better Area Under the ROC Curve (AUC) values. For ex-
ample, if we are able to estimate that a prospective access
control system gives proper access 95% of the time (true
positives), but only if we accept that 10% of the time it
will grant access where access should not have been granted
(false positives), then we are on the path to quantify whether
one type of prospective access is better than another. How-
ever, this does not oﬀer a clear way to compare prospective
techniques with retrospective ones. The latter, which can
use information from both before and after a user has ac-
cessed a record, is expected to have better AUC values. The
problem is that we do not have a cost model that allows us
to judge tradeoﬀs between a pair of ROCs.

The aim of this paper is introduce a technique called bis-
pective analysis that can be used to compare prospective and
retrospective techniques for access control by a model that
accounts for the diﬀerent costs associated with false positives
and negatives associated with each model. This is accom-
plished by weighting the ROC models for prospective and
retrospective techniques by their costs and, subsequently,
combining these in a way that enables direct comparison
to see which is better in which circumstance. The primary
contributions are:

• A Novel Cost Analysis Technique We devise a
novel cost comparison method called bispective analy-
sis that allows for an explicit comparison of classiﬁca-
tion models with diﬀerent costs. Once provided with
the knowledge of the variables (i.e., the costs of false
positive and false negative for prospective model, the
costs of false positive and false negative for retrospec-
tive, and the receiver operator characteristic (ROC)
curves for both models), bispective analysis allows ad-
ministrators to calculate which is the better option.
Moreover, bispective analysis provides insight about
the distribution of results under varying cost mod-
els, such that administrators can make decisions when
their conﬁdence in the variables is uncertain (e.g., only
a range of costs are known or only partial costs are
known).

• Classiﬁcation Models for Prospective and Ret-
rospective Security We develop a technique to rep-
resent and evaluate both prospective and retrospective
models. To do so, we translate the context associ-
ated (e.g., other users who accessed a record, when
the access was committed, and where the entity asso-

ciated with the record was located) with each access
into a vector space representation. We then subject
such vectors to a classical machine learning model to
build classiﬁers.
In this way, prospective model and
retrospective model are mapped to a common frame-
work, such that comparable results can be generated.
In addition, due to its simplicity and compactness in
representation, this technique is scalable and adapt-
able to most information systems.

• Empirical Analysis and Case Study We illustrate
how to apply bispective analysis to analyze tradeoﬀs
for a large urban hospital system based on its EMR
audit logs to provide assessments for various positions
at the hospital. We deploy prospective and retrospec-
tive models implemented by the proposed technique in
this system, and then obtain detection results (i.e, false
positive rate, false negative rate) respectively. With
bispective analysis and our detection results, we con-
duct illustrative case studies about the model selection
with diﬀerent assumptions on costs. In doing so, we
assess how the model plays out for ten care provider
positions in the system. The results show how cost
weighting can yields diﬀerent guidance in comparison
to a standard ROC analysis.

The remainder of this paper is organized as follows. Sec-
tion 2 provides a survey of cost-driven security models and
comparison methods for classiﬁcation methods. Section 3
describes foundational concepts, cost function, ROC curve
and traditional cost analysis methods, including ROC con-
vex hull methods and cost curves. Section 4 introduces bis-
pective analysis. Section 5 presents the dataset preparation
and experimental design. Section 6 introduces analysis on
dataset by traditional methods. Section 7 presents exper-
iment with bispective analysis on dataset and case studies
showing the application of bispective analysis in real envi-
ronment. Section 8 discusses potential extensions and varia-
tions of the proposed technique and some limitations of this
work. Finally, Section 9 concludes the paper and suggests
next steps for extending this work.

2. RELATED WORK

In this section, we review how existing cost-based security
models diﬀer from our work. We then review methodologies
to compare classiﬁcation models and the limitations asso-
ciated with applying them to the prospective versus retro-
spective model analysis.
2.1 Cost-based Security Models

According to the National Institutes of Standards and
Technologies [6], organizations should rate their informa-
tion systems in terms of risk across three class:
i) low, ii)
medium, or iii) high. An organization should then adopt
their security protections proportional to such risk. How-
ever, the selected security control may not be appropriate
in that the rating for impact is highly subjective.

To reduce subjectivity in decision making, several risk-
based strategies have been suggested for information secu-
rity management.
In particular, based on the recognition
that business processes are often disrupted by static and
rigid policies, many of these strategies are focused on access
control. Here we review the approaches most related to our
own. First, [5] proposed an adaptive access control model

1183to balance the tradeoﬀ between risk and utility in dynamic
environments. They create a system that encourages infor-
mation sharing among multiple organizations while keeping
its users accountable for their actions and capping the ex-
pected damage an organization could suﬀer due to sensitive
information disclosure. In relation to our own work, they in-
troduce a method to compute the expected risk based on 1)
the uncertainty and 2) the cost associated with an incorrect
decision. Second, [13], introduced a policy-based access con-
trol model to infer a decision for an incoming access. This
is achieved by training classiﬁers, using machine learning,
on known decisions and subsequently inferring the new de-
cision when there is no exact matching pattern. By doing
so, each access decision is assigned a certain degree of risk.
Third, [21] introduced the Beneﬁt and Risk Access Control
(BARAC) system, which identiﬁes a set of correlated ac-
cess requests as a closed system. Based on this system, this
method uses a graph-based model to make a decision for each
access, such that the cost of the entire system is minimized.
All of these lines of research are signiﬁcantly diﬀerent from
our own in that they focus on decisions between prospective
access control models with constant misclassiﬁcation costs,
whereas we investigate a decision between prospective and
retrospective models with varying costs.

2.2 Comparison of Classiﬁcation Models

There are a number of performance measures that can be
applied to assess the robustness of a classiﬁcation model. For
instance, one could assess the accuracy; i.e., the proportion
of total instances that are correctly labeled by the model.
However, accuracy is a biased assessment because it assumes
that false positives and negatives occur at the same rate and
are equally costly. As such, a more nuanced strategy for as-
sessing classiﬁcation models is to measure the ROC under a
range of acceptance levels for false positive and false nega-
tive thresholds. In doing so, the AUC indicates the agility of
a classiﬁer, where the “best” classiﬁer is the one that maxi-
mizes this value. The AUC has been invoked as a common
approach for assessing various classiﬁcation models for in-
formation security, such as intrusion detection systems (e.g.,
[12]), malware detection (e.g., [11]), and auditing techniques
for EMRs (e.g., [1]). We recognize the relevance of machine
learning (for which AUC is a popular evaluation measure),
for information security has been questioned [19]. Yet, we
stress that our goal is to assess how misclassiﬁcation costs,
rather than the machine learning algorithm itself, inﬂuence
information security decisions. AUC also has serious deﬁ-
cencies in itself, 1)it is misleading when ROC curves cross
and 2) it makes an unrealistic assumption on costs [8].

[16] proposed using a method to analyze the ROC convex
hull to compose a dominant classiﬁcation strategy over a
set of classiﬁers and class frequencies (in the form of prior
probabilities). This method begins by constructing a convex
hull from all ROC curves (classiﬁers) to be compared, and
then determines which point in the convex hull corresponds
to the least overall cost, given the costs of each classiﬁer and
prior probabilities. A key advantage of this method is that
it needs only the ratio of costs and ratio of class frequencies
to compose the optimal classiﬁer, such that it is robust to a
changing environment.

Subsequently, [7] introduced an alternative to traditional
ROC analysis, which is called a cost curve. In this model,
the expected cost of a classiﬁer is represented as a function

of costs and class frequencies, such that the expected cost
can be computed explicitly. A cost curve provides several
beneﬁts over the traditional ROC convex hull, including:
1) given speciﬁc cost estimates and prior probabilities, it
is easy to “read-oﬀ” the expected cost, 2) it is immediately
clear which, if any, classiﬁer is the dominant strategy, and
3) it is straightforward to determine how much one classiﬁer
outperforms another. Building on this work, [8] introduced
an approach to compares classiﬁers by computing their ex-
pected overall cost, in terms of a uniﬁed assumption on the
probability density function of the costs of false positives
(negatives).

However, in all of these techniques, it is assumed that the
costs (or cost distributions) of false positive (false negative)
for both classiﬁers are equivalent. Yet, this is clearly not
the case in our situation, which implies that such strategies
could incorrectly select a model. In fact, we verify this to
be the case in our empirical analysis.

3. PRELIMINARIES

This section begins by reviewing basic concepts in classi-
ﬁer performance evaluation that are relevant to our strategy.
Next, we introduce the deﬁnition of the cost of a classiﬁer.
This is followed by a review of the concept of an ROC curve,
and several ROC-based comparison methods for classiﬁers.
Finally, we review the notion of context, which is used in the
implementation of our prospective and retrospective models.
3.1 Basic Concepts

The application of a classiﬁer to a test instance results
in either a correct or an incorrect decision. To assess the
performance of a classiﬁer, we consider the rates of these
results over a set of cases. In doing so, the following sim-
ple measures are relevant: 1) True Positive Rate (tpr): the
fraction of positive samples correctly classiﬁed; 2) False Neg-
ative Rate (f nr = 1 − tpr): the fraction of positive samples
misclassiﬁed; 3) True Negative Rate (tnr): the fraction of
negative samples correctly classiﬁed; and 4) False Positive
Rate (f pr = 1 − tnr): the fraction of negative samples mis-
classiﬁed. Finally we report 5) Accuracy: the fraction of all
samples correctly classiﬁed.

For orientation, it should be made clear that false posi-
tive and negatives have diﬀerent implications (and thus dif-
ferent costs) in prospective and retrospective systems.
In
the prospective system, a false positive indicates the system
approves an illegitimate access, while a false negative indi-
cates the system denies access to a legitimate request. In the
retrospective system, a false positive indicates that no inves-
tigation is performed for an illegitimate access, while a false
negative means the system recommends an investigation for
a legitimate access.
3.2 Cost Function

The cost of a classiﬁer can be represented by Equation
1 [16]. Let π1 and π0 be the prior probabilities of positive
and negative cases, respectively, such that π0 = 1 − π1.
Let p10 and p01 be the f nr and f pr, respectively. And, let
c10 ∈ (0, ∞) and c01 ∈ (0,∞) be the associated costs for the
f nr and f pr, respectively. In the remainder of this paper,
we refer to c10 and c01 as the false negative cost and false
positive cost, respectively.

cost = π1p10c10 + π0p01c01

(1)

11843.3 ROC Curve

The result of a probabilistic classiﬁer is dependent on its
parameterization. For example, the na¨ıve Bayes classiﬁer
incorporates a threshold for the probability with which it
claims a class label (e.g., negative versus positive) corre-
sponds to a certain instance. Traditionally, the result of
a classiﬁer is represented by a (f pr, tpr) pair. The ROC
curve can be obtained by plotting these pairs with respect
to a range of parameterizations of the classiﬁer. And, the
AUC [2] is a commonly used measure for the evaluation of
classiﬁcation models. The larger the AU C of a classiﬁer, the
better its performance.

Now, in this setting, a classiﬁer A is said to dominate
another classiﬁer B if for any point (f prA, tprA), there exists
a point (f prB, tprB), such that tprB > tprA and f prB <
tprA. For example, in Figure 5(a), it can be seen that the
ROC of the retrospective model dominates the ROC of the
prospective model.

Given any combination of π1, π0, c10 and c01, M IN (costA)
< M IN (costB) will be true if A dominates B [16], where
M IN (costX ) is the minimal value of cost over the ROC
curve of classiﬁer X. This proposition is true because the
ROC of A forms the convex hull for both A and B, and
the point (f pr, tpr) that minimizes cost, for any combina-
tion of π1, π0, c10 and c01, is only located on the convex
hull [16]. As noted in Section 2.2, a premise for the convex
hull method is that the cost of a false positive (negative)
is equivalent for both classiﬁer A and B. However, as we
will show in our empirical analysis, selecting security mod-
els by identifying dominance is inappropriate in situations
for which this premise fails to hold.
3.4 Cost Curve

In this section, we review the cost curve introduced in
[7]. As mentioned in the previous section, the cost curve
retains all the merits of the ROC curve, but provides for
several notable beneﬁts. Though it is also hampered by
the assumption of equivalent costs (as mentioned above), it
serves as a foundation of our cost analysis.

Given estimates for π1, c10, π0 and c01, we can discover
It has been
π1c10 is needed to determine the

a point on the ROC curve to minimize cost.
proven that only W = π0c01
point (1 − ¯p10, ¯p01) of ROC that can minimize cost[16].

[7] introduced the concept of a normalized expected cost,
(π1c10 + π0c01) in Equa-
which is deﬁned in equation 2.
tion 2 is the maximized cost because it indicates both p10
and p01 are equal to 1.
In other words, the classiﬁer has
misclassiﬁed all samples. Thus, computing normcost cor-
responds to normalizing cost into the (0,1) range. In this
model, (1 − ¯p10, ¯p01) in the ROC minimizes normcost as
well.

the cost curve). We directly employ this method when a
computation of normcost
is required, but, due to space
limitations, we refer the reader to [7] for the details.

∗

normcost

∗

(K) = ¯p10 · (1 − K) + ¯p01 · K

(3)

K can be interpreted as the false positive cost ratio. In-
formally, this corresponds to the proportion of cost resulting
from false positives.
3.5 Context

In this paper, we refer to the access event that is under
review as the target. This event can be associated with a
wide range of semantics, which we call the context around
the target access. The access itself is a request to a resource
that is issued by a user, but there is a variety of contextual
information that surrounds the target.

We assume that the target access takes place in the midst
of a workﬂow, which we represent as a sequence of accesses,
such that each is associated with the same underlying re-
source. We will represent a workﬂow as  = (cid:4)e1, e2, . . .,
ei, . . ., el(cid:5). For illustration, Figure 1 depicts a series of
accesses to a speciﬁc patient’s EMR from the point of ad-
mission to discharge from a hospital. Here, e3 is the target
access and the corresponding workﬂow is (cid:4)e1,e2,e3,e4,e5,e6(cid:5).
Context can be extracted from the target access itself (e.g.,
the time this access occurs). It can also be extracted from
the corresponding workﬂow (e.g, users participated in the
workﬂow). Note the availability of context in a workﬂow for
the prospective model and the retrospective model are dif-
ferent. The retrospective model can take advantage of the
entire workﬂow, while the prospective model can only take
advantage of the parts of the workﬂow that occur before the
target access.

Figure 1: An example of a workﬂow of accesses to
a patient’s medical record. Here, the target access
e3 is surrounded by a solid rectangle. The other ac-
cesses in the workﬂow are surrounded by a dashed
rectangle. Parts contained by brackets represent
context.

normcost =

π1p10c10 + π0p01c01

π1c10 + π0c01

1

+ p01 · W
= p10 ·
= p10 · (1 − K) + p01 · K

W + 1

W + 1

4. FRAMEWORK

(2)

4.1 Framework Overview

From Equation 2, we can state K = W/(W +1) = π0c01/(π1
c10 + π0c01), which means K and W constitute a one-to-one
mapping. So, the values for ¯p10 and ¯p01 can be determined
by K. Thus, the minimized normcost, denoted by norm-
cost
[7] provides
a detailed method for deriving the curve of normcost
(i.e.,

(K), can be represented by Equation 3.

∗

∗

To orient the reader, Figure 2 provides a high-level view
of the proposed decision process for a speciﬁc user. As pre-
vious work shown [13], reliable access control policies (i.e.,
a prospective model) can be learned by a machine learn-
ing algorithm. We extend this notion for implementation
of both the prospective model and the retrospective model.
To do so, ﬁrst, we extract workﬂows of targeted user from

1185a database of transactions. Next, we construct vectors from
the workﬂows to represent all accesses issued by the user.
For the prospective model, the vectors are composed of con-
textual information that occurs at or before the point of a
target access. For the retrospective model, the vectors are
composed of context observed at any time (i.e., before, at
or after the time of the target access). Next, the vectors are
subject to a standard machine learning framework to build
classiﬁers that are representative of prospective and retro-
spective models. Finally, a decision support system uses
the ROC curves for the classiﬁers and their associated costs
and returns an answer for which classiﬁer (model) should be
adopted to manage this speciﬁc user.

equation 6.

comp(P, R) = ln(

∗
P
∗
R

cost
cost

)

(6)

∗
P and cost

∗
Here, cost
R correspond to the minimized overall
costs given:
i) the false positive (negative) costs estimates
and ii) the prior distributions of positives and negatives. iii)
the ROC curves. When comp(P, R) > 0, the prospective
model incurs greater cost than the retrospective model (de-
noted by R (cid:6) P). When comp(P, R) < 0, the retrospective
model incurs greater cost than the prospective model (de-
noted by R ≺ P). And, when comp(P, R) = 0, the prospec-
tive and retrospective models have equivalent costs (denoted
by R (cid:8) P).

The comparison function contains too many variables to
be visualized in an interpretable manner. Thus, we reduce
the number of variables via a mathematical deduction in
Equation 7. Note we use the cost curve normcost
(K) in
It can be seen that comp(P, R) is a function
Equation 7.
(P )
(P )
(R)
of KP = π0c
10 + π0c
01 /(π1c
10 +
(R)
(P )
(R)
π0c
01 /c
01 . When ratio is a constant z,
01 ) and ratio = c
the comparison function can be represented as M agnitude(
KP ,KR), as shown in Equation 8. Given this representation,
we can then compose a contour for M agnitude(KP , KR) to
investigate the tradeoﬀs under various cost conditions.

(P )
01 ) , KR = π0c

(R)
01 /(π1c

∗

comp(P, R) = ln(

= ln(

π1 ¯p

π1 ¯p

(P )
10 c
(R)
10 c

(P )
10 + π0 ¯p
(R)
10 + π0 ¯p

(P )
01 c
(R)
01 c

π1c

π1c

(P )
10 + π0c
(R)
10 + π0c

(P )
01
(R)
01

·

)

(P )
01
(R)
01
10 c(P )
π1 ¯p(P )
π1c(P )
10 c(R)
π1c(R)
∗
P (KP )
∗
R(KR)

π1 ¯p(R)

)

01 c(P )

01

)

01 c(R)

01

10 +π0 ¯p(P )
10 +π0c(P )
10 +π0 ¯p(R)
10 +π0c(R)

01

01

(7)

(8)

)

Figure 2: An architectual view of the Bispective
Analysis

= ln(

(P )
c
01
(R)
c
01

· KR
KP

· normcost
normcost

4.2 Decision Support

4.2.1 Bispective Analysis

As mentioned earlier, the prospective and retrospective se-
curity models are based on as machine learning algorithms.
Traditional methods (e.g., ROC analysis) for comparing clas-
siﬁers work under the belief that the costs for false posi-
tives (false negatives) are equivalent. However, this premise
does not hold in the prospective versus retrospective secu-
rity decision. Thus, we propose an analytic method called
bispective analysis that extends cost curves to account for
classiﬁers with diﬀering misclassiﬁcation costs. As will be
illustrated, this method has a natural visual interpretation
that can facilitate the decision making process.

To begin, equations 4 and 5 provide formulations for the
overall cost of a prospective (P) and retrospective (R) model,
respectively.

costP = π1p

(P )
10 c

(P )
10 + π0p

(P )
01 c

(P )
01

costR = π1p

(R)
10 c

(R)
10 + π0p

(R)
01 c

(R)
01

(4)

(5)

These functions allow us to derive a comparison function to
compare the costs caused by the two models, denoted by

M agnitude(KP , KR) = comp(P, R)|ratio=z

= ln(z · KR
KP

· normcost
normcost

∗
P (KP )
∗
R(KR)

Figure 3(a) depicts an example of such a contour for one
user associated with the job title of NMH Physician CPOE
(Computerized Provider Order Entry) in the EMR dataset
of our case study. Each line in the contour plot, which we
call a contour line, consists of the points (KP , KR) for which
M agnitude(KP , KR) has a constant value. This value is
represented by the number on the contour line.
To further simplify the decision making process, we can
compose a contour using Equation 9, where sgn(·) is the
sign function. The value of Threshold() must be drawn from
{−1, 0, 1}, which corresponds to R ≺ P , R (cid:8) P and R (cid:6) P ,
respectively. Figure 3(b) provides an example of the contour
after applying this threshold, where the red region corre-
sponds to R (cid:6) P , the blue region corresponds to R ≺ P and
the boundary between them corresponds to R (cid:8) P . To pro-
vide guidance, the former contour should be utilized when
the magnitude of diﬀerence between the prospective and re-
spective models is of interest to an administrator (e.g., the
trends of comparison results when KP and KR changes),
while the latter should be chosen when the administrator is
interested only in which model is dominant.

1186T hreshold(KP , KR) = sgn(M agnitude(KP , KR))

(9)

4.2.2 Probability Computation with Comparison Func-

tion

Intuitively, in contour plot, the proportion of the area de-
termined by T hreshold() = 1 reﬂects the probability that
the retrospective model will be the dominant strategy. For
illustration, in Figure 3(b), the region shaded in red indi-
cates the probability that retrospective is the dominant so-
lution for the NMH Physician CPOE is very high.

This type of contour can enable an administrator to ascer-
tain which model has a higher probability of eﬀectiveness.
To understand how, let us assume that f (KP , KR) corre-
sponds to the joint density function of KP and KR. Now,
KP and KR can be considered independent because they are
derived from two distinct classiﬁcation models. As a conse-
quence, the probability that the retrospective model domi-
nates the prospective model can be represented by Equation
10, where fP () and fR() indicate the density functions of KP
and KR, respectively.
P r(R (cid:6) P ) =

f (KP , KR)dKP dKR

T hreshold(KP ,KR)=1

(cid:2)
(cid:2)

(cid:2)
(cid:2)
(cid:2)

=

=

=

fP (KP )fR(KR)dKP dKR

T hreshold(KP ,KR)=1

(10)

A common and reasonable assumption for fP () and fR() is
the density function of the uniform distribution with range
(0,1) [7, 14]. This is useful because, in combination with
Equation 10, it follows that P r(R (cid:6) P ) corresponds to the
proportion of the contour where T hreshold(KP , KR) = 1.
More formally, this is derived as follows
P r(R (cid:6) P ) =

T hreshold(KP ,KR)=1

fP (KP )fR(KR)dKP dKR
1 · 1dKP dKR

T hreshold(KP ,KR)=1

T hreshold(KP ,KR)=1

dKP dKR.

(11)

4.3 Context-based Classiﬁcation

The context-based classiﬁcation consists of three steps: i)
construct vectors from the workﬂows; ii) train a classiﬁer
on a subset of the vectors; and iii) test the classiﬁer on the
remainder of the vectors. Since this paper does not focus on
a speciﬁc machine learning algorithm, here we focus on the
process by which we construct vectors used for prospective
and retrospective models.

4.3.1 Prospective Model
We use C = {C1, C2, . . . , Ch} to denote the set of context
that is associated with a target access. Cr is composed of
elements from dom(Cr), which is the domain of elements
associated with this type of context. For example, let U ∈ C
denote all users that attend the workﬂow of target access.
As such, we have dom(U ) = {u1, u2, . . . , ud}, such that ui
is a certain user in the system.

In a prospective model, the system needs to make a de-
cision once the target access ei has been issued. At the

moment ei is issued, we only know the accesses transpir-
ing beforehand, which corresponds to 1 = (cid:4)e1, e2, . . . , ei−1(cid:5).
For ei, we can use vectors as representations of all h types of
context. Equation 12 denotes V (U ), the vector correspond-
ing to context U .

V (U ) = (vu1 , vu2 , . . . , vud )

(12)

In this model, vux is set to 1 if ux is observed when at least
one ej ∈ 1 transpires, otherwise it is set to 0.

For example, imagine we want to construct a vector corre-
sponding to U (i.e., V (U )), for the target access e3 in Figure
1. Let dom(U ) = {u1, u2, u3, u4, u5, u6, u7, u8} in the system
and (cid:4)u2, u4, u5, u1, u3, u8(cid:5) be the user sequence correspond-
ing to the workﬂow in Figure 1. 1 = (cid:4)e1, e2(cid:5) is the access
sequence occurring before e3, where e1 and e2 are executed
by u2 and u4 respectively. Thus, the vector corresponding
to U for target user is (0, 1, 0, 1, 0, 0, 0, 0).
We use ⊕ to denote the union of two vectors1. As such,
the vector for all h context can be represented as CV =
V (C1)⊕V (C2)⊕. . .⊕V (Ch).
4.3.2 Retrospective Model

A retrospective model is employed to review the target
access using accesses occurring in the entire workﬂow. These
accesses correspond to 0 = (cid:4)e1, e2, . . . , ei−1, ei+1, . . . , el(cid:5). In
this case, during construction of V (U ), vux is assigned 1, if
user ux exists when at least one ej ∈ 0 transpires (i.e., ej
is executed by ux).
In Figure 1, the user context vector
of the retrospective model is (1, 1, 1, 1, 0, 0, 0, 1).
It is not
necessary for the vector V (Cr) in the prospective model and
retrospective model to be diﬀerent. For example, V (Cr) will
be identical for two models when Cr denotes the time the
target access was issued.

5. EXPERIMENTAL DESIGN

This section provides an overview of the experiments de-
signed for this study.
It begins with a description of the
real electronic medical record (EMR) data. This is followed
by an explanation of how context was modeled to train the
prospective and the retrospective security models. We then
introduce the machine learning algorithm used for training
the models and the speciﬁc measures used for assessing their
performance.
5.1 Electronic Medical Record

The dataset was extracted from three consecutive months
of access logs from the Cerner Corporation’s PowerChart
EMR system in use at Northwestern Memorial Hospital,
which is an 854 bed primary teaching aﬃliate of Northwest-
ern University. All clinicians retrieve clinical information
and enter inpatient notes and orders using the system. Each
entry of the log contains information about a distinct access
made to the EMR is associated with seven pieces of infor-
mation: i) encounter-id, ii) user-id, iii) patient-id, iv) time,
v) user job title, vi) service , and vii) location where the
associated patient is located.

Let us take a moment to provide more detail on what this
information corresponds to. Each (patient-id, encounter-id )
pair deﬁnes a unique workﬂow for patient treatment. This
1For example, vector C = (cid:4)a1, a2, . . . , am, b1, b2, . . . , bn(cid:5) is
the union of vector A = (cid:4)a1, a2, . . . , am(cid:5) and vector B =
(cid:4)b1, b2, . . . , bn(cid:5) (i.e., C = A ⊕ B)

11872 . 9 6 6
2 . 6 9 3 2
2 . 4

2

4

0

2.1 4 7 6

1.8749

1.6021

1.3 2 9 3

5

6

5

1 . 0

0.9

0.8

0.7

0.6

R

K

0.5

0.4

0.3

0.2

0.1

2.4204

2.1476

1.8749

5
1
1
3.5

7
8
3
3.2

2.966
2.6932

6
6
2.9

2
3
9
2.6
2.4204
2.1476
1.8749

0.1

1.6 0 2 1

1 . 3 2 9 3
0.2

1 . 0 5 6 5

0 78375

0.3

0.4

0.5
KP

4.8754
4.6026
4.3298
1
7
4
8
4 . 0

3 . 7

3

5

3 . 5 1 1 5
3 . 2 3 8 7

4.0571

3.7843

3.5115

3.2387

6

2.966
2.6932
2 . 4 2 0 4
7
4
2 . 1
9
1.6 0 2 1
1.3293

1 . 8

4

7

7
9
5
0
7
1
1.0565
3
0.5
8
0.7

6
3
7
0
3
.
0
−

3
8
5
4
3
0
.
0
−

9
1
8
3
2
.
0

1
9
7
2
5
5
2
8
1
.
0
1
−
−

4
1
0
8
5
.
0
−

6
3
7
0
3
.
0
−

3
8
5
4
3
0.0
−

1.6021

3
9
2
1.3

5
6
5
1.0

5
7
3
8
0.7

7
9
0
1
0.5

0.23819

5

7

3

8

0 . 7

0.51097

0 23819
0.6

−
0.7

4

3

0 . 0

6

3

7

5

−

8

3
0
0 . 3
0.8

2
1
5
1
7
8
9
6
9
7
2
.
3
4
1
5
5
.
4
−
2
1
0.8
9
1.1
−
.
1
−
−
−

−0.58014

0.9

0.9

0.8

0.7

0.6

R

K

0.5

0.4

0.3

0.2

0.1

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

0.5
Kp

(a) Contour Plot for M agnitude(KP , KR)

(b) Contour Plot for T hreshold(KP , KR))

Figure 3: Contour plots for the NMH Physician CPOE role in the NMH dataset. The red and blue regions
correspond to when the retrospective and prospective models dominate, respectively.

encounter begins when the patient is admitted to the hos-
pital and ends two weeks after discharge (to ensure that
accesses associated with medical billing are captured). Ta-
ble 1 summarizes each context we use to represent an access
and the size of its domain. Of the remaining information,
there are ﬁve types of context:
i) the time a target access
was issued (Time)2, ii) the hospital service the patient was
on at the time of the target access (e.g., General Medicine
vs. Obstetrics), iii) location in the medical center where the
patient resided when the target access was issued, iv) the
users who commit accesses in the workﬂow of target access
and v) the job titles associated with these users.

set. The samples generated for all 10 users are then com-
bined to form a single dataset for this job title and the overall
performance across the 10 users is measured to evalute the
entire dataset. To ensure the results are representative, we
select job titles from 10 diﬀerent hospital departments. The
job titles and summary statistics are shown in Table 2.

We train a classiﬁer for each user using a support vector
machine (SVM) using an RBF kernel [10]. We utilize a grid
search technique [10] to ﬁnd values for parameters to enable
a robust SVM. For each user in the job title, we use the
classiﬁer trained on the training set of this user to assess the
corresponding test set.

Table 1: The context used in experiment

|Dom|

Users Time
8095

4

Job Titles

Services

Locations

140

43

58

5.2 Dataset Preparation

1 , CV +

2 , . . . , CV +

Without loss of generality, assume target user t partici-
pates in N patient workﬂows. The corresponding context
vectors are CV +
N , which are composed us-
ing the approach described in Section 4.3. These vectors are
associated with a positive label class. We use the follow-
ing process to generate a corresponding set of N negative
labeled instances. We randomly select a workﬂow in which
user t failed to issue an access. From this workﬂow, we ran-
domly select an access and build a corresponding context
−
vector. Doing so N times yields a set of vectors CV
2 ,
−
. . . , CV
N , which are associated with the negative class.
−
Note that we create diﬀerent CV +
for prospective
i
model and retrospective model respectively.

−
1 , CV

i and CV

To conduct our evaluation, we construct 10 datasets, each
of which corresponds to a diﬀerent job title. Let us use Pa-
tient Care Staﬀ Nurse as an example. We randomly pick 10
users whose job titles are Patient Care Staﬀ Nurse. For each
user, we construct N positive samples and N negative sam-
ples using the process described above. We select 80% of the
vectors from the positive and negative samples, respectively,
for the training set, and use the remaining 20% as the test
2For this work, dom(T ime) consists of four values: a) Morn-
ing (6am - 12pm), b) Afternoon (12pm - 6pm), c) Evening
(6pm - 12am), and d) Night (12am - 6am)

y
c
a
r
u
c
c
A

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

P
R

PN

PC

RC

EN

QA

US

Datasets

AC

RR

RE

PS

Figure 4: Accuracy of the prospective and retro-
spective security models

6. TRADITIONAL METHODS TO COMPARE

MODELS

In this section, we compare prospective and retrospective
security models using traditional evaluation strategies to set
a baseline. We observe what kind of decision would be made
by these traditional strategies, and ﬁgure out they may make
unwise decision sometimes.

First, Figure 4 presents the accuracy of both the prospec-
tive model and the retrospective model on 10 datasets. It
can be seen that the retrospective model has a higher accu-
racy than the prospective model for each job title. This ev-
idence supports the hypothesis that contextual information

1188Table 2: Datasets per job titles and the AU C for their corresponding prospective and retrospective models.

Abbrev.

US
QA
PS
RE
RC
AC
PC
PN
EN
RR

Job Title

Unit Secretary

Utilization Review/Quality Assurance 1

Patient Care Assistive Staﬀ

Rehabilitation - Physical Therapist

Resident/Fellow CPOE

Anesthesia CPOE

NMH Physician CPOE
Patient Care Staﬀ Nurse

Emergency Department Patient Care Staﬀ Nurse

Radiology Resident/Fellow

Instances Per Class AUCP
0.984
0.959
0.979
0.944
0.925
0.932
0.953
0.939
0.961
0.919

1839
1069
777
712
504
456
448
382
366
364

AUCR
0.994
0.972
0.983
0.964
0.967
0.953
0.979
0.959
0.976
0.944

e

t

 

a
R
e
v
i
t
i
s
o
P
e
u
r
T

 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.1

0.2

0.3

Prospective Model
Retrospective Model

e

t

 

a
R
e
v
i
t
i
s
o
P
e
u
r
T

 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

Prospective Model
Retrospective Model

e

t

 

a
R
e
v
i
t
i
s
o
P
e
u
r
T

 

Prospective Model
Retrospective Model

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.4

0.5

0.6

False Positive Rate

0.7

0.8

0.9

1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

False Positive Rate

0.7

0.8

0.9

1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

False Positive Rate

(a) ROC of AC

(b) ROC of P S

(c) ROC of RE

Figure 5: ROC curves for the prospective and retrospective models of three job titles.

obtained after a target access can lead to better classiﬁcation
performance. Simply put, an retrospective model can yield
a more correct assessment of an access request. Moreover,
from Table 2 it can be observed that AU CR is larger than
AU CP for every job title, which further indicates that retro-
spective security models are better than prospective security
models under a traditional assumption of costs.

Next, we inspected the ROC curves of the prospective and
retrospective models. The curves for three of the job titles
are depicted in Figure 5. From the ROC curves, we ﬁnd that
the retrospective model dominates the prospective model for
the three datasets. This indicates that, if the assumption of
equal costs for false positive (negative) holds true, then the
retrospective model will always be chosen regardless of the
false positive (negative) cost estimation and prior positive
(negative) probability. The cost curve is considered a dual
representation of the ROC curve. This means using cost
curve would reach the same conclusion (i.e., retrospective
model wins) as the ROC curve for the job titles studied. As
such, we do not present the cost curve in this section.

The assumption of equal costs for security-related classi-
ﬁers is made in almost all previous research. And, if a se-
curity professional worked under this belief, then retrospec-
tive protections would almost be utilized over prospective
models. However, as has been alluded to, this assumption
certainly does not hold and, as the following results will il-
lustrate, can unnecessarily justify costly behavior.

7. HOW THE BISPECTIVE ANALYSIS IN-

FLUENCES SECURITY DECISIONS

This section shows how our proposed technique aﬀects
the prospective versus retrospective decision model. First,
we draw a series of contour plots for M agnitude(KP , KR) or

(P )
01 /c

(R)
01

T hreshold(KP , KR) under a diﬀerent ratio = c
for
job title Radiology Resident/Fellow. We demonstrate how
prospective and retrospective models can be compared from
various pespective. Then, we present several case studies to
show the application of our cost analysis technique in real
environments, which demonstrates our technique can make
a more reasonable decision than traditional methods.
7.1 Make Decision with Bispective Analysis

Figure 6 shows the contour plots of T hreshold(KP , KR)
for the Radiology Resident/Fellow job title. With full knowl-
edge about costs and the prior distribution of positive and
negative instances, we can determine which security is best
by pinpointing the corresponding coordinate in the plot. We
will present case studies later to show this process in detail.
With uncertainty in costs and prior distributions, bispective
analysis can still be conducted through the contour plots
from various perspectives, as we now illustrate.

7.1.1 Probability Analysis

According to section 4.2.2, the area of the region in the
contour plot determined by T hreshold(KP , KR) = 1 equals
the probability that R (cid:6) P . Now, assume that we already
know ratio = 0.3. Then, if we look at the contour plot
corresponding to ratio = 0.3 in Figure 6(a), it is clear that
P (R (cid:6) P ) < 0.5. This means that an administrator should
choose a prospective model to manage the accesses from
Radiology Resident/Fellow when only ratio = 0.3 is known.

7.1.2 Range Narrowing Analysis

In certain instances, with limited knowledge of costs and
prior distributions, the search space can be narrowed into
a small area. When this is possible, it can provide a clear
solution to which model should be selected, even if such a

11890.9

0.8

0.7

0.6

R

K

0.5

0.4

0.3

0.2

0.1

0.9

0.8

0.7

0.6

R

K

0.5

0.4

0.3

0.2

0.1

R

K

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

KP
ratio

(a)

= 0.3

KP

(b) ratio = 1.0

KP

(c) ratio = 3.0

Figure 6: Contour plots for T hreshold(KP , KR) with diﬀerent ratio for the Radiology Resident/Fellow. The red
and blue regions correspond to when the retrospective and prospective models dominate, respectively.

decision was not possible in general. For instance, in an
hospital system, the following assumptions about costs for
misclassiﬁcation in prospective and retrospective systems:

01 ≈ c
(P )
c

(R)
01

(P )
10 > c
c

(R)
10

(13)

(14)

The ﬁrst assumption (Equation 13) states that the cost of
the prospective system allowing a malicious access and the
cost of the retrospective system failing to identify a mali-
cious access are approximately equal. The second assump-
tion (Inequation 14) states that the cost of a prospective
system blocking an access from Radiology Resident/Fellow
would be greater than that of a retrospective system incor-
rectly identifying a normal and historical access from this
job title as malicious. We will discuss how these assumptions
are justiﬁed in our case studies. When such an assumption
holds, we should look at Figure 6(b), which is a contour
(R)
plot of T hreshold(KP , KR) given ratio = c
01 = 1.0.
Additionally, based on these assumptions, it follows that
KP − KR < 0 because the numerator of KP and KR are
(R)
equal according to c
01 , and denominator of KP
(R)
would be larger than that of KR according to c
01
and c
In Figure 6(b), it can be seen that the
KP − KR < 0 is always located at the left of the diagonal
(i.e., the black dashed line in the ﬁgure), a region where the
retrospective security model is always dominant.
(R)
10

(i.e., the
premise that false positive (negative) costs are equal across
two models holds), we have KP −KR = 0, which corresponds
to the dashed line in Figure 6(b). That means our bispective
analysis can still work under the permise as is believed in
traditional ROC analysis.
7.2 Case Studies

Note that when c

01 ≈ c

01 ≈ c

(R)
01 and c

(P )
10 = c

(P )
01 = c

(P )
10 > c

(R)
10 .

(P )

(P )
01 /c

(P )

In this section, we show three examples of bispective anal-
ysis in the domain of healthcare. We consider three job
titles, Patient Care Assistive Staﬀ and Anesthesia CPOE,
(P )
and Rehabilitation - Physical Therapist, estimating c
01 ,
(R)
(R)
01 , c
c
10 for each job title, and then apply bispec-
tive analysis to determine if a prospective or a retrospective
models should be applied on this job title. We show that,
for some jobs, choosing a prospective model will minimize

(P )
10 , and c

cost, disagreeing with techniques that do not take cost into
account. The estimations described are by no means ex-
haustive; rather they exist to demonstrate the utility of a
cost-based decision support.

7.2.1 Cost Estimation

(P )
c
01

(R)
01

(R)
01

(P )
01 and c

for the three job titles.

represents the costs of allowing an inappropriate ac-
cess under a prospective model, while c
represents the
costs of deciding not to review an illegitimate access under
a retrospective model. These costs are generally the result of
ﬁnes under HIPAA, HITECH, and other heathcare security
statues. As the ﬁnes associated with inappropriate access
are likely relatively independent of the security model that
(P )
they were performed under, we assume equality of c
01 and
(R)
c
01 . We also assume that ﬁnes due to inappropriate accesses
are equivalent regardless of who makes them. For the sake
of example, ﬁnes for inappropriate access over eight sepa-
rate incidents in California hospitals ranged from $5,000 to
$225,000, averaging $18,546 per inappropriate access [4, 17].
Costs associated with inappropriate access will vary due to
jurisdiction and individual details, we use this average as
both c
(P )
c
10 represents denying a legitimate access under a prospec-
tive model. This is likely the most diﬃcult cost to estimate,
as it alters behavior in a way that is not currently present
in medical settings. For Patient Care Assistive Staﬀ, which
generally would be assisting another employee that has chart
(P )
access permission, we can estimate c
10 as an hour of person-
nel time with no other costs. The national average wage for
medical assistive staﬀ is $11.73 [3]. For Anesthesia CPOE,
in the best case, withholding physician access to a patient
chart would cause the physician to wait, incurring a cost
of only an hour of personnel time. The national average
hourly compensation for anesthesiologists is $183 [9]. How-
ever, withholding access during a high-risk, high-urgency sit-
uation could result in a number of adverse outcomes, such as
misdiagnosis or drug interactions, reducing quality of care
and introducing the prospect of legal action. There is very
little data on such a scenario. We estimate c
for Anes-
thesia CPOE to be $500, although it could range from our
conservative estimate of $183 to something orders of mag-
nitude higher depending on physician behavior. Physical
therapists generally work in low-urgency situations, so ad-

(P )
10

1190(0.33, 0.46) 

R

K

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.9

0.8

0.7

0.6

R

K

0.5

0.4

0.3

0.2

0.1

0.9

0.8

0.7

0.6

(0.94, 0.81) 

R

K

0.5

(0.82, 0.72) 

0.4

0.3

0.2

0.1

0.1

0.2

0.3

0.4

0.5
KP

0.6

0.7

0.8

0.9

0.1

0.2

0.3

0.4

0.5
KP

0.6

0.7

0.8

0.9

0.1

0.2

0.3

0.4

0.6

0.7

0.8

0.9

0.5
KP

(a) Contour for AC when ratio = 1

(b) Contour for PS when ratio = 1

(c) Contour for RE when ratio = 1

Figure 7: Case Study Contour Plots

(P )
10

verse outcomes are signiﬁcantly less likely. We estimate c
for them as $39.51, the national average wage [3].

(R)
c
10

represents the costs associated with auditing a legiti-
mate access. We assume that this decision only incurs costs
related to personnel time, speciﬁcally an hour of auditor
time at $32.10 [3], again the national average for compli-
ance oﬃcers, and an hour of time from the individual be-
ing audited. Thus c
for Patient Care Assistive Staﬀ is
(R)
approximately $43.83, while c
for Anesthesia CPOE is
10
(R)
approximately $215, and c
for Rehabilitation - Physical
10
Therapist is $71.61.

(R)
10

Table 3: Cost Estimation

c(P )
01

c(R)
01

c(P )
10

c(R)
10

KP

KR

PS

$18,546

$18,546

$11.73

$43.84

0.94

0.81

AC $18,546

$18,546

$183

$215.10

0.33

0.46

RE

$18,546

$18,546

$39.51

$71.61

0.82

0.72

7.2.2 Bispective Analysis on Three Job titles

The resulting values of KP and KR for Patient Care Assis-
tive Staﬀ (PS), Anesthesia CPOE (AC) and Rehabilitation-
Physical Therapist (RE) are in Table 3, assuming 1% of
accesses are inappropriate. Using the contour plots in Fig-
ure 7, we can make the following observations. For AC, a
retrospective model minimizes cost. For PS, a prospective
model minimizes cost. For RE, bispective analysis shows the
prospective model minimizes cost (or at least no preference
between the two). Remember if we use traditional methods,
retrospective models would be chosen for all three job titles.

8. DISCUSSION

8.1 Extensions

In Section 4.2.1, we derive functions for contour plot draw-
ing by ﬁxing ratio. Likewise, we can also derive two other
two-variables functions by ﬁxing KP or KR. This assures
that we can still produce contour plots for decision mak-
ing when we only have the estimation of KP or KR rather
than ratio. In addition, we note that the comp() function
(R)
can also be a function of ratio1 = c
10 , KP and KR.
That means we can obtain a contour plot when only ratio1
is known. In summary, the visualized analysis for decision

(P )
10 /c

can be performed when value of only one of ratio, ratio1,
KP and KR is known.

In addition, note that the ﬁnal decision is made on a role-
basis in our experiment. That means once one of the two
security models is selected by applying our technique and
deployed in the system, all access requests from users in
this role would be evaluated by this model.
In practice,
there may exist a big variance among users in the same role.
Speciﬁcally, there can be diﬀerent ROC curves for diﬀerent
users in the same role.
In this situation, it is inappropri-
ate to apply an uniﬁed security model for all users of the
role. Instead, administrator would need to conduct person-
alized model selection by applying our framework to each
user separately.
8.2 Limitations

Our decision support method relies heavily on the contour
plot of comparison function of two models. That means
n=n(n − 1)/2 contour plots when there are
we may need C 2
options of n models. When n is a large number, we would
need to study too many contour plots to make a decision,
which would oﬀset the visual convenience of contour plot.

Another limitation is that the cost function used in this
paper assumes correct classiﬁcation does not incur cost, which
however is not the case in reality. For example, let us con-
sider retrospective model in hospital system. Assume a user
issued a malicious access to a patient’s record in the sys-
tem, and was identiﬁed later by retrospective system. Even
though the user would be penalized, it is possible the pa-
tient’s information has already been leaked to the public,
which would lead to costly consequence.

9. CONCLUSIONS

This paper proposed a novel framework that enables or-
ganizations to perform comparison between prospective and
retrospective models on a quantitative scale. Developing
such a framework addresses two challenges. First, existing
prospective and retrospective models are semantically dif-
ferent such that their results are not directly comparable.
Second, the assumption that costs of false positive (and false
negative) are equivalent across the classiﬁers needs to hold
for existing technique to conduct cost analysis of multiple
classiﬁers. To address the ﬁrst challenge, we converted the
two security models (i.e., prospective and retrospective) into
a uniﬁed classiﬁcation models by training the same classi-
ﬁers on the data represented by the same set of features

1191(contexts). To address the second challenge, we devise a
visualized analysis method, named bispective analysis, that
leverage contour plot of a comparison function to provide
a direct decision support for administrator. We then ex-
perimented on a real hospital information system with this
framework to show that it can provide good decision sup-
port quality. Somewhat surprisingly, we also found it can
provide decision support even when knowledge about costs
are insuﬃcient.

This work opens up a wide array of opportunities for fea-
ture security research. First, we assumed that prospective
analysis would be done after a workﬂow ends. Yet, in prac-
tice, it could start at any time after the target access hap-
pens. It is worth extending our work to implement such a
prospective system for comparison. Second, cost analysis
method can be extended to handle the situation that cost
for correct classiﬁcation is not equal to zero.

10. ACKNOWLEDGEMENTS

This research was supported by grants CCF-0424422, CNS-
0964063, CNS-0964392 and CNS-1330491 from the National
Science Foundation (NSF), R01-LM010207 from the Na-
tional Institutes of Health (NIH), and HHS-90TR0003/01
from the Oﬃce of the National Coordinator for Health In-
formation Technology at the Department of Health and Hu-
man Services (HHS). Its contents are solely the responsi-
bility of the authors and do not necessarily represent the
oﬃcial views of the HHS, NIH, or NSF. The authors also
thank Nathan Sisterson for supplying the dataset used in
this study.

11. REFERENCES
[1] A. A. Boxwala, J. Kim, J. M. Grillo, and

L. Ohno-Machado. Using statistical and machine
learning to help institutions detect suspicious access to
electronic health records. Journal of the American
Medical Informatics Association, 18(4):498–505, 2011.
[2] A. Bradley. The use of the area under the roc curve in
the evaluation of machine learning algorithms. Pattern
Recognition, 30:1145–1159, 1997.

[3] Bureau of Labor Statistics and U.S. Department of

Labor. Occupational outlook handbook, 2014-15
edition, 2014.

[4] California Department of Public Health. California
Department of Public Health Issues Privacy Breach
Fines to 7 California Health Facilities, 2010.

[5] P. Cheng, P. Rohatgi, C. Keser, P. Karger,

G. Wagner, and A. Reninger. Fuzzy multi-level
security: An experiment on quantiﬁed risk-adaptive
access control. In Proceedings of the IEEE Symposium
on Security and Privacy, pages 222–230, 2007.

[6] K. Dempsey, G. Witte, and D. Rike. Security and

privacy controls for federal information system and
organizations. Technical Report Special Publication
800-53, Revision 4, Washington, DC, 2014.

[7] C. Drummond and R. Holte. Explicitly representing

expected cost: an alternative to roc representation. In
Proceedings of the 6th international conference on
Knowledge Discovery and Data Mining, pages
198–207, 2000.

[8] D. Hand. Measuring classiﬁer performance: a coherent

alternative to the area under the roc curve. Machine
Learning, 77:103–123, 2009.

[9] B. Herman. 72 Statistics on Hourly Physician

Compensation, 2013.

[10] C. W. Hsu, C. C. Chang, and C. J. Lin. A practical

guide to support vector classiﬁcation. Technical
report, Dept. of Computer Science and Information
Engineering, National Taiwan University, Taipei,
Taiwan, 2003.

[11] J. Z. Kolter and M. A. Maloof. Learning to detect
malicious executables in the wild. In Proceedings of
the 10th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages
470–478, 2004.

[12] R. Lippmann, D. Fried, I. Grad, et al. Evaluating

intrusion detection systems: the 1998 darpa oﬀ-line
intrusion detection evaluation. In Proceedings of the
DARPA Information Survivability Conference and
Exposition, pages 12–26, 2000.

[13] I. Molloy, P. Cheng, J. Dicken, A. Russo, and

C. Morisset. Risk-based access control decisions under
uncertainty. In Proceedings of the 2nd ACM conference
on Data and Application Security and Privacy, pages
157–168, 2012.

[14] J. H. Orallo, P. Flach, and C. Ferri. A uniﬁed view of

performance metrics: Translating threshold choices
into expected classiﬁcation loss. Journal of Machine
Learning Research, 13:2813–2869, 2012.

[15] D. Povey. Optimistic security: a new access control
paradigm. In Proceedings of the Workshop on New
Security Paradigms, pages 40–45, 1999.

[16] F. Provost and T. Fawcett. Analysis and visualization
of classiﬁer performance: Comparison under imprecise
class and cost distributions. In Proceedings of the 3rd
international conference on Knowledge Discovery and
Data Mining, pages 43–48, 1997.

[17] K. Robertson. Kaiser ﬁned for employees checking
medical records of octuplets - Sacramento Business
Journal, 2009.

[18] R. Snelick, U. Uludag, A. Mink, M. Indovina, and

A. Jain. Large-scale evaluation of multimodal
biometric authentication using state-of-the-art
systems. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 27(3):450–455, 2005.

[19] R. Sommer and V. Paxson. Outside the closed world:

On using machine learning for network intrusion
detection. In Proceedings of the IEEE Symposium on
Security and Privacy, pages 305–316, 2010.

[20] D. Weitzner. Information accountability.

Communications of the ACM, 37(6):82–87, 2008.

[21] L. Zhang, A. Brodsky, and S. Jajodia. Toward

information sharing: Beneﬁt and risk access control.
In Proceedings of the 7th IEEE International
Workshop on Policies for Distributed Systems and
Networks, pages 45–53, 2006.

1192