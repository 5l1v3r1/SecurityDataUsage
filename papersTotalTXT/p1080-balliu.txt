Automating Information Flow Analysis of Low Level Code

Musard Balliu, Mads Dam, Roberto Guanciale

KTH Royal Institute of Technology
SE-100 44, Stockholm, Sweden
{musard,mfd,robertog}@kth.se

ABSTRACT
It lacks structure, it uses
Low level code is challenging:
jumps and symbolic addresses, the control ﬂow is often highly
optimized, and registers and memory locations may be reused
in ways that make typing extremely challenging. Informa-
tion ﬂow properties create additional complications: They
are hyperproperties relating multiple executions, and the
possibility of interrupts and concurrency, and use of devices
and features like memory-mapped I/O requires a departure
from the usual initial-state ﬁnal-state account of noninterfer-
ence. In this work we propose a novel approach to relational
veriﬁcation for machine code. Veriﬁcation goals are ex-
pressed as equivalence of traces decorated with observation
points. Relational veriﬁcation conditions are propagated be-
tween observation points using symbolic execution, and dis-
charged using ﬁrst-order reasoning. We have implemented
an automated tool that integrates with SMT solvers to au-
tomate the veriﬁcation task. The tool transforms ARMv7
binaries into an intermediate, architecture-independent for-
mat using the BAP toolset by means of a veriﬁed translator.
We demonstrate the capabilities of the tool on a separation
kernel system call handler, which mixes hand-written assem-
bly with gcc-optimized output, a UART device driver and a
crypto service modular exponentiation routine.

Categories and Subject Descriptors
D4.6 [Operating Systems]: Security and Protection—In-
formation ﬂow controls, Security kernels, Veriﬁcation; D.2.4
[Software Engineering]: Software/Program Veriﬁcation—
Formal methods, Model checking

Keywords
Information Flow Security; Formal Veriﬁcation; Symbolic
Execution; Machine Code

INTRODUCTION

1.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660322.

The ultimate goal of information ﬂow analysis is to es-
tablish conﬁdentiality and integrity properties of real code
executing on commodity CPUs. In the literature, normally
this problem is addressed at the source code level. There it
may be more forgiving to ignore messy low level problems,
e.g.
regarding timing, complex control ﬂow, or hardware
speciﬁcs. Also, one may appeal to special compilers that
avoid diﬃcult optimizations, or work around machine fea-
tures such as caching, instruction reordering, concurrency,
I/O, interrupts, bus contention and so on, that are diﬃcult
to handle in a precise manner.

Sometimes, however, source level analysis is less suitable.
This is certainly the case when dealing with third-party
code, but it applies in other cases too, for instance, for
heavily optimized or obfuscated code, and for kernel han-
dler routines that manipulate security sensitive peripherals
such as privileged processor registers, MMUs, and bus and
interrupt controllers.

The literature has two “standard” approaches to informa-
tion ﬂow control (IFC) for low level languages: (a) For static
veriﬁcation, most authors, cf.
[33, 12], have attempted to
reimpose typing and high level structure at the assembly or
byte code level, in order to reuse standard type-based tech-
niques for high level languages. For instance, [33], uses this
approach for typed assembly language, and [12] takes a re-
lated approach to Java bytecode. (b) Most work, however,
has focused on dynamic techniques, often using some com-
bination with static analysis to generate labels, or tags, to
help minimize the dynamic overhead, cf.
[13, 29, 46, 31,
21]. For instance, [21] proposes a machine architecture with
hardware-supported tag propagation to support dynamic in-
formation ﬂow tracking.

Neither of these schools are very helpful, though, when it
comes to the problem we have set out to study: Information
ﬂow analysis for low level code on commodity processors. In
this domain, existing static approaches are too imprecise due
to lightweight (data/ﬂow/path/timing-insensitive) analysis,
while dynamic approaches suﬀer from the well known prob-
lem of label creep and introduce undesired runtime overhead
[42]. Security testing-like techniques [37, 5], which we dis-
cuss later, provide impressive results in terms of scalability,
however, they are in general unsound and can not directly
be used for full veriﬁcation.

Instead we propose to directly verify relational (i.e.

in-
formation ﬂow) properties at machine code level, leveraging
as much as possible recent progress on low level code anal-
ysis tools such as BAP [14], McVeto [48], Vine/BitBlaze
[45]. Code for our target machine, ARMv7, is ﬁrst lifted to

1080a machine-independent intermediary form, BIL, using the
BAP tool [14]. This process uses a lifter that is produced
from the Cambridge HOL4 model of ARMv7 [28]. This al-
lows the reuse and extension of BAPs program veriﬁcation
back end to symbolically execute the resulting BIL code. We
use this to ﬁrst perform unary analysis and then verify re-
lational properties by propagating relational preconditions
through each of a pair of related programs until a pair of
observation points are reached, that need to be matched, in
order for the relational property to hold. These observation
points are memory write events, to locations that are stat-
ically determined to be observable by some external agent,
because of multithreading, or memory-mapped I/O, or for
some other reason. Matching is done by SMT solving using
STP [24], on formulas that tend to grow huge, but generally
rely only on linear arithmetic, uninterpreted functions, and
arrays, and so are not too costly to check. Special care is
needed for memory accesses which introduce quantiﬁer alter-
nation, hence we propose an instantiation technique which
ensures the resulting formulas are quantiﬁer free.

Three distinguishing features make our information ﬂow
loop invariants, tim-
analysis both useful and challenging:
ing and traces. Loops are handled using (relational) invari-
ants/widening. We point out that relational invariants can
be signiﬁcantly simpler than state invariants as they may
not require proving functional correctness of the loop. Our
case studies show that the invariants we provide are con-
junctions of linear equalities, which, as shown in recent work
[44], can be generated automatically. Timing is particularly
critical. The timing information is included in the symbolic
state and propagated with the other constraints. The model
used here scales to functional cost models, i.e. models where
the timing cost can be calculated as function of the input
instruction, independent of the history. This is evidently re-
alistic only for simple processor architectures such as ARM
Cortex-M (but we note that a vast number of such proces-
sors are in use today in critical control applications). Richer
and tractable timing models that can take into account also
features like caches and instruction pipelines are, however,
currently not available at ISA level, and we leave this for
future work. Finally, the trace-based analysis broadens the
number of target applications handled by our technique, in-
cluding preemptive environments and scheduling.

We are the ﬁrst to admit that the approach will suﬀer
from scalability problems, for instance due to path explo-
sion, and due to the generally complex and detailed machine
state. However, our primary application is separation ker-
nel handler veriﬁcation, and this domain is generally char-
acterized by critical machine code fragments that are rather
small (generally under 1K instructions per handler), but also
tricky. The case studies reported in this paper are based on
syscall handlers and device drivers of slightly more than 250
lines of ARMv7, produced by a mix of hand-crafted assem-
bly and GCC-optimized C.

Overall, this paper makes both theoretical and practical
contributions. On the theoretical side, we present a novel ap-
proach for formal relational machine code veriﬁcation, with
focus on information ﬂow security properties. The combi-
nation of unary and relational analysis makes our approach
appealing for precise security analysis of machine code. We
provide a new angle with the inclusion of timing information
into the state and with the invariant handling which ensures
a nice compositional property over traces. On the practical

side, we present the ﬁrst automated toolset for information
ﬂow analysis of ARMv7 binaries. We exercise the tool on
non-trivial case studies including separation kernel syscalls,
device drivers and crypto routines. For a more thorough
discussion of related work, please refer to Sect. 8.

2. THREAT MODEL AND SECURITY

In our target applications, trusted and untrusted agents
share and control parts of the system memory. Our goal is to
ensure that the only information channels connecting agents
to each other are the intended ones. These intended chan-
nels can be shared buﬀers, network connections, or speciﬁc
communication devices such as the message sending syscall
handler considered later in the case study. They can also be
memory-mapped devices connecting agents to the external
world such as a UART device.
In the special case where
channels form the usual security lattice, the goal reduces to
classical Goguen-Meseguer information ﬂow [25], which re-
quires the state of the untrusted program be unaﬀected by
the state of the trusted one.

We use the ARMv7 program in Fig. 1 to elucidate our
threat model. The program loads a memory pointer from
the address 2048 into the register R3. Subsequently, the
memory referenced by the pointer is updated three times.
The program always terminates with the following eﬀect on
the system state: (i) the memory pointer is loaded into the
register R3, (ii) the registers R1 and R2 are updated to zero
(lines 0x110 and 0x118), (iii) the “zero ﬂag” Z is enabled (the
instruction at line 0x110 contains the S suﬃx, thus over-
riding Z according to the result of the executed arithmetic
operation [4]) and (iv) zero is written (line 0x114) into the
memory referenced by the pointer.

0x0f4 MOV R2, #0
0x0f8 LDR R3, [PC+#0x700] //2048
0x0fc STR R2, [R3]
0x100 LDR R1, [PC+#0x2f8] //1024
0x104 ADDS R1, R1, R2
0x108 MOVEQ R2, #1
0x10c STR R2, [R3]
0x110 MOVS R2, #0
0x114 STR R2, [R3]
0x118 MOV R1, #0

Figure 1: ARMv7 Program

Assume that the system memory from address 0 to 2047
contains the state of a trusted agent and that the remaining
part of the memory contains the state of an untrusted agent.
If an observing agent is not able to access its own memory
while the program is executed, then the above program can
be considered secure, since after termination the state of the
untrusted agent is unaﬀected by the state of the trusted one.
However, in several scenarios this requirement is not satis-
ﬁed: (i) the observer controls a device that is mapped to a
memory area that belongs to the untrusted agent, (ii) the
code is interrupted and scheduled in a preemptive environ-
ment, (iii) memory stores have side eﬀects, or (iv) the code
is executed in a multi-core setting.

In all these cases, a departure from the usual initial-state
ﬁnal-state account of noninterference is required. In partic-
ular, all updates to the untrusted memory (e.g. lines 0x0fc,
0x10c and 0x114) can be monitored by the attacker and re-

1081veal secret information. Hence our example program can
not be considered secure. In fact, depending on the content
of the memory of the trusted agent, the assembly fragment
can have the following executions:

1) If the memory at address 1024 is zero (line 0x100) then
(i) the instruction 0x104 enables the ﬂag Z, (ii) the instruc-
tion 0x108 updates the register R2 to 1, (iii) thus the address
referenced by the pointer is updated three times, with the
values 0, 1 and 0, respectively.

2) Otherwise (i) the instruction 0x104 disables the ﬂag Z,
(ii) the instruction 0x108 has no eﬀect, (iii) thus the memory
referenced by the pointer is updated three times, always with
the value zero.

Consequently, an attacker capable of observing the rele-
vant memory state will in one case see the referenced mem-
ory location ﬂicker, and in another case not. The security
condition must prevent this phenomenon.

In this paper we work with observational determinism
[32], deﬁned as follows. Assume a set of conﬁgurations C
and a transition relation → ⊆ C × C. An execution is a
maximal ﬁnite or inﬁnite sequence

π = C0 → ··· → Cn → ···

(1)

of conﬁgurations related by the transition relation. The ini-
tial conﬁguration of π in (1) is C0. A transition may give rise
to an observation obs. In our case, observations are timed
writes to observer readable memory. An observation trace,
or just trace, trc(π), of π extracts from π the sequence of
observations produced by π. The traces π1 and π2 are then
trace equivalent, if trc(π1) = trc(π2).
Since we assume a concept of observer readable memory,
it makes sense to deﬁne the relation C ≡ C(cid:48) by requiring
that the observer readable memory of C and C(cid:48) are the
same. This is the familiar notion of observational, or low
conﬁguration (state) equivalence. We can then proceed to
deﬁne observational determinism.

Definition 1

(Observational Determinism). A set
P of executions π is observational deterministic, if for any
pair of executions π1, π2 ∈ P with initial conﬁgurations C1
and C2, respectively, if C1 ≡ C2 then π1 and π2 are trace
equivalent.

Observational determinism works well for a class of nonde-
terministic programs, and it is preserved under reﬁnement
[32].
In particular it avoids the quantiﬁer alternation in
bisimulation-oriented unwinding conditions. To be accu-
rate, observational determinism presupposes that all non-
determinism can be relegated to the initial state. This is
true in our case. However, observational determinism is less
suitable when alternation is essential, for instance in the case
of strategic reasoning.
3. MACHINE MODEL

For the theory development we consider deterministic pro-
grams written in a simple machine language (SiML) with the
following instruction syntax

ι

exp

::= reg := exp | cjmp(exp, exp, exp) | assert(exp)
| assume(exp) | store(exp, exp) | halt
::= load(exp) | bop(exp, exp) | reg | P C
| uop(exp) | v

There are registers reg ∈ Reg, the program counter P C and
values v ∈ Val that are, for simplicity, taken as primitive.

Instructions include register assignments, memory stores,
conditional jumps, assertions, assumptions, and halt. Ex-
pressions include unary and binary operations on constants,
register lookups, and memory loads.
A SiML program is evaluated in the context of a regis-
ter state ∆ : Reg (cid:55)→ Val , a program counter pc ∈ V al and
(cid:55)→ Val . We assume a SiML pro-
a memory state µ : Val
grams be non-self modifying, thus instructions are stored
(cid:55)→ ι which is
in a separate instruction memory Π : Val
usually addressed via the program counter. The instruc-
tion memory is a total function and we assume that for
each possible address v outside the executable part of the
memory Π(v) = assert(0). A conﬁguration C is either a
tuple (Π, ∆, µ, pc, t) of instruction memory, register state,
data memory, program counter and current execution time
t ∈ N, or an error conﬁguration ⊥, used to handle failing as-
serts. The transition relation has the shape C → C(cid:48) where
C (cid:54)=⊥. SiML is a subset of the BAP Intermediate Language
(BIL), which is used for lifting the ARMv7 binaries. We
refer to [15] for a complete deﬁnition of the operational se-
mantics of BIL. Instructions and expressions are evaluated
in the context of a conﬁguration. For instance, the assign-
ment reg := exp assigns to register reg the value of exp, the
conditional jump cjmp(e1, e2, e3) transfers control to e2 if e1
is true (non-zero), otherwise to e3. The assert(b) statement
terminates the program abnormally if b is false, otherwise it
has no eﬀect on the state, the store(e1, e2) stores the value
of e2 at memory location e1 and the expression load(e1)
loads the value at location e1. We distinguish between nor-
mal and abnormal executions. A normal execution, if it ever
terminates, executes halt as the last instruction. Otherwise
the execution is abnormal and terminates in the error conﬁg-
uration ⊥. The length of π is len(π), the i-th conﬁguration
of π is π(i), ι(π, i) = Π(pci), pc(π, i) = pci, and t(π, i) = ti.
A model M consists of the set of executions π induced by
some set of initial conﬁgurations C0.

Our target applications require reasoning about the exe-
cution time of the program. The timing behavior is highly
architecture-dependent and is in general very diﬃcult to cap-
ture accurately.
In this paper we work with a functional
time model τ : ι → N which assigns a ﬁxed parameter-
dependent cost to each instruction. That is if we have
(Π, ∆, µ, pc, t) → (Π, ∆(cid:48), µ(cid:48), pc(cid:48), t(cid:48)) then t(cid:48) = t + τ (Π(pc)).
For instance, the execution time of a load instruction will
depend on the number of bytes to load from the mem-
ory. Under these assumptions, the execution time is history-
independent and the timing model is deterministic.

4. UNARY SYMBOLIC ANALYSIS

We reason about the behavior of a program by means of
forward symbolic analysis. The analysis allows us to build a
logical formula, which corresponds to multiple program exe-
cutions, and leverage ﬁrst-order reasoning to statically prove
program properties. The program is executed on symbolic
inputs and, consequently, the state is also symbolic.
Ini-
tially, registers are mapped to fresh variables, the memory
is a variable representing an uninterpreted function and the
program counter is a constant. We use exps and es to range
over symbolic expressions, which are built over these ini-
tial variables and constants using the standard machinery
In particu-
and have either type memory or type value.
lar, if exps is a memory expression and exps
2 are

1 and exps

1082expressions of type value, then exps(exps
of type value representing the lookup of exps
exps[exps
corresponding update.

1) is an expression
1 in exps, and
2] is a memory expression representing the

1 (cid:55)→ exps

A symbolic state is a tuple Σs = (∆s, µs, pc), where ∆s
maps registers to symbolic expressions, µs is a symbolic
memory expression and pc is the concrete value represent-
ing the program counter. A symbolic conﬁguration C s is
a tuple (Π, ∆s, µs, φ, pc, t), which extends a symbolic state
with a path predicate φ, the instruction memory Π and the
execution time t. The path predicate φ, also path condition,
is a symbolic boolean expression built over the initial vari-
ables and constrains the set of concrete initial states that
execute the path. Usually, the path condition of the initial
conﬁguration entails the program preconditions.

Forward symbolic semantics is given by the transition
rules on symbolic conﬁgurations depicted in Fig. 2. Here, we
use ∆s, µs (cid:96) exp ⇓ exps to represent the symbolic evaluation
of an expression exp in the context (∆s, µs). For instance,
if ∆s, µs (cid:96) exp ⇓ exps then ∆s, µs (cid:96) load(exp) ⇓ µs(exps).
Notice that, since we assume non-self modifying code, we
omit the constant instruction memory Π from the rules and
the time is increased independently of the processor state.
The cjmp rules evaluate the jump target in the current sym-
bolic state and then update the program counter and the
path condition depending on whether the jump condition is
satisﬁable. The jump target can be a symbolic expression
which requires to resolve all possible targets in the current
context. This can be addressed by enumerating all concrete
jump targets that are consistent with the path condition,
for example using a decision procedure that returns all sat-
isfying assignments of the formula φ(cid:48) in state Σs. Another
complication arises when considering memory load and store
operations. Memory addresses can be symbolic as reported
in the rules for load and store instructions. This would
require to evaluate the symbolic expression in the context of
a symbolic state and a path predicate φ, and then compute
all concrete addresses as for the cjmp rules. This process
can in general be infeasible due to the huge amount of pos-
sible concrete addresses at a given point, most of which will
be irrelevant to the ﬁnal analysis. The solution we adopt in
Fig. 2 is to propagate the symbolic expression and postpone
the address resolution when needed. The assert rules use
a ﬁrst order oracle to decide the validity of the asserted ex-
pression, while the assume rule propagates the constraint
as expected. Another problem that arises with the proof sys-
tem is the possible nontermination due to unbounded loops,
which we tackle by providing invariants, as discussed later.
The correctness of forward symbolic execution can be jus-
tiﬁed in terms of the strongest postcondition transformer
[22]. We start with a SiML program Π and a property vec-
tor F , both having the same length. The property vector
assigns to each program location l a formula Fl, which rep-
resents a property of executions reaching l. The strongest
postcondition vector sp(Π, F ) consists of entries sp(Π, F )l
representing the pointwise strongest, i.e.
smallest, condi-
tion which guarantees that, when property Fj holds in the
prestate and control passes from instruction j to l, then
sp(Π, F )l holds in the poststate. The construction uses the
iterator spstep(ι, φ, j, l) which handles the case of control
transfers from j, with φ holding at j, to l, with ι the instruc-
tion being executed. The sp function is pointwise monotone,
and hence, using standard techniques, the largest cumula-

tive ﬁxed point Flim satisfying Flim = sp(Π, Flim) (cid:118) Finit
can be obtained from an initial property vector Finit. The
iterative computation is evidently not guaranteed to termi-
nate, but, by choosing the Finit vector in an intelligent way
and providing invariants, it is in fact possible to compute
sp(Finit) in many concrete situations [20], even for programs
with convoluted control ﬂow.

5. RELATIONAL SYMBOLIC ANALYSIS

We now turn to the relational analysis for proving infor-
mation ﬂow properties deﬁned in the previous sections. The
main idea is to perform forward symbolic execution on a
pair of programs and verify the information ﬂow relation at
each observation point.
5.1 Symbolic Observation Trees

The threat model assumes the attacker has access to part
of the memory, can observe any store on his memory ad-
dresses and count the time elapsed up to the point where
an observation occurs. The symbolic analysis accounts for
the observation points, which are represented as symbolic
constraints. We use a predicate PO to deﬁne the range of
the observable memory addresses. For instance, in Exam-
ple 1, the untrusted agent has assigned memory addresses
higher than 2K, i.e., PO(v) = v ≥ 2048, hence an explicit
enumeration is quite expensive. Tracking dependencies on
observable memory is tricky because the store instructions
can be symbolic and thus potentially write to both observ-
able and unobservable addresses. Therefore it is necessary
to distinguish between observable stores, which aﬀect the
attackers state and unobservable stores, which do not aﬀect
the attackers state. We solve the issue by forking the sym-
bolic execution engine each time we consider a store instruc-
tion. As reported in Fig. 3, we ﬁrst evaluate address exp1
and expression exp2 in the symbolic context, and then dis-
tinguish between stores at observable addresses and stores
at unobservable addresses. The predicate PO partitions the
symbolic execution into one branch where the store is always
observable and one where the store is always unobservable.
This process is important to guarantee the correctness of the
entire approach.

The ﬁrst rule captures the paths where the store instruc-
tion only aﬀects observable addresses and thus is relevant for
the subsequent security analysis. The second rule captures
the paths where the store instruction aﬀects the unobserv-
able addresses, hence the analysis proceeds normally. The
ﬁrst rule is used to extract a symbolic observation tuple.

Definition 2

(Symbolic observation). Consider a
symbolic conﬁguration C s = (Π, ∆s, µs, φ, pc, t) such that
Π(pc) = store(exp1, exp2). Then a symbolic observation is
the tuple obs = (φ, exps
2, t) obtained after applying the
ﬁrst rule in Fig. 3.

1, exps

Intuitively, a symbolic observation captures how the con-
crete executions, starting from initial states that satisfy φ,
aﬀect the observable memory when they reach control point
pc. This is done by recording the execution timestamp t and
the possible values (exps
2) stored in each observable address
(exps

1).

Alg. 1 tracks all symbolic observations occurring in the
program by building a symbolic observation tree for a start-
ing conﬁguration C s
0 and a range predicate PO. We use fse
to represent all conﬁgurations produced in one step by the

1083Π(pc) = (reg := e) ∆s, µs (cid:96) e ⇓ es

(∆s, µs, φ, pc, t) → (∆s[reg (cid:55)→ es], µs, pc + 1, t(cid:48))

Π(pc) = assume(e) ∆s, µs (cid:96) e ⇓ es φ(cid:48) = (φ ∧ es (cid:54)= 0)

(∆s, µs, φ, pc, t) → (∆s, µs, φ(cid:48), pc + 1, t(cid:48))

Π(pc) = assert(e) ∆s, µs (cid:96) e ⇓ es

|= (φ ⇒ es (cid:54)= 0)

(∆s, µs, φ, pc.t) → (∆s, µs, φ, pc + 1, t(cid:48))

Π(pc) = cjmp(e1, e2, e3) ∆s, µs (cid:96) e1 ⇓ es

1 φ(cid:48) = (φ ∧ es
φ(cid:48) ∧ (pcs = pc(cid:48)) consistent

1 (cid:54)= 0)

∆s, µs (cid:96) e2 ⇓ pcs

(∆s, µs, φ, pc, t) → (∆s, µs, φ(cid:48), pc(cid:48), t(cid:48))

Π(pc) = store(e1, e2) ∆s, µs (cid:96) e1 ⇓ es
1 (cid:55)→ es
Π(pc) = cjmp(e1, e2, e3) ∆s, µs (cid:96) e1 ⇓ es

(∆s, µs, φ, pc, t) → (∆, µs[es

1 ∆s, µs (cid:96) e2 ⇓ es
2], φ, pc + 1, t(cid:48))
1 φ(cid:48) = (φ ∧ es

∆s, µs (cid:96) e3 ⇓ pcs φ(cid:48) ∧ (pcs = pc(cid:48)) consistent

(∆s, µs, φ, pc.t) → (∆s, µs, φ(cid:48), pc(cid:48), t(cid:48))

2

1 = 0)

Π(pc) = assert(e) ∆s, µs (cid:96) e ⇓ es
(∆s, µs, φ, pc, t) →⊥

(cid:54)|= (φ ⇒ es = 0)

Figure 2: Symbolic Semantics of Instructions, where t(cid:48) = t + τ (Π(pc))

Π(pc) = store(exp1, exp2) ∆s, µs (cid:96) exp1 ⇓ exps
(∆s, µs, φ, pc, t) → (∆, µs[exps
Π(pc) = store(exp1, exp2) ∆s, µs (cid:96) exp1 ⇓ exps

2], (φ ∧ PO(exps

1 (cid:55)→ exps

(∆s, µs, φ, pc, t) → (∆, µs[exps

1 (cid:55)→ exps

2], (φ ∧ ¬PO(exps

1 ∆s, µs (cid:96) exp2 ⇓ exps

2

1)), pc + 1, t + τ (Π(pc)))

1 ∆s, µs (cid:96) exp2 ⇓ exps

2

1)), pc + 1, t + τ (Π(pc)))

Figure 3: Observable and Unobservable Stores

0 , createN ode(Start))]

Algorithm 1 Program to Symbolic Observation Tree
INPUT: Predicate PO, C s
0
OUTPUT: Tree T
1. W := [(C s
2. While W (cid:54)= ∅
3.
4.
5.
6.
7.
8.
9.
10.

If Π(pc) == store(e1, e2)
u) := fse(C s, PO)
o, µs
TreeN := createNode(obs(φo, (∆s, µs (cid:96) e1),

pop (C s, T ) from W
(Π, ∆s, µs, φ, pc, t) := C s
If φ is satisﬁable

(C s
o , C s
(Π, ∆s
If φo is satisﬁable

o, φo, pco, to) := C s
o

(∆s, µs (cid:96) e2), to))

11.
12.
13.
14.
15.
16.
17.
18.

push TreeN to T.children
push (C s

o , T reeN ) to W

push (C s

u, T ) to W

Else If Π(pc) == halt

push createN ode(End) to T.children

Else

for C s

1 ∈ fse(C s, PO)
1 , T ) to W

push (C s

unary analysis described in Sect. 4. In particular, we assume
that for store instructions, the ﬁrst element of the output
of fse corresponds to the ﬁrst rule in Fig. 3 (which uses
PO). The procedure evaluates program instructions one by
one and creates a tree node each time it reaches an observ-
able store (line 10). The algorithm starts with the tree root
T.Start, the observation range predicate PO and the worklist
containing the initial conﬁguration C s
0 . For each statement,
the algorithm updates the symbolic states by calling fse (line
7 and 17). We discard all symbolic states that have a non

u. If C s

feasible path (line 5). When considering a store (line 6-13),
the symbolic execution applies the rules in Fig. 3 and poten-
tially produces an observable state C s
o and an unobservable
state C s
o is feasible, (line 9), the algorithm creates a
new node containing the symbolic observation and attaches
it to the current tree node. Subsequent observations will be
attached to the freshly created node. If a halt statement is
reached (line 14-15), the current branch is terminated, else
the worklist is updated with all symbolic states obtained by
executing the non-store instruction (line 16-19).

We prove correctness of Alg. 1 only for bounded programs
and discuss generalizations in Sect. 5.4. In a nutshell, we
show that the tree produced by the algorithm contains all
the information needed to verify the security of the origi-
nal program Π. The symbolic observation tree T contains
the observation traces induced by the model of Π. Given
a concrete initial conﬁguration C0, the trace is extracted
by considering a path in T (the node sequence from Start
to End) such that C0 satisﬁes all path predicates and the
observation trace is obtained by the evaluation of symbolic
observations in C0, for each node along the path.

Theorem 1. Consider a SiML program Π and the cor-
responding symbolic observation tree T . Then, the set of
observation traces of the model MΠ of Π and the set of ob-
servation traces of T are the same.
5.2 Relational Analysis

Relational analysis relates two symbolic observations by
means of a relation Ψ which we call a connector. A con-
nector Ψ is a predicate over free variables of the symbolic
observation pair. In our setting, the connector Ψ forces the
equality of initial observable parts of the memory. Moreover,
the connector can cope with declassiﬁcation policies with no
additional eﬀort [7]. Indeed, one can reﬁne the initial state
indistinguishability relation in Def. 1 and Ψ, accordingly, to
express the declassiﬁcation policy.

1084Definition 3

(Relational Validity). Consider obs1 =

1,1, es

1,2, t1) and obs2 = (φ2, es

(φ1, es
2,2, t2), two symbolic
observations, and a connector Ψ. The triple (obs1, obs2, Ψ)
is relationally valid if

2,1, es

R := (Ψ ∧ φ1 ∧ φ2 ∧ PO(es
2,1 ∧ es

1,1 = es

1,1) ∧ PO(es
1,2 = es

(es

2,1)) ⇒

2,2 ∧ t1 = t2) is valid

2,1)),

1,1 = es

1,2 = es

1,1), PO(es

Relational validity is a key property for enforcing the se-
curity condition over traces. It basically states that a pair
of symbolic observations is secure if for any execution pair
which initially agrees on observable memory (enforced by
the connector Ψ) and reaches the observable program points
(enforced by the path conditions φ1, φ2), if the stores are per-
formed on observable memory (enforced by PO(es
then they write the same values (es
2,2) at the same
observable addresses (es
2,1) at the same time (t1 = t2).
This implies that the observable memory is not aﬀected by
changes on the secret memory, hence the program is secure
wrt. that observation pair.
Relational symbolic analysis on symbolic trees is described
in Alg. 2. The algorithm takes as input a tree T , a copy T (cid:48)
with all variables renamed and a connector Ψ which de-
ﬁnes the relation between variables, in the usual style of
self-composition [10]. It then calls the procedure Validity-
Check which visits the tree per levels and checks relational
validity for each observation pair (the Cartesian product on
sets of nodes T (l) and T (cid:48)(l) in line 1-2). It is worth noting
that the End node is considered as a special observation,
which corresponds to normal termination. A ﬁrst order or-
acle is used to determine the validity of the condition R. If
R is not valid, the oracle returns a counterexample. This
corresponds to a pair of concrete initial states giving rise to
a pair of concrete executions that falsify the security condi-
tion, i.e. a security attack. Otherwise, if all pairs are valid
for all levels, then the program is secure.

Algorithm 2 Relational Veriﬁcation on SOTs
INPUT: Symbolic Tree pair T, T’, Connector Ψ
OUTPUT: Secure or Insecure + Attack
1. level := l
2. Call ValidityCheck(T(l), T’(l), Ψ)

1,1, es

2,1, es

ValidityCheck(T(l), T’(l), Ψ)
1. For all (TreeN, TreeN’) in T(l) × T’(l)
2.
2,2, t2)) :=
3. A := Valid (Ψ ∧ φ1 ∧ φ2 ∧ PO(es

((φ1, es
1,2, t1), (φ2, es
(TreeN.obs, TreeN’.obs)

2,1 ∧ es

2,2 ∧ t1 = t2))
(es
If Invalid(A) return Insecure, A
4.
5. ValidityCheck(T(l+1), T’(l+1), Ψ)
6. return Valid

1,1 = es

1,2 = es

1,1) ∧ PO(es

2,1)) ⇒

Theorem 2

(Tree Security). Let T be a symbolic ob-
servation tree and MT the set of observation traces asso-
ciated with T . Then MT is observational deterministic if
Alg. 2 returns Valid in line 6.

Theorem 3

(Security). Let Π be an SiML program
and T the symbolic observation tree obtained by running
Alg. 1 on Π . Let also Alg. 2 return Valid on input T and

connector Ψ. Then MΠ is observational deterministic if
Alg. 2 returns Valid in line 6.

Example 1. We demonstrate our approach using the pro-
gram in Fig. 1 and omit the equivalent SiML program. Sup-
pose all memory addresses higher than 2KB are observable
by the attacker. That is PO(v) = (v ≥ 2048). Algorithm 1
yields the symbolic observation tree depicted in Fig. 4. Each
root-leaf path represents observation traces of the program.
The ﬁrst branch is introduced by the line 7 of the algorithm:
the left path is taken if the address updated by instruction
0x0fc is observable, otherwise the right path is taken. The
second branch is introduced by the conditional instruction
0x108, which updates the register R2 only if the content of
the memory at the address 1024 is zero (since the instruction
contains the EQ suﬃx).

Figure 4: Symbolic Observation Trees

Suppose Alg. 1 has started with an initial symbolic con-
ﬁguration that bounds the i-th register to the fresh variable
Ri and the memory to the fresh variable M . The observa-
tions introduced by the instruction 0x10c are obs2 and obs4
according to the branch taken by the conditional instruction
0x108:

obs2 = ((M (R3) ≥ 2048 ∧ M (1024) = 0), M (R3), 1)
obs4 = ((M (R3) ≥ 2048 ∧ M (1024) (cid:54)= 0), M (R3), 0)
Algorithm 2 takes the symbolic tree T and a copy T (cid:48) with
all variables in symbolic observations renamed, say obs(cid:48)
i, and
the connector Ψ. Assuming that the registers R1 and R3 do
not contain secret information, then Ψ := R1 = R1’ ∧ R3 =
R3’ ∧ (∀v.PO(v) ⇒ M (v) = M(cid:48)(v)). The symbolic tree has
four levels (excluding the root) and the Cartesian product
leads to 16 cases to be considered (i.e. four for each level).
We consider the second level of the tree and the corre-
sponding observations obs2 and obs4 and obs(cid:48)
2 and obs(cid:48)
4 as
depicted in Fig. 4. In particular, R2,2 = (Ψ, obs2, obs(cid:48)
2) is
relationally valid, while R2,4 = (Ψ, obs2, obs(cid:48)
4) is not. For
instance, if R3 = R3’ = 2048, M (2048) = M(cid:48)(2048) = 2052,
M (1024) = 0 and M(cid:48)(1024) = 1 then R2,4 is false. In fact,
the program writes into the observable address 2052 diﬀer-
ent values depending on the content of the secret memory
address 1024.
5.3 Instantiation
lowing predicate R = (Ψ∧ φ1 ∧ φ2 ∧ PO(es
1,1 = es
(es

Relational validity requires proving the validity of the fol-
2,1)) ⇒
2,2∧t1 = t2) where the free variables can

1,1)∧ PO(es

2,1∧es

1,2 = es

1085introduce the constraint Ψ = (cid:86)

include the variables used to represent the initial registers
and memories (we write Ri, R(cid:48)
i, M and M(cid:48) for registers and
memories respectively). A connector Ψ is the predicate that
forces the (relational) equality of initial observable parts of
the memory and the equality of registers that do not contain
1∧···∧(∀v.PO(v) ⇒
secret information, that is Ψ := R1 = R(cid:48)
M (v) = M(cid:48)(v)). The resulting formula is clearly not quanti-
ﬁer free, hence it may result diﬃcult for automatic theorem
provers. This mainly depends on the observable predicate
PO which deﬁnes the range of observable addresses. If the
range is small, one can simply enumerate the addresses and
v∈PO (v)(M (v) = M(cid:48)(v)).
Since this range can be up to 232 concrete addresses (i.e.
4GB), we extract from R all expressions that correspond to
memory accesses, M (e), and instantiate e for v in Ψ. This
is recursively repeated for e and for all expressions in R.
Clearly, the number of such expressions can be huge, but
still bounded by the number of memory accesses in the pro-
gram code.
We illustrate the instantiation process with an example.
Let the R predicate to contain the expression M (M (R1) +
M (R2)), then the instantiation includes the constraints (i)
PO(R1) ⇒ M (R1) = M(cid:48)(R1), (ii) PO(R2) ⇒ M (R2) =
M(cid:48)(R2) and (iii) PO(M (R1) + M (R2)) ⇒ M (M (R1) +
M (R2)) = M(cid:48)(M (R1) + M (R2)). Namely, for all expres-
sions in R which represent memory accesses, we generate
a constraint stating that if the address is observable, then
the initial memory values are the same. The constraints we
generate are suﬃcient to conclude about relational validity.
5.4

Invariants

The approach presented so far may not terminate due to
the unbounded loops that might occur in the program. We
handle this issue by decorating program loops with loop in-
variants [20]. Let ΠLoop be the program slice corresponding
to the loop and let the loop be uniquely identiﬁed by a pair
(pci, pce). We remove all back edges to cut the loop, namely
the edge from pce to pci and apply the transformations from
[9], which allow to cut the loop in a sound manner.

Proving invariants in this fashion is not suﬃcient for rela-
tional analysis. The main reason is that the approach only
accounts for state invariants and fails to capture the number
of observations that might be produced in each loop itera-
tion. Therefore, a naive application of invariants in Alg. 2
would be unsound. Moreover, state invariants may require
proving functional correctness of the loop. In fact, if a vari-
able is updated in the loop body and later it contributes
to an observation, the invariant must be suﬃciently strong
to identify the exact value of the variable. This may not
be needed for proving security, therefore we use relational
invariants which are in general simpler.

We propose a modiﬁcation of Alg. 1 and Alg. 2, which
is correct for programs with natural
loops [9] (no jumps
escaping the loop body). The main idea is to enforce re-
lational invariants during the analysis of Alg. 2 and ensure
that the loop pair is executed the same number of times.
This requires to prove that not only the invariant but also
the equivalence of the branch condition pair is preserved at
each iteration. We ﬁrst modify Alg. 1 to create a tree TLoop,
which consists of the symbolic observation tree of the loop
body, the branch condition B annotating the root node and
the symbolic conﬁgurations C s annotating the leaf nodes.
The tree TLoop is uniquely identiﬁed and represents an ap-

1 ∧ C

proximated model of traces of ΠLoop. Similarly, non-loop
trees are extended with the corresponding symbolic conﬁgu-
rations annotating their leaf nodes. As a result, we obtain a
tree which can be a normal tree, i.e. labeled with symbolic
observations and symbolic states on leaf nodes or a loop tree,
which in addition contains the branch condition labeling the
root node. At this point, it is possible to generalize Alg. 2 to
handle loops by means of relational invariants Ψ. To illus-
trate this, consider a program P := P1; (while B do P2; )P3
and the corresponding trees T := T1; T2; T3 obtained as de-
scribed above. Let T , T (cid:48) be the input tree pair to Alg. 2,
Ψ1 be the initial connector, and Ψ2 the relational invariant
of the loop tree T2. As for traditional invariant veriﬁcation,
we ﬁrst check that the (relational) invariant Ψ2 holds before
(cid:48)s
the loop entry, i.e. Ψ1 ∧ C s
1 ⇒ Ψ2, and it is pre-
(cid:48)s
2 ∧ C
served by the loop body, i.e. Ψ2 ∧ C s
2 ⇒ Ψ(cid:48)
2. This
is done using the symbolic conﬁgurations C s
from the
i , C
leaves of the observation trees Ti, T (cid:48)
i , where Ψ(cid:48)
i denotes the
connector after the execution of the pair (Ti, T (cid:48)
i ).
In ad-
(cid:48)s
dition, we enforce that Ψ1 ∧ C s
1 ⇒ (B ⇔ B(cid:48)) and
1 ∧ C
(cid:48)s
Ψ2 ∧ C s
2 ⇒ (B ⇔ B(cid:48)) to ensure that the loops are
executed the same number of times. Finally, Alg. 2 can be
applied to the symbolic observation trees (T1, T (cid:48)
1), (T2, T (cid:48)
2)
3), using the connectors Ψ1, (Ψ2 ∧ B ∧ B(cid:48)) and
and (T3, T (cid:48)
(Ψ2 ∧ ¬B ∧ ¬B(cid:48)), respectively. Two diﬀerent cases can be
encountered during a run of Alg. 2. If a pair of normal nodes
is reached, the relational validity is checked as before. If a
pair of a loop node and a normal node is reached, they must
be inconsistent. These conditions make our approach com-
positional with respect to observation traces. The process
is repeated recursively for all pairs of nodes and, if success-
fully veriﬁed, it guarantees the security condition in Def. 1.
The following example illustrates the relational veriﬁcation
of the UART driver routine of a separation kernel. For sake
of clarity, here we reason at the C level and describe the case
study more in detail later.

(cid:48)s
i

2 ∧ C

Example 2. This code snippet transforms a 32 bit integer
n into a hexadecimal number and, at each iteration, notiﬁes
the UART by updating three observable addresses (line 8-10).

void printf_hex(uint32_t n) {
int h, i = 32 / 4 - 1;
1.
2.
do {
3.
4.
5.
6.
7.
8.
9.
10.
11.

h = (n >> 28); n <<= 4;
if(h < 10) h += ’0’;
else h += ’A’ - 10;
usart_registers *usart0 = USART0_BASE;
while(usart0->tcr != 0){ ; }
buffer_out[0] = h;
usart0->tpr = (uint32_t)buffer_out;
usart0->tcr = 1;

}while(i--); }

The internal loop, which we discuss later, implements a
polling routine on register tcr, which is externally modiﬁed
whenever the UART is ready to receive the next digit. Let
Ψ1 = (n = n’) be the initial connector relation and Ψ2 =
(n = n’ ∧ i = i’) be the relational invariant of the external
loop. The connector Ψ1 holds of line 1 by the assumption
that n is low. Moreover no observations occur, hence the ﬁrst
part is secure. The connector Ψ2 holds of the external loop
(lines 11-2) since the value of h written at the low address
buffer_out[0] only depends on n, which is low, while the

1086Figure 5: Veriﬁcation process

next two observations are ﬁxed constants. The loop iterates
the same number of times since the value of i is preserved
by the relational invariant Ψ2. Finally, the initial connector
(n = n’) and the symbolic state pair at the loop entry (i =
i’ = 7) trivially entail Ψ2. Observe that if n were a secret
location , then Ψ2 = true, and the veriﬁcation would fail.
Indeed, it is possible an execution pair goes through lines 4
and 5, and writes diﬀerent values in buffer_out[0].

It is worth noting that the proposed veriﬁcation approach
ensures termination-sensitive noninterference, even for time-
insensitive attacker models. We didn’t ﬁnd the security con-
dition restrictive in our case studies. However, for loops
without observations, one can relax the requirement on equal
number of loop iterations and ensure termination-insensitive
noninterference.

6. PROTOTYPE IMPLEMENTATION

We implemented the relational analysis as a new back-end
for the CMU Binary Analysis Platform framework [14]. The
analogies between the BAP Intermediate Language (BIL)
and SiML make the implementation of the prototype tool
easier after enforcing minor syntactical constraints over the
input programs. For instance, we do not allow multiple write
accesses from a single instruction (i.e. each BIL instruction
that writes in the memory always updates 4 bytes). Imple-
menting the analysis as a new BAP back-end provided us
several additional beneﬁts: (i) the resulting prototype tool
is architecture independent, (ii) there exists a veriﬁed trans-
former from ARMv7 assembly to BIL [20] and (iii) we can
take beneﬁt of the existing exporters to SMT solvers. We
had to reimplement the symbolic execution engine of BAP
v0.7 in order to handle our case studies. Variable substitu-
tion and conditional jumps constitute the main sources of
exponential blowup and thus need special care. Fig. 6 de-
picts the workﬂow of the veriﬁcation process. We start from
GCC compiled machine code and lift it to the BIL language.
The resulting BIL program is transformed into a single static
assignment form (SSA) to enable eﬃcient symbolic analysis
and avoid the expensive substitution operation.
In addi-
tion, several simpliﬁcation and constant propagation rou-
tines have been implemented to further speed up the anal-
ysis. Loop invariants are currently provided manually and
symbolic jumps are resolved statically. This phase produces
the symbolic observation tree as described in Alg. 1. Subse-
quently, the relational analysis module generates quantiﬁer-
free formulas for a given pair of symbolic observation trees
and a connector relation, as described in Alg. 2. Finally,
using existing BAP exporters, the formulas are sent to the
STP solver [24], which either validates the security property
or provides a counterexample which violates the policy.

In the worst case, the size of observation tree can be ex-
ponential due to the well-known path explosion problem.

We leverage standard optimizations such as expression sub-
stitution and constant propagation to reduce the tree size
when possible. In addition, one can make use of the CFG
to further control the exponential blow-up.

However, scalability issues are expected when fully veri-
fying machine code. Consider symbolic jumps or symbolic
memory. A bug-ﬁnding tool would simply concretize the
symbols and continue the analysis. For veriﬁcation one has
to resolve all possible concretizations or carry the symbolic
constraints throughout the analysis.

7. CASE STUDIES

The tool has been used to verify several programs.

In
all cases, the analysis has been performed directly on the
ARMv7-A machine code produced by the GCC compiler.
The benchmark of these experiments is summarized in Ta-
ble 1. Among other statistics we report the memory foot-
prints and the number of SOT nodes to get an understanding
of how big a piece of code we can currently check. Here, we
summarize three case studies: (i) the IPC syscall of a sepa-
ration kernel, (ii) a UART device driver and (iii) a modular
exponentiation routine used by crypto services. The case
studies are taken from real software. Our experience shows
that small programs (order of 1000s instructions) are per-
fectly reasonable in high assurance contexts, and well within
the reach of our tool with some standard engineering.
7.1 Case Study 1: Send syscall

The target separation kernel is a low level execution plat-
form for ARMv7. The kernel implements minimal function-
alities and consists of 1028 machine code instructions, mix-
ing hand written assembly with GCC optimized output. The
kernel must execute the partitions in isolation and control
the communication appropriately. Each partition is allowed
to access a non-overlapping part of the system resources: (a)
a contiguous part of the physical memory, that contains the
partition’s executable and data, (b) the logical message box,
stored in the kernel memory and, (c) the virtual registers,
which are stored in the kernel memory while the partition
is suspended, and are stored in the standard registers while
the partition is active.

The IPC mechanism is provided by the “send” syscall; ﬁrst
the active partition (the sender) stores the message in the
register R1 and raises a software interrupt, then the kernel
handler stores the message in the message box of the receiver
and restores the sender. While executing, the kernel backs
up the sender’s CPU state into its own memory and restores
it when the syscall terminates. To appropriately control the
communication, the kernel must ensure that: (1) the sender
infers no information about the receiver and (2) the receiver
only infers the content of sender’s register R1 (the delivered
message) and nothing more.

The above requirements have been veriﬁed by executing
the relational analysis of the “send” syscall twice; considering
observable the resources allocated to either the sender and
the receiver. The resources allocated to the observing agent
directly drive the deﬁnition of the connector relation (con-
sisting in 30 lines of statements). Moreover, to take into
account the designed declassiﬁcation, the initial connector
guarantees that the value of R1 is equal in the two initial
conﬁgurations. In the other experiments, we have modiﬁed
the preconditions to test the tool with non-secure versions
of the send syscall.

1087Software

send syscall

sender

send syscall

receiver

send syscall

sender 1

send syscall

sender 2

UART print char
UART print hex
UART print bin

Exp. timing
Exp. timing

ARM
(LOC)

80

80

80

80

13
32
30
19
19

BIL

(LOC)
1017

Tool
(Sec)
220

SMT Memory
(Byte)
(Sec)
599M

17

1017

220

753

27

1015

215

100
305
286
151
160

1
5
2
1
1

16

16

18

1
1
1
1
1

599M

77M

599M

85K
29 M
30M
319K
463K

Table 1: Experimental results

SOT Size
(Nodes)

Secure
(Y/N)

31

31

31

31

3
7
7
8
7

Y

Y

N

N

Y
Y
Y
Y
N

The absence of loop in the syscall freed us from deﬁning
the corresponding invariants. We also took beneﬁt from the
existing results obtained by a previous veriﬁcation of func-
tional correctness of the kernel: (i) the resolution of indirect
jumps, (ii) the identiﬁcation of data structures invariants
and (iii) the analysis of constant parts of the memory.

These results reduced the set of reachable code of the
syscall to 80 instructions (which corresponds to reducing
the BIL code from 10K lines to 1017 lines) and provided us
the necessary handler precondition (consisting of 400 lines
of statements).

7.2 Case Study 2: UART device driver

The UART (Universal asynchronous receiver/transmitter)
is a hardware device for communication over a serial inter-
face. The driver is implemented in C and resembles the
functionality of the well known printf. We limit our veri-
ﬁcation to the low level interface of the driver. Example 2
provides the C code that sends to the UART a 32 bit in-
teger in the form of an hexadecimal number. The function
contains two loops: the outer loop computes the eight hex-
adecimal digits of the number, the nested loop polls on the
device register tcr before writing the current digit to the
UART.

The function binary code consists of 32 instructions, that
are lifted to 305 BIL lines. Initially the parameter n is repre-
sented by the register R0 and the outer loop uses the register
R4 and R5 to store the variable n and i, respectively. Since
the UART delivers the written characters to the external
world, we consider observable the UART registers (64 bytes
starting from USART0_BASE) and the DMA buﬀer (32 bytes
starting from buffer_out). Moreover, since the input “must
be sent” to the UART, we consider non secret the initial
value of R0.

We ﬁrst veriﬁed the nested loop. This fragment polls on
the device register tcr, which is updated externally by the
device driver. To emulate this external eﬀect on the system
memory, we inline the behavior of the device in the loop
body. At each iteration a shadow variable tcrWait (which
models an oracle knowing the number of iterations needed
by the UART to receive a message) is decremented and tcr
is resetted if the tcrWait value is zero. The given relational
invariant tcrWait=tcrWait’ states that the oracle provides
the same answer in both executions. The tool automatically

checks that the relational invariant is preserved and that
the loop conditions are equivalent in both conﬁgurations.
Notice that the nested loop does not produce observations,
thus its veriﬁcation is required only to guarantee termination
sensitive noninterference.

Next we verify the outer loop. The relational invariant,
R4=R4’ & R5=R5’ & tcrWait=tcrWait’, states that the val-
ues of n, i and the oracle answer are consistent in both
conﬁgurations. The tool automatically checks that the rela-
tional invariant is preserved, the loop conditions are equiva-
lent, the nested loop invariant is satisﬁed and the relational
validity of the three observations (the update of the output
buﬀer and the two UART registers). The relational veri-
ﬁcation is signiﬁcantly simpler than the functional (total-
)correctness of the loop; the relational invariant does not
need to relate the values of n and i, the value of h is not
constrained and no variant is needed.

The last veriﬁcation step must ensure that starting from
the initial connector Ψ, the condition of the outer loop is
equivalent in both conﬁgurations and that the outer invari-
ant is established. The initial connector Ψ simply relates the
integer sent to the UART (the initial value of R0) and the
content of the observable memory. The tool spotted that
without further preconditions the code is not secure: before
using the registers R3, R4 and R5 to represent the local vari-
ables, the function pushes their initial (secret) values on the
stack. This yields an non (relationally) valid observation if
the stack pointer is unconstrained.

7.3 Case Study 3: Modular exponentiation

The modular exponentiation routine is a simpliﬁed version
of the case studies in [34]. The authors provide two programs
with the same functionality. The insecure version branches
on the secret boolean variable i, while the secure version
computes the results independent of the branch condition,
stores them in an array A, and returns A[i]. We point out
that it is critical to verify this code at the machine level,
as the compiler can perform optimizations that break the
security.

We assume the attacker can only observe the execution
time of the two routines. The elapse of time is modeled by
a shadow variable, which is incremented for each instruc-
tion, following the functional time model described earlier.
The tool detects the control-ﬂow side channel of the non-

1088secure routine and validates the secure one, even if we do
not require program counter equivalence between every pair
of possible conﬁguration.

8. DISCUSSION AND RELATED WORK

Formal Veriﬁcation of Low Level Code. Related
kernel information ﬂow veriﬁcation eﬀorts have been reported
recently by several authors. For instance, [36] showed a non-
interference property of the seL4 microkernel, essentially re-
ducing to show absence of information ﬂow from the sched-
uler to the next scheduled thread state. Similarly, [19] es-
tablished system-level information ﬂow security of a simple
hypervisor at the level of ARMv7 assembly, by proving a
trace equivalence property with respect to an ideal model
that reﬂects the isolation properties that are desired of the
hypervisor. Other machine code veriﬁcation work includes
the work by Heitmeyer et al.
[27] and a series of works on
the INTEGRITY kernel [41]. All these works use interactive
theorem proving (ITP) techniques to establish the desired
security property and consequently require serious manual
eﬀort. By contrast, this paper shows that automatic veri-
ﬁcation of small kernel routines is possible with less eﬀort.
Formal veriﬁcation of device drivers has been applied to se-
rial interfaces such as UART and USB devices. These works
focus on functional correctness and ignore information ﬂow
properties. Moreover, the veriﬁcation task is performed at
C level [3, 35] or uses ITP [23].

Relational Veriﬁcation. Relational program veriﬁca-
tion has been used to prove non-functional properties such as
compiler optimization correctness [10], program equivalence
[39, 38] and information ﬂow security [11, 8, 30]. Neither ad-
dresses veriﬁcation at the machine level. Barthe et al. [11]
introduce self-composition as a method for checking 2-safety
properties, including information ﬂow. A related paper [10]
presents product programs as a mean for reducing relational
veriﬁcation to classical functional veriﬁcation. Several au-
thors have studied algorithms for constructing and verifying
over/under approximations of product programs automati-
cally using typing [47], abstract interpretation [30, 38] and
symbolic execution [8, 39, 44]. This work diﬀers in several
aspects. First, prior works do not consider timing channels
and trace-based observations, giving rise to weaker security
guarantees and simpler computational models. Second, we
combine unary and relational analysis to avoid the expensive
construction of the product program and reuse previously
computed results. The unary analysis extracts necessary
program dependences, while the relational one performs the
veriﬁcation. Breadth ﬁrst search algorithms enable the al-
ternation of these steps and allow eﬃcient veriﬁcation on
the ﬂy. As our case studies show, machine code veriﬁcation
requires ﬂow and path sensitive techniques due to register
reuse and complex data/control ﬂow. Hence, compared to
[30, 38], our techniques is more precise. Third, our approach
addresses additional complications due to the lack of support
for data structures. For instance, the secret state cannot be
tied to program variables, and it may depend on complex
pointer arithmetic. Consequently, it is unknown a priori
whether an instruction accesses a secret or public memory
location. Recently, Caselden et al. [17] presented a way to
recover a hybrid information ﬂow/control ﬂow graph using
trace based analysis of machine code. This graph is used to
ﬁnd paths that trigger a given vulnerability condition. We
ﬁnd this work relevant and believe that their ideas can be

applied to our setting and speed up the symbolic execution.
However, the technique requires structural knowledge which
one may not always have and ignores timing channels.

Timing.

Eliminating timing channels by purely soft-
ware approaches is diﬃcult due to architecture dependent
features such as caches, pipelines and more[1]. However, the
timing constraints generated by our analysis can be used as a
software contract to be enforced by hardware features. Our
execution time model is history-independent and determin-
istic. We can not precisely represent the eﬀect of caches and
pipeline data dependencies. However, we can verify absence
of side channels for simple architectures (e.g. ARM Cortex-
M) or under the assumption that the attacker is not able
to access to information that are aﬀected by the wall-clock.
Moreover, symbolic analysis provides memory access pat-
terns which can be later validated wrt. a given architecture
model. The model is similar to [26], which considers tim-
ing analysis for JavaCard-like bytecode. Molnar et al. [34]
introduce the notion of PC-security which can avoid con-
trol ﬂow side channels for crypto operations. Their security
model can be easily accommodated in our work. Several au-
thors propose mitigation [49] and padding [2] techniques to
address timing channels. The results produced by our anal-
ysis are complementary to mitigation and padding. Indeed,
they can be combined with mitigation to reduce the leakage
bandwidth or to enable the required padding. We point out
that worst case execution time is insuﬃcient to verify that
the execution time is independent of the secret, although it
can remove information ﬂows using mitigation. Finally, our
model is suitable for systems where the external scheduler
is instruction-based.

Loop Invariants. Finding loop invariants is deﬁnitely
the most time consuming veriﬁcation task and, for tricky
examples, this is inherited by our works as well. However,
relational invariants are in general simpler than functional
invariants. We only need to enforce that the loop pair has
the same low memory eﬀects, without saying what these
eﬀects are. Recent work considers automatic generation of
relational loop invariants for machine code using data driven
techniques [44]. After executing the programs a certain
number of times, concrete memory and register values are
used to determine linear equality relationships between vari-
ables. Our approach can be used to check if the inferred in-
variants are suﬃcient to enforce equivalence for traces. The
fact that we consider traces makes automatic invariant gen-
eration harder since traces do not compose in general. In
[43], Saxena et al. introduce loop-extended symbolic execu-
tion which relates number of iterations of diﬀerent program
loops. This can be used to make our trace analysis more
precise, although it was not needed in our case studies.

Security Analysis For Machine Code. Security anal-
ysis for machine code is a well studied research area [6]. The
majority of works focus on bug ﬁnding techniques for mal-
ware analysis, vulnerability checking, automatic exploit gen-
eration and more [37, 5, 18, 16]. Typically, they use typing,
taint analysis or lightweight symbolic execution to ensure
good path coverage and still maintain scalability. Other
works take a more formal approach to machine code veriﬁ-
cation [40]. All these approaches fail to capture the infor-
mation ﬂows considered in this paper. Our focus is on full
veriﬁcation of small kernel handlers and device drivers. We
admit that scalability remains an issue for larger programs
and the techniques used by cited works can improve our tool.

1089Information Flow Analysis.

Information ﬂow has
been pervasively applied to software security using static
and dynamic veriﬁcation techniques [42, 31]. If applicable,
security type systems (TS) would be very eﬃcient. Un-
fortunately, none of our case studies can be handled with
TSs, at least not without signiﬁcant modiﬁcation. There
are several case where a TS approach would fail: (i) Low
observations are memory writes of shape M [Ri] = exp,
hence dataﬂow analysis is needed to determine the values
of Ri to know which address is updated. (ii)TSs donˆa ˘A´Zt
support low memory writes under high branches. (iii)Our
case studies use preconditions that guarantee certain invari-
ants (describing the execution context). Data/control ﬂow
analysis is needed to determine the observations enabled in
those contexts.
(iv)Since traces do not compose in gen-
eral, a global analysis through the symbolic trees is needed.
(v)Unreachable or semantically secure code is also problem-
atic for TSs.
(vi)Declassiﬁcation can be challenging and
the timing analysis may require the TS to perform symbolic
computation.

The veriﬁcation approach for trace-based information ﬂow
analysis of ARMv7 machine code is novel. We leverage
symbolic execution to reduce relational veriﬁcation to au-
tomatic theorem proving of quantiﬁer-free formulas. We are
not aware of any tool that performs full veriﬁcation of infor-
mation ﬂows for machine code.

Self-Modifying Code. Our analysis requires the code
to be non self-modifying. For programs executed in user
space this property is usually enforced at run-time by the un-
derlying OS, for instance by conﬁguring as non-writable the
virtual memory containing the program code. On the other
hand, privileged code can dynamically change its behavior
by writing into its instruction memory, changing the copro-
cessor registers that control the MMU or updating the page
tables. Considering these events as observable enables our
analysis to verify that privileged code is non self-modifying.
This can be done by checking that the corresponding sym-
bolic observation tree is empty. It also enables the use of
relational analysis for low level code that does not reconﬁg-
ure the memory layout, but it accesses protected memory
areas in privileged mode (e.g. the send syscall accesses the
message boxes stored in the kernel memory) or performs
privileged instructions (e.g. the syscall accesses the ARM
banked registers to back up and restore the CPU context of
the interrupted partition).

9. CONCLUSIONS

We presented a novel approach to relational veriﬁcation
for machine code. A distinguishing feature of our proposal is
the ability to precisely verify information ﬂow properties in
the presence of features like a preemptive execution environ-
ment and memory mapped devices. We have implemented
a tool and veriﬁed several real world case studies, including
separation kernel routines and device drivers. This shows
that information ﬂow analysis for security critical routines
is not only important, but also feasible.

There are several challenges we leave out as future work.
The technique introduced in Alg. 1 can be combined with a
security type system to automatically infer and reﬁne types.
This would improve the relational analysis by using Alg. 2
only when the typing fails. Timing is particularly critical
to apply our approach to real processor architecture. We
also are conﬁdent that, due to their simplicity, relational in-

variant generation can be automated. Other future plans
include engineering the tool and improving on symbolic ex-
ecution.

Acknowledgments
The authors thank the anonymous reviewers for valuable
comments. This work is supported by framework grant “IT
2010” from the Swedish Foundation for Strategic Research.

10. REFERENCES
[1] O. Acii¸cmez and ¸Cetin Kaya Ko¸c. Microarchitectural

attacks and countermeasures. In Cryptographic
Engineering, pages 475–504. 2009.

[2] J. Agat. Transforming out timing leaks. In POPL,

pages 40–53, 2000.

[3] E. Alkassar, M. A. Hillebrand, S. Knapp, R. Rusev,
and S. Tverdyshev. Formal device and programming
model for a serial interface. In VERIFY, 2007.

[4] ARMv7-A architecture reference manual.
[5] T. Avgerinos, S. K. Cha, B. L. T. Hao, and

D. Brumley. Aeg: Automatic exploit generation. In
NDSS, 2011.

[6] G. Balakrishnan and T. Reps. Wysinwyx: What you

see is not what you execute. ACM Trans. Program.
Lang. Syst., 32:23:1–23:84, August 2010.

[7] M. Balliu, M. Dam, and G. L. Guernic. Epistemic
Temporal Logic for Information Flow Security. In
Proceedings of the ACM SIGPLAN Programming
Languages and Analysis for Security, june 2011.

[8] M. Balliu, M. Dam, and G. L. Guernic. ENCoVer:

Symbolic Exploration for Information Flow Security.
In Proceedings of the IEEE Computer Security
Foundations Symposium, pages 30–44, june 2012.

[9] M. Barnett and K. R. M. Leino. Weakest-precondition

of unstructured programs. In PASTE, pages 82–87,
2005.

[10] G. Barthe, J. M. Crespo, and C. Kunz. Relational
veriﬁcation using product programs. In FM, pages
200–214, 2011.

[11] G. Barthe, P. R. D’Argenio, and T. Rezk. Secure

information ﬂow by self-composition. Mathematical
Structures in Computer Science, 21(6):1207–1252,
2011.

[12] G. Barthe, D. Pichardie, and T. Rezk. A certiﬁed

lightweight non-interference java bytecode veriﬁer. In
Proceedings of the 16th European Conference on
Programming, ESOP’07, pages 125–140, Berlin,
Heidelberg, 2007. Springer-Verlag.

[13] J. Brown and T. F. Knight Jr. A minimal trusted

computing base for dynamically ensuring secure
information ﬂow. 2001.

[14] D. Brumley, I. Jager, T. Avgerinos, and E. J.

Schwartz. Bap: A binary analysis platform. In CAV,
pages 463–469, 2011.

[15] D. Brumley, I. Jager, E. J. Schwartz, and S. Whitman.

The bap handbook.
http://bap.ece.cmu.edu/doc/bap.pdf, October
2013.

[16] D. Brumley, H. Wang, S. Jha, and D. X. Song.
Creating vulnerability signatures using weakest
preconditions. In CSF, pages 311–325, 2007.

1090[17] D. Caselden, A. Bazhanyuk, M. Payer, S. McCamant,
and D. Song. Hi-cfg: Construction by binary analysis
and application to attack polymorphism. In
ESORICS, pages 164–181, 2013.

[18] S. K. Cha, T. Avgerinos, A. Rebert, and D. Brumley.

Unleashing mayhem on binary code. In IEEE
Symposium on Security and Privacy, pages 380–394,
2012.

[19] M. Dam, R. Guanciale, N. Khakpour, H. Nemati, and

O. Schwarz. Formal veriﬁcation of information ﬂow
security for a simple arm-based separation kernel. In
ACM Conference on Computer and Communications
Security, pages 223–234, 2013.

[20] M. Dam, R. Guanciale, and H. Nemati. Machine code

veriﬁcation of a tiny arm hypervisor. In
TrustED@CCS, pages 3–12, 2013.

[21] A. A. de Amorim, N. Collins, A. DeHon, D. Demange,
C. Hritcu, D. Pichardie, B. C. Pierce, R. Pollack, and
A. Tolmach. A veriﬁed information-ﬂow architecture.
In POPL, pages 165–178, 2014.

[22] E. W. Dijkstra. Guarded commands, nondeterminacy

and formal derivation of programs. Commun. ACM,
18(8):453–457, Aug. 1975.

[23] J. Duan and J. Regehr. Correctness proofs for device

drivers in embedded systems. In Proceedings of the 5th
International Conference on Systems Software
Veriﬁcation, SSV’10, pages 5–5, Berkeley, CA, USA,
2010. USENIX Association.

[24] V. Ganesh and D. L. Dill. A decision procedure for

bit-vectors and arrays. In Proc. CAV’07, volume 4590
of Lecture Notes in Computer Science, pages 519–531.
Springer, 2007.

of the 9th Italian Conference on Theoretical Computer
Science, ICTCS’05, pages 360–374. Springer-Verlag,
2005.

[34] D. Molnar, M. Piotrowski, D. Schultz, and D. Wagner.

The program counter security model: Automatic
detection and removal of control-ﬂow side channel
attacks. In ICISC, pages 156–168, 2005.

[35] D. Monniaux. Veriﬁcation of device drivers and

intelligent controllers: a case study. In EMSOFT,
pages 30–36, 2007.

[36] T. C. Murray, D. Matichuk, M. Brassil, P. Gammie,
and G. Klein. Noninterference for operating system
kernels. In CPP, pages 126–142, 2012.

[37] J. Newsome and D. X. Song. Dynamic taint analysis

for automatic detection, analysis, and
signaturegeneration of exploits on commodity
software. In NDSS, 2005.

[38] N. Partush and E. Yahav. Abstract semantic

diﬀerencing for numerical programs. In SAS, pages
238–258, 2013.

[39] D. A. Ramos and D. R. Engler. Practical, low-eﬀort
equivalence veriﬁcation of real code. In CAV, pages
669–685, 2011.

[40] T. W. Reps, J. Lim, A. V. Thakur, G. Balakrishnan,

and A. Lal. There’s plenty of room at the bottom:
Analyzing and verifying machine code. In CAV, pages
41–56, 2010.

[41] R. Richards. Modeling and security analysis of a

commercial real-time operating system kernel. In D. S.
Hardin, editor, Design and Veriﬁcation of
Microprocessor Systems for High-Assurance
Applications, pages 301–322. Springer US, 2010.

[25] J. A. Goguen and J. Meseguer. Security policies and

[42] A. Sabelfeld and A. Myers. Language-based

security models. In Proc. IEEE Symp. on Security and
Privacy, pages 11–20, Los Alamitos, Calif., 1982.
IEEE Comp. Soc. Press.

[26] D. Hedin and D. Sands. Timing aware information

ﬂow security for a javacard-like bytecode. Electr.
Notes Theor. Comput. Sci., 141(1):163–182, 2005.

[27] C. L. Heitmeyer, M. Archer, E. I. Leonard, and

J. Mclean. Formal speciﬁcation and veriﬁcation of data
separation in a separation kernel for an embedded
system. In CCS, pages 346–355. ACM, 2006.

[28] HOL4. http://hol.sourceforge.net/.
[29] C. Hritcu, M. Greenberg, B. Karel, B. C. Pierce, and

G. Morrisett. All your ifcexception are belong to us. In
IEEE Symposium on Security and Privacy, pages
3–17, 2013.

information-ﬂow security. IEEE J. on Selected Areas
in Communications, 21(1):5–19, 2003.

[43] P. Saxena, P. Poosankam, S. McCamant, and D. Song.

Loop-extended symbolic execution on binary
programs. In ISSTA, pages 225–236, 2009.

[44] R. Sharma, E. Schkufza, B. R. Churchill, and

A. Aiken. Data-driven equivalence checking. In
OOPSLA, pages 391–406, 2013.

[45] D. Song, D. Brumley, H. Yin, J. Caballero, I. Jager,
M. G. Kang, Z. Liang, J. Newsome, P. Poosankam,
and P. Saxena. BitBlaze: A new approach to
computer security via binary analysis. In Proceedings
of the 4th International Conference on Information
Systems Security. Keynote invited paper., Hyderabad,
India, Dec. 2008.

[30] M. Kov´acs, H. Seidl, and B. Finkbeiner. Relational

[46] G. E. Suh, J. W. Lee, D. Zhang, and S. Devadas.

abstract interpretation for the veriﬁcation of
2-hypersafety properties. In ACM Conference on
Computer and Communications Security, pages
211–222, 2013.

[31] G. Le Guernic. Conﬁdentiality Enforcement Using
Dynamic Information Flow Analyses. PhD thesis,
Kansas State University, 2007.

[32] J. McLean. Proving noninterference and functional

correctness using traces. Journal of Computer
Security, 1(1):37–58, 1992.

Secure program execution via dynamic information
ﬂow tracking. In ACM SIGOPS Operating Systems
Review, volume 38, pages 85–96. ACM, 2004.

[47] T. Terauchi and A. Aiken. Secure information ﬂow as

a safety problem. In SAS, pages 352–367, 2005.

[48] A. V. Thakur, J. Lim, A. Lal, A. Burton, E. Driscoll,

M. Elder, T. Andersen, and T. W. Reps. Directed
proof generation for machine code. In CAV, pages
288–305, 2010.

[49] D. Zhang, A. Askarov, and A. C. Myers.

[33] R. Medel, A. Compagnoni, and E. Bonelli. A typed

assembly language for non-interference. In Proceedings

Language-based control and mitigation of timing
channels. In PLDI, pages 99–110, 2012.

1091