Alternative Implementations of Secure Real Numbers

Vassil Dimitrov
University of Calgary

Department of Electrical and

Computer Engineering

vdvsd103@gmail.com

Liisi Kerik
Cybernetica

Ülikooli 2, Tartu, Estonia
liisi.kerik@cyber.ee

Toomas Krips

STACC

Ülikooli 2, Tartu, Estonia

toomaskrips@gmail.com

Jaak Randmets

Cybernetica

Ülikooli 2, Tartu, Estonia

jaak.randmets@cyber.ee

Jan Willemson
Cybernetica, STACC

Ülikooli 2, Tartu, Estonia

jan.willemson@stacc.ee

ABSTRACT
This paper extends the choice available for secure real num-
ber implementations with two new contributions. We will
consider the numbers represented in form a − ϕb where ϕ
is the golden ratio, and in form (−1)s · 2e where e is a
ﬁxed-point number. We develop basic arithmetic operations
together with some frequently used elementary functions.
All the operations are implemented and benchmarked on
Sharemind secure multi-party computation framework. It
turns out that the new proposals provide viable alternatives
to standard ﬂoating- and ﬁxed-point implementations from
the performance/error viewpoint in various settings. How-
ever, the optimal choice still depends on the exact require-
ments of the numerical algorithm to be implemented.

Keywords
Secure ﬁxed- and ﬂoating-point arithmetic, privacy-preserving
data analysis, secure computations

1.

INTRODUCTION

It is estimated that currently human knowledge doubles
approximately every year [21]. It means that data analysis
facilities should keep up with this pace. However, it is not
conceivable that all the organisations depending on big data
analysis would upgrade their server farms every year.

Consequently, the only way to manage this growth is to
rely on computation as a service, and this is the core reason
behind the success of cloud computing business idea.

On the other hand, outsourcing computations has other

restrictions with data privacy being on top of the list. Privacy-
preserving data analysis is the holy grail of cloud computing,
but unfortunately it is easier said than done.

There are several paradigms proposed enabling to oﬄoad
some of the computations to another party in a way that
some privacy guarantees could be given.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’16, October 24–28, 2016, Vienna, Austria.
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978348

Historically, the ﬁrst framework was the garbled circuits
(GC) approach originally proposed by Yao in 1982 [3,18,23].
At its core, GC provides secure function evaluation capa-
bility for two mutually distrusting parties, one acting as a
garbler and another as evaluator.

Soon, secure multi-party computation protocols were de-
veloped in a series of seminal papers [4, 9, 12]. These pro-
tocols can be implemented on top of several basic technolo-
gies, with secret sharing being one of the most often used
ones [5, 22].

The most recent breakthrough in secure computation out-
sourcing was achieved by Gentry who proposed the ﬁrst so-
lution to achieve fully homomorphic encryption in 2009 [11].
All of these approaches have their strengths and weak-
nesses, but their common characteristic is that they intro-
duce a remarkable performance penalty to achieve privacy-
preserving features.

Archer et al. have compared the main secure computation
implementations of AES-128 block cipher [2] and conclude
that the fastest alternative is secure multi-party computing
based on linear secret sharing.

However, an eﬃcient secure computation framework is
only the ﬁrst step, providing basic elementary operations
like bit manipulation or integer addition and multiplication.
Statistical data analysis methods require higher level proto-
cols like division and roots. To implement those, data types
richer than bits and (modular) integers are required.

This paper presents a contribution on secure data types,

more precisely, secure real number implementations.

The paper is organised as follows. First, we will review
the state of the art in secure real number implementations
in Section 2 and list the required preliminaries in Section 3.
Then, Sections 4 and 5 describe two new proposed approaches
based on golden section and logarithmic representations.
Performance benchmarks and error analysis are presented
in Section 6 and some conclusions are drawn in Section 7.

2. STATE OF THE ART

The ﬁrst attempt to extend modular integer domains pro-
vided by basic secure computation frameworks was made by
Catrina et al. They implemented ﬁxed-point arithmetic and
applied it to linear programming [6–8].

In 2011, Franz and Katzenbeisser [10] proposed a solu-
tion to implementation of ﬂoating-point arithmetic for se-
cure signal processing. Their approach relies on two-party

553computations working over garbled circuits, and they have
not provided any actual implementation or benchmarking
results.

In 2013, Aliasgari et al. designed and evaluated ﬂoating-
point computation techniques and protocols for square root,
logarithm and exponentiation in a standard linear secret
sharing framework [1].

This approach was extended in 2015 by Kamm and Wil-
lemson who implemented and optimised a number of nu-
meric primitives, building a complete solution for privacy-
preserving satellite collision analysis [13].

In order to provide further improvements, Krips and Wil-
lemson [15, 16], and Kerik et al. [14] have proposed and im-
plemented several tricks that increase the eﬃciency of spe-
ciﬁc elementary functions used under various restrictions.

The standardised IEEE 754 format is not well suited for
secure computations on oblivious values. For example, it has
an exceptional not-a-number value and it assumes explicit
bit access that is ineﬃcient to implement on top of modular
integer arithmetic. Thus, all of the aforementioned imple-
mentations use a custom format, ignoring some of the details
of IEEE 754 standard. The ﬁrst full standard-compliant im-
plementation was achieved by Pullonen and Siim who used a
hybrid of garbled circuits and secret sharing [20]. However,
the overhead required to support the whole standard is too
high for larger data volumes.
3. PRELIMINARIES

Our protocols will make use of both public and private
(protected) values. To express that x is private, we will de-

note it by(cid:74)x(cid:75). Concrete instantiation of the value protection

mechanism may vary between implementations. In princi-
ple, any secure computation framework (say, garbled cir-
cuits, fully homomorphic encryption or secret-sharing based
multi-party computation) may be used.

However, we will make some assumptions about the un-
derlying framework. We will assume that the framework
provides access to modular integer arithmetic. In addition
to integers, we use several kinds of ﬁxed-point numbers.

Two’s complement and biased ﬁxed-point numbers will be
represented and hence also notated as integers. Fixed-point
numbers that use a sign bit instead of two’s complement
or biased representation will be denoted as tuples (s, a). A
tuple (s, a) that is a signed ﬁxed-point number with radix-
point m signiﬁes the value (−1)s · a · 2−m.

We will also assume access to the following operations.

• Addition of two private values(cid:74)x(cid:75) and(cid:74)y(cid:75) denoted as
(cid:74)x(cid:75) +(cid:74)y(cid:75). If the underlying protection framework is
linear, this is equal to(cid:74)x + y(cid:75).
• Multiplication of a private value(cid:74)x(cid:75) by a public scalar
c denoted as c·(cid:74)x(cid:75). If the underlying protection frame-
work is linear, this is equal to(cid:74)c · x(cid:75).
• Multiplication of two private values (cid:74)x(cid:75) and (cid:74)y(cid:75) de-
noted as(cid:74)x(cid:75) ·(cid:74)y(cid:75). If the underlying framework is not

fully homomorphic, evaluating this primitive generally
requires communication between the computing par-
ties.

• Standard comparison operators >, ≥, etc. The inputs
of these operators will be protected integer values and
outputs will be protected bits containing the values of
the corresponding predicate evaluations.

if the bit b = 1 then

• Standard Boolean operations (conjunction, disjunction,
xor) on one-bit protected values with the output being
a protected bit again.

type consisting of several integer values.

Note that while usually x and y are integers, they
might also refer to more complicated types that consist
of several integer values.

• ObliviousChoice((cid:74)b(cid:75),(cid:74)x(cid:75),(cid:74)y(cid:75)):
this function outputs (cid:74)x(cid:75), otherwise it outputs (cid:74)y(cid:75).
• Swap((cid:74)c(cid:75),(cid:74)x(cid:75),(cid:74)y(cid:75)) outputting ((cid:74)x(cid:75),(cid:74)y(cid:75)) if c = 0 and
((cid:74)y(cid:75),(cid:74)x(cid:75)) if c = 1. Here also x and y can refer to a
• ConjBit({(cid:74)xi(cid:75)}n
i=0,(cid:74)y(cid:75)) takes an array of bits x and a
• PublicBitShiftRightProtocol((cid:74)x(cid:75), k). This function
Takes a protected value(cid:74)x(cid:75) and a public integer k, and
outputs(cid:74)x (cid:29) k(cid:75) where x (cid:29) k is equal to x shifted right
• BitExtract((cid:74)x(cid:75)) takes a protected value(cid:74)x(cid:75) and out-

by k bits. x (cid:29) k is also equal to x/2k rounded down, so
sometimes in the protocol we will use syntactic sugar
like x/2k to denote x (cid:29) k.

single bit y and ﬁnds the conjunction of every bit of x
with y.

puts a vector of protected bits corresponding to the
bit representation of x. Note that when we use linear
integer protection mechanism (like secret sharing) this
operation is rather expensive.

• MSNZB({(cid:74)bi(cid:75)}n−1

i=0 ) (Most Signiﬁcant Non-Zero Bit) takes
a vector of protected bits and outputs a similar vec-
tor, where only the highest 1-bit (i.e. the 1-bit with
the largest index value) has remained and all the other
bits are set to 0. If all the input bits are 0, they will
also remain so in the output.

j=0}k

{{pi,j}l

• Vectorised ﬁxed-point polynomial evaluation protocol

cPoly but more eﬃcient than evaluating each polyno-
mial independently.

Therefore, both the range and the domain are [0, 1).
Implementation details can be found in [14].

a public polynomial p = {pi}l
sider that the inputs are ﬁxed-point numbers and that

• Polynomial evaluation protocol cPoly(p,(cid:74)x(cid:75)) evaluates
i=0 on (cid:74)x(cid:75). We con-
both(cid:74)x(cid:75) and the output have 0 bits before radix point.
cPolyArr(p,(cid:74)x(cid:75)) takes an array of polynomials p =
i=0 and an argument(cid:74)x(cid:75) and evaluates all
the polynomials on (cid:74)x(cid:75). This protocol is similar to
• Pick({(cid:74)xi(cid:75)}l
i=0,(cid:74)n(cid:75)) takes a shared array {(cid:74)xi(cid:75)}l
a shared index(cid:74)n(cid:75) and returns(cid:74)xn(cid:75).
• Truncate((cid:74)x(cid:75), n) takes an integer(cid:74)x(cid:75) and casts it down
that the length of (cid:74)x(cid:75) is no less than n bits.
• ConvertUp((cid:74)x(cid:75), k, l) takes in a shared k-bit two’s com-
plement integer (cid:74)x(cid:75) and returns a shared l-bit two’s

to n bits by discarding the highest bits. We presume
If the
underlying protection framework is linear then trun-
cating a shared integer is achieved by truncating all
the shares.

complement integer that has the same value. We pre-
sume that k < l.

i=0 and

554• FixSubtract(((cid:74)s0(cid:75),(cid:74)a0(cid:75)), ((cid:74)s1(cid:75),(cid:74)a1(cid:75))) takes two signed
ﬁxed-point numbers ((cid:74)s0(cid:75),(cid:74)a0(cid:75)) and ((cid:74)s1(cid:75),(cid:74)a1(cid:75)) and

returns their diﬀerence as a signed ﬁxed-point number.

For benchmarking, we will implement our algorithms on
Sharemind1 multi-party computation engine that relies on
linear secret sharing and provides all the primitive opera-
tions listed above.

It was recently shown by Pettai and Laud that, when
properly composed, Sharemind protocols provide privacy
against active adversaries [19]. They also produced a soft-
ware toolkit allowing for the respective analysis to run on
the Sharemind protocols implemented in the domain spe-
ciﬁc language developed by Randmets and Laud [17]. All the
protocols described in this paper have been formally veriﬁed
using this toolkit.

4. GOLDEN SECTION NUMBERS

We shall now describe a real number type that can depict
signed real numbers, has free addition, and is reasonably
eﬃcient for other operations.

We use a tuple of secret signed two’s complement integers

((cid:74)a(cid:75),(cid:74)b(cid:75)) to denote the positive real number a− ϕb. We call

these numbers golden section numbers or golden numbers.
We may refer to these numbers as either a− ϕb or (a, b). For
a given real number x, we denote its (approximate) golden
representation as gx. For a golden section number a − ϕb,
we refer to a as its integer representand and to b as its ϕ-
representand. Note that we will be using a and b throughout
this section to refer to the integer representand and the ϕ-
representand of the number being considered, respectively.
We will now see how addition and multiplication work on
golden section numbers. Addition is quite straightforward:

a − ϕb + c − ϕd = (a + c) − ϕ(b + d).

For multiplication, we note that ϕ2 = ϕ + 1 and thus obtain

(a − ϕb) · (c − ϕd) = (ac + bd) − ϕ(bc + ad − bd) .
Golden numbers are not monotone with respect to repre-
sentands. Hence, ﬁnding a good approximation for a given
number is a non-trivial problem on its own.

(1)

Definition 1. Given a real number x, we say that the tu-
ple of integers (a, b) is a (k, ε)-approximation of x if |a| ,|b| ≤
2k and |a − ϕb − x|≤ ε. If k is implied by the context or not
important in the context, we shall refer to (a, b) as just an
ε-representation of x. If ε is implied or not important, we
shall refer to (a, b) as a (k,·)-representation of x.

If neither are important (or are implied) we refer to (a, b)

as just as a representation of x.

It is preferable to use such a (k, ε)-approximation of a
number where both k and ε are relatively small. While it is
clear that a small ε implies a small error and is thus better,
the reason why a small k is good is a bit more diﬃcult.
Namely, we observe that when we multiply two golden sec-
tion numbers x and y with (k,·)-approximation, then their
product has a (2k + 1,·)-approximation. We will later see
how we can replace a golden section number x with a (k, )-
representation of it.

We shall assume throughout the section that the error ε
is quite a small number, several orders of magnitude smaller
1https://sharemind.cyber.ee/

than 1. Thus, when we discuss how either the number or
the representands need to be bounded by some quite large
numbers, we shall ignore ε in those analyses as rounding
down used in ﬁnding those large numbers will cover any
overﬂow ε might cause.

Golden numbers are also relatively dense in real numbers.
Lemma 1. For a real number x which satisﬁes |x| < ϕs+1,
and a positive integer k, there exists a (ϕs+1 + ϕk+1, ϕ−k)-
approximation of x.

no i so that ai = 1 and ai+1 = 1 would both hold.

Proof. We note that we can write every positive real
number as a (possibly inﬁnite) sum of powers of ϕ. We can
i=−∞ aiϕi where ai ∈ {0, 1} and where there is

write x =(cid:80)s
We also note that given such a requirement,(cid:80)s(cid:48)
Thus, if we choose to represent x as x = (cid:80)s
(cid:80)s
i=s(cid:48) aiϕi|≤ ϕs(cid:48)
The following three facts about Fibonacci numbers Fk

ϕs(cid:48)+1 holds for any s(cid:48).
the error we make is no greater than ϕs(cid:48)

(k ∈ Z) are common knowledge and are easily proven:

i=−∞ aiϕi <

i=s(cid:48) aiϕi,
|x −

, that is,

.

• ϕk = Fkϕ + Fk−1 for every k,
• Fk ≈ ϕk for every positive k, and

• (cid:80)k
s(cid:88)

i =s(cid:48)

aiϕi =

s(cid:88)
s(cid:48)(cid:88)

i=0

i=0 Fk = Fk+1 − 1 for every positive k.

From these facts it follows that

ai(Fiϕ + Fi−1) ≤ s(cid:88)

|Fi|ϕ + |Fi−1|=

i=s(cid:48)
|Fi|ϕ + |Fi−1|+

i=s(cid:48)
|Fi|ϕ + |Fi−1|=

s(cid:88)

i=1

(Fs(cid:48)+1 − 1)ϕ + (Fs(cid:48) ) + (Fs+1 − 2)ϕ + Fs − 1
= (Fs(cid:48)+1 + Fs+1 − 3)ϕ + Fs + Fs(cid:48) − 1.

We see that taking k = −s(cid:48) gives us the result.

However, there is a problem with this number system.
Namely, we may use large integers to depict small real num-
bers. This, however, means that when multiplying several
small real numbers, the representands may grow exponen-
tially and may overﬂow very fast. We would like to keep the
absolute value of the representands to be smaller than some
reasonable constant.

The solution for this comes from the fact that there may
be several diﬀerent (k, ε)-approximations of a number. Thus
we want a method for replacing a (k1, ε1)-approximation
with a (k2, ε2)-approximation where ε2 may be slightly greater
than ε1, but where k2 < k1. We shall use a method that we
shall call normalization, which is, in essence, subtracting a
suitable representation of 0 from the golden section number.

Definition 2. We say that a golden section number is
(cid:96)-normalized if the absolute value of its integer representand
is not greater than (cid:96).

Note that this deﬁnition depends only on the integer rep-
resentand and not the ϕ-representand of a golden number,
and that a too large ϕ-representand could also cause similar
overﬂow problems as a too large integer representand does.

555However, we shall see that if the absolute value of integer
representand is smaller than (cid:96) then, provided that we put
an extra constraint on the size of the numbers that we can
represent, the absolute value of the ϕ-representand shall also
be smaller than (cid:96). More precisely, we shall require that
|a − ϕb|≤ (cid:96)(ϕ − 1).

Lemma 2. Let a golden section number a−ϕb satisfy also

|a − ϕb|≤ (cid:96)(ϕ − 1) and be (cid:96)-normalized. Then |b|≤ (cid:96).

Proof. Using the reverse triangle inequality, we obtain
||a| − |bϕ|| ≤ |a − bϕ| ≤ (cid:96)(ϕ − 1). From this we obtain
|bϕ|− (cid:96)(ϕ− 1) ≤ |a| ≤ |bϕ| + (cid:96)(ϕ− 1). Consider the ﬁrst half
— |bϕ|−(cid:96)(ϕ−1) ≤ |a|, from which we obtain |bϕ|−(cid:96)ϕ+(cid:96) ≤ (cid:96),
i.e. |bϕ| ≤ (cid:96)ϕ. This is equivalent to |b| ≤ (cid:96).

We thus will assume from this point on that if we are
talking about a golden section number, then |a− ϕb|≤ (cid:96)(ϕ−
1).

We want to settle for a general value for (cid:96) that we will use
across the article. The main idea for choosing (cid:96) shall be the
idea that if we multiply together two (cid:96)-normalized numbers,
no overﬂow should happen.

We shall generally take (cid:96) =

, where n refers

(cid:22)(cid:113)

(cid:23)

2n−1−1

2

to the bit length of a and b.

(cid:22)(cid:113)

(cid:23)

Lemma 3. If two golden section numbers a−ϕb and c−ϕd
-normalized, then both the integer represen-

2n−1−1

are

2

tand and the ϕ-representand of their product are smaller
than 2n−1.

Proof. Let us denote a − ϕb · c − ϕd with x − ϕy, i.e.
x = ac + bd and y = ad + bc − bd. We give the proof for
x ≥ 0 as the proof for x < 0 is analogous. We assume that
|a| ,|b| ,|c| ,|d| ≤ (cid:96). Thus x = ac + bd ≤ 2(cid:96)2 < 2n−1. We
assume that the absolute value of the product x − ϕy is no
greater than (cid:96)(ϕ − 1).
In a similar way as we did in the
proof of Lemma 2, we obtain that |yϕ|−(cid:96)(ϕ − 1) ≤ x. Thus
|yϕ|≤ 2(cid:96)2 + (cid:96)(ϕ − 1) which gives us |y|< 2(cid:96)2 < 2n−1.

From now on, we shall speak of normalized numbers which

shall mean

-normalized numbers. Likewise, (cid:96)

(cid:22)(cid:113)
(cid:22)(cid:113)

(cid:23)
(cid:23)

2n−1−1

2

2n−1−1

2

.

shall refer to

4.1 Normalization

In the previous subsection we described a problem where

representands grow exponentially on multiplication and threaten
to overﬂow really fast.

Thus, we want to signiﬁcantly reduce the absolute values
of the representands of the number while keeping its value
relatively the same.

There are various possibilities for this, but due to the na-
ture of the task, they are equivalent to deducing suitable
¯ε-representations of zero from the number. Namely, sup-
pose that we normalized a − ϕb and ended up with a(cid:48) − ϕb(cid:48).
Let the error made in this process be no greater than ¯ε,
|a − ϕb − (a(cid:48) − ϕb(cid:48))| ≤ ¯ε. This means that |(a − a(cid:48)) −
i.e.
ϕ(b−b(cid:48))|≤ ¯ε, i.e. |(a − a(cid:48)) − ϕ(b − b(cid:48))| is an ¯ε-representation
of 0. The smaller the ¯ε, the smaller the error arising from
the normalization process and thus it is desirable to obtain
¯ε-representations of 0 where ¯ε is very small. The process
should also be not very resource-consuming.

We ﬁrst note that, thanks to Lemma 2, it suﬃces to nor-
malize only the integer representand of the number. If the
normalization error is small, then the result will still be an
ε-representation of a golden number, and thus the absolute
value of the ϕ-representand of the result will also be smaller
than (cid:96).
Note also that in order to normalize an integer represen-
tand down to 2k, we need either the n − k most signiﬁ-
cant bits to be zero (if this repesentand is positive) or the
n − k most signiﬁcant bits to be one (if it is negative) in
the end-result. (In principle, k can be chosen freely, but we
2 − 1.) We note that using the proto-
col BitExtract((cid:74)·(cid:75)), we can have access to the individual bits
generally take k = n
−an−12n−1 +(cid:80)n−2

If the bit representation of a is a0a1 . . . an−1 then a =
i=0 ai2i (recall we are using two’s comple-
ment representation). In the positive case we could make
the i-th bit of a zero by deducting 2i from a. We will later
conﬁrm that in the negative case in a similar fashion we
could make the ith bit zero by adding 2i to a. In the end we
would use an−1 to obliviously choose between the negative
and positive cases.

of a.

ϕ

(cid:105)

(cid:105)

(cid:104) 2i

(cid:104) 2i

Thus we shall describe a method where for every bit ai
), i.e.

. We would perform the BitExtract((cid:74)a(cid:75)) protocol

of a we have a representation of 0 that is (2i,
2i − ϕ
and then use the higher bits of a to perform oblivious choice
about whether to deduce or add (2i,
) from a or not. In
i=k ai2i =

the positive case we would end up with a −(cid:80)n−2
(cid:80)k−1
i=0 ai2i ≤ 2k − 1 and in the negative case with
n−1(cid:88)
n−2(cid:88)
n−2(cid:88)

(1 − ai)2i = −2n−1 +

(1 − ai)2i

(1 − ai)2i

n−2(cid:88)

n−2(cid:88)

n−2(cid:88)

ai2i +

ai2i +

2i +

a +

i =k

i=k

i=k

i=0

i=0

ϕ

(cid:105)

(cid:104) 2i

ϕ

(ai − 1)2i +

(1 − ai)2i

i=0

= −1 +

= −1 − n−2(cid:88)
n−2(cid:88)
k−1(cid:88)
k−1(cid:88)

= −1 +

≥ −1 +

i=0

i=0

i=k

(ai − 1)2i

−2i = −2k.

i=0

(cid:105)

(cid:104) 2i

We would then perform oblivious choice between them using
an−1. We would end up with a number where the absolute
value of a is no greater than 2k which is small enough for a
suitable k.

ϕ

(cid:105)

(cid:104) k

However, this approach has a problem – namely, (2i,
ϕ ]||k ∈ Z} is quite uniformly distributed in [0, ϕ

)
tends to be a very poor representation of 0. The set {|k −
ϕ[ k
2 ) and thus
for most integers k, (k − ϕ
) is a poor representation of
0 as the error is too large.
Thus we want to modify our algorithm somewhat — we
want to ﬁnd numbers xi that are close to 2i but where |xi −
ϕ
|2i − xi| to be small, since if |2i − xi| is too large, then the
number that we obtain after the normalization process is
still not normalized.

(cid:105)| is very small. On the other hand, we also wish

(cid:104) xi

ϕ

ϕ

556We shall now describe the algorithm for normalization.
We use a number of pairs of public constants (xi, yi) such
that xi − ϕyi ≈ 0 and that xi is close to 2i. After describing
the algorithm we shall see what properties they must satisfy.
The protocol is formalized in Algorithm 1.

We are given a golden section number(cid:74)a(cid:75)− ϕ(cid:74)b(cid:75). We per-
form bit decomposition on(cid:74)a(cid:75) and obtain its bits(cid:74)a0(cid:75), . . . ,
(cid:74)an−1(cid:75). Out of these, we are interested in bits with large

indices as the less signiﬁcant bits will not be important in
normalizing. Let us consider the positive and negative cases
separately for easier reading. In the algorithm, we will com-
pute both of them and then use an−1 to obliviously choose
between them. First let us consider the case where a is pos-
itive. If ai = 1 and n − 1 > i ≥ k, we will deduce xi from a

and yi from b. This is done by multiplying xi with(cid:74)ai(cid:75) and
deducing(cid:74)xiai(cid:75) from(cid:74)a(cid:75), and likewise, multiplying yi with
(cid:74)ai(cid:75) and deducing(cid:74)yiai(cid:75) from(cid:74)b(cid:75). Note that these protocols

are local and thus practically free.

Likewise, in the negative case, if ai = 0 and n− 1 > i ≥ k,

we will add xi to a and yi to b.

following inequalities:

(cid:88)

(2i − xi) ≤ k−1(cid:88)

i:2i<xi
k≤i≤n−2

i=0

≤ 2k − 1 +

n−2(cid:88)
(cid:88)

i=k

ai2i +

ai(2i − xi)

(cid:88)

(xi − 2i) ≥ −1 +

k−1(cid:88)

i:2i<xi
k≤i≤n−2

i=0

≥ −2k +

i(cid:48):2i(cid:48)
>xi
k≤i≤n−2

(2i(cid:48) − xi(cid:48) ) ,
n−2(cid:88)

(ai − 1)2i +

(1− ai)(xi − 2i)

(cid:88)

i=k

(xi − 2i) .

in order to achieve that a −(cid:80)n−2

(cid:80)n−2
i=k aixi or a +
Thus,
i=k (1 − ai)xi belongs the interval (−(cid:96), (cid:96)), it suﬃces for

i:2i>xi
k≤i≤n−2

both cases that

(2i − xi)

−(cid:96) ≤ (cid:88)
(cid:88)

i:2i<xi
k≤i≤n−2

i(cid:48):2i(cid:48)
>xi
k≤i≤n−2

2k +

(2i(cid:48) − xi(cid:48) ) ≤ (cid:96) .

Thus we arrive to the following deﬁnition.

Definition 3. A (k, (cid:96), ε, n)-normalization set is a set of
integers {xk, . . . , xn−1, yk, . . . , yn−1} with the following prop-
erties:

1. (cid:80)n−2
2. (cid:80)
3. 2k +(cid:80)

i:2i<xi
k≤i≤n−2

i=k |xi − ϕ · yi|≤ ε

(2i − xi) ≥ −(cid:96)

(2i(cid:48) − xi(cid:48) ) ≤ (cid:96)

i(cid:48):2i(cid:48)
>xi
k≤i≤n−2

There is some freedom in choosing k, with lower values
giving us more freedom in choosing the normalization set,
but reducing the number of values that a normalized number
can have.
4.2 Finding Normalization Sets

The key part of ﬁnding normalization sets is ﬁnding in-
teger pairs (xi, yi) where on the one hand xi
is very close
yi
to ϕ and on the other hand xi must be very close to 2i. In
essence, we have to ﬁnd suitable constants xi, since yi is
deﬁned by xi as [ xi
ϕ ]. Note that this can be considered a
minimization problem – we have to guarantee that proper-
ties 2 and 3 in Deﬁnition 3 hold, but how loosely or strictly
they hold is not really important for us.
i=k |xi − ϕ·
yi| as much as possible in order to minimize the error caused
by the normalization protocol. Thus we can see ﬁnding the
normalization set as an optimisation problem to minimize

On the other hand, we do want to minimize(cid:80)n−1
(cid:80)n−1
i=k |xi − ϕ · [ xi
For easier notation, let us deﬁne err(x) := x − ϕ[ x
ϕ ].

ϕ ]| with the constraints 2. and 3. in Deﬁni-

tion 3 holding.

Result: Given a golden section number and a
normalization set, returns the number
normalized according to the set.

and

i=k

i=k,{yi}n

Algorithm 1: GoldenNorm

Data: (cid:74)a(cid:75),(cid:74)b(cid:75),{xi}n
i=0 ← BitExtract((cid:74)a(cid:75));
1 {(cid:74)ai(cid:75)}n−1
i=k ← {(cid:74)ai(cid:75)}n−2
2 {(cid:74)zi(cid:75)}n−2
i=k · {xi}n−2
i=k ← {(cid:74)ai(cid:75)}n−2
3 {(cid:74)wi(cid:75)}n−2
i=k ;
i=k · {yi}n−2
i(cid:75)}n−2
i=k ← {(cid:74)1 − ai)(cid:75)}n−2
4 {(cid:74)z(cid:48)
i=k ;
i(cid:75)}n−2
5 {(cid:74)w(cid:48)
i=k ← {(cid:74)(1 − ai)(cid:75)}n−2
(cid:74)a(cid:48)(cid:75) ←(cid:74)a(cid:75) −(cid:74)zi(cid:75);
(cid:74)b(cid:48)(cid:75) ←(cid:74)b(cid:75) −(cid:74)wi(cid:75);
(cid:74)a(cid:48)(cid:48)(cid:75) ←(cid:74)a(cid:75) +(cid:74)z(cid:48)
i(cid:75);
i(cid:75);
(cid:74)b(cid:48)(cid:48)(cid:75) ←(cid:74)b(cid:75) +(cid:74)w(cid:48)

6 for i ← k to n − 2 do

7

8

9

i=k · {xi}n−2
i=k ;
i=k · {yi}n−2
i=k ;

10
11 end

12 (cid:74)a(cid:75) ← ObliviousChoice((cid:74)an−1(cid:75),(cid:74)a(cid:48)(cid:75),(cid:74)a(cid:48)(cid:48)(cid:75));
13 (cid:74)b(cid:75) ← ObliviousChoice((cid:74)an−1(cid:75),(cid:74)b(cid:48)(cid:75),(cid:74)b(cid:48)(cid:48)(cid:75));
14 return(cid:74)a(cid:75),(cid:74)b(cid:75)

Now we shall see what properties the pairs (xi, yi) must
satisfy so that the ﬁnal result would have an absolute value
no greater than (cid:96) and that its diﬀerence from the original
golden number would be no greater than .

We want the end result, which is a −(cid:80)n−2
positive case and a +(cid:80)n−2
i=k aixi in the
i=k (1 − ai)xi in the negative case,
to be in the interval (−(cid:96), (cid:96)). We note that in the positive
k−1(cid:88)
case the following equality holds.

n−2(cid:88)

a − n−2(cid:88)

ai(2i − xi).

ai2i +

aixi =

i=k

i=0

i=k

Likewise, in the negative case this holds.

k−1(cid:88)

n−2(cid:88)

n−2(cid:88)

a +

(1− ai)xi = −1 +

(ai − 1)2i +

(ai − 1)(2i − xi).

i=k

i=0

i=k

In attempting to estimate these quantities with inequal-
ities, it is important whether 2i is smaller or greater than
xi. Thus, by distinguishing these cases, we arrive at the

557We searched for these coeﬃcients using the facts that
err(Fk) tend to be small and that when when x and x(cid:48) are
such that if err(x) and err(x(cid:48)) are small, then also err(x+x(cid:48))
is small, more precisely, |err(x + x(cid:48))| ≤ |err(x)| + |err(x(cid:48))|.
Thus, we would take a small interval around a power of 2
and ﬁnd all elements zi for which err(zi) was suitably small.
Then, in a larger interval, the only possible candidates w for
a minimal |err(w)| had to have the format zi + j · Fk. Thus
we needed to check these elements to ﬁnd a minimal one. If
necessary, this process could be iterated.
4.3 Protocols for golden section numbers

We shall now present a few protocols on golden numbers.
We have already described addition, multiplication and nor-
malization protocols and thus we will not discuss them any
further here.

We will denote with GoldenMult((cid:74)x(cid:75),(cid:74)y(cid:75)) golden number

multiplication as described in equation (1). Generally we as-
sume that all functions get normalized inputs, unless speci-
ﬁed otherwise. We will thus not normalize the inputs before
performing multiplication, but will normalize the product.
In some cases, such as when it is the ﬁnal result that will be
declassiﬁed, the product can be left unnormalized.

refer to computing the product(cid:81)k−1

We will also use the function GoldenProd(x0, . . . , xk−1) to
i=0 xi using GoldenMult.
Computing the product of k−1 golden numbers takes l·log k
rounds where l is the number of rounds required for a single
multiplication.

4.3.1 Multiplication by ϕ
We will describe now a protocol for multiplying an inte-
ger by the golden ratio. This protocol, presented in Algo-
rithm 2, will be useful for performing golden-to-ﬁx conver-
sion described in Section 4.3.2.

i=0

Algorithm 2: MultWithPhi

Data: (cid:74)x(cid:75), n, m, (m > n),{pi}∞
Result: (cid:74)xϕ(cid:75).
1 {(cid:74)xi(cid:75)}n−1
i=0 ← BitExtract((cid:74)x(cid:75));
2 (cid:74)s0(cid:75) ←(cid:80)m
i=0 pi · ((cid:80)
j=0(cid:74)xj(cid:75) · 2m+j−i);
i=m+1 pi · ((cid:80)
3 (cid:74)s1(cid:75) ←(cid:80)m+n
j=i−m(cid:74)xj(cid:75) · 2m+j−i);
4 (cid:74)s(cid:75) ←(cid:74)s0(cid:75) +(cid:74)s1(cid:75);
i=0 ← BitExtract((cid:74)−x(cid:75));
i(cid:75)}n−1
5 {(cid:74)x(cid:48)
0(cid:75) ←(cid:80)m
i=0 pi · ((cid:80)
j=0(cid:74)x(cid:48)
j(cid:75) · 2m+j−i);
6 (cid:74)s(cid:48)
1(cid:75) ←(cid:80)m+n
i=m+1 pi · ((cid:80)
j(cid:75) · 2m+j−i);
7 (cid:74)s(cid:48)
j=i−m(cid:74)x(cid:48)
0(cid:75) +(cid:74)s(cid:48)
8 (cid:74)s(cid:48)(cid:75) ←(cid:74)s(cid:48)
1(cid:75);
9 (cid:74)r(cid:75) ← ObliviousChoice((cid:74)xn−1(cid:75),(cid:74)s(cid:48)(cid:75),(cid:74)s(cid:75));
10 return ((cid:74)xn−1(cid:75),(cid:74)r(cid:75))
The protocol takes in a secret signed integer (cid:74)x(cid:75) and re-
turns a signed ﬁxed-point number that represents(cid:74)xϕ(cid:75). This
choice. We start with a secret integer (cid:74)x(cid:75). We also have
put(cid:74)x(cid:75). We then compute(cid:80)m
i=0. We extract the bits(cid:74)xi(cid:75) from the in-
j=0(cid:74)xj(cid:75) · 2m+j−i) +
(cid:80)m+n
i=m+1 pi·((cid:80)
j=i−m(cid:74)xj(cid:75)·2m+j−i) that represents xϕ if x is

protocol needs one bit-extraction protocol and one oblivious
the bits of φ, {pi}∞

positive. We then do the same for −x and obliviously choose
between the two cases based on the last bit of x. The last
bit of x is also the sign of the resulting ﬁxed-point number,
as multiplication with ϕ does not change the sign.

i=0 pi · ((cid:80)

4.3.2 Conversion to a ﬁxed-point number
Algorithm 3 presents the protocol for converting a golden

section number to a ﬁxed-point number.

Result: A ﬁxed-point number that represents the same

2 // we will also obtain an−1 as a side product

value as the golden number input.

Algorithm 3: GoldToFix

Data: (cid:74)a(cid:75) − ϕ(cid:74)b(cid:75), n, m, (n > m)
1 (cid:74)bigA(cid:75) ← ConvertUp((cid:74)a(cid:75), n, n + m);
3 (cid:74)f ixA(cid:75) ← ((cid:74)an−1(cid:75),(cid:74)bigA(cid:75) · 2m);
4 (cid:74)f ixB(cid:75) ← MultWithPhi((cid:74)b(cid:75), n, m);
5 (cid:74)f ix(cid:75) ← FixSubtract((cid:74)f ixA(cid:75),(cid:74)f ixB(cid:75));
6 return(cid:74)f ix(cid:75)

from the ConvertUp function.

While conversion functions are important on their own,
here we will also use them as subprotocols in more compli-
cated algorithms.

Since we have access to MultWithPhi function, convert-
ing a golden number to a ﬁxed-point number is trivial. We
need to convert both the integer representand and the ϕ-
representand to a respective ﬁxed-point number and deduce
the second from the ﬁrst.
4.3.3 Return a constant based on power of two
We will see that in both the inverse protocol and the

square root protocol, we get a secret golden number (cid:74)gx(cid:75)
return a golden number(cid:74)gzi(cid:75).

and, based on the interval [2i, 2i+1) its absolute value is in,

The protocol for performing this operation is presented in

Algorithm 4.

Result: Will return the sign of the input and (xj, yj) if

gx ∈ [2j, 2j+1).

Algorithm 4: TwoPowerConst

Data: (cid:74)gx(cid:75),{gzi} = {(xi, yi)}, n, m < n
1 (cid:74)sign(cid:75),(cid:74)f(cid:75) ← GoldToFix((cid:74)gx(cid:75));
2 {(cid:74)bi(cid:75)}n+m−1
i=0 ← MSNZB((cid:74)f(cid:75));
3 ((cid:74)s(cid:75),(cid:74)t(cid:75)) ←(cid:80)n−1
4 return(cid:74)sign(cid:75), ((cid:74)s(cid:75),(cid:74)t(cid:75))

i=−m((cid:74)bi(cid:75) · xi,(cid:74)bi(cid:75) · yi);

The computation is performed the following way. We con-
vert the input to a ﬁxed-point number. We then perform
MSNZB on the integer representative of the ﬁxed-point num-
ber and compute the scalar product with the set of public
coeﬃcients {gzi}. Note that ﬁnding the scalar product is a
local operation.
Inverse
4.3.4
We shall now describe the protocol for computing the in-

verse of a secret golden number (cid:74)gx(cid:75). A protocol for com-

puting the inverse of numbers in [0.5, 1] is presented in Al-
gorithm 5. It uses the approximation 1
+ 1)
that works well in the neighbourhood of 1 (being equivalent
to the respective Taylor series).

x =(cid:81)((1 − x)2i

The protocol for computing the inverse of a golden number

is presented in Algorithm 6.
We use Algorithm 5 as a subprotocol. Given gx as an
input, we need to ﬁnd gx(cid:48) and gy so that x(cid:48) ∈ [0.5, 1] and

5584
5 end

Algorithm 5: HalfToOneInv

3 for i ← 0 to k − 1 do

Data: (cid:74)gx(cid:75)(x ∈ [0.5, 1]), n, m, (n > m), k
Result: (cid:74)g 1
x(cid:75)
1 (cid:74)gy(cid:75) ← 1 −(cid:74)gx(cid:75);
2 (cid:74)gy0(cid:75) ←(cid:74)gy(cid:75);
(cid:74)gyi+1(cid:75) ← GoldenMult((cid:74)gyi(cid:75),(cid:74)gyi(cid:75));
6 (cid:74)gz(cid:75) ← GoldenProd((cid:74)gy0(cid:75) + 1,(cid:74)gy1(cid:75) + 1, . . . ,(cid:74)gyk(cid:75) + 1);
7 return(cid:74)gz(cid:75)
Data: (cid:74)gx(cid:75), n, m, (n > m),{(xi, yi)}
x(cid:75)
Result: (cid:74)g 1
1 ((cid:74)sign(cid:75),(cid:74)gy(cid:75)) ← TwoPowerConst((cid:74)gx(cid:75),{gzi});
2 (cid:74)gx(cid:48)(cid:75) ← GoldenMult((cid:74)gx(cid:75),(cid:74)gy(cid:75));
3 (cid:74)gz(cid:75) ← HalfToOneInv((cid:74)gx(cid:48)(cid:75));
4 (cid:74)gw(cid:75) ← GoldenMult((cid:74)gy(cid:75),(cid:74)gz(cid:75));
5 (cid:74)gu(cid:75) ← ObliviousChoice((cid:74)sign(cid:75),−(cid:74)gw(cid:75),(cid:74)gw(cid:75));
6 return(cid:74)gu(cid:75)

Algorithm 6: GoldInv

x · 1

x(cid:48) = 1

that x · y = x(cid:48). We can then use the HalfToOneInv function
to compute 1
y which we shall then multiply with y
to obtain 1
x . y is an approximation of a suitable power of 2.

Here the gzi are approximations of diﬀerent powers of 2 –
when x ∈ [2j, 2j+1), then TwoPowerConst should return ap-
proximately 2−j−1.

We compute y using the function TwoPowerConst((cid:74)gx(cid:75),{gzi}).
We compute(cid:74)gx(cid:48)(cid:75) by multiplying(cid:74)gx(cid:75) and(cid:74)gy(cid:75). We then
use the HalfToOneInv protocol on(cid:74)gx(cid:48)(cid:75), obtaining(cid:74)g 1
x(cid:48)(cid:75). To
Finally, since our current result is approximately(cid:74)g(cid:12)(cid:12) 1
(cid:12)(cid:12)(cid:75),
get this back to the correct range, we multiply it by(cid:74)gy(cid:75).

we have to make an oblivious choice between the result and
its additive inverse so that it would have the correct sign.
4.3.5 Square Root Protocol
Algorithm 7 presents the protocol for ﬁnding the square

x(cid:48)

root of a golden section number.

4

3

Algorithm 7: GoldSqrt

2 for i ← 0 to k − 1 do

Data: (cid:74)gx(cid:75), m, n, (n > m), k,{gwi}
x(cid:75)
Result: (cid:74)g√
1 (cid:74)gy0(cid:75) ← TwoPowerConst((cid:74)gx(cid:75),{gwi});
(cid:74)gz0(cid:75) ← GoldenMult((cid:74)gyi(cid:75),(cid:74)gx(cid:75));
(cid:74)gz1(cid:75) ← GoldenMult((cid:74)gyi(cid:75),(cid:74)gz0(cid:75));
(cid:74)gz1(cid:75) ← 3 −(cid:74)gz1(cid:75);
(cid:74)gz2(cid:75) ← GoldenMult((cid:74)gyi(cid:75),g 0.5);
(cid:74)gyi+1(cid:75) ← GoldenMult((cid:74)gz1(cid:75),(cid:74)gz2(cid:75));
9 (cid:74)gw(cid:75) ← GoldenMult((cid:74)gx(cid:75),(cid:74)gyk(cid:75));
10 return(cid:74)gw(cid:75)

7
8 end

5

6

The protocol is following. We ﬁrst compute the inverse
square root of the input x and then multiply it with x. There
x where the formula for
exists an iterative algorithm for
the nth approximation is yn+1 = 0.5yn(3−xy2
n). The reason
why we use inverse square root to compute square root is

1√

that general iterative methods for square root need division,
which is too costly in out setting.

To obtain the starting approximations, we shall use the
2 – if x ∈

function TwoPowerConst where the constants are 2
[2j, 2j+1), the function will return 2

j
2 .

i

5. LOGARITHMIC NUMBERS

In this section we will present logarithmic numbers. We
will explain the format, and then describe algorithms for
computing the inverse, product, square root, logarithm, ex-
ponential function and sum.
5.1 Logarithmic number format

We represent a logarithmic number x as a triple (zx, sx, ex).
Zero-bit zx is 0 if x is zero and 1 if x is non-zero. Sign bit
sx is 0 if x is positive and 1 if x is negative. Exponent
ex is an (m + n)-bit integer which represents a ﬁxed-point
number with m bits before and n bits after the radix point.
The exponent is biased and so can be both positive and
negative. The value of the number is computed as follows:
(z, s, e) → z · (−1)s · 2(e−Bias)/2n
, where Bias is 2m+n−2 − 1.
The larger m, the larger the range of numbers we can rep-
resent. The larger n, the more precision we have.
While the length of the exponent is m + n bits, only the
lowest m + n − 1 bits are used. The highest bit is always
zero to achieve faster comparisons between exponents.
5.2 Inverse

Algorithm 8 presents the protocol for ﬁnding the inverse

of a logarithmic number.

Algorithm 8: LogNumInv

Data: (cid:74)x(cid:75) = ((cid:74)zx(cid:75),(cid:74)sx(cid:75),(cid:74)ex(cid:75))
Result: (cid:74)1/x(cid:75)
1 return ((cid:74)1(cid:75),(cid:74)sx(cid:75), 2 ·(cid:74)Bias(cid:75) −(cid:74)ex(cid:75))

Inverse is computed by leaving the sign unchanged and
negating the exponent, based on the formula ((−1)sx·2ex )−1 =
(−1)sx · 2−ex . We assume that the input is not zero. The
zero-bit of the result is set to 1 to indicate that the result is
non-zero. We also have to account for the bias when com-
puting the exponent. When changing the sign of a biased
integer, we have to not only change the sign but also add
the double of the bias.
5.3 Multiplication

Algorithm 9 presents the protocol for multiplying loga-

rithmic numbers.

Algorithm 9: LogNumMult

Data: (cid:74)x(cid:75) = ((cid:74)zx(cid:75),(cid:74)sx(cid:75),(cid:74)ex(cid:75)),(cid:74)y(cid:75) = ((cid:74)zy(cid:75),(cid:74)sy(cid:75),(cid:74)ey(cid:75))
Result: (cid:74)x · y(cid:75)
1 (cid:74)e(cid:75) ←(cid:74)ex(cid:75) +(cid:74)ey(cid:75);
2 (cid:74)z(cid:75) ←(cid:74)zx(cid:75) ∧(cid:74)zy(cid:75) ∧ ((cid:74)e(cid:75) ≥(cid:74)Bias(cid:75));
3 return ((cid:74)z(cid:75),(cid:74)sx(cid:75) ⊕(cid:74)sy(cid:75),(cid:74)e(cid:75) −(cid:74)Bias(cid:75))

Multiplication of logarithmic numbers is based on the for-
mula 2ex 2ey = 2ex+ey . Because our exponents are biased,
we have to subtract the bias when adding them. To get
the sign of the result, we compute the XOR of the signs of

559the operands. The zero-bit of the end result is computed
as follows: the result is non-zero iﬀ both operands are non-
zero and their product does not underﬂow. Therefore, any
underﬂows are rounded down to zero.
5.4 Square root

Algorithm 10 presents the protocol for ﬁnding the square

root of a logarithmic number.

Algorithm 10: LogNumSqrt

Data: (cid:74)x(cid:75) = ((cid:74)zx(cid:75),(cid:74)sx(cid:75),(cid:74)ex(cid:75))
x(cid:75)
Result: (cid:74)√
1 return ((cid:74)zx(cid:75),(cid:74)0(cid:75), ((cid:74)ex(cid:75) +(cid:74)Bias(cid:75))/2)

√

We assume that the input is non-negative. If the input
is zero we return zero, and if the input is non-zero then we
2ex = 2ex/2. When
divide the exponent by two because
dividing a biased integer by two we have to double the bias
before division in order to get the end result with the correct
bias.
5.5 Logarithm

Algorithm 11 presents the protocol for ﬁnding the binary

logarithm of a logarithmic number.

i=0

Algorithm 11: LogNumLg

Data: m, n,(cid:74)x(cid:75) = ((cid:74)zx(cid:75),(cid:74)sx(cid:75),(cid:74)ex(cid:75)), p = {pi}l
Result: (cid:74)lg x(cid:75)
1 (cid:74)s(cid:75) ←(cid:74)ex(cid:75) <(cid:74)Bias(cid:75);
2 (cid:74)e(cid:48)(cid:75) ←(cid:74)ex(cid:75) −(cid:74)Bias(cid:75);
3 (cid:74)e(cid:48)(cid:48)(cid:75) ←(cid:74)Bias(cid:75) −(cid:74)ex(cid:75);
4 (cid:74)e(cid:75) ← ObliviousChoice((cid:74)s(cid:75),(cid:74)e(cid:48)(cid:48)(cid:75),(cid:74)e(cid:48)(cid:75));
i=0 ← MSNZB(BitExtract((cid:74)e(cid:75)));
5 {(cid:74)ji(cid:75)}m+n−3
6 (cid:74)v(cid:75) ←(cid:74)e(cid:75) −(cid:80)m+n−3
(cid:74)ji(cid:75) · 2i;
7 (cid:74)w(cid:75) ←(cid:80)m+n−3
((cid:74)ji(cid:75) · 2m+n−i) ·(cid:74)v(cid:75);
8 (cid:74)z(cid:75) ←(cid:74)ex(cid:75) (cid:54)=(cid:74)Bias(cid:75);
9 (cid:74)t(cid:75) ← cPoly(p,(cid:74)w(cid:75))/2m−1;
10 (cid:74)u(cid:75) ← 2n+1 ·(cid:80)m+n−3
((cid:74)ji(cid:75) · (n + 1 − i));
11 return ((cid:74)z(cid:75),(cid:74)s(cid:75),(cid:74)t(cid:75) −(cid:74)u(cid:75) +(cid:74)Bias(cid:75))

i=0

i=0

i=0

To compute the binary logarithm of a logarithmic num-
ber, we assume that the input is positive. We note that if
2ey = lg 2ex then ey = lg ex. Therefore, the exponent of
the output is the binary logarithm of the exponent of the
input, which means that the problem is reduced to comput-
ing the binary logarithm of a ﬁxed-point number. However,
logarithmic numbers with negative and zero exponents (log-
arithmic numbers that lie in (0, 1]) need special handling, be-
cause we do not want to deal with computing the logarithms
of negative numbers. If the exponent of the input is negative,
we ﬁnd the result using the formula lg 2−ex = −2lg ex . Thus,
to compute the binary logarithm of a logarithmic number we
compute the logarithm of the absolute value of the exponent,
and set the sign bit of the end result to 0 if the exponent
is positive, and 1 if the exponent is negative. If the expo-
nent is equal to 0 then we set the zero-bit of the result to 0,
otherwise we set it to 1.
We compute the binary logarithm of a ﬁxed-point number
with the help of a polynomial p that approximates f(cid:48)(x) =

log4 (x + 1) + 1/2 in [0, 1) (obtained using Chebychev inter-
polation). Our polynomial evaluation protocol only allows
inputs and outputs in range [0, 1), therefore, instead of ap-
proximating f (x) = lg x directly, we ﬁrst shift the number to
the left so that we shift out all leading zeroes and the most
signiﬁcant non-zero bit (via BitExtract and MSNZB protocols
and multiplications). Then we consider the resulting num-
ber as a ﬁxed-point number with 0 bits before radix point
and approximate the function f(cid:48)(x) = log4 (x + 1)+1/2, the
values of which are in [0.5, 1). In order to derive the loga-
rithm of the original number from this intermediate result,
we divide it by 2m−1 and subtract a constant which depends
on the most signiﬁcant non-zero bit of the original number.
In order to compute natural logarithm via binary log-
arithm, we multiply the result by ln 2, because loga x =
loga 2 · lg x.
5.6 Exponent

Algorithm 12 presents the protocol for ﬁnding the base 2

exponent of a logarithmic number.

i=0

Algorithm 12: LogNumExp

Data: m, n,(cid:74)x(cid:75) = ((cid:74)zx(cid:75),(cid:74)sx(cid:75),(cid:74)ex(cid:75)), p = {pi}l
Result: (cid:74)2x(cid:75)
1 (cid:74)g(cid:75) ← Truncate((cid:74)ex(cid:75), n);
2 (cid:74)w(cid:75) ←(cid:74)2m−2 + m − 3(cid:75) −(cid:74)ex(cid:75)/2n;
3 (cid:74)y(cid:75) ← cPoly(p, 2m−2 ·(cid:74)g(cid:75));
4 (cid:74)t(cid:75) ← Pick({(cid:74)y(cid:75)/2i}m+n−3
,(cid:74)w(cid:75));
5 (cid:74)z(cid:75) ←(cid:74)zx(cid:75) ∨ ((cid:74)ex(cid:75) <(cid:74)2n · (m − 2) + Bias(cid:75));
6 (cid:74)u(cid:75) ←(cid:74)zx(cid:75) · ObliviousChoice((cid:74)sx(cid:75),−(cid:74)t(cid:75),(cid:74)t(cid:75)) +(cid:74)Bias(cid:75);
7 return ((cid:74)z(cid:75),(cid:74)0(cid:75),(cid:74)u(cid:75))

i=0

To compute the base 2 exponential function on logarith-
mic numbers, we note that if 2ey = 22ex then ey = 2ex .
Therefore, the exponent of the output is 2 to the power of
the exponent of the input, and the problem is reduced to
computing base 2 exponential function on ﬁxed-point num-
bers.
To ﬁnd the base 2 exponent of a ﬁxed-point number, we
use the formula 2ex = 2(cid:98)ex(cid:99)+1·2{ex}−1. Separating the input
into whole and fractional part allows us to approximate the
function f (x) = 2x−1 on the fractional part, with inputs in
range [0, 1) and outputs in range [0.5, 1), which is suitable for
our interpolation protocol. In the algorithm, approximation
polynomial for f (x) = 2x−1 is denoted as p.

After exponentiating the fractional part, we shift the re-
sult right by a number of bits which depends on the whole
part. This is done by computing all public shifts and obliv-
iously picking the right one (line 4).
Note that in order to achieve good precision when approxi-
mating 2{ex}−1, we do not just truncate the exponent to get
the fractional part. We truncate and then perform a left
shift so that polynomial evaluation protocol has more bits
of precision to work with. Therefore, after polynomial eval-
uation we do not have to perform a left shift, which shifts in
zeroes from the right and therefore means lost precision, but
a right shift, which shifts out bits to the right and therefore
means ridding ourselves of excess precision achieved with
polynomial approximation.
lg e, because ax = 2lg a·x.

In order to compute exp x, we multiply the argument by

5605.7 Addition

Algorithm 13 presents the protocol for ﬁnding the sum of

logarithmic numbers.

i=0

))

{di}m+n−3

i=0

Algorithm 13: LogNumAdd

, d =
, c = 2n(2m + lg (1 − 2−2−n

Data: (cid:74)x(cid:75) = ((cid:74)zx(cid:75),(cid:74)sx(cid:75),(cid:74)ex(cid:75)),(cid:74)y(cid:75) =
((cid:74)zy(cid:75),(cid:74)sy(cid:75),(cid:74)ey(cid:75)), a = {ai}m+n−3
Result: (cid:74)x + y(cid:75)
1 (cid:74)l(cid:75) ←(cid:74)zx(cid:75) ∨(cid:74)zy(cid:75) ∧ ((cid:74)ex(cid:75) <(cid:74)ey(cid:75));
2 ((cid:74)x(cid:75),(cid:74)y(cid:75)) ← Swap((cid:74)l(cid:75),(cid:74)x(cid:75),(cid:74)y(cid:75));
3 (cid:74)e(cid:75) ←(cid:74)ex(cid:75) −(cid:74)ey(cid:75);
4 {(cid:74)bi(cid:75)}m+n−1
i=0 ← BitExtract(e);
5 (cid:74)t(cid:75) ←(cid:74)sx(cid:75) ⊕(cid:74)sy(cid:75);
6 {(cid:74)pi(cid:75)}m+n−2
i=0 ← MSNZB({(cid:74)bi(cid:75)}m+n−2
i=0 ← ConjBit({(cid:74)pi(cid:75)}m+n−2
7 {(cid:74)ri(cid:75)}m+n−2
,(cid:74)t(cid:75));
i=0 ← {(cid:74)ri(cid:75)}m+n−2
8 {(cid:74)qi(cid:75)}m+n−2
⊕ {(cid:74)pi(cid:75)}m+n−2
9 (cid:74)k(cid:75) ←(cid:80)m+n−2
((cid:74)pi(cid:75) · 2m+n−1−i · ((cid:74)e(cid:75) −(cid:74)2i(cid:75)));
i=0 ← cPolyArr(a,(cid:74)k(cid:75));
10 {(cid:74)vi(cid:75)}m+n−3
i=0 ← cPolyArr(d,(cid:74)k(cid:75));
11 {(cid:74)wi(cid:75)}m+n−3
a(cid:75) ← ((cid:87)m+n−2
(cid:74)bi(cid:75) ⊕(cid:74)q0(cid:75)) · 2n;
12 (cid:74)g(cid:48)
13 (cid:74)ga(cid:75) ←(cid:80)m+n−3
((cid:74)qi+1(cid:75) ·(cid:74)vi(cid:75));
14 (cid:74)g(cid:48)
d(cid:75) ←(cid:74)r0(cid:75) · c;
15 (cid:74)gd(cid:75) ←(cid:80)m+n−3
((cid:74)ri+1(cid:75) ·(cid:74)wi(cid:75));
16 (cid:74)u(cid:75) ←(cid:74)zy(cid:75) · ((cid:74)g(cid:48)
a(cid:75) +(cid:74)ga(cid:75) +(cid:74)g(cid:48)
17 (cid:74)z(cid:75) ←(cid:74)zx(cid:75) ⊕(cid:74)zx(cid:75) ∧(cid:74)zy(cid:75) ∧(cid:74)t(cid:75) ∧ ((cid:74)ex(cid:75) < −(cid:74)u(cid:75));
18 return ((cid:74)z(cid:75),(cid:74)sx(cid:75),(cid:74)ex(cid:75) +(cid:74)u(cid:75))

d(cid:75) +(cid:74)gd(cid:75));

i=0

;

i=0

);

i=0

i=0

i=0

i=0

i=0

i=1

First, we sort the operands by their absolute value.

If
the absolute value of x is smaller than the absolute value
of y then we swap them (lines 1 and 2). Now we know
that |x| ≥ |y|.
In order to reduce addition to a single-
operand function, we factorise 2ex ±2ey as 2ex (1±2ey−ex ) =
2ex+lg (1±2ey−ex ). Knowing that the ﬁrst operand is larger
is beneﬁcial for two reasons: it gives us the sign of the end
result (which is equal to the sign of the larger operand), and
it ensures that lg (1 − 2ey−ex ) is a real-valued function. Now
that the operands are sorted, we approximate two diﬀerent
functions, one for addition and one for subtraction.
To compute lg (1 ± 2ey−ex ) we ﬁnd the diﬀerence of the
exponents (line 3) and denote it with e. We also ﬁnd all its
bits (line 4). We denote with t (line 5) a bit which is 0 if we
perform addition and 1 if we subtract.
Both fa(x) = lg (1 + 2x) and fd(x) = lg (1 − 2x) are func-
tions for which interpolating with a single polynomial though
Chebyshev nodes yields poor precision, especially for the val-
ues of the argument near zero. Therefore, our approach for
approximating these functions is to ﬁnd the most signiﬁcant
non-zero bit of the argument (line 6) and shift the argu-
ment to the left so that we shift out all leading zeroes and
the most signiﬁcant non-zero bit (line 9). On the resulting
number we compute a separate polynomial for each function
and each possible position of the most signiﬁcant non-zero
bit (lines 10 and 11). (In the algorithm, we denote the array
of polynomials for addition as a and the array of polynomials
for subtraction as d.)

There are also two special cases which are not covered by
the polynomials:
if the argument is zero, and if the most
signiﬁcant non-zero bit of the argument is in the lowest pos-

sible position. If the argument is zero then it means that
x and y have an equal absolute value, in which case for
addition we return as the value of fa(x) = lg (1 + 2x) a
constant representing 1 (line 12), and for subtraction we re-
turn 0 as the ﬁnal result. If the most signiﬁcant non-zero
bit of the argument is in the lowest possible position then
for addition we return as the value of fa(x) = lg (1 + 2x)
a constant representing 1, and for subtraction we return as
the value of fd(x) = lg (1 − 2x) a constant c representing
2m + lg (1 − 2−2−n

) (line 14).

In lines 13 and 15 we pick the right interpolation result de-
pending on the most signiﬁcant nonzero bit of the argument.
From this collection of constants and polynomial approxima-
tion results, we pick the correct one based on whether we
are performing addition or subtraction, and depending on

the most signiﬁcant non-zero bit of (cid:74)ex(cid:75) −(cid:74)ey(cid:75). In line 16

we pick the value u which is added to the exponent of the
larger logarithmic number to achieve the ﬁnal result. Note
that if the smaller operand is zero then u is also set to zero.
In case of subtraction, we check for underﬂow, and if the
result of subtraction is smaller than is possible to accurately
represent with a non-zero number we round the result down
to zero (line 17).

The approach to addition presented in Algorithm 13 re-
sults in precision which is reasonable but still far from ideal.
One way to perform precise logarithmic addition is based on
formula x + y = x· lg (2 · 2y/x) where we ﬁnd the sum of two
numbers with the help of division, exponentiation, multipli-
cation, and logarithm.
In order to achieve good precision
with this method, the operands have to be cast up before
computing the sum. As this method involves a composition
of exponentiation and logarithm, both performed on num-
bers twice the size of the original inputs, it is extremely
ineﬃcient, but it allows for near-ideal precision.

6. ANALYSIS OF ERROR AND PERFOR-

MANCE

In this section, we compare golden and logarithmic num-
bers against existing implementations of ﬂoating-point and
ﬁxed-point representations. Comparisons are made in both
performance and accuracy for diﬀerent operations and bit-
widths. We look at addition, multiplication, reciprocal and
square root. For ﬂoating-point and logarithmic numbers,
we additionally measure the performance and accuracy of
exponentiation and natural logarithm.

We have implemented logarithmic and golden section num-
bers on the Sharemind SMC platform. We chose Share-
mind because of its maturity, tooling, and availability of
ﬁxed-point and ﬂoating-point numbers. As existing num-
ber systems were already implemented using Sharemind’s
domain-speciﬁc language [17], we decided to also use it for
golden section and logarithmic representations. The proto-
col language provides us with directly comparable perfor-
mance and allows to avoid many complexities that a direct
C++ implementation would have.

To provide a clear overview of accuracy and speed trade-
oﬀs, we measured the performance of each number system
on multiple bit-widths. Generally, higher bit-widths oﬀer us
better accuracy for the cost of performance.
We implemented three diﬀerent versions of logarithmic
numbers: Lh, Ls and Ld (with h, s and d standing for half,
single and double precision). For Lh, we chose m = 6 and

561n = 16 (Section 5.1) so that it oﬀers at least as much range
and precision as IEEE 754 half-precision ﬂoating-point num-
bers and also aligns to a byte boundary (2 + 6 + 16 = 24 bits
or 3 bytes). For Ls, we chose m = 9 and n = 29 for a size of
40 bits and accuracy comparable to single-precision ﬂoating-
point numbers. For Ld, we chose m = 12 and n = 58 for a
size of 72 bits and accuracy comparable to double-precision
ﬂoating-point numbers.
We also implemented tree versions of golden numbers (Sec-
tion 4): G32, G64 and G128 where for Gn we store two n-bit
components to provide comparable accuracy to n-bit ﬁxed-
point numbers with radix point at (cid:98)n/2(cid:99).

We compare our results against existing secure real num-
ber implementations that Sharemind already provides. Two
ﬂoating-point number representations are used: ﬂoats, pro-
viding comparable accuracy to single-precision ﬂoating-point
numbers, and ﬂoatd, providing comparable accuracy to double-
precision ﬂoating-point numbers. See [13–15] for implemen-
tation details. Logarithmic numbers compare well with ﬂoating-
point numbers as both are designed to provide good relative
errors. Additionally, Sharemind provides 32-bit and 64-bit
ﬁxed-point numbers with radix point in the middle (denoted
with ﬁx32 and ﬁx64 respectively). Golden numbers compare
well with ﬁxed-point numbers as both are designed to pro-
vide good absolute errors.

Accuracy was measured experimentally, by identifying the
range of inputs in which the largest errors should be found,
and then uniformly sampling this range to ﬁnd maximum
error.

Performance measurements were made on a cluster of three
computers connected with 10Gbps Ethernet. Each cluster
node was equipped with 128GB DDR4 memory and two 8-
core Intel Xeon (E5-2640 v3) processors, and was running
Debian 8.2 Jessie with memory overcommit and swap dis-
abled.

We measured each operation on various input sizes, exe-
cuting the operation in parallel on the inputs. Each mea-
surement was repeated a minimum of ten times and the
mean of the measurements was recorded. Measurements
were performed in randomized order. Note that due to the
networked nature of the protocols, parallel execution im-
proves performance drastically up to the so called saturation
point.

We recorded maximum achieved operations per second
that states how many parallel operations can be evaluated
on given input size per second. For example, if we can per-
form a single parallel operation on 100-element vectors per
second this gives us 100 operations per second.

Our performance and accuracy measurements are displayed
in Figure 1. For every variant of every real number rep-
resentation, we plot its maximum achieved performance in
operations per second (OP/s) on the y-axis and its error on
the x-axis. Note that performance increases on the y-axis
and accuracy improves on the x-axis. Logarithmic numbers
are represented with squares, golden section with diamonds,
ﬂoating-point numbers with circles and ﬁxed-point numbers
with triangles. The accuracy of operations increases with
shade, so that white shapes denote least precision and best
performance.

We have measured addition, multiplication, reciprocal and
square root. For ﬂoating-point and logarithmic numbers we
also benchmarked exponentiation and natural logarihm. In
most cases, maximum relative error was measured, but for

ﬁxed-point numbers and some golden number protocols this
is not reasonable. In these cases maximum absolute error
was measured, and this is denoted by adding label “A” to
the mark.

Some operations, such as ﬁxed-point addition, achieve
perfect accuracy within their number representation. These
cases are marked with a “(cid:63)”. Instead of maximum error, we
plot the value of half of the step between two consecutive
numbers in this representation

In Figure 1 we can see that golden section numbers com-
pare relatively well with ﬁxed-point numbers. They achieve
somewhat worse performance in our aggregated benchmarks,
but the true picture is actually more detailed.

What is not reﬂected on the graph is the fact that golden
section multiplication requires signiﬁcantly fewer communi-
cation rounds than ﬁxed-point multiplication. For instance,
ﬁx32 multiplication requires 16 communication rounds, but
comparable G64 multiplication requires only 11. This makes
golden section numbers more suitable for high latency and
high throughput situations, and also better for applications
that perform many consecutive operations on small inputs.
The worse performance of golden section numbers after
the saturation point can be wholly attributed to increased
communication cost. Namely, every multiplication of G64
numbers requires 6852 bits of network communication, but
a single ﬁx32 multiplication requires only 2970.

We can also see that compared to ﬂoating-point numbers,
logarithmic numbers perform signiﬁcantly better in case of
multiplication (Figure 1b), reciprocal (Figure 1d) and square
root (Figure 1c), while oﬀering similar accuracy. Unfor-
tunately, logarithmic numbers do not compare favourably
to ﬂoating-point numbers with both lower performance and
worse accuracy of addition (Figure 1a).
In case of natu-
ral logarithm and exponentiation, logarithmic numbers are
close to ﬂoating-point numbers in both performance and ac-
curacy. This means that logarithmic numbers are a poor
choice for applications that are very addition heavy but an
excellent choice for applications that require many multi-
plicative operations.

7. CONCLUSIONS AND FURTHER WORK
Technically, protected computation domains are very dif-
ferent from the classical open ones. Many low-level bit ma-
nipulation techniques are too cumbersome to implement and
hence standard numeric algorithms do not work very well.
This holds true even for basic arithmetic operations. A full
IEEE 754 ﬂoating-point number speciﬁcation is too complex
to be eﬃcient in an oblivious setting. Even a reimplementa-
tion of the signif icand· 2exponent representation is too slow,
even in case of simple addition, since oblivious radix point
alignment is very ineﬃcient. Hence, alternatives need to be
studied.

This paper proposed two new candidate representations
for oblivious real numbers – golden and logarithmic repre-
sentations. The corresponding algorithms were implemented
on the Sharemind SMC engine and benchmarked for vari-
ous precision levels and input sizes.

The results show that we still do not have a clear winner.
Since logarithmic representation is multiplicative, adding
two logarithmic numbers is slow. However, signiﬁcant per-
formance improvements can be achieved for several elemen-
tary functions like multiplication, inverse, and square root.

562Golden number representation allows for very fast (actu-
ally, local) addition, and its multiplication speed is compara-
ble with that of ﬁxed-point numbers. However, this format
only allows for relatively slow elementary function compu-
tations.

Thus the choice of real number representation depends on

application domain and computations to be performed.

Another aspect to consider is precision. Our analysis
shows that logarithmic representation achieves the best rela-
tive error for most of the operations (except addition). How-
ever, precision achieved by our other implementations seems
more than suﬃcient for practical statistical applications.

In this paper we have developed only the most basic math-
ematical tools. In order to be applied to actual data analysis
tasks (e.g. statistical tests, ﬁnding correlation coeﬃcients,
variances, etc.), higher-level operations need to be imple-
mented. It is an interesting open question which real num-
ber representations perform optimally for various operations
and input sizes. This study will be a subject for our future
research.

8. ACKNOWLEDGEMENTS

This research has been supported by Estonian Research

Council under the grant no. IUT27-1.

9. REFERENCES
[1] Mehrdad Aliasgari, Marina Blanton, Yihua Zhang,
and Aaron Steele. Secure computation on ﬂoating
point numbers. In NDSS, 2013.

[2] David W. Archer, Dan Bogdanov, Benny Pinkas, and

Pille Pullonen. Maturity and performance of
programmable secure computation. Cryptology ePrint
Archive, Report 2015/1039, 2015.
http://eprint.iacr.org/. Journal version accepted to
IEEE Security & Privacy, to appear in 2017.

[3] Mihir Bellare, Viet Tung Hoang, and Phillip Rogaway.

Foundations of garbled circuits. In Proceedings of the
2012 ACM conference on Computer and
communications security, pages 784–796. ACM, 2012.

[4] Michael Ben-Or, Shaﬁ Goldwasser, and Avi

Wigderson. Completeness theorems for
non-cryptographic fault-tolerant distributed
computation. In Proceedings of the twentieth annual
ACM symposium on Theory of computing, pages 1–10.
ACM, 1988.

[5] George Robert Blakley. Safeguarding cryptographic

keys. In Proceedings of the 1979 AFIPS National
Computer Conference, pages 313–317, 1979.

[6] Octavian Catrina and Sebastiaan De Hoogh. Improved

primitives for secure multiparty integer computation.
In Security and Cryptography for Networks, pages
182–199. Springer, 2010.

[7] Octavian Catrina and Sebastiaan De Hoogh. Secure

multiparty linear programming using ﬁxed-point
arithmetic. In Computer Security–ESORICS 2010,
volume 6345 of Lecture Notes in Computer Science,
pages 134–150. Springer, 2010.

[8] Octavian Catrina and Amitabh Saxena. Secure

computation with ﬁxed-point numbers. In Financial
Cryptography and Data Security, volume 6052 of
Lecture Notes in Computer Science, pages 35–50.
Springer, 2010.

[9] David Chaum, Claude Cr´epeau, and Ivan Damg˚ard.

Multiparty unconditionally secure protocols. In
Proceedings of the twentieth annual ACM symposium
on Theory of computing, pages 11–19. ACM, 1988.
[10] Martin Franz and Stefan Katzenbeisser. Processing

encrypted ﬂoating point signals. In Proceedings of the
thirteenth ACM multimedia workshop on Multimedia
and security, pages 103–108. ACM, 2011.

[11] Craig Gentry. A fully homomorphic encryption
scheme. PhD thesis, Stanford University, 2009.

[12] Oded Goldreich, Silvio Micali, and Avi Wigderson.

How to play any mental game. In Proceedings of the
nineteenth annual ACM symposium on Theory of
computing, pages 218–229. ACM, 1987.

[13] Liina Kamm and Jan Willemson. Secure ﬂoating point

arithmetic and private satellite collision analysis.
International Journal of Information Security,
14(6):531–548, 2015.

[14] Liisi Kerik, Peeter Laud, and Jaak Randmets.

Optimizing MPC for robust and scalable integer and
ﬂoating-point arithmetic. LNCS. Springer, 2016.
Accepted to Workshop on Applied Homomorphic
Cryptography 2016.

[15] Toomas Krips and Jan Willemson. Hybrid model of

ﬁxed and ﬂoating point numbers in secure multiparty
computations. In Information Security: 17th
International Conference, ISC 2014, volume 8783 of
LNCS, pages 179–197. Springer, 2014.

[16] Toomas Krips and Jan Willemson. Point-counting

method for embarrassingly parallel evaluation in
secure computation. In FPS 2015, volume 9482 of
LNCS, pages 66–82. Springer, 2016.

[17] Peeter Laud and Jaak Randmets. A Domain-Speciﬁc

Language for Low-Level Secure Multiparty
Computation Protocols. In Proceedings of the 22nd
ACM SIGSAC Conference on Computer and
Communications Security, 2015, pages 1492–1503.
ACM, 2015.

[18] Yehuda Lindell and Benny Pinkas. A proof of security
of Yao’s protocol for two-party computation. Journal
of Cryptology, 22(2):161–188, 2009.

[19] Martin Pettai and Peeter Laud. Automatic Proofs of
Privacy of Secure Multi-party Computation Protocols
against Active Adversaries. In IEEE 28th Computer
Security Foundations Symposium, CSF 2015, Verona,
Italy, 13-17 July, 2015, pages 75–89. IEEE, 2015.
[20] Pille Pullonen and Sander Siim. Combining Secret
Sharing and Garbled Circuits for Eﬃcient Private
IEEE 754 Floating-Point Computations. In FC 2015
Workshops, volume 8976 of LNCS, pages 172–183.
Springer, 2015.

[21] David Russell Schilling. Knowledge doubling every 12
months, soon to be every 12 hours. Industry Tap, 2013.
http://www.industrytap.com/knowledge-doubling-
every-12-months-soon-to-be-every-12-hours/3950.

[22] Adi Shamir. How to share a secret. Communications

of the ACM, 22(11):612–613, 1979.

[23] Andrew C Yao. Protocols for secure computations. In

Foundations of Computer Science, 1982. SFCS’08.
23rd Annual Symposium on, pages 160–164. IEEE,
1982.

563(a) Addition

(cid:63)A

(cid:63)A

(cid:63)A

(cid:63)A

(cid:63)A

109

108

107

106

105

104

103

(b) Multiplication

(cid:63)

A

A

(cid:63)

A

A

)
s
/
P
O
(

e
c
n
a
m
r
o
f
r
e
P

106

105

(cid:63)

A

10−2

10−5

10−8

10−11
Error

10−14

10−17

10−20

10−2

10−5

10−8

10−14

10−17

10−20

10−11
Error

(c) Reciprocal

(d) Square root

(cid:63)

(cid:63)

(cid:63)

108

107

106

105

104

103

105

104

A

10−4

10−7

A

10−10
Error

10−13

10−16

10−19

(e) Exponentiation

106

105

104

103

105

104

)
s
/
P
O

(

e
c
n
a
m
r
o
f
r
e
P

)
s
/
P
O
(

e
c
n
a
m
r
o
f
r
e
P

A

A

10−3

10−6

10−9
Error

10−12

10−15

10−18

(f) Natural logarithm

Lh
Ls
Ld
ﬂoats
ﬂoatd

G32
G64
G128
ﬁx32
ﬁx64

)
s
/
P
O
(

e
c
n
a
m
r
o
f
r
e
P

)
s
/
P
O

(

e
c
n
a
m
r
o
f
r
e
P

)
s
/
P
O
(

e
c
n
a
m
r
o
f
r
e
P

10−5

10−8

10−11

10−14

10−17

Error

10−5

10−7

10−9
Error

10−11

10−13

10−15

Figure 1: Performance and accuracy trade-oﬀs between various real number representations. Performance in operations per
second is denoted on the y-axis and absolute or relative error on the x-axis.
In most cases maximum relative error was
measured but in a few cases maximum absolute error was measured – we denote this using “A”. Performance increases on the
y-axis and accuracy improves on the x-axis. We annotate operations that achieve perfect accuracy within the representation
with (cid:63) and plot the value of half of the step between two consecutive numbers in this representation.

564