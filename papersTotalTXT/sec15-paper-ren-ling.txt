Constants Count: Practical Improvements  

to Oblivious RAM

Ling Ren, Christopher Fletcher, and Albert Kwon, Massachusetts Institute of Technology; 
Emil Stefanov, University of California, Berkeley; Elaine Shi, Cornell University; Marten van 
Dijk, University of Connecticut; Srinivas Devadas, Massachusetts Institute of Technology

https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/ren-ling

This paper is included in the Proceedings of the 24th USENIX Security SymposiumAugust 12–14, 2015 • Washington, D.C.ISBN 978-1-931971-232Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXConstants Count: Practical Improvements to Oblivious RAM

Ling Ren

MIT

Christopher Fletcher

MIT

Albert Kwon

MIT

Emil Stefanov
UC Berkeley

Elaine Shi

Cornell University

Marten van Dijk

UConn

Srinivas Devadas

MIT

Abstract

Oblivious RAM (ORAM) is a cryptographic primitive
that hides memory access patterns as seen by untrusted
storage. This paper proposes Ring ORAM, the most
bandwidth-efﬁcient ORAM scheme for the small client
storage setting in both theory and practice. Ring ORAM
is the ﬁrst tree-based ORAM whose bandwidth is in-
dependent of the ORAM bucket size, a property that
unlocks multiple performance improvements.
First,
Ring ORAM’s overall bandwidth is 2.3× to 4× better
than Path ORAM, the prior-art scheme for small client
storage. Second, if memory can perform simple un-
trusted computation, Ring ORAM achieves constant on-
line bandwidth (∼ 60× improvement over Path ORAM
for practical parameters). As a case study, we show Ring
ORAM speeds up program completion time in a secure
processor by 1.5× relative to Path ORAM. On the the-
ory side, Ring ORAM features a tighter and signiﬁcantly
simpler analysis than Path ORAM.

1

Introduction

With cloud computing and storage gaining popularity,
privacy of users’ sensitive data has become a large con-
cern. It is well known, however, that encryption alone
is not enough to ensure data privacy. Even after encryp-
tion, a malicious server still learns a user’s access pattern,
e.g., how frequently each piece of data is accessed, if the
user scans, binary searches or randomly accesses her data
at different stages. Prior works have shown that access
patterns can reveal a lot of information about encrypted
ﬁles [14] or private user data in computation outsourc-
ing [32, 18].

Oblivious RAM (ORAM) is a cryptographic primi-
tive that completely eliminates the information leakage
in memory access traces. In an ORAM scheme, a client
(e.g., a local machine) accesses data blocks residing on
a server, such that for any two logical access sequences

of the same length, the observable communications be-
tween the client and the server are computationally in-
distinguishable.

ORAMs are traditionally evaluated by bandwidth—
the number of blocks that have to be transferred between
the client and the server to access one block, client stor-
age—the amount of trusted local memory required at the
client side, and server storage—the amount of untrusted
memory required at the server side. All three metrics
are measured as functions of N, the total number of data
blocks in the ORAM.

A factor that determines which ORAM scheme to use
is whether the client has a large (GigaBytes or larger) or
small (KiloBytes to MegaBytes) storage budget. An ex-
ample of large client storage setting is remote oblivious
ﬁle servers [30, 17, 24, 3]. In this setting, a user runs on
a local desktop machine and can use its main memory
or disk for client storage. Given this large client storage
budget, the preferred ORAM scheme to date is the SSS
construction [25], which has about 1 · logN bandwidth
and typically requires GigaBytes of client storage.

In the same ﬁle server application, however, if the user
is instead on a mobile phone, the client storage will have
to be small. A more dramatic example for small client
storage is when the client is a remote secure processor
— in which case client storage is restricted to the pro-
cessor’s scarce on-chip memory. Partly for this reason,
all secure processor proposals [18, 16, 8, 31, 22, 7, 5, 6]
have adopted Path ORAM [27] which allows for small
(typically KiloBytes of) client storage.

The majority of this paper focuses on the small client
storage setting and Path ORAM. In fact, our construc-
tion is an improvement to Path ORAM. However, in
Section 7, we show that our techniques can be eas-
ily extended to obtain a competitive large client storage
ORAM.

USENIX Association  

24th USENIX Security Symposium  415

Path ORAM
Ring ORAM

Ring ORAM + XOR

Online Bandwidth
Z logN = 4logN

∼ 1· logN

∼ 1

Overall Bandwidth
2Z logN = 8logN

3-3.5logN
2-2.5logN

log N levels

~

Table 1: Our contributions. Overheads are relative to an in-
secure system. Ranges in constants for Ring ORAM are due to
different parameter settings. The bandwidth cost of tree ORAM
recursion [23, 26] is small (< 3%) and thus excluded. XOR
refers to the XOR technique from [3].

Position map

Stash

Z = 4

Server

Client

Figure 1: Path ORAM server and client storage. Sup-
pose the black block is mapped to the shaded path. In
that case, the block may reside in any slot along the path
or in the stash (client storage).

1.1 Path ORAM and Challenges
We now give a brief overview of Path ORAM (for more
details, see [27]). Path ORAM follows the tree-based
ORAM paradigm [23] where server storage is structured
as a binary tree of roughly logN levels. Each node in
the tree is a bucket that can hold up to a small num-
ber Z of data blocks. Each path in the tree is deﬁned
as the sequence of buckets from the root of the tree to
some leaf node. Each block is mapped to a random path,
and must reside somewhere on that path. To access a
block, the Path ORAM algorithm ﬁrst looks up a posi-
tion map, a table in client storage which tracks the path
each block is currently mapped to, and then reads all the
(∼ Z logN) blocks on that path into a client-side data
structure called the stash. The requested block is then
remapped to a new random path and the position map
is updated accordingly. Lastly, the algorithm invokes an
eviction procedure which writes the same path we just
read from, percolating blocks down that path.
(Other
tree-based ORAMs use different eviction algorithms that
are less effective than Path ORAM, and hence the worse
performance.)

The bandwidth of Path ORAM is 2Z logN because
each access reads and writes a path in the tree. To
prevent blocks from accumulating in client storage, the
bucket size Z has to be at least 4 (experimentally veri-
ﬁed [27, 18]) or 5 (theoretically proven [26]).

We remind readers not to confuse the above read/write
path operation with reading/writing data blocks.
In
ORAM, both reads and writes to a data block are served
by the read path operation, which moves the requested
block into client storage to be operated upon secretly.
The sole purpose of the write path operation is to evict
blocks from the stash and percolate blocks down the
tree.

Despite being a huge improvement over prior

schemes, Path ORAM is still plagued with several im-
portant challenges. First, the constant factor 2Z ≥ 8 is
substantial, and brings Path ORAM’s bandwidth over-
head to > 150× for practical parameterizations. In con-
trast, the SSS construction does not have this bucket size
parameter and can achieve close to 1· logN bandwidth.
(This bucket-size-dependent bandwidth is exactly why
Path ORAM is dismissed in the large client storage set-
ting.)

Second, despite the importance of overall bandwidth,
online bandwidth—which determines response time—
is equally, if not more, important in practice. For Path
ORAM, half of the overall bandwidth must be incurred
online. Again in contrast, an earlier work [3] reduced
the SSS ORAM’s online bandwidth to O(1) by grant-
ing the server the ability to perform simple XOR compu-
tations. Unfortunately, their techniques do not apply to
Path ORAM.

1.2 Our Contributions
In this paper, we propose Ring ORAM to address both
challenges simultaneously. Our key technical achieve-
ment is to carefully re-design the tree-based ORAM such
that the online bandwidth is O(1), and the amortized
overall bandwidth is independent of the bucket size. We
compare bandwidth overhead with Path ORAM in Ta-
ble 1. The major contributions of Ring ORAM include:

• Small online bandwidth. We provide the ﬁrst
tree-based ORAM scheme that achieves ∼ 1 online
bandwidth, relying only on very simple, untrusted
computation logic on the server side. This repre-
sents at least 60× improvement over Path ORAM
for reasonable parameters.

• Bucket-size independent overall bandwidth.
While all known tree-based ORAMs incur an over-
all bandwidth cost that depends on the bucket size,
Ring ORAM eliminates this dependence, and im-
proves overall bandwidth by 2.3× to 4× relative to
Path ORAM.

• Simple and tight theoretical analysis. Using novel
proof techniques based on Ring ORAM’s eviction

416  24th USENIX Security Symposium 

USENIX Association

2

algorithm, we obtain a much simpler and tighter
theoretical analysis than that of Path ORAM. Of in-
dependent interest, we note that the proof of Lemma
1 in [27], a crucial lemma for both Path ORAM and
this paper, is incomplete (the lemma itself is cor-
rect). We give a rigorous proof for that lemma in
this paper.

As mentioned, one main application of small client
storage ORAM is for the secure processor setting. We
simulate Ring ORAM in the secure processor setting and
conﬁrm that the improvement in bandwidth over Path
ORAM translates to a 1.5× speedup in program comple-
tion time. Combined with all other known techniques,
the average program slowdown from using an ORAM is
2.4× over a set of SPEC and database benchmarks.

Extension to larger client storage. Although our ini-
tial motivation was to design an optimized ORAM
scheme under small client storage, as an interesting by-
product, Ring ORAM can be easily extended to achieve
competitive performance in the large client storage set-
ting. This makes Ring ORAM a good candidate in obliv-
ious cloud storage, because as a tree-based ORAM, Ring
ORAM is easier to analyze, implement and de-amortize
than hierarchical ORAMs like SSS [25]. Therefore, Ring
ORAM is essentially a united paradigm for ORAM con-
structions in both large and small client storage settings.

Organization.
In the rest of this introduction, we give
an overview of our techniques to improve ORAM’s on-
line and overall bandwidth. Section 2 gives a formal se-
curity deﬁnition for ORAM. Section 3 explains the Ring
ORAM protocol in detail. Section 4 gives a complete for-
mal analysis for bounding Ring ORAM’s client storage.
Section 5 analyzes Ring ORAM’s bandwidth and gives
a methodology for setting parameters optimally. Section
6 compares Ring ORAM to prior work in terms of band-
width vs. client storage and performance in a secure pro-
cessor setting. Section 7 describes how to extend Ring
ORAM to the large client storage setting. Section 8 gives
related work and Section 9 concludes.

1.3 Overview of Techniques
We now explain our key technical insights. At a high
level, our scheme also follows the tree-based ORAM
paradigm [23]. Server storage is a binary tree where each
node (a bucket) contains up to Z blocks and blocks per-
colate down the tree during ORAM evictions. We intro-
duce the following non-trivial techniques that allow us
to achieve signiﬁcant savings in both online and overall
bandwidth costs.

Eliminating online bandwidth’s dependence on
bucket size.
In Path ORAM, reading a block would
amount to reading and writing all Z slots in all buckets on
a path. Our ﬁrst goal is to read only one block from each
bucket on the path. To do this, we randomly permute
each bucket and store the permutation in each bucket as
additional metadata. Then, by reading only metadata,
the client can determine whether the requested block is
in the present bucket or not. If so, the client relies on
the stored permutation to read the block of interest from
its random offset. Otherwise, the client reads a “fresh”
(unread) dummy block, also from a random offset. We
stress that the metadata size is typically much smaller
than the block size, so the cost of reading metadata can
be ignored.

For the above approach to be secure, it is impera-
tive that each block in a bucket should be read at most
once—a key idea also adopted by Goldreich and Ostro-
vsky in their early ORAM constructions [11]. Notice that
any real block is naturally read only once, since once a
real block is read, it will be invalidated from the present
bucket, and relocated somewhere else in the ORAM tree.
But dummy blocks in a bucket can be exhausted if the
bucket is read many times. When this happens (which
is public information), Ring ORAM introduces an early
reshufﬂe procedure to reshufﬂe the buckets that have
been read too many times. Speciﬁcally, suppose that
each bucket is guaranteed to have S dummy blocks, then
a bucket must be reshufﬂed every S times it is read.

We note that the above technique also gives an addi-
tional nice property: out of the O(logN) blocks the client
reads, only 1 of them is a real block (i.e., the block of
interest); all the others are dummy blocks. If we allow
some simple computation on the memory side, we can
immediately apply the XOR trick from Burst ORAM [3]
to get O(1) online bandwidth.
In the XOR trick, the
server simply XORs these encrypted blocks and sends a
single, XOR’ed block to the client. The client can recon-
struct the ciphertext of all the dummy blocks, and XOR
them away to get back the encrypted real block.

Eliminating overall bandwidth’s dependence on
bucket size. Unfortunately, na¨ıvely applying the above
strategy will dramatically increase ofﬂine and overall
bandwidth. The more dummy slots we reserve in each
bucket (i.e., a large S), the more expensive ORAM evic-
tions become, since they have to read and write all the
blocks in a bucket. But if we reserve too few dummy
slots, we will frequently run out of dummy blocks and
have to call early reshufﬂe, also increasing overall band-
width.

We solve the above problem with several additional
techniques. First, we design a new eviction procedure
that improves eviction quality. At a high level, Ring

USENIX Association  

24th USENIX Security Symposium  417

3

ORAM performs evictions on a path in a similar fashion
as Path ORAM, but eviction paths are selected based on
a reverse lexicographical order [9], which evenly spreads
eviction paths over the entire tree. The improved eviction
quality allows us to perform evictions less frequently,
only once every A ORAM accesses, where A is a new
parameter. We then develop a proof that crucially shows
A can approach 2Z while still ensuring negligible ORAM
failure probability. The proof may be of independent
interest as it uses novel proof techniques and is signiﬁ-
cantly simpler than Path ORAM’s proof. The amortized
ofﬂine bandwidth is now roughly 2Z
A logN, which does
not depend on the bucket size Z either.

Second, bucket reshufﬂes can naturally piggyback on
ORAM evictions. The balanced eviction order further
ensures that every bucket will be reshufﬂed regularly.
Therefore, we can set the reserved dummy slots S in ac-
cordance with the eviction frequency A, such that early
reshufﬂes contribute little (< 3%) to the overall band-
width.

Putting it all Together. None of the aforementioned
ideas would work alone. Our ﬁnal product, Ring ORAM,
stems from intricately combining these ideas in a non-
trivial manner.
For example, observe how our two
main techniques act like two sides of a lever: (1) per-
muted buckets such that only 1 block is read per bucket;
and (2) high quality and hence less frequent evictions.
While permuted buckets make reads cheaper, they re-
quire adding dummy slots and would dramatically in-
crease eviction overhead without the second technique.
At the same time, less frequent evictions require increas-
ing bucket size Z; without permuted buckets, ORAM
reads blow up and nullify any saving on evictions. Addi-
tional techniques are needed to complete the construc-
tion. For example, early reshufﬂes keep the number
of dummy slots small; piggyback reshufﬂes and load-
balancing evictions keep the early reshufﬂe rate low.
Without all of the above techniques, one can hardly get
any improvement.

2 Security Deﬁnition

We adopt the standard ORAM security deﬁnition.
In-
formally, the server should not learn anything about: 1)
which data the client is accessing; 2) how old it is (when
it was last accessed); 3) whether the same data is be-
ing accessed (linkability); 4) access pattern (sequential,
random, etc); or 5) whether the access is a read or a
write. Like previous work, we do not consider informa-
tion leakage through the timing channel, such as when or
how frequently the client makes data requests.

Notation

N
L
Z
S
B
A
P(l)
P(l,i)
P(l,i, j)

Meaning

Number of real data blocks in ORAM

Depth of the ORAM tree

Maximum number of real blocks per bucket

Number of slots reserved for dummies per bucket

Data block size (in bits)

Eviction rate (larger means less frequent)

Path l

The i-th bucket (towards the root) on P(l)

The j-th slot in bucket P(l,i)

Table 2: ORAM parameters and notations.

Deﬁnition 1. (ORAM Deﬁnition) Let

←−y = ((opM, addrM, dataM), . . . ,(op 1, addr1, data1))

denote a data sequence of length M, where opi denotes
whether the i-th operation is a read or a write, addri de-
notes the address for that access and datai denotes the
data (if a write). Let ORAM(←−y ) be the resulting se-
quence of operations between the client and server under
an ORAM algorithm. The ORAM protocol guarantees
that for any ←−y and ←−y (cid:29), ORAM(←−y ) and ORAM(←−y (cid:29))
are computationally indistinguishable if |←−y | = |←−y (cid:29)|, and
also that for any ←−y the data returned to the client by
ORAM is consistent with ←−y (i.e., the ORAM behaves like
a valid RAM) with overwhelming probability.

We remark that for the server to perform computations
on data blocks [3], ORAM(←−y ) and ORAM(←−y (cid:29)) include
those operations. To satisfy the above security deﬁnition,
it is implied that these operations also cannot leak any
information about the access pattern.

3 Ring ORAM Protocol

3.1 Overview
We ﬁrst describe Ring ORAM in terms of its server and
client data structures. All notation used throughout the
rest of the paper is summarized in Table 2.

Server storage
is organized as a binary tree of buckets
where each bucket has a small number of slots to hold
blocks. Levels in the tree are numbered from 0 (the root)
to L (inclusive, the leaves) where L = O(logN) and N is
the number of blocks in the ORAM. Each bucket has Z +
S slots and a small amount of metadata. Of these slots,
up to Z slots may contain real blocks and the remaining
S slots are reserved for dummy blocks as described in
Section 1.3. Our theoretical analysis in Section 4 will
show that to store N blocks in Ring ORAM, the physical
ORAM tree needs roughly 6N to 8N slots. Experiments

418  24th USENIX Security Symposium 

USENIX Association

4

Algorithm 1 Non-recursive Ring ORAM.
1: function ACCESS(a, op, data(cid:29))
2:

Global/persistent variables: round
l(cid:29) ← UniformRandom(0,2L − 1)
l ← PositionMap[a]
PositionMap[a] ← l(cid:29)
data ← ReadPath(l,a)
if data = ⊥ then
(cid:31) If block a is not found on path l, it must
be in Stash (cid:30)
data ← read and remove a from Stash
return data to client

if op = read then

if op = write then
data ← data(cid:29)

Stash ← Stash∪ (a,l(cid:29), data)
round ← round + 1 mod A
if round

?
= 0 then

EvictPath()

EarlyReshuﬄe(l)

3:
4:
5:

6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

16:
17:
18:

19:

show that server storage in practice for both Ring ORAM
and Path ORAM can be 2N or even smaller.

Client storage
is made up of a position map and a
stash. The position map is a dictionary that maps each
block in the ORAM to a random leaf in the ORAM tree
(each leaf is given a unique identiﬁer). The stash buffers
blocks that have not been evicted to the ORAM tree and
additionally stores Z(L + 1) blocks on the eviction path
during an eviction operation. We will prove in Section 4
that stash overﬂow probability decreases exponentially
as stash capacity increases, which means our required
stash size is the same as Path ORAM. The position map
stores N ∗ L bits, but can be squashed to constant storage
using the standard recursion technique (Section 3.7).

Main invariants. Ring ORAM has two main invari-
ants:

1. (Same as Path ORAM): Every block is mapped to a
leaf chosen uniformly at random in the ORAM tree.
If a block a is mapped to leaf l, block a is contained
either in the stash or in some bucket along the path
from the root of the tree to leaf l.

2. (Permuted buckets) For every bucket in the tree,
the physical positions of the Z + S dummy and real
blocks in each bucket are randomly permuted with
respect to all past and future writes to that bucket.

Since a leaf uniquely determines a path in a binary tree,
we will use leaves/paths interchangeably when the con-
text is clear, and denote path l as P(l).
Access and Eviction Operations. The Ring ORAM
access protocol is shown in Algorithm 1. Each access
is broken into the following four steps:

1.) Position Map lookup (Lines 3-5): Look up the po-
sition map to learn which path l the block being accessed
is currently mapped to. Remap that block to a new ran-
dom path l(cid:29).
This ﬁrst

tree-based
ORAMs [23, 27]. But the rest of the protocol differs
substantially from previous tree-based schemes, and we
highlight our key innovations in bold.

to other

step is

identical

2.) Read Path (Lines 6-15): The ReadPath(l,a) oper-
ation reads all buckets along P(l) to look for the block
of interest (block a), and then reads that block into the
stash. The block of interest is then updated in stash on a
write, or is returned to the client on a read. We remind
readers again that both reading and writing a data block
are served by a ReadPath operation.

Unlike prior tree-based schemes, our ReadPath op-
eration only reads one block from each bucket—the

block of interest if found or a previously-unread
dummy block otherwise. This is safe because of In-
variant 2, above: each bucket is permuted randomly, so
the slot being read looks random to an observer. This
lowers the bandwidth overhead of ReadPath (i.e., online
bandwidth) to L + 1 blocks (the number of levels in the
tree) or even a single block if the XOR trick is applied
(Section 3.2).

3.) Evict Path (Line 16-18): The EvictPath operation
reads Z blocks (all the remaining real blocks, and po-
tentially some dummy blocks) from each bucket along a
path into the stash, and then ﬁlls that path with blocks
from the stash, trying to push blocks as far down towards
the leaves as possible. The sole purpose of an eviction
operation is to push blocks back to the ORAM tree to
keep the stash occupancy low.

Unlike Path ORAM, eviction in Ring ORAM selects
paths in the reverse lexicographical order, and does
not happen on every access. Its rate is controlled by
a public parameter A: every A ReadPath operations
trigger a single EvictPath operation. This means Ring
ORAM needs much fewer eviction operations than Path
ORAM. We will theoretically derive a tight relationship
between A and Z in Section 4.

USENIX Association  

24th USENIX Security Symposium  419

5

4.) Early Reshufﬂes
(Line 19): Finally, we perform
a maintenance task called EarlyReshuﬄe on P(l), the
path accessed by ReadPath. This step is crucial in
maintaining blocks randomly shufﬂed in each bucket,
which enables ReadPath to securely read only one block
from each bucket.

We will present details of ReadPath, EvictPath and
EarlyReshuﬄe in the next three subsections. We de-
fer low-level details for helper functions needed in these
three subroutines to Appendix A. We explain the security
for each subroutine in Section 3.5. Finally, we discuss
additional optimizations in Section 3.6 and recursion in
Section 3.7.

3.2 Read Path Operation

data ← ⊥
for i ← 0 to L do

Algorithm 2 ReadPath procedure.
1: function ReadPath(l,a)
2:
3:
4:
5:
6:
7:
8:
9:

oﬀset ← GetBlockOﬀset(P(l,i),a)
data(cid:28) ← P(l,i, oﬀset)
Invalidate P(l,i, oﬀset)
if data(cid:28) (cid:27)= ⊥ then
data ← data(cid:28)
P(l,i).count ← P(l,i).count + 1

return data

The ReadPath operation is shown in Algorithm 2. For
each bucket along the current path, ReadPath selects a
single block to read from that bucket. For a given bucket,
if the block of interest lives in that bucket, we read and
invalidate the block of interest. Otherwise, we read and
invalidate a randomly-chosen dummy block that is still
valid at that point. The index of the block to read (either
real or random) is returned by the GetBlockOﬀset func-
tion whose detailed description is given in Appendix A.
Reading a single block per bucket is crucial for our
bandwidth improvements. In addition to reducing online
bandwidth by a factor of Z, it allows us to use larger Z
and A to decrease overall bandwidth (Section 5). Without
this, read bandwidth is proportional to Z, and the cost of
larger Z on reads outweighs the beneﬁts.

Bucket Metadata. Because the position map only
tracks the path containing the block of interest, the client
does not know where in each bucket to look for the block
of interest. Thus, for each bucket we must store the
permutation in the bucket metadata that maps each real
block in the bucket to one of the Z + S slots (Lines 4,
GetBlockOﬀset) as well as some additional metadata.
Once we know the offset into the bucket, Line 5 reads

the block in the slot, and invalidates it. We describe all
metadata in Appendix A, but make the important point
that the metadata is small and independent of the block
size.

One important piece of metadata to mention now is a
counter which tracks how many times it has been read
since its last eviction (Line 9). If a bucket is read too
many (S) times, it may run out of dummy blocks (i.e.,
all the dummy blocks have been invalidated). On fu-
ture accesses, if additional dummy blocks are requested
from this bucket, we cannot re-read a previously inval-
idated dummy block: doing so reveals to the adversary
that the block of interest is not in this bucket. Therefore,
we need to reshufﬂe single buckets on-demand as soon as
they are touched more than S times using EarlyReshuﬄe
(Section 3.4).

XOR Technique. We further make the following key
observation: during our ReadPath operation, each block
returned to the client is a dummy block except for
the block of interest. This means our scheme can
also take advantage of the XOR technique introduced
in [3] to reduce online bandwidth overhead to O(1).
To be more concrete, on each access ReadPath re-
turns L + 1 blocks in ciphertext, one from each bucket,
Enc(b0,r0), Enc(b2,r2),··· , Enc(bL,rL). Enc is a ran-
domized symmetric scheme such as AES counter mode
with nonce ri. With the XOR technique, ReadPath
will return a single ciphertext — the ciphertext of
all the blocks XORed together, namely Enc(b0,r0) ⊕
Enc(b2,r2) ⊕ ··· ⊕ Enc(bL,rL). The client can recover
the encrypted block of interest by XORing the returned
ciphertext with the encryptions of all the dummy blocks.
To make computing each dummy block’s encryption
easy, the client can set the plaintext of all dummy blocks
to a ﬁxed value of its choosing (e.g., 0).

3.3 Evict Path Operation

Algorithm 3 EvictPath procedure.
1: function EvictPath
2:
3:
4:
5:
6:

Global/persistent variables G initialized to 0
l ← G mod 2L
G ← G + 1
for i ← 0 to L do
for i ← L to 0 do

Stash ← Stash∪ ReadBucket(P(l,i))
WriteBucket(P(l,i), Stash)
P(l,i).count ← 0

7:
8:
9:

The EvictPath routine is shown in Algorithm 3. As
mentioned, evictions are scheduled statically: one evic-

6

420  24th USENIX Security Symposium 

USENIX Association

G = 0

G = 1

G = 2

G = 3

Time

Figure 2: Reverse-lexicographic order of paths used by
EvictPath. After path G = 3 is evicted to, the order re-
peats.

tion operation happens after every A reads. At a high
level, an eviction operation reads all remaining real
blocks on a path (in a secure fashion), and tries to push
them down that path as far as possible. The leaf-to-root
order in the writeback step (Lines 7) reﬂects that we wish
to ﬁll the deepest buckets as fully as possible. (For read-
ers who are familiar with Path ORAM, EvictPath is like
a Path ORAM access where no block is accessed and
therefore no block is remapped to a new leaf.)

We emphasize two unique features of Ring ORAM
eviction operations. First, evictions in Ring ORAM are
performed to paths in a speciﬁc order called the reverse-
lexicographic order, ﬁrst proposed by Gentry et al. [9]
and shown in Figure 2. The reverse-lexicographic order
eviction aims to minimize the overlap between consecu-
tive eviction paths, because (intuitively) evictions to the
same bucket in consecutive accesses are less useful. This
improves eviction quality and allows us to reduce the fre-
quency of eviction. Evicting using this static order is also
a key component in simplifying our theoretical analysis
in Section 4.

Second, buckets in Ring ORAM need to be randomly
shufﬂed (Invariant 2), and we mostly rely on EvictPath
operations to keep them shufﬂed. An EvictPath oper-
ation reads Z blocks from each bucket on a path into
the stash, and writes out Z + S blocks (only up to Z
are real blocks) to each bucket, randomly permuted.
The details of reading/writing buckets (ReadBucket and
WriteBucket) are deferred to Appendix A.

3.4 Early Reshufﬂe Operation

Algorithm 4 EarlyReshuﬄe procedure.
1: function EarlyReshuﬄe(l)
2:
3:
4:
5:
6:

if P(l,i).count ≥ S then

for i ← 0 to L do

Stash ← Stash∪ ReadBucket(P(l,i))
WriteBucket(P(l,i), Stash)
P(l,i).count ← 0

Due to randomness, a bucket can be touched > S
times by ReadPath operations before it is reshufﬂed

7

If this happens, we call
by the scheduled EvictPath.
EarlyReshuﬄe on that bucket to reshufﬂe it before the
bucket is read again (see Section 3.2). More precisely,
after each ORAM access EarlyReshuﬄe goes over all
the buckets on the read path, and reshufﬂes all the buck-
ets that have been accessed more than S times by per-
forming ReadBucket and WriteBucket. ReadBucket
and WriteBucket are the same as in EvictPath:
that
is, ReadBucket reads exactly Z slots in the bucket
and WriteBucket re-permutes and writes back Z + S
real/dummy blocks. We note that though S does not af-
fect security (Section 3.5), it clearly has an impact on
performance (how often we shufﬂe, the extra cost per
reshufﬂe, etc.). We discuss how to optimally select S in
Section 5.

3.5 Security Analysis
Claim 1. ReadPath leaks no information.

The path selected for reading will look random to
any adversary due to Invariant 1 (leaves are chosen
uniformly at random). From Invariant 2, we know that
every bucket is randomly shufﬂed. Moreover, because
we invalidate any block we read, we will never read the
same slot. Thus, any sequence of reads (real or dummy)
to a bucket between two shufﬂes is indistinguishable.
Thus the adversary learns nothing during ReadPath. (cid:31)

Claim 2. EvictPath leaks no information.

The path selected for eviction is chosen statically,
and is public (reverse-lexicographic order). ReadBucket
always reads exactly Z blocks from random slots.
WriteBucket similarly writes Z + S encrypted blocks in
a data-independent fashion. (cid:31)

Claim 3. EarlyShuﬄe leaks no information.

To which buckets EarlyShuﬄe operations occur is
publicly known: the adversary knows how many times a
bucket has been accessed since the last EvictPath to that
bucket. ReadBucket and WriteBucket are secure as per
observations in Claim 2. (cid:31)

The three subroutines of the Ring ORAM algorithm
are the only operations that cause externally observable
behaviors. Claims 1, 2, and 3 show that the subroutines
are secure. We have so far assumed that path remap-
ping and bucket permutation are truly random, which
gives unconditional security. If pseudorandom numbers
are used instead, we have computational security through
similar arguments.

USENIX Association  

24th USENIX Security Symposium  421

3.6 Other Optimizations
Minimizing roundtrips. To keep the presentation sim-
ple, we wrote the ReadPath (EvictPath) algorithms to
process buckets one by one.
In fact, they can be per-
formed for all buckets on the path in parallel which re-
duces the number of roundtrips to 2 (one for metadata
and one for data blocks).

Tree-top caching. The idea of tree-top caching [18] is
simple: we can reduce the bandwidth for ReadPath and
EvictPath by storing the top t (a new parameter) levels of
the Ring ORAM tree at the client as an extension of the
stash1. For a given t, the stash grows by approximately
2tZ blocks.

De-amortization. We can de-amortize the expensive
EvictPath operation through a period of A accesses, sim-
ply by reading/writing a small number of blocks on the
eviction path after each access. After de-amortization,
worst-case overall bandwidth equals average overall
bandwidth.

3.7 Recursive Construction
With the construction given thus far, the client needs to
store a large position map. To achieve small client stor-
age, we follow the standard recursion idea in tree-based
ORAMs [23]: instead of storing the position map on the
client, we store the position map on a smaller ORAM
on the server, and store only the position map for the
smaller ORAM. The client can recurse until the ﬁnal
position map becomes small enough to ﬁt in its stor-
age. For reasonably block sizes (e.g., 4 KB), recursion
contributes very little to overall bandwidth (e.g., < 5%
for a 1 TB ORAM) because the position map ORAMs
use much smaller blocks [26]. Since recursion for Ring
ORAM behaves in the same way as all the other tree-
based ORAMs, we omit the details.

4 Stash Analysis

In this section we analyze the stash occupancy for a non-
recursive Ring ORAM. Following the notations in Path
ORAM [27], by ORAMZ,A
L we denote a non-recursive
Ring ORAM with L + 1 levels, bucket size Z and one
eviction per A accesses. The root is at level 0 and the
leaves are at level L. We deﬁne the stash occupancy
st (SZ) to be the number of real blocks in the stash after a
sequence of ORAM sequences (this notation will be fur-
ther explained later). We will prove that Pr [st (SZ) > R]
1We call this optimization tree-top caching following prior work.
But the word cache is a misnomer: the top t levels of the tree are per-
manently stored by the client.

decreases exponentially in R for certain Z and A combi-
nations. As it turns out, the deterministic eviction pattern
in Ring ORAM dramatically simpliﬁes the proof.

We note here that the reshufﬂing of a bucket does not
affect the occupancy of the bucket, and is thus irrelevant
to the proof we present here.

4.1 Proof outline
The proof consists of the two steps. The ﬁrst step is the
same as Path ORAM, and needs Lemma 1 and Lemma 2
in the Path ORAM paper [27], which we restate in Sec-
tion 4.2. We introduce ∞-ORAM, which has an inﬁnite
bucket size and after a post-processing step has exactly
the same distribution of blocks over all buckets and the
stash (Lemma 1). Lemma 2 says the stash occupancy
of ∞-ORAM after post-processing is greater than R if
and only if there exists a subtree T in ∞-ORAM whose
“occupancy” exceeds its “capacity” by more than R. We
note, however, that the Path ORAM [27] paper only gave
intuition for the proof of Lemma 1, and unfortunately
did not capture of all the subtleties. We will rigorously
prove that lemma, which turns out to be quite tricky and
requires signiﬁcant changes to the post-processing algo-
rithm.

The second step (Section 4.3) is much simpler than
the rest of Path ORAM’s proof, thanks to Ring ORAM’s
static eviction pattern. We simply need to calculate the
expected occupancy of subtrees in ∞-ORAM, and apply
a Chernoff-like bound on their actual occupancy to com-
plete the proof. We do not need the complicated eviction
game, negative association, stochastic dominance, etc.,
as in the Path ORAM proof [26].

For readability, we will defer the proofs of all lemmas

to Appendix B.

L

L

L

as bZ

4.2 ∞-ORAM
We ﬁrst introduce ∞-ORAM, denoted as ORAM∞,A
. Its
buckets have inﬁnite capacity. It receives the same input
request sequence as ORAMZ,A
. We then label buckets
linearly such that the two children of bucket bi are b2i and
b2i+1, with the root bucket being b1. We deﬁne the stash
to be b0. We refer to bi of ORAM∞,A
i , and bi of
ORAMZ,A
i . We further deﬁne ORAM state, which
consists of the states of all the buckets in the ORAM, i.e.,
the blocks contained by each bucket. Let S∞ be the state
of ORAM∞,A
We now propose a new greedy post-processing algo-
rithm G (different from the one in [27]), which by re-
assigning blocks in buckets makes each bucket b∞
in ∞-
i
ORAM contain the same set of blocks as bZ
i . Formally, G
takes as input S∞ and SZ after the same access sequence
with the same randomness. For i from 2L+1 − 1 down to

and SZ be the state of ORAMZ,A

as b∞

.

L

L

L

422  24th USENIX Security Symposium 

USENIX Association

8

1 (note that the decreasing order ensures that a parent is
always processed later than its children), G processes the
blocks in bucket b∞
i

in the following way:
1. For those blocks that are also in bZ

i , keep them in

b∞
i .

2. For those blocks that are not in bZ
i , move them from b∞
i

i but in some an-
to b∞
cestors of bZ
i/2 (the parent
of b∞
i , and note that the division includes ﬂooring).
If such blocks exist and the number of blocks re-
maining in b∞
i

is less than Z, raise an error.

3. If there exists a block in b∞
i

that is in neither bZ

i nor

any ancestor of bZ

i , raise an error.

i after G contains the same set of blocks as bZ

We say GSZ (S∞) =S Z, if no error occurs during G
and b∞
i for
i = 0,1,···2 L+1.
Lemma 1. GSZ (S∞) = SZ after the same ORAM access
sequence with the same randomness.

L

L

L

. This means that if a node in ORAM∞,A

Next, we investigate what state S∞ will lead to the
stash occupancy of more than R blocks in a post-
processed ∞-ORAM. We say a subtree T is a rooted sub-
tree, denoted as T ∈ ORAM∞,A
if T contains the root of
ORAM∞,A
is
in T , then so are all its ancestors. We deﬁne n(T ) to
be the total number of nodes in T . We deﬁne c(T ) (the
capacity of T ) to be the maximum number of blocks T
can hold; for Ring ORAM c(T ) =n(T )· Z. Lastly, we
deﬁne X(T ) (the occupancy of T ) to be the actual num-
ber of real blocks that are stored in T . The following
lemma characterizes the stash size of a post-processed
∞-ORAM:
Lemma 2. st (GSZ (S∞)) > R if and only if ∃T ∈
ORAM∞,A
s.t. X(T ) > c(T ) +R before post-processing.

L

By Lemma 1 and Lemma 2, we have

Pr [X(T ) > c(T ) +R]

Pr [st (SZ) > R] = Pr [st (GSZ (S∞)) > R]
≤ ∑
∞,A
T∈ORAM
L
< ∑
n≥1

Pr [X(T ) > c(T ) +R]

4n max

T :n(T )=n

Let X(T ) = ∑i Xi(T ), where each Xi(T ) ∈ {0,1} and
indicates whether the i-th block (can be either real or
stale) is in T . Let pi = Pr [Xi(T ) =1]. Xi(T ) is com-
pletely determined by its time stamp i and the leaf label
assigned to block i, so they are independent from each
other (refer to the proof of Lemma 3). Thus, we can
apply a Chernoff-like bound to get an exponentially de-
creasing bound on the tail distribution. To do so, we ﬁrst

establish a bound on E(cid:31)etX(T )(cid:30) where t > 0,
E(cid:31)etX(T )(cid:30) = E(cid:31)et ∑i Xi(T )(cid:30) = E(cid:31)ΠietXi(T )(cid:30)

(by independence)

= ΠiE(cid:31)etXi(T )(cid:30)
= Πi(cid:29)pi(et − 1) +1(cid:28)
≤ Πi(cid:27)epi(et−1)(cid:26) = e(et−1)Σi pi

= e(et−1)E[X(T )]

(2)

For simplicity, we write n = n(T ) and a = A/2. By
Lemma 3, E[X(T )] ≤ n · a. By the Markov Inequality,
we have for all t > 0,

Pr [X(T ) > c(T ) +R] = Pr(cid:31)etX(T ) > et(nZ+R)(cid:30)
≤ E(cid:31)etX(T )(cid:30)· e−t(nZ+R)
≤ e(et−1)an · e−t(nZ+R)
= e−tR · e−n[tZ−a(et−1)]

Let t = ln(Z/a),
Pr [X(T ) > c(T ) +R] ≤ (a/Z)R · e−n[Z ln(Z/a)+a−Z]
Now we will choose Z and A such that Z > a and q =
Z ln(Z/a)+a−Z−ln4 > 0. If these two conditions hold,
from Equation (1) we have t = ln(Z/a) > 0 and that the
stash overﬂow probability decreases exponentially in the
stash size R:

(3)

(1)

Pr [st (SZ) > R] ≤ ∑
n≥1

(a/Z)R · e−qn <

(a/Z)R
1− e−q .

The above inequalities used a union bound and a bound
on Catalan sequences.

4.3 Bounding the Stash Size
We ﬁrst give a bound on the expected bucket load:
Lemma 3. For any rooted subtree T in ORAM∞,A
, if the
number of distinct blocks in the ORAM N ≤ A· 2L−1, the
expected load of T has the following upper bound:

L

∀T ∈ ORAM∞,A

L

,E[X(T )] ≤ n(T )· A/2.

4.4 Stash Size in Practice
Now that we have established that Z ln(2Z/A) +A/2 −
Z − ln4 > 0 ensures an exponentially decreasing stash
overﬂow probability, we would like to know how tight
this requirement is and what the stash size should be in
practice.

We simulate Ring ORAM with L = 20 for over 1 Bil-
lion accesses in a random access pattern, and measure
the stash occupancy (excluding the transient storage of a
path). For several Z values, we look for the maximum A
that results in an exponentially decreasing stash overﬂow

USENIX Association  

24th USENIX Security Symposium  423

9

1000

900

800

700

600

A

500

400

300

200

100

0

Analytical
Empirical

Zoomed in:

60

50

40

30

20

10

0

0

100

200

300
Z

0

10

20

30

40

400

500

600

Figure 3: For each Z, determine analytically and em-
pirically the maximum A that results in an exponentially
decreasing stash failure probability.

4,3

32
51
103

λ

80
128
256

16,20

Z,A Parameters
32,46
Max Stash Size
113
155
272

65
93
171

8,8

41
62
120

16,23

197
302
595

Table 3: Maximum stash occupancy for realistic security
parameters (stash overﬂow probability 2−λ ) and several
choices of A and Z. A = 23 is the maximum achievable
A for Z = 16 according to simulation.

probability. In Figure 3, we plot both the empirical curve
based on simulation and the theoretical curve based on
the proof. In all cases, the theoretical curve indicates a
only slightly smaller A than we are able to achieve in
simulation, indicating that our analysis is tight.

To determine required stash size in practice, Table 3
shows the extrapolated required stash size for a stash
overﬂow probability of 2−λ for several realistic λ . We
show Z = 16, A = 23 for completeness: this is an aggres-
sive setting that works for Z = 16 according to simulation
but does not satisfy the theoretical analysis; observe that
this point requires roughly 3× the stash occupancy for a
given λ .

5 Bandwidth Analysis

In this section, we answer an important question: how
do Z (the maximum number of real blocks per bucket),
A (the eviction rate) and S (the number of extra dummies
per bucket) impact Ring ORAM’s performance (band-
width)? By the end of the section, we will have a
theoretically-backed analytic model that, given Z, selects
optimal A and S to minimize bandwidth.

We ﬁrst state an intuitive trade-off: for a given Z, in-
creasing A causes stash occupancy to increase and band-

9

8

7

6

)
𝑁𝑁
𝑁𝑁

2

g
o
l
(
𝑂𝑂

5

i

h
t
d
w
d
n
a
b

 
r
o
f
 
t
n
a
t
s
n
o
C

Z=4, A=3
Z=8, A=8
Z=16, A=20
Z=32, A=46

4

3

0

10

20

S − A

30

40

50

Figure 4: For different Z, and the corresponding optimal
A, vary S and plot bandwidth overhead. We only consider
S ≥ A

width overhead to decrease. Let us ﬁrst ignore early
reshufﬂes and the XOR technique. Then, the overall
bandwidth of Ring ORAM consists of ReadPath and
EvictPath. ReadPath transfers L + 1 blocks, one from
each bucket. EvictPath reads Z blocks per bucket and
writes Z + S blocks per bucket, (2Z + S)(L + 1) blocks
in total, but happens every A accesses. From the re-
quirement of Lemma 3, we have L = log(2N/A), so
the ideal amortized overall bandwidth of Ring ORAM
is (1 + (2Z + S)/A)log(4N/A). Clearly, a larger A im-
proves bandwidth for a given Z as it reduces both evic-
tion frequency and tree depth L. So we simply choose
the largest A that satisﬁes the requirement from the stash
analysis in Section 4.3.

as S increases,

Now we consider the extra overhead from early
reshufﬂes. We have the following trade-off in choos-
ing S:
the early reshufﬂe rate de-
creases (since we have more dummies per bucket) but
the cost to read+write buckets during an EvictPath and
EarlyReshuﬄe increases. This effect is shown in Figure 4
through simulation: for S too small, early shufﬂe rate is
high and bandwidth increases; for S too large, eviction
bandwidth dominates.

To analytically choose a good S, we analyze the early
reshufﬂe rate. First, notice a bucket at level l in the Ring
ORAM tree will be processed by EvictPath exactly once
for every 2lA ReadPath operations, due to the reverse-
lexicographic order of eviction paths (Section 3.3). Sec-
ond, each ReadPath operation is to an independent and
uniformly random path and thus will touch any bucket in
level l with equal probability of 2−l. Thus, the distribu-
tion on the expected number of times ReadPath opera-
tions touch a given bucket in level l, between two consec-
utive EvictPath calls, is given by a binomial distribution
of 2lA trials and success probability 2−l. The probabil-
ity that a bucket needs to be early reshufﬂed before an
EvictPath is given by a binomial distribution cumula-

424  24th USENIX Security Symposium 

USENIX Association

10

Find largest A ≤ 2Z such that
Z ln(2Z/A) +A/2 − Z − ln4 > 0 holds.
Find S ≥ 0 that minimizes
(2Z + S)(1 + Poiss cdf(S,A))
Ring ORAM ofﬂine bandwidth is

(2Z+S)(1+Poiss cdf(S,A))

A

· log(4N/A)

Table 4: Analytic model for choosing parameters, given
Z.

250

200

r
e

i
l

B = 64 Bytes

B = 4 KiloBytes

Path ORAM

 

p
i
t
l
u
m
h
t
d
w
d
n
a
B

i

2X

150

100

50

0

Ring ORAM

2.7X

0

1000

2000

Storage (in KiloBytes)

3000

4000

Figure 6: Bandwidth overhead vs. data block storage for
1 TB ORAM capacities and ORAM failure probability
2−80.

2logN for very large Z.

i

h
t
d
w
d
n
a
b

9
8
7
6
)
5
𝑁𝑁
𝑁𝑁
4
3
2
1
0

g
o
l
(
𝑂𝑂

2

 
r
o
f
 
t
n
a
t
s
n
o
C

Path ORAM (overall)

Path ORAM (online)

Ring ORAM (overall)

Ring ORAM (online)

0

10

20

30
Z

40

50

60

6 Evaluation

Figure 5: Overall bandwidth as a function of Z. Kinks
are present in the graph because we always round A to the
nearest integer. For Path ORAM, we only study Z = 4
since a larger Z strictly hurts bandwidth.

tive density function Binom cdf(S,2lA,2−l).2 Based on
this analysis, the expected number of times any bucket
is involved in ReadPath operations between consecutive
EvictPath operations is A. Thus, we will only consider
S ≥ A as shown in Figure 4 (S < A is clearly bad as it
needs too much early reshufﬂing).
We remark that the binomial distribution quickly con-
verges to a Poisson distribution. So the amortized overall
bandwidth, taking early reshufﬂes into account, can be
accurately approximated as (L +1) + (L +1)(2Z +S)/A·
(1 + Poiss cdf(S,A)). We should then choose the S that
minimizes the above formula. This method always ﬁnds
the optimal S and perfectly matches the overall band-
width in our simulation in Figure 4.

We recap how to choose A and S for a given Z in Ta-
ble 4. For the rest of the paper, we will choose A and S
this way unless otherwise stated. Using this method to
set A and S, we show online and overall bandwidth as a
function of Z in Figure 5. In the ﬁgure, Ring ORAM does
not use the XOR technique on reads. For Z = 50, we
achieve ∼ 3.5logN bandwidth; for very large Z, band-
width approaches 3logN. Applying the XOR technique,
online bandwidth overhead drops to close to 1 which re-
duces overall bandwidth to ∼ 2.5logN for Z = 50 and
2The possibility that a bucket needs to be early reshufﬂed twice

before an eviction is negligible.

6.1 Bandwidth vs. Client Storage

To give a holistic comparison between schemes, Figure 6
shows the best achievable bandwidth, for different client
storage budgets, for Path ORAM and Ring ORAM. For
each scheme in the ﬁgure, we apply all known optimiza-
tions and tune parameters to minimize overall bandwidth
given a storage budget. For Path ORAM we choose Z = 4
(increasing Z strictly hurts bandwidth) and tree-top cache
to ﬁll remaining space. For Ring ORAM we adjust Z, A
and S, tree-top cache and apply the XOR technique.

To simplify the presentation, “client storage” includes
all ORAM data structures except for the position map
– which has the same space/bandwidth cost for both
Path ORAM and Ring ORAM. We remark that applying
the recursion technique (Section 3.7) to get a small on-
chip position map is cheap for reasonably large blocks.
For example, recursing the on-chip position map down
to 256 KiloBytes of space when the data block size
is 4 KiloBytes increases overall bandwidth for Ring
ORAM and Path ORAM by < 3%.

The high order bit is that across different block sizes
and client storage budgets, Ring ORAM consistently re-
duces overall bandwidth relative to Path ORAM by 2-
2.7×. We give a summary of these results for several rep-
resentative client storage budgets in Table 5. We remark
that for smaller block sizes, Ring ORAM’s improvement
over Path ORAM (∼ 2× for 64 Byte blocks) is smaller
relative to when we use larger blocks (2.7× for 4 Kilo-
Byte blocks). The reason is that with small blocks, the
cost to read bucket metadata cannot be ignored, forcing
Ring ORAM to use smaller Z.

USENIX Association  

24th USENIX Security Symposium  425

11

Z, A (Ring ORAM only) Ring ORAM Ring ORAM (XOR)

Online, Overall Bandwidth overhead

Block Size (Bytes)

64
4096

10,11
33,48

48×, 144×
20×, 82×

24×, 118×
∼ 1×, 60×

Path ORAM
120×, 240×
80×, 160×

Table 5: Breakdown between online and ofﬂine bandwidth given a client storage budget of 1000× the block size for several
representative points (Section 6.1). Overheads are relative to an insecure system. Parameter meaning is given in Table 2.

7 Ring ORAM with Large Client Storage

If given a large client storage budget, we can ﬁrst choose
very large A and Z for Ring ORAM, which means band-
width approaches 2logN (Section 5).3 Then remaining
client storage can be used to tree-top cache (Section 3.6).
For example, tree-top caching t = L/2 levels requires
O(√N) storage and bandwidth drops by a factor of 2
to 1 · logN—which roughly matches the SSS construc-
tion [25].
Burst ORAM [3] extends the SSS construction to han-
dle millions of accesses in a short period, followed by
a relatively long idle time where there are few requests.
The idea to adapt Ring ORAM to handle bursts is to de-
lay multiple (potentially millions of) EvictPath opera-
tions until after the burst of requests. Unfortunately, this
strategy means we will experience a much higher early
reshufﬂe rate in levels towards the root. The solution
is to coordinate tree-top caching with delayed evictions:
For a given tree-top size t, we allow at most 2t delayed
EvictPath operations. This ensures that for levels ≥ t,
the early reshufﬂe rate matches our analysis in Section 5.
We experimentally compared this methodology to the
dataset used by Burst ORAM and veriﬁed that it gives
comparable performance to that work.

8 Related Work

ORAM was ﬁrst proposed by Goldreich and Ostro-
vsky [10, 11]. Since then, there have been numerous
follow-up works that signiﬁcantly improved ORAM’s ef-
ﬁciency in the past three decades [21, 20, 2, 1, 29, 12,
13, 15, 25, 23, 9, 27, 28]. We have already reviewed two
state-of-the-art schemes with different client storage re-
quirements: Path ORAM [27] and the SSS ORAM [25].
Circuit ORAM [28] is another recent tree-based ORAM,
which requires only O(1) client storage, but its band-
width is a constant factor worse than Path ORAM.

Reducing online bandwidth.

Two recent works
[3, 19] have made efforts to reduce online bandwidth
(response time). Unfortunately, the techniques in Burst
ORAM [3] do not work with Path ORAM (or more
generally any existing tree-based ORAMs). On the

3We assume the XOR technique because large client storage implies

a ﬁle server setting.

Figure 7: SPEC benchmark slowdown.

6.2 Case Study: Secure Processors
In this study, we show how Ring ORAM improves the
performance of secure processors over Path ORAM. We
assume the same processor/cache architecture as [5],
given in Table 4 of that work. We evaluate a 4 GigaByte
ORAM with 64-Byte block size (matching a typical pro-
cessor’s cache line size). Due to the small block size,
we parameterize Ring ORAM at Z = 5, A = 5, X = 2
to reduce metadata overhead. We use the optimized
ORAM recursion techniques [22]: we apply recursion
three times with 32-Byte position map block size and get
a 256 KB ﬁnal position map. We evaluate performance
for SPEC-int benchmarks and two database benchmarks,
and simulate 3 billion instructions for each benchmark.
We assume a ﬂat 50-cycle DRAM latency, and com-
pute ORAM latency assuming 128 bits/cycle processor-
memory bandwidth. We do not use tree-top caching
since it proportionally beneﬁts both Ring ORAM and
Path ORAM. Today’s DRAM DIMMs cannot perform
any computation, but it is not hard to imagine having
simple XOR logic either inside memory, or connected to
O(logN) parallel DIMMs so as not to occupy processor-
memory bandwidth. Thus, we show results with and
without the XOR technique.

Figure 7 shows program slowdown over an insecure
DRAM. The high order bit is that using Ring ORAM
with XOR results in a geometric average slowdown of
2.8× relative to an insecure system. This is a 1.5× im-
provement over Path ORAM. If XOR is not available, the
slowdown over an insecure system is 3.2×.
We have also repeated the experiment with the uniﬁed
ORAM recursion technique and its parameters [5]. The
geometric average slowdown over an insecure system is
2.4× (2.5× without XOR).

426  24th USENIX Security Symposium 

USENIX Association

12

other hand, Path-PIR [19], while featuring a tree-based
ORAM, employs heavy primitives like Private Informa-
tion Retrieval (PIR) or even FHE, and thus requires a
signiﬁcant amount of server computation. In compari-
son, our techniques efﬁciently achieve O(1) online cost
for tree-based ORAMs without resorting to PIR/FHE,
and also improve bursty workload performance similar
to Burst ORAM.

Subsequent work. Techniques proposed in this paper
have been adopted by subsequent works. For example,
Tiny ORAM [6] and Onion ORAM [4] used part of our
eviction strategy in their design for different purposes.

9 Conclusion

This paper proposes Ring ORAM, the most bandwidth-
efﬁcient ORAM scheme for the small (constant or poly-
log) client storage setting. Ring ORAM is simple, ﬂexi-
ble and backed by a tight theoretic analysis.

Ring ORAM is the ﬁrst tree-based ORAM whose
online and overall bandwidth are independent of tree
ORAM bucket size. With this and additional proper-
ties of the algorithm, we show that Ring ORAM im-
proves online bandwidth by 60× (if simple computa-
tion such as XOR is available at memory), and overall
bandwidth by 2.3× to 4× relative to Path ORAM. In a
secure processor case study, we show that Ring ORAM’s
bandwidth improvement translates to an overall program
performance improvement of 1.5×. By increasing Ring
ORAM’s client storage, Ring ORAM is competitive in
the cloud storage setting as well.

Acknowledgement

This research was partially by NSF grant CNS-
1413996 and CNS-1314857, the QCRI-CSAIL partner-
ship, a Sloan Fellowship, and Google Research Awards.
Christopher Fletcher was supported by a DoD National
Defense Science and Engineering Graduate Fellowship.

References

[1] BONEH, D., MAZIERES, D., AND POPA, R. A. Remote
oblivious storage: Making oblivious RAM practical.
Manuscript,
http://dspace.mit.edu/bitstream/
handle/1721.1/62006/MIT-CSAIL-TR-2011-018.
pdf, 2011.

[2] DAMG ˚ARD, I., MELDGAARD, S., AND NIELSEN, J. B.
Perfectly secure oblivious RAM without random oracles.
In TCC (2011).

[4] DEVADAS, S., VAN DIJK, M., FLETCHER, C. W., REN,
L., SHI, E., AND WICHS, D. Onion oram: A con-
stant bandwidth blowup oblivious ram. Cryptology ePrint
Archive, 2015. http://eprint.iacr.org/2015/005.
[5] FLETCHER, C., REN, L., KWON, A., VAN DIJK, M.,
AND DEVADAS, S. Freecursive oram: [nearly] free recur-
sion and integrity veriﬁcation for position-based oblivious
ram. In ASPLOS (2015).

[6] FLETCHER, C., REN, L., KWON, A., VAN DIJK, M.,
STEFANOV, E., SERPANOS, D., AND DEVADAS, S. A
low-latency, low-area hardware oblivious ram controller.
In FCCM (2015).

[7] FLETCHER, C., REN, L., YU, X., VAN DIJK, M.,
KHAN, O., AND DEVADAS, S. Suppressing the obliv-
ious ram timing channel while making information leak-
age and program efﬁciency trade-offs. In HPCA (2014).
[8] FLETCHER, C., VAN DIJK, M., AND DEVADAS, S. Se-
cure Processor Architecture for Encrypted Computation
on Untrusted Programs. In STC (2012).

[9] GENTRY, C., GOLDMAN, K. A., HALEVI, S., JUTLA,
C. S., RAYKOVA, M., AND WICHS, D. Optimizing oram
and using it efﬁciently for secure computation.
In PET
(2013).

[10] GOLDREICH, O. Towards a theory of software protection

and simulation on oblivious rams. In STOC (1987).

[11] GOLDREICH, O., AND OSTROVSKY, R. Software pro-
In J. ACM

tection and simulation on oblivious rams.
(1996).

[12] GOODRICH, M. T., AND MITZENMACHER, M. Privacy-
preserving access of outsourced data via oblivious ram
simulation. In ICALP (2011).

[13] GOODRICH, M. T., MITZENMACHER, M., OHRI-
MENKO, O., AND TAMASSIA, R. Privacy-preserving
group data access via stateless oblivious RAM simula-
tion. In SODA (2012).

[14] ISLAM, M., KUZU, M., AND KANTARCIOGLU, M. Ac-
cess pattern disclosure on searchable encryption: Ramiﬁ-
cation, attack and mitigation. In NDSS (2012).

[15] KUSHILEVITZ, E., LU, S., AND OSTROVSKY, R. On
the (in) security of hash-based oblivious ram and a new
balancing scheme. In SODA (2012).

[16] LIU, C., HARRIS, A., MAAS, M., HICKS, M., TIWARI,
M., AND SHI, E. Ghostrider: A hardware-software sys-
tem for memory trace oblivious computation. In ASPLOS
(2015).

[17] LORCH, J. R., PARNO, B., MICKENS, J. W., RAYKOVA,
M., AND SCHIFFMAN, J. Shroud: Ensuring private ac-
cess to large-scale data in the data center. In FAST (2013).
[18] MAAS, M., LOVE, E., STEFANOV, E., TIWARI, M.,
SHI, E., ASANOVIC, K., KUBIATOWICZ, J., AND
SONG, D. Phantom: Practical oblivious computation in a
secure processor. In CCS (2013).

[3] DAUTRICH, J., STEFANOV, E., AND SHI, E. Burst oram:
Minimizing oram response times for bursty access pat-
terns. In USENIX (2014).

[19] MAYBERRY, T., BLASS, E.-O., AND CHAN, A. H. Ef-
ﬁcient private ﬁle retrieval by combining oram and pir. In
NDSS (2014).

USENIX Association  

24th USENIX Security Symposium  427

13

[20] OSTROVSKY, R. Efﬁcient computation on oblivious

rams. In STOC (1990).

[21] OSTROVSKY, R., AND SHOUP, V. Private information

storage (extended abstract). In STOC (1997).

[22] REN, L., YU, X., FLETCHER, C., VAN DIJK, M., AND
DEVADAS, S. Design space exploration and optimiza-
tion of path oblivious ram in secure processors. In ISCA
(2013).

[23] SHI, E., CHAN, T.-H. H., STEFANOV, E., AND LI, M.
Oblivious ram with o((logn)3) worst-case cost. In Asi-
acrypt (2011).

[24] STEFANOV, E., AND SHI, E. Oblivistore: High perfor-

mance oblivious cloud storage. In S&P (2013).

[25] STEFANOV, E., SHI, E., AND SONG, D. Towards practi-

cal oblivious RAM. In NDSS (2012).

[26] STEFANOV, E., VAN DIJK, M., SHI, E., CHAN, T.-
H. H., FLETCHER, C., REN, L., YU, X., AND DE-
VADAS, S. Path oram: An extremely simple oblivious
ram protocol. Cryptology ePrint Archive, 2013. http:
//eprint.iacr.org/2013/280.

[27] STEFANOV, E., VAN DIJK, M., SHI, E., FLETCHER, C.,
REN, L., YU, X., AND DEVADAS, S. Path oram: An
extremely simple oblivious ram protocol. In CCS (2013).
[28] WANG, X. S., CHAN, T.-H. H., AND SHI, E. Cir-
cuit oram: On tightness of the goldreich-ostrovsky lower
bound. Cryptology ePrint Archive, 2014.
http://
eprint.iacr.org/2014/672.

[29] WILLIAMS, P., AND SION, R. Single round access pri-

vacy on outsourced storage. In CCS (2012).

[30] WILLIAMS, P., SION, R., AND TOMESCU, A. Privatefs:

A parallel oblivious ﬁle system. In CCS (2012).

[31] YU, X., FLETCHER, C. W., REN, L., VAN DIJK, M.,
AND DEVADAS, S. Generalized external interaction
with tamper-resistant hardware with bounded information
leakage. In CCSW (2013).

[32] ZHUANG, X., ZHANG, T., AND PANDE, S. HIDE: an in-
frastructure for efﬁciently protecting information leakage
on the address bus. In ASPLOS (2004).

A Bucket Structure

Table 6 lists all the ﬁelds in a Ring ORAM bucket and
their size. We would like to make two remarks. First,
only the data ﬁelds are permuted and that permutation
is stored in ptrs. Other bucket ﬁelds do not need to
be permuted because when they are needed, they will
be read in their entirety. Second, count and valids are
stored in plaintext. There is no need to encrypt them
since the server can see which bucket is accessed (deduc-
ing count for each bucket), and which slot is accessed in
each bucket (deducing valids for each bucket). In fact,
if the server can do computation and is trusted to follow

Algorithm 5 Helper functions.

count, valids, addrs, leaves, ptrs, data are ﬁelds of the
input bucket in each of the following three functions

1: function GetBlockOﬀset(bucket,a)
2:
3:
4:
5:
6:

read in valids, addrs, ptrs
decrypt addrs, ptrs
for j ← 0 to Z − 1 do
return ptrs[ j]

if a = addrs[ j] and valids[ptrs[ j]] then

return a pointer to a random valid dummy

(cid:30) block of interest

(cid:30) track # of remaining real blocks

if valids[ptrs[ j]] then

read in valids, addrs, leaves, ptrs
decrypt addrs, leaves, ptrs
z ← 0
for j ← 0 to Z − 1 do

1: function ReadBucket(bucket)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

for j ← z to Z − 1 do

read a random valid dummy

data(cid:29) ← read and decrypt data[ptrs[ j]]
z ← z + 1
if addrs[ j] (cid:28)= ⊥ then

block ← (addr[ j], leaf[ j], data(cid:29))
Stash ← Stash∪ block

1: function WriteBucket(bucket, Stash)
2:
3:
4:
5:
6:

ﬁnd up to Z blocks from Stash that can reside
in this bucket, to form addrs, leaves, data(cid:29)
ptrs ← PRP(0,Z + S)
for j ← 0 to Z − 1 do
valids ← {1}Z+S
count ← 0
encrypt addrs, leaves, ptrs, data
write out count, valids, addrs, leaves, ptrs, data

data[ptrs[ j]] ← data(cid:29)[ j]

(cid:30) or truly random

7:
8:
9:
10:

the protocol faithfully, the client can let the server up-
date count and valids. All the other structures should be
probabilistically encrypted.

For example,

Having deﬁned the bucket structure, we can be more
speciﬁc about some of the operations in earlier sec-
tions.
in Algorithm 2 Line 5 means
reading P(l,i).data[oﬀset], and Line 6 means setting
P(l,i).valids[oﬀset] to 0.
Now we describe the helper functions in detail.
GetBlockOﬀset reads in the valids, addrs, ptrs ﬁeld, and
looks for the block of interest. If it ﬁnds the block of
interest, meaning that the address of a still valid block
matches the block of interest, it returns the permuted lo-
cation of that block (stored in ptrs). If it does not ﬁnd
the block of interest, it returns the permuted location of
a random valid dummy block.

428  24th USENIX Security Symposium 

USENIX Association

14

Notation

Size (bits)

Meaning

count
valids
addrs
leaves
ptrs
data

EncSeed

log(S)
(Z + S)∗ 1
Z ∗ log(N)

Z ∗ L

Z ∗ log(Z + S)
(Z + S)∗ B

λ (security parameter)

# of times this bucket has been touched by ReadPath since it was last shufﬂed

Indicates whether each of the Z + S blocks is valid
Address for each of the Z (potentially) real blocks
Leaf label for each of the Z (potentially) real blocks

Offset in the bucket for each of the Z (potentially) real blocks

Data ﬁeld for each of the Z + S blocks, permuted according to ptrs
Encryption seed for the bucket; count and valids are stored in the clear

Table 6: Ring ORAM bucket format. All logs are taken to their ceiling.

ReadBucket reads all of the remaining real blocks in a
bucket into the stash. For security reasons, ReadBucket
always reads exactly Z blocks from that bucket. If the
bucket contains less than Z valid real blocks, the remain-
ing blocks read out are random valid dummy blocks. Im-
portantly, since we allow at most S reads to each bucket
before reshufﬂing it, it is guaranteed that there are at least
Z valid (real + dummy) blocks left that have not been
touched since the last reshufﬂe.

WriteBucket evicts as many blocks as possible (up to
Z) from the stash to a certain bucket. If there are z(cid:30) ≤ Z
real blocks to be evicted to that bucket, Z +S−z(cid:30) dummy
blocks are added. The Z + S blocks are then randomly
shufﬂed based on either a truly random permutation or a
Pseudo Random Permutation (PRP). The permutation is
stored in the bucket ﬁeld ptrs. Then, the function resets
count to 0 and all valid bits to 1, since this bucket has
just been reshufﬂed and no blocks have been touched.
Finally, the permuted data ﬁeld along with its metadata
are encrypted (except count and valids) and written out
to the bucket.

B Proof of the Lemmas

To prove Lemma 1, we made a little change to the Ring
ORAM algorithm. In Ring ORAM, a ReadPath opera-
tion adds the block of interest to the stash and replaces it
with a dummy block in the tree. Instead of making the
block of interest in the tree dummy, we turn it into a stale
block. On an EvictPath operation to path l, all the stale
blocks that are mapped to leaf l are turned into dummy
blocks. Stale blocks are treated as real blocks in both
ORAMZ,A
(including GZ) until they are
turned into dummy blocks. Note that this trick of stale
blocks is only to make the proof go through.
It hurts
the stash occupancy and we will not use it in practice.
With the stale block trick, we can use induction to prove
Lemma 1.

and ORAM∞,A

L

L

Proof of Lemma 1. Initially, the lemma obviously holds.
Suppose GSZ (S∞) =S Z after some accesses. We need to

15

S(cid:30)Z

(S∞) = S(cid:30)Z where S(cid:30)Z and S(cid:30)∞ are the states
show that G
after the next operation (either ReadPath or EvictPath).
A ReadPath operation adds a block to the stash (the root
bucket) for both ORAMZ,A
, and does not
move any blocks in the tree except turning a real block
into a stale block. Since stale blocks are treated as real
blocks, G

and ORAM∞,A

L

L

Now we show the induction holds for an EvictPath
l be an EvictPath operation to P(l)
l be an EvictPath operation
l (SZ) and S(cid:30)∞ =
l has the same effect as EP∞
l

operation. Let EPZ
(path l) in ORAMZ,A
L
to P(l) in ORAM∞,A
L
EP∞
followed by post-processing, so

and EP∞
. Then, S(cid:30)Z = EPZ

l (S∞). Note that EPZ

(S∞) =S (cid:30)Z holds.

S(cid:30)Z

S(cid:30)Z = EPZ
l (SZ) =G
(EP∞
= G
S(cid:30)Z

(EP∞
l (GSZ (S∞)))

S(cid:30)Z

l (SZ))

The last equation is due to the induction hypothesis.

It remains to show that

G

(EP∞

(EP∞

S(cid:30)Z

S(cid:30)Z

(EP∞

l (S∞)) ,

l (GSZ (S∞))) = G
S(cid:30)Z
(S(cid:30)∞). To show this, we decompose G into
which is G
steps for each bucket, i.e., GSZ (S∞) = g1g2···g 2L+1 (S∞)
where gi processes bucket b∞
in reference to bZ
i . Sim-
i
into g(cid:30)1g(cid:30)2···g(cid:30)2L+1 where
ilarly, we decompose G
S(cid:30)Z
each g(cid:30)i processes bucket b(cid:30)∞
of S(cid:30)∞ in reference
i
to b(cid:30)Z
of S(cid:30)Z. We now only need to show that
i
for any 0 < i < 2L+1, G
l (g1g2···g i (S∞))) =
(EP∞
G
l (g1g2···g i−1 (S∞))). This is obvious if we
S(cid:30)Z
consider the following three cases separately:
1. If bi ∈ P(l), then gi before EP∞
l has no effect since
l moves all blocks on P(l) into the stash before
(cid:23)∈ P(l) and bi/2 (cid:23)∈ P(l) (neither bi nor
then gi and EP∞
l
in-
their order
l (g0g1g2···g i (S∞))) =
Furthermore,

EP∞
evicting them to P(l).
its parent
touch non-overlapping buckets and do not
terfere with each other.
(EP∞
can be swapped, G
G

is on Path l),

2. If bi

Hence,

l (g0g1g2···g i−1 (S∞))).

gi (EP∞

S(cid:30)Z

S(cid:30)Z

S(cid:30)Z

USENIX Association  

24th USENIX Security Symposium  429

S(cid:31)Z

(since EP∞

bZ
i = b(cid:31)Z
l does not change the content
i
of bi), so gi has the same effect as g(cid:31)i and can be
merged into G

.

3. If bi (cid:30)∈ P(l) but bi/2 ∈ P(l), the blocks moved into
bi/2 by gi will stay in bi/2 after EP∞
l since bi/2 is
the highest intersection (towards the leaf) that these
blocks can go to. So gi can be swapped with EP∞
l
and can be merged into G
as in the second case.
We remind the readers that because we only remove stale
blocks that are mapped to P(l), the ﬁrst case is the only
case where some stale blocks in bi may turn into dummy
blocks. And the same set of stale blocks are removed
from ORAMZ,A

and ORAM∞,A

S(cid:31)Z

.

L

L
This shows

(EP∞

G

S(cid:31)Z

l (GSZ (S∞))) = G
= G

(EP∞

l (S∞))

S(cid:31)Z

S(cid:31)Z(cid:31)S(cid:31)∞(cid:30)

and completes the proof.

The proof of Lemma 2 remains unchanged from the
Path ORAM paper [27], and is replicated here for com-
pleteness.

Proof of Lemma 2. If part: Suppose T ∈ ORAM∞,A
and
X(T ) > c(T ) + R. Observe that G can assign the blocks
in a bucket only to an ancestor bucket. Since T can store
at most c(T ) blocks, more than R blocks must be as-
signed to the stash by G.

L

Only if part: Suppose that st (GSZ (S∞)) > R. Let T
be the maximal rooted subtree such that all the buck-
ets in T contain exactly Z blocks after post-processing
G. Suppose b is a bucket not in T . By the maximality
of T , there is an ancestor (not necessarily proper ances-
tor) bucket b(cid:31) of b that contains less than Z blocks after
post-processing, which implies that no block from b can
go to the stash. Hence, all blocks that are in the stash
must have originated from T . Therefore, it follows that
X(T ) > c(T ) +R.

L

L

Proof of Lemma 3. For a bucket b in ORAM∞,A
, de-
ﬁne Y (b) to be the number of blocks in b before post-
It sufﬁces to prove that ∀b ∈ ORAM∞,A
,
processing.
E[Y (b)] ≤ A/2.
If b is a leaf bucket, the blocks in it are put there by the
last EvictPath operation to that leaf/path. Note that only
real blocks could be put in b by that operation, although
some of them may have turned into stale blocks. Stale
blocks can never be moved into a leaf by an EvictPath
operation, because that EvictPath operation would re-
move all the stale blocks mapped to that leaf. There are
at most N distinct real blocks and each block has a prob-
ability of 2−L to be mapped to b independently. Thus
E[Y (b)] ≤ N · 2−L ≤ A/2.
If b is not a leaf bucket, we deﬁne two variables m1 and
m2: the last EvictPath operation to b’s left child is the
m1-th EvictPath operation, and the last EvictPath oper-
ation to b’s right child is the m2-th EvictPath operation.
Without loss of generality, assume m1 < m2. We then
time-stamp the blocks as follows. When a block is ac-
cessed and remapped, it gets time stamp m∗, which is
the number of EvictPath operations that have happened.
Blocks with m∗ ≤ m1 will not be in b as they will go
to either the left child or the right child of b. Blocks
with m∗ > m2 will not be in b as the last access to b
(m2-th) has already passed. Therefore, only blocks with
time stamp m1 < m∗ ≤ m2 will be put in b by the m2-
th access. (Some of them may be accessed again after
the m2-th access and become stale, but this does not af-
fect the total number of blocks in b as stale blocks are
treated as real blocks.) There are at most d = A|m1− m2|
such blocks, and each goes to b independently with a
probability of 2−(i+1), where i is the level of b. The de-
terministic nature of evictions in Ring ORAM ensures
|m1 − m2| = 2i. (One way to see this is that a bucket b
at level i will be written every 2i EvictPath operations,
and two consecutive EvictPath operations to b always
travel down the two different children of b.) Therefore,
E[Y (b)] ≤ d · 2−(i+1) = A/2 for any non-leaf bucket as
well.

430  24th USENIX Security Symposium 

USENIX Association

16

