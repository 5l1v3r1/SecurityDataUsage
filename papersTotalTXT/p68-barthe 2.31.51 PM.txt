Differentially Private Bayesian Programming

Gilles Barthe
IMDEA Software
˚
Marco Gaboardi
University at Buffalo

˚
Gian Pietro Farina
University at Buffalo

Emilio Jesús Gallego Arias

CRI Mines-ParisTech
:

Andy Gordon

Microsoft Research

Justin Hsu

University of Pennsylvania

Pierre-Yves Strub

IMDEA Software

ABSTRACT
We present PrivInfer, an expressive framework for writing
and verifying diﬀerentially private Bayesian machine learning
algorithms. Programs in PrivInfer are written in a rich func-
tional probabilistic programming language with constructs
for performing Bayesian inference. Then, diﬀerential pri-
vacy of programs is established using a relational reﬁnement
type system, in which reﬁnements on probability types are
indexed by a metric on distributions. Our framework lever-
ages recent developments in Bayesian inference, probabilistic
programming languages, and in relational reﬁnement types.
We demonstrate the expressiveness of PrivInfer by verifying
privacy for several examples of private Bayesian inference.

1.

INTRODUCTION

Diﬀerential privacy [17] is emerging as a gold standard in
data privacy. Its statistical guarantee ensures that the prob-
ability distribution on outputs of a data analysis is almost
the same as the distribution on outputs obtained by per-
forming the same data analysis on a (hypothetical) dataset
diﬀering in one individual. A standard way to ensure dif-
ferential privacy is by perturbing the data analysis adding
some statistical noise. The magnitude and the shape of noise
must provide a protection to the inﬂuence of an individual
on the result of the analysis, while ensuring that the algo-
rithm provides useful results. Two properties of diﬀerential
privacy are especially relevant for this work: (1) composabil-
ity, (2) the fact that diﬀerential privacy works well on large
datasets, where the presence or absence of an individual has
limited impact. These two properties have led to the design
˚
:
CNS1565365 and by EPSRC grant EP/M022358/1.
Partially supported by NSF grants TC-065060 and TWC-
1513694, and a grant from the Simons Foundation (#360368
to Justin Hsu).

supported by NSF grants CNS1237235,

Partially

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24 - 28, 2016, Vienna, Austria
© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978371

of tools for diﬀerentially private data analysis. Many of these
tools use programming language techniques to ensure that
the resulting programs are indeed diﬀerentially private [2–
4, 6, 19–21, 29, 33]. Moreover, property (2) has encouraged
the interaction of the diﬀerential privacy community with the
machine learning community to design privacy-preserving
machine learning techniques, e.g. [12, 18, 25, 39]. At the same
time, researchers in probabilistic programming are exploring
programming languages as tools for machine learning. For
example, in Bayesian inference, probabilistic programming
allows data analysts to represent the probabilistic model, its
parameters, and the data observations as a specially crafted
program. Given this program as input, we can then use
inference algorithms to produce a distribution over the pa-
rameters of the model representing our updated beliefs on
them. Several works have explored the design of program-
ming languages to compute eﬃciently the updated beliefs
in order to produce eﬃcient and usable tools for machine
learning, e.g. [22, 24, 28, 31, 32, 36].

Recently, research in Bayesian inference and machine
learning has turned to privacy-preserving Bayesian infer-
ence [15, 38, 40, 41], where the observed data is private.
Bayesian inference is a deterministic process, and directly
releasing the posterior distribution would violate diﬀerential
privacy. Hence, researchers have developed techniques to
make Bayesian inference diﬀerentially private. Basic tech-
niques add noise on the input data, or add noise on the
result of the data analysis, while more advanced techniques
can ensure diﬀerential privacy by releasing random samples
from the posterior distribution instead of releasing the pos-
terior distribution itself. The diversity of approaches makes
Bayesian inference an attractive target for veriﬁcation tools
for diﬀerential privacy.

In this work we present PrivInfer, a programming frame-
work combining veriﬁcation techniques for diﬀerential privacy
with learning techniques for Bayesian inference. PrivInfer
consists of two main components: a probabilistic functional
language extending PCF for Bayesian inference, and a rela-
tional higher-order type system that can verify diﬀerential
privacy for programs written in this language.

The core idea of Bayesian learning is to use conditional
distributions to represent the beliefs updated after some
observations. PrivInfer, similarly to other programming lan-
guages for inference models conditioning on data explicitly
using an observe statement. An interesting aspect of Bayesian
inference is that although the inferred output is a probability

68distribution, the process to generate it is deterministic. To
guarantee diﬀerential privacy, we need to inject some ran-
domness into the inference process. To handle these two roles
of distributions, PrivInfer distinguishes between symbolic dis-
tributions and actual distributions. The former represent the
result of an inference, while the latter are used to represent
random computations, e.g. diﬀerentially private computa-
tions (mechanisms). Moreover, we parametrize our language
with an algorithm to perform Bayesian inference returning
symbolic distributions, expressed by the statement infer, and
mechanisms to ensure diﬀerential privacy returning actual
distributions.

Diﬀerential privacy is a probabilistic 2-property, i.e. a
property expressed over pairs of execution traces of the pro-
gram. To address this challenge, we use an approach based on
approximate relational higher-order reﬁnement type system,
like the ones used in the system HOARe2 [6].

PrivInfer extends this approach to deal with the construc-
tions that are needed for Bayesian inference like the observe
and infer constructs and the distinction between symbolic
and actual distributions. Another important aspect of the
veriﬁcation of diﬀerential privacy is reasoning about the sen-
sitivity of a data analysis. This measures the inﬂuence that
two databases diﬀering in one individual can have on the
output. Calibrating noise to sensitivity ensures that the data
analysis provides suﬃcient privacy. In Bayesian inference,
the output of the computation is a distribution (often deﬁned
by a few numeric parameters) for which one can consider dif-
ferent measures. A simple approach is to considered standard
metrics (Euclidean, Hamming, etc.) to measure the distance
between the parameters. Another more interesting approach
is to consider distances between distributions, rather than
the parameters. The type system of PrivInfer allows one to
reason about the parameters of a distribution, using stan-
dard metrics, but also about the distribution itself using
f -divergences, a class of probability metrics including some
well known examples like total variation distance, Hellinger
distance, KL divergence, etc.

In summary, PrivInfer extends the relational type system

approach of HOARe2 in three directions:

‚ providing relational typing rules for observe and infer,
‚ providing relational typing rules to reason about sym-
‚ generalizing the probability polymonad of HOARe2 to

bolic and actual distributions,

reason about general f -divergences.

The combination of these three contributions allows us to ad-
dress Bayesian inference, which is not supported by HOARe2.
To illustrate the diﬀerent features of our approach we show
how diﬀerent basic Bayesian data analysis can be guaranteed
diﬀerentially private in three diﬀerent ways: by adding noise
on the input, by adding noise on the parameters with sen-
sitivity measured using the (cid:2)p-norms, and ﬁnally by adding
noise on the distributions with sensitivity measured using
f -divergences. This shows that PrivInfer can be used for a
diverse set of Bayesian data analyses.

Summing up, the contributions of our work are:

‚ A probabilistic extension PCFp of PCF for Bayesian
inference that serves as the language underlying our
framework PrivInfer (§ 4). This includes observe and
infer statements as well as primitives for handling sym-
bolic and actual distributions.

‚ A higher-order approximate relational type system for
reasoning about properties of two runs of programs from
PrivInfer (§ 5). In particular, the type system permits
to reason about f -divergences. The f -divergences can
be used to reason about diﬀerential privacy as well as
about program sensitivity for Bayesian inference.

‚ We show on several examples how PrivInfer can be
used to reason about diﬀerential privacy (§ 6). We will
explore three ways to guarantee diﬀerential privacy:
by adding noise on the input, by adding noise on the
output parameters based on (cid:2)p-norms, and by adding
noise on the output parameters based on f -divergences.

Our work is motivated by Bayesian inference, a statisti-

2. BAYESIAN INFERENCE
cal method which takes a prior distribution Prpξq over a
posterior distribution Prpξ | xq, an updated version of the

parameter ξ and some observed data x, and produces the

prior distribution. Bayesian inference is based on Bayes’
theorem, which gives a formula for the posterior distribution:

Prpξ | xq “ Prpx | ξq ¨ Prpξq

Prpxq

The expression Prpx | ξq is the likelihood of ξ when x is
observed. This is a function Lxpξq of the parameter ξ for

ﬁxed data x, describing the probability of observing the data
x given a speciﬁc value of the parameter ξ. Since the data

x is considered ﬁxed, the expression Prpxq denotes a nor-
malization constant ensuring that Prpξ | xq is a probability

distribution. The choice of the prior reﬂects the prior knowl-
edge or belief on the parameter ξ before any observation has
been performed. In practice, the prior and the likelihood
are typically chosen so that the posterior belongs to the
same family of distributions as the prior. In this case the
prior is said to be conjugate prior for the likelihood. Using
conjugate priors, besides being mathematically convenient
in the derivations, ensures that Bayesian inference can be
performed by a recursive process over the data.

Our goal is to perform Bayesian inference under diﬀerential
privacy. We provide the formal deﬁnition of diﬀerential
privacy in Deﬁnition 3.1, but for the purpose of this section
it is enough to know that diﬀerential privacy is a statistical
guarantee that requires the answers of a data analysis to
be statistically close when run on two adjacent databases,
i.e. databases that diﬀer in one individual. In the vanilla
version of diﬀerential privacy, the notion of “statistically close”
is measured by a parameter . A typical way to achieve
diﬀerential privacy is to add random noise, and we present
several primitives for doing this in § 3. For one example,
the exponential mechanism (denoted ExpMech) returns a
possible output with probability proportional to a quality
score function Q. The function Q takes in input a database
and a potential output for the statistic computed on the
database, and gives each output a score representing how
good that output is for that database. The privacy and the
utility of the mechanism depend on  and on the sensitivity
of the quality score function, i.e., how much the quality score
can diﬀer for two adjacent databases.

As a motivating example we will consider a simple Bayesian
inference task: learning the bias of a coin from some obser-
vations. For example, we can think of the observations as
medical records asserting whether patients from a sample

69population have a disease or not. We can perform Bayesian
inference to establish how likely it is to have the disease in the
population. We will show how to make this task diﬀerentially
private, and verify privacy in PrivInfer.

First, the input of this example is a set of binary observa-
tions describing whether any given patient has the disease—
this is the private information that we want to protect. We
assume that the number of patients n is public and that the
adjacency condition for diﬀerential privacy states that two
databases diﬀer in the data of one patient. In our concrete
case this means that two databases d, d
are adjacent if all of
their records are the same except for one record that is 0 in
one database and 1 in the other.

1

While in abstract our problem can be described as esti-
mating the bias of a coin, we need to be more formal and
provide the precise model and the parameters that we want
to estimate. We can incorporate our initial belief on the
fairness of the coin using a prior distribution on the bias ξ

given by a beta distribution. This is a distribution over r0, 1s

with probability density:

betapξ | a, bq “ ξa´1p1 ´ ξb´1q

Bpa, bq

where a, b P R`

are parameters and B denotes the beta
function. The likelihood is the probability that a series of
i.i.d samples from a Bernoulli distributed random variable
with bias ξ matches the observations. Using an informal
notation,1 we can write the following program in PrivInfer:

˘

¯
betapa, bq

(1)

infer

observe

λr.bernoulliprq “ obs

´

`

The term infer represents an inference algorithm and the
observe statement is used to describe the model. Expression
(1) denotes the posterior distribution that is computed using

Bayes’ theorem with prior betapa, bq and with likelihood
pλr.bernoulliprq “ obsq.

Now, we want to ensure diﬀerential privacy. We have
several options. A ﬁrst natural idea is to perturbate the
input data using the exponential mechanism, corresponding
to the following program:

¯
observepλr.bernoulliprq “ ExpMech Q obsq betapa, bq

´

infer

The fact that diﬀerential privacy is closed under post-
processing ensures that this guarantees diﬀerential privacy
for the whole program. In more detail, in the notation above
we denoted by Q the scoring function. Since obs is a boolean,
we can use a quality score function that gives score 1 to b

if b “ obs and 0 otherwise. This function has sensitivity 1
and so one achieves p, 0q-diﬀerential privacy. This is a very

simple approach, but in some situations it can already be
very useful [38].

posterior which is a betapa

A diﬀerent way of guaranteeing diﬀerential privacy is by
adding noise on the output. In this case the output is the
. Us-
ing again the exponential mechanism we can consider the
following program:

observepλr.bernoulliprq “obsqbeta pa, bq˘¯

1q for some values a

ExpMech Q

infer

´

`

, b

, b

1

1

1

So, a natural question is which Q we can use as quality score
function and what is its sensitivity in this case.

1

, b

There are two natural choices. The ﬁrst one is to consider

distance in term of some metric on vectors, e.g. the one given

the parameters pa
1q as a vector and measure the possible
by (cid:2)1 norm dppa, bq,pa
1|. The second is
1
to consider betapa
1q as an actual distribution and then use
ΔHpbetapa, bq, betapa
1

a notion of distance on distributions, e.g. Hellinger distance

1qq “ |a´ a

1|`|b´ b

1qq.

, b

, b

, b

1

These two approaches both guarantee privacy, but they
have diﬀerent utility properties. Our system PrivInfer can
prove privacy for both approaches.

3. BACKGROUND
3.1 Probability and Distributions

In our work we will consider discrete distributions. Fol-
lowing Dwork and Roth [16] we will use standard names for
several continuous distributions but we will consider them to
be the approximate discrete versions of these distributions
up to arbitrary precision.

ř
We deﬁne the set DpAq of distributions over a set A as the
set of functions μ : A Ñ r0, 1s with discrete supportpμq “ tx |
μ x ‰ 0u, such that
xPA μ x “ 1. In our language we will
consider only distribution over basic types, this guarantees
that all our distributions are discrete (see § 4).

We will use several basic distributions like uniform, bernoulli,
normal, beta, etc. These are all standard distributions and we
omit their deﬁnition here. We will also use some notation to

describe distributions. For instance, given an element a P A,
we will denote by 1a the probability distribution that assigns
all mass to the value a. We will also denote by bind μ M
the composition of a distribution μ over the set A with a
function M that takes a value in A and returns a distribution
over the set B.
3.2 Differential Privacy

1

1

Diﬀerential privacy is a strong, quantitative notion of
statistical privacy proposed by Dwork et al. [17].
In the
standard setting, we consider a program (sometimes called
a mechanism) that takes aprivate database d as input, and
produces a distribution over outputs. Intuitively, d represents
a collection of data from diﬀerent individuals. When two
are identical except for a single individual’s
databases d, d
are adjacent 2, and we write
record, we say that d and d
. Then, diﬀerential privacy states that the output
d Φ d
distributions obtained from running the program on two
adjacent databases should be statistically similar. More
formally:

numeric parameters, let D be the set of databases, and let

Deﬁnition 3.1 (Dwork et al. [17]). Let , δ ą 0 be two
R be the set of possible outputs. A program M : D Ñ DpRq
satisﬁes p, δq-diﬀerential privacy if

1

PrpMpdq P Sq ď e PrpMpd
and for every subset of outputs S Ď R.

for all pairs of adjacent databases d, d

1q P Sq ` δ
1 P D such that d Φ d

1

,

In this case, the exponential mechanism is not applied to

booleans but instead to distributions of the shape betapa
1q.
1We omit in particular the monadic probabilistic construc-
tions. A formal description of this example is in § 6.

, b

1

As shown by Barthe et al. [3], we can reformulate diﬀeren-

tial privacy using a speciﬁc statistical -distance -D:
2In our concrete examples we will consider sometime as adja-
cent also two databases that diﬀer by at most one individual.

70Lemma 3.1. Let , δ P R`
. Let D be the set of databases,
and let R be the set of possible outputs. A program M : D Ñ
DpRq satisﬁes p, δq-diﬀerential privacy iﬀ -DpMpdq, Mpd
1qq ď
`

are adjacent databases and

δ, whered, d

1

-Dpμ1, μ2q ”max
EĎR

rx P Es ´ e ¨ Pr
xÐμ2

Pr
xÐμ1

rx P Es˘

for μ1, μ2 P DpRq.

Diﬀerential privacy is an unusually robust notion of privacy.
It degrades smoothly when private mechanisms are composed
in sequence or in parallel, and it is preserved under any post-
processing that does not depend on the private database.
The following lemmas capture these properties:

Lemma 3.2 (Post-processing). Let M : D Ñ DpRq be an
p, δq-diﬀerentially private program. Let N : R Ñ DpR
1q be
an arbitrary randomized program. Then λd.bindpM dq N :
D Ñ DpR

1q is p, δq-diﬀerentially private.

Diﬀerential privacy enjoys diﬀerent composition schemes,

we report here one of the simpler and most used.

Lemma 3.3 (Composition). Let M1 : D Ñ DpR1q, and M2 :
D Ñ DpR2q respectively p1, δ1q and p2, δ2q diﬀerentially
private programs. Let M : D Ñ DpR1 ˆ R2q the program
deﬁned as Mpxq ” pM1pxq, M2pxqq. Then, M is p1` 2, δ1`
δ2q diﬀerentially private.

Accordingly, complex diﬀerentially private programs can
be easily assembled from simpler private components, and
researchers have proposed a staggering variety of private
algorithms which we cannot hope to summarize here. (Inter-
ested readers can consult Dwork and Roth [16] for a textbook
treatment.)

While these algorithms serve many diﬀerent purposes,
the vast majority are constructed from just three private
operations, which we call primitives. These primitives oﬀer
diﬀerent ways to create private mechanisms from non-private
functions. Crucially, the function must satisfy the following
sensitivity property:

. Suppose f : A Ñ B is a
Deﬁnition 3.2. Let k P R`
function, where A and B are equipped with distancesd A
and dB. Then f is k-sensitive if

dBpfpaq, fpa
1 P A.

1qq ď k ¨ dApa, a

1q

for every a, a

Intuitively, k-sensitivity bounds the eﬀect of a small change
in the input, a property that is similar in spirit to the diﬀer-
ential privacy guarantee. With this property in hand, we can
describe the three basic primitive operations in diﬀerential
privacy, named after their noise distributions.

The Laplace mechanism. The ﬁrst primitive is the stan-
dard way to construct a private version of a function that
maps databases to numbers. Such functions are also called
numeric queries, and are fundamental tools for statistical
analysis. For instance, the function that computes the aver-
age age of all the individuals in a database is a numeric query.
When the numeric query has bounded sensitivity, we can use
the Laplace mechanism to guarantee diﬀerential privacy.

Deﬁnition 3.3. Let  P R`
and let f : D Ñ R be a numeric
query. Then, the Laplace mechanism maps a database d P D

to fpdq ` ν, where ν is drawn form the Laplace distribution
with scale 1{. This distribution has the following probability

density function:

Lap1{

pxq “ 

2

expp´|x|q.

If f is a k-sensitive function, then the Laplace mechanism is

pk, 0q-diﬀerentially private.

The Gaussian mechanism. The Gaussian mechanism is
an alternative to the Laplace mechanism, adding Gaussian
noise with an appropriate standard deviation to release a
numeric query. Unlike the Laplace mechanism, the Gaussian

mechanism does not satisfy p, 0q-privacy for any . However,
it satisﬁes p, δq-diﬀerential privacy for δ P R`
Deﬁnition 3.4. Let , δ P R and let f : D Ñ R be a numeric
d P D to fpdq ` ν, where ν is a drawn from the Gaussian

query. Then, the Gaussian mechanism maps a database

.

distribution with standard deviation

2 lnp1.25{δq{.
If f is a k-sensitive function for k ă 1{, then the Gaussian
mechanism is pk, δq-diﬀerentially private.

σp, δq “a

The exponential mechanism. The ﬁrst two primitives can
make numeric queries private, but in many situations we may
want to privately release a non-numeric value. To accomplish
this goal, the typical tool is the exponential mechanism [30],
our ﬁnal primitive. This mechanism is parameterized by
a setR , representing the range of possible outputs, and a

quality score function q : D ˆ R Ñ R, assigning a real-valued
The exponential mechanism releases an output r P R

score to each possible output given a database.

with approximately the largest quality score on the private
database. The level of privacy depends on the sensitivity of
q in the database. Formally:

Deﬁnition 3.5 (McSherry and Talwar [30]). Let  P R`
Let R be the set of outputs, and q : D ˆ R Ñ R be the
d P D releases r P R with probability proportional to

quality score. Then, the exponential mechanism on database

.

ˆ

˙

Prprq „ exp

qpd, rq

2

If f is a k-sensitive function in d for any ﬁxed r P R, then
the exponential mechanism is pk, 0q-diﬀerentially private.
3.3 f-divergences

As we have seen, diﬀerential privacy is closely related to
function sensitivity. To verify diﬀerential privacy for the
result of probabilistic inferences, we will need to work with
several notions of distance between distributions. These
distances can be neatly described as f -divergences [13], a
rich class of metrics on probability distributions. Inspired by
the deﬁnition of relative entropy, f -divergences are deﬁned
by a convex function f . Formally:

Deﬁnition 3.6 (Csisz´ar and Shields [13]). Let fpxq be a
convex function deﬁned for x ą 0, with fp1q “ 0. Let μ1, μ2
ÿ
denoted Δfpμ1 | μ2q is deﬁned as:

distributions over A. Then, the f -divergence of μ1 from μ2,

¯

´

Δfpμ1 | μ2q “

μ2paqf

μ1paq
μ2paq

aPA

71Simpliﬁed form

1
2
1
2

¯2

´a
μ1paq ´a
|μ1paq ´ μ2paq|
´
¯
μ2paq
´
μ1paq ln
μ1paq ´ eμ2paq, 0

μ1paq
μ2paq

max

¯

aPA

aPA

ÿ
ÿ
ÿ
ÿ

aPA

aPA

f -diverg.

SDpxq
HDpxq
KLpxq
-Dpxq

1
2

fpxq
|x ´ 1|
p?
x ´ 1q2
x lnpxq ´ x ` 1
maxpx ´ e, 0q

1
2

Table 1: f -divergences for statistical distance (SD), Hellinger
distance (HD), KL divergence (KL), and -distance (-D)

´

¯

where we assume 0 ¨ fp 0
“ lim
tÑ0

0 ¨ f

0

a
0

´
¯
q “ 0 and
t ¨ f

a
t

“ a lim
uÑ8

´

fpuq

¯

.

u

If Δfpμ1 | μ2q ď δ we say that μ1 and μ2 are pf, δq-close.

Examples of f -divergences include KL-divergence, Hellinger
distance, and total variation distance. Moreover, Barthe and
Olmedo [2] showed how the -distance of Lemma 3.1 can
be seen as an f -divergence for diﬀerential privacy. These
f -divergences are summarized in Table 1. Notice that some
of the f -divergences in the table above are not symmetric. In
particular, this is the case for KL-divergence and -distance,
which we use to describe p, δq-diﬀerential privacy. We will
denote by F the class of functions meeting the requirements
of Deﬁnition 3.6.

Not only do f -divergences measure useful statistical quan-
tities, they also enjoy several properties that are useful for
formal veriﬁcation. (e.g. see [13]). A property that is worth
mentioning and that will be used implicitly in our example
is the following.

Theorem 3.1 (Data processing inequality). Let f P F,

μ1, μ2 be two distributions over A, and M be a function
(potentially randomized) mapping values in A to distributions
over B. Then, we have:

Δfpbind μ1 M, bind μ2 Mq ď Δfpμ1, μ2q

Another important property for our framework is compo-
sition. As shown by Barthe and Olmedo [2] we can compose
f -divergences in an additive way. More speciﬁcally, they give
the following deﬁnition.

Deﬁnition 3.7 (Barthe and Olmedo [2]). Let f1, f2, f3 P F.
We say that pf1, f2q are f3 composable if and only if for every

A, B, two distributions μ1, μ2 over A, and two functions
M1, M2 mapping values in A to distributions over B we have

Δf3pbind μ1 M1, bind μ2 M2q ď

Δf1pμ1, μ2q ` sup

Δf2pM1 v, M2 vq

v
In particular, we have the following.

Lemma 3.4 (Barthe and Olmedo [2]).

‚ p1-D, 2-Dq are p1 ` 2q-DP composable.
‚ pSD, SDq are SD composable.
‚ pHD, HDq are HD composable.
‚ pKL, KLq are KL composable.

This form of composition will be internalized by the rela-

tional reﬁnement type system that we will present in § 5.

4. PrivInfer

The main components of PrivInfer are a language that
permits to express Bayesian inference models and a type
system for reasoning in a relational way about programs
from the language.
4.1 The language

e

The language underlying PrivInfer is a probabilistic pro-
gramming extension of PCF that we will call PCFp. Ex-
pressions of PCFp are deﬁned by the following grammar

::“ x | c | e e | λx. e
|
|
|
|
|

letrec f x “ e | case e with rdi xi ñ eisi
return e | mlet x “ e in e
observe x ñ e in e | inferpeq | ranpeq
bernoullipeq | normalpe, eq | betape, eq | uniformpq
lapMechpe, eq | gaussMechpe, eq | expMechpe, e, eq
where c represents a constant from a set C and x a variable.
We will denote by PCFp (X ) the set of expression of PrivInfer
where the variables are taken from the set X .

simple types of the form

We will consider only expressions that are well typed using

τ, σ ::“ rτ | Mrrτs | MrDrrτss | Drrτs |τ Ñ σ
rτ
` | r0, 1s |rτ list.

::“ ‚ | B | N | R | R` | R

where rτ are basic types. As usual a typing judgment is a
judgment of the shape Γ $ e : τ where an environment Γ

`

is an assignment of types to variables. The simply typed
system of PrivInfer is an extension of the one in Barthe et al.
[6]; in Figure 1 we only present the rules speciﬁc to PrivInfer.
The syntax and types of PCFp extend the ones of PCF by
means of several constructors. Basic types include the unit
type ‚ and types for booleans B and natural numbers N. We
also have types for real numbers R, positive real numbers R`
,
positive real number plus inﬁnity R
and for real numbers in
the unit interval r0, 1s. Finally we have lists over basic types.
ability monad Mrrτs over the basic type rτ , and a type Drrτs
representing symbolic distributions over the basic typerτ . The
probabilistic monad Mrrτs that can be manipulated by the let-
binder mlet x “ e1 in e2 and by the unit return e. Symbolic
like bernoullipeq for Bernoulli distributions, normalpe1, e2q for

probability monad can also be over symbolic distributions.
Probabilities (actual distributions) are encapsulated in the

Simple types combines basic types using arrow types, a prob-

distributions are built using basic probabilistic primitives

normal distribution, etc. These primitives are assigned types
as described in Figure 2. For symbolic distributions we
also assume that we have an operation getParams to extract

the parameters. We also have primitives lapMechpe1, e2q,
gaussMechpe1, e2q and expMechpe1, e2, e3q that provide im-
plementations for the mechanism ensuring diﬀerential privacy
as described in § 3.2.
learning. The primitive observe x ñ e1 in e2 can be used to

Finally, we have three special constructs for representing

describe conditional distributions. This is a functional version
of a similar primitive used in languages like Fun [23]. This
primitive takes two arguments, a prior e2 and a predicate e1
over x. The intended semantics is the one provided by Bayes’
theorem:
it ﬁlters the prior by means of the observation
provided by e1 and renormalize the obtained distribution

(see Section § 4.2 for more details). The primitives inferpeq
and ranpeq are used to transform symbolic distributions in

72Γ $ e1 : MrT1s

Γ, x : T1 $ e2 : MrT2s

BindM

Γ $ mlet x “ e1 in e2 : MrT2s

Γ $ e1 : Mrrτs
Γ, x :rτ $ e2 : MrBs
Γ $ observe x ñ e2 in e1 : Mrrτs

Ran

UnitM

Γ $ e : Drrτs
Γ $ ranpeq : Mrrτs

Observe

Γ $ e : T

Γ $ return e : MrTs

Γ $ e : Mrrτs
Γ $ inferpeq : Drrτs

Infer

Figure 1: PCFp type system (selected rules)

uniform : Drr0, 1ss
bernoulli : r0, 1s Ñ DrBs
beta : R` ˆ R` Ñ Drr0, 1ss
normal : R ˆ R` Ñ DrRs
lapMech : R` ˆ R Ñ MrRs
gaussMech : R` ˆ R Ñ MrRs
expMech : R ˆ ppD, Rq Ñ Rq ˆ D Ñ MrRs

Figure 2: Primitive distributions types.

actual distributions and vice versa. In particular, inferpeq is

the main component performing probabilistic inference.
4.2 Denotational Semantics

The semantics of PCFp is largely standard. We con-
sider only terminating programs and hence we can interpret
them in a set-theoretic way. Basic types are interpreted in

the corresponding sets, e.g. (cid:2)‚(cid:3) “ t‚u, (cid:2)B(cid:3) “ ttrue, falseu,
(cid:2)N(cid:3) “ t0, 1, 2, . . .u, etc. As usual, arrow types (cid:2)τ Ñ σ(cid:3) are
Mrτs for τ P trτ , Drrτsu is interpreted as the set of discrete
interpreted as set of functions (cid:2)τ (cid:3) Ñ (cid:2)σ(cid:3). A monadic type
(
(cid:2)Mrτs(cid:3) “(cid:3)
μ x “ 1
Types of the shape Drrτs are interpreted in set of symbolic
an example, DrBs is interpreted as:

μ : (cid:2)τ (cid:3) Ñ R` | supppμq discrete^

representations for distributions parametrized by values. As

probabilities over τ , i.e.:

ÿ

xP(cid:2)τ (cid:3)

(cid:2)DrBs(cid:3) “(cid:3)

bernoullipvq | v P r0, 1s(

The interpretation of expressions is given as usual under a
validation θ which is a ﬁnite map from variables to values
in the interpretation of types. We will say that θ validates

an environment Γ if @x : τ P Γ we haveθ pxq P (cid:2)τ (cid:3). For most

of the expressions the interpretation is standard, we detail
the less standard interpretations in Figure 3. Probabilistic
expressions are interpreted into discrete probabilities. In
particular, (cid:2)return e(cid:3)θ is deﬁned as the Dirac distribution
returning (cid:2)e(cid:3)θ with probability one. The binding construct
mlet x “ e1 in e2 composes probabilities. The expression
observe x ñ t in u ﬁlters the distribution (cid:2)u(cid:3)θ using the pred-
icate x ñ t and rescales it in order to obtain a distribution.

The observe is the key component to have conditional distri-
butions and to update a prior using Bayes’ theorem. The
semantics of infer relies on a given algorithm3 AlgInf for in-
ference. We leave the algorithm unspeciﬁed because it is not
3In this work we consider only exact inference, and we leave
for future works to consider approximate inference. We also
consider only terminating programs with a well deﬁned se-
mantics (e.g. no observations of events with zero probability)
and where the inference algorithms never fail (this could be
easily simulated by using the maybe monad).

central to our veriﬁcation task. Symbolic distributions are
syntactic constructions, this is reﬂected in their interpreta-
tion. For an example, we give in Figure 3 the interpretation

(cid:2)bernoullipeq(cid:3)θ. The operator ran turns a symbolic distribu-

tion in an actual distribution. Its semantics is deﬁned by
cases on the given symbolic distribution. In Figure 3 we
give its interpretation for the case when the given symbolic

distribution is bernoullipeq. The cases for the other symbolic

distributions are similar.

The soundness of the semantics is given by the following:

Lemma 4.1. If Γ $ e : τ and θ validates Γ, then (cid:2)e(cid:3)θ P (cid:2)τ (cid:3).

5. RELATIONAL TYPE SYSTEM
5.1 Relational Typing

To reason about diﬀerential privacy as well as about f -
divergences we will consider a higher-order relational re-
ﬁnement type system. We follow the approach proposed
by Barthe et al. [6].
We will distinguish two sets of variables: XR (relational )
Ť
and XP (plain). Associated with every relational variable
x P XR, we have a left instance xŸ and a right instance xŹ.
xPXRtxŸ, xŹu and X ’ for XR’ Y XP .
We write XR’ for
The set of PrivInfer expressions E is the set of expressions
deﬁned over plain variables, i.e. expressions in PCFppXPq.
The set of PrivInfer relational expressions E ’ is the set of
expressions, deﬁned over plain and relational variables (ex-
pressions in PCFppX ’q) where only non-relational variables
The sets of relational types T “ tT, U, . . .u and assertions
A “ tφ, ψ, . . .u are deﬁned by the following grammars:
::“ rτ | Mf,δrtx ::rτ | φus | Mf,δrtx :: Drrτs | φus
| Drrτs | Πpx :: Tq. T | tx :: T | φu
T, U P T
px P XPq
φ, ψ P A ::“ Q px : τq. φ
px P XRq
Q px :: Tq. φ
| ΔD
| e’ “ e’ | e’ ď e’ | Cpφ1, . . . , φnq

f pe’, e’q ď δ | f P F
“ tJ{0, K{0, (cid:4){1, _{2, ^{2, ñ{2u,

can be bound.

where f, δ, e’ P E ’, and Q P t@,Du.
reﬁnements of the shape tx :: T | φu. This is a reﬁnement

Relational types extend simple types by means of relational

C

type that uses a relational assertion φ stating some rela-
tional property that the inhabitants of this type have to
satisfy. Relational assertions are ﬁrst order formulas over
some basic predicates: ΔD
on a speciﬁc f -divergence, f P F asserting that f
and e’ “ e’ and e’ ď e’ for the equality and inequal-

is a
convex function meeting the requirements of Deﬁnition 3.6,

f pe’, e’q ď δ asserting a bound

ity of relational expressions, respectively. Relational types

73(cid:2)Γ $ return e : Mrτs(cid:3)θ “ 1(cid:2)e(cid:3)θ

(cid:2)Γ $ mlet x “ e1 in e2 : Mrσs(cid:3)θ “ d ÞÑ

(cid:2)Γ $ observe x ñ t in u : Mrτs(cid:3)θ “ d ÞÑ (cid:2)u(cid:3)θpdq ¨ p(cid:2)t(cid:3)
p(cid:2)u(cid:3)θpgq ¨ p(cid:2)t(cid:3)

θd
x

ptrueqq

ptrueqqq

θg
x

ÿ

gP(cid:2)τ (cid:3)θ

(cid:2)Γ $ bernoullipeq : DrBs(cid:3)θ “ bernoullip(cid:2)e(cid:3)θq

(cid:2)Γ $ ranpbernoullipeqq : MrBs(cid:3)θ “ d ÞÑ

ÿ

gP(cid:2)τ (cid:3)θ

p(cid:2)e1(cid:3)θpgq ¨ (cid:2)e2(cid:3)

θg
x

pdqq

(cid:2)Γ $ infer e : Drτs(cid:3)θ “ AlgInf e

#
(cid:2)e(cid:3)θ
1 ´ (cid:2)e(cid:3)θ

if d “ true

otherwise

Figure 3: Interpretation for some of PCFp expressions (selected rules).

Mf,δrtx :: T | φus, for T P trτ , Drrτsu, and it corresponds to

also reﬁnes the probability monad. This has now the shape

a polymonad [26] or parametric eﬀect monads [27] where f
and δ are parameters useful to reason about f -divergences.
Relational expressions can appear in relational reﬁnements
and in the parameters of the probabilistic monad, so the
usual arrow type constructor is replaced by the dependent

type constructor Πpx :: Tq. S.
Before introducing the typing rule of PrivInfer we need to
introduce some notation. A relational environment G is a
ﬁnite sequence of bindings px :: Tq such that each variable
x is never bound twice and only relational variables from
XR are bound. We write xG for the type of x in G. We will
denote by |¨| the type erasure from relational types to simple
the notation }¨}, to describe the following map from relational
environments to environments xs}G} “ x|G| iﬀ x P dompGq,
where s P tŸ,Źu. For example, given a relational binding
px :: Tq, we have }px :: Tq} “ xŸ : |T|, xŹ : |T|.
proves typing judgment of the form G $ e1 „ e2 :: T . We will
use Γ $ e :: T as a shorthand for Γ $ e „ e :: T . Several of

types and its extension to environments. We will use instead

We can now present our relational type system. PrivInfer

the typing rules of PrivInfer come from the system proposed
by Barthe et al. [6]. We report some of them in Figure 5.
We also extend the subtyping relation of Barthe et al. [6]
with the rule for monadic subtyping in Figure 4.

We present the rules that are speciﬁc to PrivInfer in Fig-
ure 5. The rules UnitM and BindM correspond to the unit and
the composition of the probabilistic polymonad. These are
similar to the usual rule for the unit and composition of mon-
ads but additionally they require the indices to well behave.

In particular pf1, f2q are required to be f3 composable to

guarantee that the composition is respected. The rules Infer
and Ran are similar to their simply typed version but they
also transfer the information on the f -divergence from the
indices to the reﬁnements and viceversa. The rule Observe
requires that the sub-expressions are well typed relationally
and it further requires the validity of the assertion:

}G} $ Δfpobserve xŸ ñ eŸ in e

Ÿ, observe xŹ ñ eŹ in e
1

Źq ď δ
1

2

2

for the δ
that can then be used as a bound for the f -
divergence in the conclusion. This assertion may be surpris-
ing at ﬁrst, since it doesn’t consider the bounds δ and δ
for
the sub-expressions but instead requires to provide directly a
bound for the conclusion—it is not really compositional. The
reason for this presentation is that a generic bound in term
of δ and δ
would be often too large to say something useful

1

1

G $ T ĺ U
}G} $ fi P F
@θ. θ ( G, x :: T ñ (cid:2)f1 ď f2 ^ δ1 ď δ2 ă 8(cid:3)θ

}G} $ δi : R

`

G $ Mf1,δ1rTs ĺ Mf2,δ2rUs

S-M

Figure 4: Relational Subtyping (rule for monadic subtyping)

about the conclusion.
In fact, estimating bounds on the
f -divergence of conditional distributions is a hard task and
often it is only expressed in term of prior perturbations [14].
So, instead we prefer to postpone the task to verify a bound
to the concrete application where a tighter bound can be
found with some calculation. We will see some uses of this
rule in the examples in § 6.
5.2 Relational Interpretation

We want now to give a relational interpretation of relational
types so that we can prove the soundness of the relational type
system of PrivInfer. Before doing this we need to introduce
an important component of our interpretation, the notion of

pf, δq-lifting of a relation, inspired from the relational lifting

of f -divergences by Barthe and Olmedo [2].

T2, let f be a convex function providing an f -divergence and

Deﬁnition 5.1 (pf, δq-Lifting of a relation Ψ). Let Ψ Ď T1ˆ
let δ P R`
. Then, we have that μ1 P MrT1s and μ2 P MrT2s
are in the pf, δq-lifting of Ψ, denoted Lpf,δqpΨq iﬀ there exist
two distributions μL, μR P MrT1 ˆ T2s such that
1. μi pa, bq ą0 implies pa, bq P Ψ, for i P tL, Ru,
2. π1 μL “ μ1 ^ π2 μR “ μ2, and
ř
3. ΔfpμL, μRq ď δ.
y μpx, yq and π2 μ “ λy.

where π1 μ “ λx.
We will call the distributions μL and μR the left and right

ř
x μpx, yq.

witnesses for the lifting, respectively.

This notion of lifting will be used to give a relational
interpretation of monadic types. We say that a valuation θ

validates a relational environment G, denoted θ ( G, ifθ (
}G} and @x P dompGq, pxŸθ, xŹθq P (cid:4)xG(cid:5)θ. The relational
interpretation (cid:2)φ(cid:3)θ P tJ,Ku of assertions φ with respect to
a valuationθ ( Γ is an extension of the the one provided

in Barthe et al. [6]. In Figure 7 we provide the extensions
speciﬁc to PrivInfer. Notice that we interpret the assertion
f pe’
ΔD

2 q ď δ with the corresponding f -divergence.

1 , e’

74UnitM

BindM

Observe

}G} $ f P F

}G} $ δ : R

`

G $ return e :: Mf,δrTs

G $ e :: T

Infer

pf1, f2q are f3-composable
G, x :: T1 $ e2 :: Mf2,δ2rT2s

G $ mlet x “ e1 in e2 :: Mf3,δ1`δ2rT2s

G $ e1 :: Mf1,δ1rT1s
G $ Mf2,δ2rT2s
G, x :rτ $ e

1

:: Mf,δ1rty :: B | yŸ “ yŹus

G $ e :: Mf,δrty ::rτ | yŸ “ yŹus

:: Mf,δ2rty ::rτ | yŸ “ yŹus
}G} $ Δfpobserve xŸ ñ eŸ in e

G $ observe x ñ e in e
1

Figure 5: PrivInfer Relational Typing Rules

G $ e : Mf,δrtx ::rτ | xŸ “ xŹus
G $ inferpeq : tx :: Drrτs |Δ D
f pxŸ, xŹq ď δu
G $ e : tx :: Drrτs | ΔD
G $ ranpeq : Mf,δrtx ::rτ | xŸ “ xŹus
f pxŸ, xŹq ď δu
Źq ďδ
1

Ÿ, observe xŹ ñ eŹ in e
1

Ran

2

(cid:2)f P F(cid:3)θ “ (cid:2)f (cid:3)θ P F
p(cid:2)e’
2 q ď δ(cid:3)θ “ Δ(cid:2)f (cid:3)θ

f pe’

(cid:2)ΔD

1 , e’

1 (cid:3)θ, (cid:2)e’

2 (cid:3)θq ď (cid:2)δ(cid:3)θ

)

!
xŸ ÞÑ d1
xŹ ÞÑ d2

Figure 7: Relational interpretation of assertions (added
rules)

θ

)

!
xŸ ÞÑ d1
xŹ ÞÑ d2

(cid:2)φ(cid:3)
θ

pd1, d2q P (cid:4)tx :: T | φu(cid:5)θ

pd1, d2q P(cid:4)T (cid:5)θ

f1, f2 P (cid:2)|T| Ñ |U|(cid:3)

d1, d2 P (cid:2)rτ (cid:3)
pd1, d2q P (cid:4)rτ (cid:5)θ
@pd1, d2q P (cid:4)T (cid:5)θ.pf1pd1q, f2pd2qq P (cid:4)U (cid:5)
pf1, f2q P(cid:4)Πpx :: Tq. U (cid:5)θ
d1, d2 P (cid:2)Drrτs(cid:3)
pd1, d2q P (cid:4)Drrτs(cid:5)θ

μ1, μ2 P (cid:2)Mr|T|s(cid:3)
Lf,δp(cid:4)T (cid:5)θq μ1 μ2
pμ1, μ2q P(cid:4)M f,δrTs(cid:5)θ

Figure 6: Relational interpretation of types

We give in Figure 6 the relational interpretation (cid:4)T (cid:5)θ of
a relational type T with respect to the valuation θ ( }G}.

This corresponds to pairs of values in the standard interpre-
tation of PCFp expressions. To deﬁne this interpretation we
use both the interpretation of relational assertions given in
Figure 7 and the deﬁnition of lifting given in Deﬁnition 5.1.
The interpretation of relational assertions is used in the inter-
pretation of relational reﬁnement types, while the lifting is
used to provide interpretation to the probabilistic polymonad.

Notice that the relational interpretation of a type Drrτs is
of Drrτs. This can then be restricted by using relational

just the set of pairs of values in the standard interpretation

reﬁnement types. We can then prove that the relational re-
ﬁnement type system is sound with respect to the relational
interpretation of types.

Theorem 5.1 (Soundness). If G $ e1 „ e2 :: T , then for
every valuation θ |ù G we have p(cid:2)e1(cid:3)θ, (cid:2)e2(cid:3)θq P (cid:4)T (cid:5)θ.

The soundness theorem above give us a concrete way to

reason about f -divergences.

Corollary 5.1 (f -divergence). If $ e :: Mf,δrty :: τ | yŸ “ yŹus
then for every pμ1, μ2q P (cid:2)e(cid:3) we have Δfpμ1, μ2q ď δ.

Moreover, thanks to the characterization of diﬀerential
privacy in terms of f -divergence given by Barthe and Olmedo
[2] we can reﬁne the previous result to show that PrivInfer
accurately models diﬀerential privacy.

Corollary 5.2 (Diﬀerential Privacy). If $ e :: tx :: σ |
Φu Ñ M-D,δty :: τ | yŸ “ yŹu then (cid:2)e(cid:3) is p, δq-diﬀerentially
private w.r.t. adjacency relation (cid:2)Φ(cid:3).

6. EXAMPLES

In this section we show how we can use PrivInfer to guaran-
tee diﬀerential privacy for Bayesian learning by adding noise
on the input, noise on the output using (cid:2)1 norm, and noise
on the output using f -divergences. We will show some of
these approaches on three classical examples from Bayesian
learning: learning the bias of a coin from some observations
(as discussed in § 2), its generalized process, i.e the Dirich-
let/multinomial model and the learning of the mean of a
Gaussian. In all the example we will use pseudo code that
can be easily desugared into the language presented in § 4.
Indeed, the following examples have been type-checked with
an actual tool implementing PrivInfer.
6.1

Input perturbation

Input perturbation: Beta Learning.

Let’s start by revisiting the task of inferring the parameter
of a Bernoulli distributed random variable given a sequence
of private observations. We consider two lists of booleans
with the same length in the adjacency relation Φ iﬀ they
diﬀer in the value of at most one entry. We want to ensure
diﬀerential privacy by perturbing the input. A natural way
to do this, since the observations are boolean value is by
using the exponential mechanism. We can then learn the bias
from the perturbed data. The post-processing property of
diﬀerential privacy ensures that we will learn the parameter
in a private way.

Let’s start by considering the quality score function for
the exponential mechanism. A natural choice is to con-

sider a function score:boolÑboolÑ{0,1} mapping equal

booleans to 1 and diﬀerent booleans to 0. Remember that
the intended reading is that one of the boolean is the one to
which we want to give a quality score, while the other is the
one provided by the observation. The sensitivity of score
is 1. Using this score function we can then create a general
function for adding noise to the input list:

1.

let rec addNoise db eps = match db with

752.
3.
4.

| [] Ñ return ([])
| y::yl Ñ mlet yn = (expMech eps score y) in

mlet yln = (addNoise yl eps) in return(yn::yln)

To this function we can give the following type guaranteeing
diﬀerential privacy.

tl :: B list | lŸ Φ lŹu Ñ t :: R` |“u Ñ M-D,0tb :: B list |“u
where we use tx :: T |“u as a shorthand for tx :: T | xŸ “
xŹu. We will use this shorthand all along this section.

The bulk of the example is the following function that
recursively updates the prior distribution and learns the ﬁnal
distribution over the parameter.

1. let rec learnBias dbn prior = match dbn with
2. | [] Ñ prior
3. | d::dbs Ñ observe
4.
5.

(fun r Ñ mlet z = ran bernoulli(r) in return (d=z))

(learnBias dbs prior)

The likelihood given in Line 4 is the formal version of the
one we presented in§ 2. The function learnBias can be typed
in diﬀerent ways depending on what is our goal. For this
example we can assign to it the following type:

tl :: B list |“u Ñ MSD,0tx :: r0, 1s |“u Ñ MSD,0tx :: r0, 1s |“u

The reading of this type is that if learnBias takes two lists
of observations that are equal and two prior that are equal,
then we obtain two posterior that are equal. Thanks to
this we can type the occurrence of observe in line 3-4 using
a trivial assertion. Here we use the SD divergence but in

fact this would also hold for any other f P F. In particular,

this type allows us to compose it with addNoise using an
mlet. This type also reﬂects the fact that the prior is public.
We can then compose these two procedures in the following
program:

1. let main db a b eps = mlet noisyDB = (addNoise db eps)
2.

in return(infer (learnBias noisyDB (ran (beta(a,b)))))

Notice that in line 2 we use infer for learning from the noised
data. We can then assign to main the type

tl :: B list | lŸ Φ lŹu Ñ ta :: R` |“u Ñ tb :: R` |“u Ñ
t :: R` |“u Ñ M-D,0td :: Drr0, 1ss |“u

which guarantees us that the result is  diﬀerentially pri-
vate. Notice that the result type is a polymonadic type

over Drr0, 1ss. This because we are releasing the symbolic

distribution.

Input perturbation: Normal Learning.

An example similar to the previous one is learning the
mean of a gaussian distribution with known variance: kv,
from a list of real number observations—for instance some
medical parameters like the level of LDL of each patient. We
consider two lists of reals with the same length adjacent when
the (cid:2)1 distance between at most two elements (in the same
position) is bounded by 1. To perturb the input we may now
want to use a diﬀerent mechanism, for example we could use
the Gaussian mechanisms—this may give reasonable results
if we expect the data to come from a normal distribution.
Also in this case, the sensitivity is 1. The addNoise function
is very similar to the one we used in the previous example:

1.
2.
3.
4.

let rec addNoise db eps delta = match db with

| [] Ñ return ([])
| y::yl Ñ mlet yn = (gaussMech (sigma eps delta) y) in

mlet yln = (addNoise yl eps delta) in return(yn::yln)

The two diﬀerences are that now we also have delta as input
and that in line 3 instead of the score function we have a
function sigma computing the variance as in Deﬁnition 3.4.
The inference function become instead the following.

| [] Ñ prior
| d::dbs Ñ observe (fun (r: real) Ñ

1. let rec learnMean dbn prior = match dbn with
2.
3.
4.
5.
6. let main db hMean hVar eps delta =
7.
8.
9.

return(infer (learnMean noisyDB

mlet noisyDB = (addNoise db eps delta)in

(learnMean dbs prior) in

mlet z = ran normal(r, kv) in return (d=z))

(ran (normal(hMean,hVar)))))

Composing them we get the following type guaranteeing

p, δq-diﬀerential privacy.

tl :: R list | lŸ Φ lŹu Ñ ta :: R` |“u Ñ tb :: R` |“u Ñ
t :: R` |“u Ñ tδ :: R` |“u Ñ M-D,δtd :: DrRs |“u

6.2 Noise on Output with (cid:2)1-norm

We present examples where the privacy guarantee is achieved

by adding noise on the output. For doing this we need to
compute the sensitivity of the program. In contrast, in the
previous section the sensitivity was evident because directly
computed on the input. As discussed before we can compute
the sensitivity with respect to diﬀerent metrics. Here we
consider the sensitivity computed over the (cid:2)1-norm on the
parameters of the posterior distribution.

Output parameters perturbation: Beta Learning.

The main diﬀerence with the example in the previous
section is that here we add Laplacian noise to the parameters
of the posterior.

let d = infer (learnBias db (ran beta(a,b))) in

1. let main db a b eps=
2.
3.
4. mlet aPn = lapMech(eps, aP) in
5. mlet bPn = lapMech(eps, bP) in
6.

let (aP, bP) = getParams d in

return beta(aPn, bPn)

In line 2 we use the function learnBias from the previous
section, while in line 4 and 5 we add Laplace noise. The
formal sensitivity analysis is based on the fact that the
posterior parameters are going to be the counts of true and
false in the data respectively summed up to the parameters
of the prior. This reasoning is performed on each step of
observe. Then we can prove that the (cid:2)1-norm sensitivity of
the whole program is 2 and type the program with a type
guaranteeing 2-diﬀerentially privacy.

tl :: B list | lŸ Φ lŹu Ñ ta :: R` |“u Ñ tb :: R` |“u Ñ
t :: R` |“u Ñ M2-D,0td :: Drr0, 1ss |“u

Output parameters perturbation: Normal Learning.

For this example we use the same adjacency relation of
the example with noise on the input where in particular the

76where uk “`

hV 2 ` n

1

˘´1

number of observation n is public knowledge. The code is
very similar to the previous one.
1.let main db hM hV kV eps =
2. let mDistr = infer (learnMean db (ran normal(hM,kV))) in
3.
4. mlet meanN = lapMech(eps/s mean) in
5.
let d = normal(meanN, uk) in return(d)

let mean = getMean mDistr in

kv2

calculations this can be bound by s “ hV

. Notice that we only add noise
to the posterior mean parameter and not to the posterior
variance parameter since the latter doesn’t depend on the
data but only on public information. The diﬃculty for
verifying this example is in the sensitivity analysis. By some
kv`hV where kv is the
known variance of the gaussian distribution whose mean we
are learning and hV is the prior variance over the mean. We
use this information in line 4 when we add noise with the
Laplace mechanism. By using this information we can give
the following type to the previous program:

tl :: R list | lŸ Φ lŹu Ñ thM :: R |“u Ñ thV :: R` |“u Ñ
tkv :: R` |“u Ñ t :: R` |“u Ñ Ms-D,0td :: DrRs |“u

6.3 Noise on Output using f-divergences

We now turn to the approach of calibrating the sensitivity
according to f -divergences. We will consider once again
the example for learning privately the distribution over the
parameter of a Bernoulli distribution, but diﬀerently from
the previous section we will add noise to the output of the
inference algorithm using the exponential mechanism with
a score function using an f -divergence. So, we perturb the
output distribution and not its parameters.

We will use Hellinger distance as a metric over the output
space of our diﬀerentially private program, but any other
f -divergence could also be used. The quality score function
for the exponential mechanism can be given a type of the
shape:

tl :: B list | lŸ Φ lŹu Ñ td :: Drτs |“u Ñ tr :: R | |rŸ´rŹ| ďρu

where the bound ρ express its sensitivity. Now we can use
as a score function the following program

score (db, prior) out = -(H (infer (learnBias db prior)) out)

This quality score uses a function H computing the Hellinger
distance between the result of the inference and a potential
output to assign it a score. The closer out is to the real
distribution (using Hellinger distance), the higher the scoring
is. If we use the exponential mechanism with this score we
achieve our goal of using the Hellinger to “calibrate the noise”.
Indeed we have a program:

let main prior obs eps = expMech eps score (obs, prior)

To which we can assign type:

MHD,0tx :: r0, 1s |“u Ñ t(cid:2) :: B list | (cid:2)Ÿ Φ (cid:2)Źu

Ñ t :: R` |“u Ñ Mρ-D,0td :: Drr0, 1ss |“u

Concretely, to achieve this we can proceed by considering
ﬁrst the code for learnBias:
1.let rec learnBias db prior = match dbn with
2.| [] Ñ prior
3.| d::dbs Ñ mlet rec = (learBias dbs prior) in observe
4.

(fun r Ñ mlet z = ran bernoulli(r) in return (d=z)) rec

To have a bound for the whole learnBias we need ﬁrst to
give a bound to the diﬀerence in Hellinger distance that two
distinct observations can generate. This is described by the
following lemma.

a
Lemma 6.1. Let d1, d2 : B with d1Φd2. Let a, b P R`
Prpξq “ Betapa, bq. Then ΔHDpPrpξ | d1q, Prpξ | d2qq ď
1 ´ π

“ ρ.

. Let

4

Using this lemma we can then type theobserve statement
with the bound ρ. We still need to propagate this bound
to the whole learnBias. We can do this by using the adja-
cency relation which imposes at most one diﬀerence in the
observations, and the data processing inequality Theorem 3.1
guaranteeing that for equal observations the Hellinger dis-
tance cannot increase. Summing up, using the lemma above,
the adjacency assumption and the data processing inequality
we can give to learnBias the following type:

tl :: B list | lŸ Φ lŹu Ñ MHD,0tx :: r0, 1s |“u

Ñ MHD,ρtx :: r0, 1s |“u

This ensures that starting from the same prior and observing
l1 and l2 in the two diﬀerent runs such that l1 Φ l2 we
can achieve two beta distributions which are at distance at
most ρ. Using some additional reﬁnement for infer and H we
can guarantee that score has the intended type, and so we

can guarantee that overall this program is pρ, 0q-diﬀerential

privacy.

The reasoning above is not limited to the Hellinger distance.

For instance the following lemma:

a
Lemma 6.2. Let d1, d2 : B with d1Φd2. Let a, b P R`
Let Prpξq “ Betapa, bq. Then ΔSDpPrpξ | d1q, Prpξ | d2qq ď
2p1 ´ π

q “ ζ.

.

4

gives a type in term of statistical distance:

tl :: B list | lŸ Φ lŹu Ñ MSD,0tx :: r0, 1s |“u

Ñ MSD,ζtx :: r0, 1s |“u

The choice of which metric to use is ultimately left to the
user.

This example easily extends also to the Dirichlet example.
Indeed, Lemma 6.1 can be generalized to arbitrary Dirichlet
distributions:

Lemma 6.3. Let k P Ně2, d1, d2 : rks list with d1Φd2.
Let a1, a2, . . . , ak P R`

Then ΔHDpPrpξ | d1q, Prpξ | d2qq ďa

. Let Prpξq “ Dirichletpa1, a2, . . . , akq.

1 ´ π

“ ρ.

4

Using this lemma we can assign to the following program:

1.let rec learnP db prior = match dbn with
2.| [] Ñ prior
3.| d::dbs Ñ mlet rec = (learnP dbs prior) in observe
4. (fun r sÑ mlet z = ran multinomial(r,s) in
5.

return (d=z)) rec

the type:

tl :: r3s list | lŸ Φ lŹu Ñ MHD,0tx :: r0, 1s2 |“u

Ñ MHD,ρtx :: r0, 1s2 |“u

Similarly to the previous example we can now add noise
to the output of the inference process using the sensitivity

with respect to the Hellinger distance and obtain a pρ, 0q-

diﬀerential privacy guarantee.

777. RELATED WORK

Differential privacy and Bayesian inference. Our sys-
tem targets programs from the combination of diﬀerential
privacy and Bayesian inference. Both of these topics are ac-
tive areas of research, and their intersection is an especially
popular research direction today. We brieﬂy summarize the
most well-known work, and refer interested readers to sur-
veys for a more detailed development (Dwork and Roth [16]
for diﬀerential privacy, Bishop [9] for Bayesian inference).

Blum et al. [10] and Dwork et al. [17] proposed diﬀerential
privacy, a worst-case notion of statistical privacy, in a pair
of groundbreaking papers, initiating intense research interest
in developing diﬀerentially private algorithms. The original
works propose the Laplace and Gaussian mechanisms that
we use, while the seminal paper of McSherry and Talwar [30]
introduces the exponential mechanism. Recently, researchers
have investigated how to guarantee diﬀerential privacy when
performing Bayesian inference, a foundational technique in
machine learning. Roughly speaking, works in the literature
have explored three diﬀerent approaches to guaranteeing
diﬀerential privacy when the samples are private data. First,
we may add noise directly to the samples, and then perform
inference as usual [38]. Second, we may perform inference on
the private data, then add noise to the parameters themselves
[40]. This approach requires bounding the sensitivity of the
output parameters when we change a single data sample,
relying on speciﬁc properties of the model and the prior
distribution. The ﬁnal approach involves no noise during
inference, but outputs samples from the posterior rather
than the entire posterior distribution [15, 40, 41]. This last
approach is highly speciﬁc to the model and prior, and our
system does not handle it, yet.

Formal veriﬁcation for differential privacy. In parallel
with the development of private algorithms, researchers in
formal veriﬁcation have proposed a wide variety of tech-
niques for verifying diﬀerential privacy. For a comprehensive
discussion, interested readers can consult the recent survey
by Barthe et al. [8]. Many of these techniques rely on the
composition properties of privacy, though there are some ex-
ceptions [7]. For a brief survey, the ﬁrst systems were based
on runtime veriﬁcation of privacy [29]. The ﬁrst systems for
static veriﬁcation of privacy used linear type systems [21, 33].
There is also extensive work on relational program logics for
diﬀerential privacy [2–4], and techniques for verifying privacy
in standard Hoare logic using product programs [5]. None of
these techniques have been applied to verifying diﬀerential
privacy of Bayesian inference. Our system is most closely
related to HOARe2, a relational reﬁnement type system that
was recently proposed by Barthe et al. [6]. This system has
been used for verifying diﬀerential privacy of algorithms, and
more general relational properties like incentive compatibil-
ity from the ﬁeld of mechanism design. However, it cannot
model probabilistic inference.

Probabilistic programming. Research in probabilistic pro-
gramming has emerged early in the 60s and 70s, and is nowa-
days a very active research area. Relevant to our work is
in particular the research in probabilistic programming for
machine learning and statistics which has been very active
in recent years. Many probabilistic programming languages

have been designed for these applications, including Win-
BUGS [28], IBAL [32], Church [22], Infer.net [31], Tabu-
lar [24], Anglican [36], Dr. Bayes [37]. Our goal is not to
provide a new language but instead is to propose a frame-
work where one can reason about diﬀerential privacy for such
languages. For instance, we compiled programs written in
Tabular [24] intoPrivInfer so that diﬀerential privacy could
be veriﬁed. Another related work is the one by Adams and
Jacobs [1] proposing a type theory for Bayesian inference.
While technically their work is very diﬀerent from ours it
shares the same goal of providing reasoning principles for
Bayesian inference. Our work considers a probabilistic PCF
for discrete distributions. It would be interesting to extend
our techniques to higher-order languages with continuous
distributions and conditioning, by building on the rigorous
foundations developed in recent work [11, 35].

8. CONCLUSION

We have presented PrivInfer, a type-based framework for
diﬀerentially private Bayesian inference. Our framework
allows to write data analysis as functional programs for
Bayesian inference and to add noise to them in diﬀerent ways
using diﬀerent metrics. Besides, our framework allows to
reason about general f -divergences for Bayesian inference.
Future directions include exploring the use of this approach
to guarantee robustness for Bayesian inference and other ma-
chine learning techniques [14], to ensure diﬀerential privacy
using conditions over the prior and the likelihood similar
to the ones studied by Zhang et al. [40], Zheng [41], and
investigating further uses of f -divergences for improving the
utility of diﬀerentially private Bayesian learning. On the
programming language side it would also be interesting to
extend our framework to continuous distributions following
the approach by Sato [34]. We believe that the intersection
of programming languages, machine learning, and diﬀerential
privacy will reserve us many exciting results.

References
[1] R. Adams and B. Jacobs. A type theory for
probabilistic and bayesian reasoning. CoRR,
abs/1511.09230, 2015.

[2] G. Barthe and F. Olmedo. Beyond diﬀerential privacy:

Composition theorems and relational logic for
f-divergences between probabilistic programs. In
ICALP, 2013.

[3] G. Barthe, B. K¨opf, F. Olmedo, and

S. Zanella-B´eguelin. Probabilistic Relational Reasoning
for Diﬀerential Privacy. In POPL, 2012.

[4] G. Barthe, G. Danezis, B. Gr´egoire, C. Kunz, and

S. Zanella B´eguelin. Veriﬁed computational diﬀerential
privacy with applications to smart metering. In CSF,
2013.

[5] G. Barthe, M. Gaboardi, E. J. Gallego Arias, J. Hsu,

C. Kunz, and P.-Y. Strub. Proving diﬀerential privacy
in Hoare logic. In CSF, 2014.

[6] G. Barthe, M. Gaboardi, E. J. G. Arias, J. Hsu,

A. Roth, and P. Strub. Higher-order approximate
relational reﬁnement types for mechanism design and
diﬀerential privacy. In POPL, 2015.

78[7] G. Barthe, M. Gaboardi, B. Gr´egoire, J. Hsu, and P.-Y.

[25] M. Hardt, K. Ligett, and F. McSherry. A simple and

Strub. Proving diﬀerential privacy via probabilistic
couplings. In LICS, 2016.

practical algorithm for diﬀerentially private data
release. In NIPS, 2012.

[8] G. Barthe, M. Gaboardi, J. Hsu, and B. Pierce.

Programming language techniques for diﬀerential
privacy. ACM SIGLOG News, 2016.

[9] C. M. Bishop. Pattern Recognition and Machine

Learning (Information Science and Statistics). 2006.
ISBN 0387310738.

[10] A. Blum, C. Dwork, F. McSherry, and K. Nissim.

Practical privacy: The SuLQ framework. In PODS,
2005.

[11] J. Borgstr¨om, U. D. Lago, A. D. Gordon, and

M. Szymczak. A lambda-calculus foundation for
universal probabilistic programming. In ICFP, 2016.

[26] M. Hicks, G. M. Bierman, N. Guts, D. Leijen, and

N. Swamy. Polymonadic programming. In MSFP, 2014.

[27] S. Katsumata. Parametric eﬀect monads and semantics

of eﬀect systems. In POPL, 2014.

[28] D. J. Lunn, A. Thomas, N. Best, and D. Spiegelhalter.

WinBUGS - A bayesian modelling framework:
Concepts, structure, and extensibility. Statistics and
Computing, 2000.

[29] F. McSherry. Privacy integrated queries: an extensible

platform for privacy-preserving data analysis. In
International Conference on Management of Data,
2009.

[12] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate.

Diﬀerentially private empirical risk minimization. 2011.

[30] F. McSherry and K. Talwar. Mechanism design via

diﬀerential privacy. In FOCS, 2007.

[13] I. Csisz´ar and P. Shields. Information theory and
statistics: A tutorial. Foundations and Trends in
Communications and Information Theory, 2004.

[14] D. K. Dey and L. R. Birmiwal. Robust Bayesian
analysis using divergence measures. Statistics &
Probability Letters, 1994.

[15] C. Dimitrakakis, B. Nelson, A. Mitrokotsa, and B. I. P.
Rubinstein. Robust and Private Bayesian Inference. In
ALT, 2014.

[16] C. Dwork and A. Roth. The algorithmic foundations of

diﬀerential privacy. Foundations and Trends in
Theoretical Computer Science, 2014.

[17] C. Dwork, F. McSherry, K. Nissim, and A. Smith.

Calibrating noise to sensitivity in private data analysis.
In TCC, 2006.

[18] C. Dwork, G. N. Rothblum, and S. P. Vadhan.

Boosting and diﬀerential privacy. In FOCS, 2010.

[19] H. Ebadi, D. Sands, and G. Schneider. Diﬀerential

privacy: Now it’s getting personal. POPL, 2015.

[20] F. Eigner and M. Maﬀei. Diﬀerential privacy by typing

in security protocols. In CSF, 2013.

[21] M. Gaboardi, A. Haeberlen, J. Hsu, A. Narayan, and
B. C. Pierce. Linear dependent types for diﬀerential
privacy. In POPL, 2013.

[22] N. D. Goodman, V. K. Mansinghka, D. M. Roy,
K. Bonawitz, and J. B. Tenenbaum. Church: a
language for generative models. In UAI, 2008.

[23] A. D. Gordon, M. Aizatulin, J. Borgstr¨om, G. Claret,

T. Graepel, A. V. Nori, S. K. Rajamani, and C. V.
Russo. A model-learner pattern for bayesian reasoning.
In POPL, 2013.

[24] A. D. Gordon, T. Graepel, N. Rolland, C. V. Russo,

J. Borgstr¨om, and J. Guiver. Tabular: a schema-driven
probabilistic programming language. In POPL, 2014.

[31] T. Minka, J. Winn, J. Guiver, and D. Knowles.

Infer.NET 2.5, 2012. URL
http://research.microsoft.com/infernet. MSR.

[32] A. Pfeﬀer. IBAL: A Probabilistic Rational

Programming Language. In IJCAI, 2001.

[33] J. Reed and B. C. Pierce. Distance Makes the Types

Grow Stronger: A Calculus for Diﬀerential Privacy. In
ICFP, 2010.

[34] T. Sato. Approximate Relational Hoare Logic for

Continuous Random Samplings. CoRR,
abs/1603.01445.

[35] S. Staton, H. Yang, C. Heunen, O. Kammar, and

F. Wood. Semantics for probabilistic programming:
higher-order functions, continuous distributions, and
soft constraints. In LICS, 2016.

[36] D. Tolpin, J. van de Meent, and F. Wood. Probabilistic

Programming in Anglican. In ECML PKDD, 2015.

[37] N. Toronto, J. McCarthy, and D. V. Horn. Running
Probabilistic Programs Backwards. In ESOP, 2015.

[38] O. Williams and F. McSherry. Probabilistic Inference

and Diﬀerential Privacy. In NIPS, 2010.

[39] J. Zhang, G. Cormode, C. M. Procopiuc, D. Srivastava,

and X. Xiao. PrivBayes: Private data release via
bayesian networks. In SIGMOD, 2014.

[40] Z. Zhang, B. I. P. Rubinstein, and C. Dimitrakakis. On

the Diﬀerential Privacy of Bayesian Inference. In
AAAI, 2016.

[41] S. Zheng. The diﬀerential privacy of Bayesian inference,

2015. URL http://nrs.harvard.edu/urn-3:
HUL.InstRepos:14398533. Bachelor’s thesis, Harvard
College.

79