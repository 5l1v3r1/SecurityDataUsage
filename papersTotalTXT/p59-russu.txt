Secure Kernel Machines against Evasion Attacks

Paolo Russu

Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy

paolo.russu@
diee.unica.it

Ambra Demontis
Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy
ambra.demontis@

diee.unica.it

Battista Biggio
Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy
battista.biggio@

diee.unica.it

Giorgio Fumera
Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy

fumera@diee.unica.it

ABSTRACT
Machine learning is widely used in security-sensitive set-
tings like spam and malware detection, although it has been
shown that malicious data can be carefully modiﬁed at test
time to evade detection. To overcome this limitation, adversary-
aware learning algorithms have been developed, exploiting
robust optimization and game-theoretical models to incorpo-
rate knowledge of potential adversarial data manipulations
into the learning algorithm. Despite these techniques have
been shown to be eﬀective in some adversarial learning tasks,
their adoption in practice is hindered by diﬀerent factors, in-
cluding the diﬃculty of meeting speciﬁc theoretical require-
ments, the complexity of implementation, and scalability
issues, in terms of computational time and space required
during training.
In this work, we aim to develop secure
kernel machines against evasion attacks that are not com-
putationally more demanding than their non-secure counter-
parts. In particular, leveraging recent work on robustness
and regularization, we show that the security of a linear
classiﬁer can be drastically improved by selecting a proper
regularizer, depending on the kind of evasion attack, as well
as unbalancing the cost of classiﬁcation errors. We then
discuss the security of nonlinear kernel machines, and show
that a proper choice of the kernel function is crucial. We also
show that unbalancing the cost of classiﬁcation errors and
varying some kernel parameters can further improve classi-
ﬁer security, yielding decision functions that better enclose
the legitimate data. Our results on spam and PDF malware
detection corroborate our analysis.

Keywords
adversarial machine learning; evasion attacks; secure learn-
ing; kernel methods

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
AISec’16, October 28 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4573-6/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2996758.2996771

Fabio Roli

Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy
roli@diee.unica.it

1.

INTRODUCTION

In recent years, machine learning has been increasingly
used in security-related applications, including spam, mal-
ware and network intrusion detection [1, 4, 6–8, 15, 33].
One of the main reasons is that, thanks to its generaliza-
tion capability, machine learning has the potential to detect
never-before-seen attacks and threats. However, it has been
shown that speciﬁc vulnerabilities of learning algorithms can
be exploited by skilled attackers to mislead learning, i.e., the
learning algorithm can be itself the weakest link in the se-
curity chain.
In one relevant scenario, usually referred to
as evasion, malicious samples are carefully modiﬁed at test
time to evade detection. Another pertinent setting is re-
lated to the so-called poisoning attacks, where the attacker
can inject poisoning samples into the training data used
to learn the classiﬁer, in order to compromise the training
phase [8, 33]. In this paper we restrict our focus on evasion
attacks, and on how to learn more secure classiﬁers against
them, without increasing training complexity.

The intrinsic vulnerabilities of learning algorithms to well-
crafted attacks are essentially rooted on their underlying
stationarity assumption. In particular, learning algorithms
have been originally designed by assuming that training and
testing samples are drawn from the same distribution. How-
ever, such an assumption is clearly violated when attackers
manipulate the input data either at training or testing time.
From a very general theoretical viewpoint, this means that
the class-conditional distribution of malicious samples ob-
served at test time is diﬀerent from that observed at training
time. In the case of evasion attacks, only malicious data at
test time is aﬀected. Thus, by denoting the input samples
with x ∈ X (in a continuous feature space), and their class
labels with y ∈ {−1, +1} (respectively, for legitimate and
malicious samples), this can be formalized as:

(cid:90)

(cid:48)|y = +1) =

pts(x

(cid:48)|x, y = +1)ptr(x|y = +1)dx ,

p(x

x∈X

where x(cid:48) = a(x), being a : X (cid:55)→ X a manipulation func-
tion representing the attack strategy, i.e., deﬁning how the
attacker manipulates the initial malicious data x as x(cid:48) to
evade detection at test time. The term p(x(cid:48)|x, y = +1)
characterizes the probability of having the initial malicious
sample x modiﬁed as x(cid:48), and pts and ptr respectively denote

59the testing and training class-conditional distributions of the
malicious samples. This straightforward model, albeit being
not directly useful to design secure learning algorithms (see,
e.g., [2]), clariﬁes the connection between the manipulation
function a and the adversarial drift that it induces in the
probability distribution of malicious samples.

To account for this potential, adversarial drift between
training and testing distributions, adversary-aware learning
algorithms have been developed, based on robust optimiza-
tion, and probabilistic and game-theoretical models (see,
e.g., [9, 14, 31]). The underlying idea of these algorithms is
that of incorporating knowledge of the potential adversarial
data manipulations into the learning phase, either by simu-
lating such manipulations at training time, through the deﬁ-
nition of suitable manipulation functions a(x), or by model-
ing the distribution drift directly (in generative models). In
practice, both options reﬂect a similar eﬀect, i.e., an adver-
sarial shift of the malicious distribution, as witnessed by the
aforementioned probability model. The only diﬀerence is the
level at which assumptions on the attacker model are made,
i.e., either at the level of each malicious sample, or at the
higher level of their global probability distribution. Clearly,
making assumptions at the sample level allows one to more
ﬁnely deﬁne the potential adversarial data manipulations,
which can be advantageous when application-speciﬁc con-
straints on data manipulation can be accounted for. On
the other hand, secure learning techniques based on this ap-
proach tends to exhibit a much higher training complexity,
especially in terms of computational time and space. This
is one of the main factors that hinders the adoption of these
algorithms in practice, along with the diﬃculty of meeting
some theoretical requirements, and, in some cases, the com-
plexity of their implementation.

In this work, we aim to overcome these limitations by
developing secure kernel machines against evasion attacks
that are not computationally more demanding than their
non-secure counterparts. To this end, we ﬁrst clarify diﬀer-
ences between sparse and dense evasion attacks, depending
on whether it is convenient for the attacker to signiﬁcantly
modify few features, or slightly modify many of them. We
also intuitively explain the source of classiﬁer vulnerability
to evasion attacks, depending on the shape of the decision
function. As an aside contribution of this work, we also
discuss brieﬂy how to evade non-diﬀerentiable classiﬁcation
functions, like those exhibited by decision trees and random
forests, through the use of a surrogate classiﬁer (Sect. 2).
In Sect. 3, we summarize recent ﬁndings on regularization
and robustness properties of learning algorithms [17, 34],
which will subsequently help us to shed light on the role
of regularization and cost-sensitive learning to design more
secure linear classiﬁers, depending on the kind of evasion
attack.
In Sect. 4, we devise some upper bounds on the
worst-case impact of sparse and dense evasion attacks on
linear and nonlinear classiﬁcation functions. Besides con-
ﬁrming the ﬁndings in [17, 34], our analysis also allows us
to investigate robustness properties of nonlinear kernel ma-
chines. We indeed show that a proper choice of the kernel
function is crucial to improve security depending on the hy-
pothesized kind of evasion attack, similarly to the role of
regularization in linear classiﬁers. Our analysis also sug-
gests that unbalancing the cost of classiﬁcation errors and
varying some kernel parameters can further improve classi-
ﬁer security. In Sects. 5-6, we describe some secure (linear

and nonlinear) classiﬁers developed according to our ﬁnd-
ings.
In Sect. 7, we conduct an experimental analysis on
spam and PDF malware detection to corroborate our anal-
ysis. We conclude this paper by discussing related work
(Sect. 8) and future research directions (Sect. 9).

2. EVASION ATTACKS

To analyze possible attacks against machine learning and
devise principled countermeasures in a more systematic man-
ner, a formal model of the attacker has been proposed in [1,
4, 6, 7, 15]. The model relies upon the deﬁnition of the
attacker’s goal, knowledge of the classiﬁer, and capability
of manipulating the input data. Here we formalize evasion
attacks in terms of this model, as done in [4, 7].
Attacker’s goal. In evasion attacks, the goal is to modify
a single malicious sample (e.g., a spam email) to have it
misclassiﬁed as legitimate (with the largest conﬁdence) by
the classiﬁer.
Attacker’s knowledge. The attacker can have diﬀerent
levels of knowledge of the targeted classiﬁer; she may have
limited or perfect knowledge about the training data, the
feature set, and the classiﬁcation algorithm [4, 7]. In this
work, we focus on perfect-knowledge (worst-case) attacks.
Although it may be overly pessimistic to assume that the
attacker knows everything about the targeted system, this
often reveals interesting properties of learning algorithms,
as it highlights the worst possible performance degradation
that may be incurred under attack.
Attacker’s capability.
In evasion attacks, the attacker
can only modify malicious samples, and the amount of fea-
sible manipulations is often bounded, as the malicious data
has to preserve its intrusive functionality. For example, mal-
ware has to embed a valid exploitation code for the attack
to be eﬀective, and spam emails have to remain readable
by humans.
It is thus clear that excessive obfuscation or
manipulation of such samples is not possible without com-
promising the functionality of the attack. This behavior
has been formalized in terms of application-dependent con-
straints in previous work [4, 32]; in particular, in terms of
bounds on the distance between the initial malicious sample
x and the manipulated one x(cid:48) in feature space. As discussed
in [32], two kinds of constraints have been mostly used when
modeling real-world adversarial settings, leading one to de-
ﬁne sparse ((cid:96)1) and dense ((cid:96)2) attacks. Bounding the (cid:96)1
distance between x and x(cid:48) yields a sparse attack, as it rep-
resents the case when the cost depends on the number of
modiﬁed features. For instance, when instances correspond
to text (e.g., the email’s body) and each feature represents
the occurrences of a given term in the text, the attacker
usually aims to change as few words as possible. Instead,
the (cid:96)2 distance yields a dense attack, as it represents the
case when the cost of modifying features is proportional to
the distance between the original and modiﬁed sample in
Euclidean space. For example, when considering images as
the input data, usually the attacker prefers making small
changes to many or even all pixels, rather than signiﬁcantly
modifying only few of them. This amounts to only slightly
blurring the image, instead of obtaining a salt-and-pepper
noise eﬀect (as produced by sparse attacks), and the ﬁnal
eﬀect is less visible to the human eye (as we will show in the
examples of manipulated handwritten digits in Sect. 7).
Attack strategy. Having deﬁned the attacker’s goal, knowl-

60Figure 1: Evasion attacks against diﬀerent classiﬁers, trained on blue (legitimate) and red (malicious) samples.
A linear SVM classiﬁer against sparse (ﬁrst plot) and dense (second plot) evasion attacks, an SVM with the
RBF kernel (third plot) and a random forest (fourth plot) against sparse evasion attacks. The initial malicious
point x is found at the center of the distance constraint, while the evasion sample x(cid:63) is denoted with a green
star. For each classiﬁer, g(x) values are shown in colors, and the black line denotes the decision boundary.

edge and capability, one can ﬁnally formalize the attack
strategy, i.e., the procedure for obfuscating malicious data
to evade detection, in terms of an optimization problem.
Let us denote the legitimate and malicious class labels re-
spectively with −1 and +1, and assume that the classiﬁer’s
decision function is f (x) = sign (g(x)), where g(x) ∈ R is
the classiﬁers’ linear discriminant function, and x is the rep-
resentation of a sample in a d-dimensional feature space. For
example, for linear classiﬁer, g(x) = w(cid:62)x + b ∈ R, where
w ∈ Rd are the feature weights, and b ∈ R is the bias.
Given a malicious sample x, the goal is to ﬁnd the sample
x∗ that minimizes the classiﬁer’s discriminant function g(·)
(i.e., that is classiﬁed as legitimate with the highest possible
conﬁdence) subject to the constraint that x∗ lies within a
distance dmax from x:

∗

x

=

arg min

g(x

)

(cid:48)

(1)

x(cid:48)
, x) ≤ dmax , xlb (cid:22) x
(cid:48)
d(x

(cid:48) (cid:22) xub ,

s.t.

(2)
where xlb (cid:22) x(cid:48) (cid:22) xub represent a box constraint (as features
are often normalized onto a compact domain), and the dis-
tance measure d(·,·) is deﬁned in terms of the cost of data
manipulation (e.g., the number of modiﬁed words in each
spam) [4, 7, 13, 21, 36]. Sparse and dense evasion attacks
are simply deﬁned based on whether d(·,·) corresponds to
the (cid:96)1 or to the (cid:96)2 distance, respectively.
Solving the evasion problem. Depending on the kind of
decision function and distance metric, Problem (1)-(2) can
be casted in terms of a linear or a nonlinear programming
problem.1 For linear classiﬁers, the global minimum can be
found, either in the case of (cid:96)1 or (cid:96)2 constraints. For nonlin-
ear g(x), the solution is typically found at a local minimum
of the objective function. Problem (1)-(2) can be solved
with standard solvers in both cases, although they may not
be very eﬃcient, as they do not exploit speciﬁc knowledge
about the evasion problem (e.g., sparsity of the solution,
compact domain, etc.). We thus devise an ad-hoc solver
based on exploring a descent direction aligned with the gra-
dient of g(x) by means of a bisect line search.
Its basic
structure is given as Algorithm 1. To reduce the number
of iterations, and ensure quick convergence, we explore one

1Note that Problem (1)-(2) becomes linear only for linear
classiﬁers and sparse evasion attacks (using the (cid:96)1 distance).

Algorithm 1 Evasion Attack
Input: x: the malicious sample; x(0): the initial location
of the attack sample; dmax: the maximum distance be-
tween x and x(cid:48) (Eq. 2); xlb, xub: the box constraint
bounds (Eq. 2); : a small positive constant.

i ← i + 1
t(cid:48) = arg mint g(x(i−1) − t∇g(x(i−1))) (line search)
x(i) ← x(i−1) − t(cid:48)∇g(x(i−1))
if constraints in Eq. (2) are violated then

Output: x(cid:48): the evasion attack sample.
1: i ← 0
2: repeat
3:
4:
5:
6:
7:
8:
9: until g(x(i)) − g(x(i−1)) > 
10: return x(i)

Project x(i) onto the feasible domain

end if

feature at a time in the case of (cid:96)1 attacks (starting from the
more promising feature, i.e., the one exhibiting the highest
gradient variation), as the solution will be sparse. Con-
versely, we simultaneously explore all the features in the
case of (cid:96)2 attacks, as the solution will be likely to modify
all feature values. We also minimize the number of gradient
and function evaluations to further speed up our evasion al-
gorithm; e.g., we only re-compute the gradient of g(x) when
no better point is found on the descent direction under ex-
ploration. Finally, in the case of nonlinear g(x), we exploit
multiple initializations for x(0) to mitigate issues related to
the presence of multiple local minima.

Examples of (sparse and dense) evasion attacks against
Support Vector Machines (SVMs) and random forests are
shown in Fig. 1. As random forests have a non-diﬀerentiable
discriminant function g(x), we construct a diﬀerentiable ap-
proximation ˆg(x) by learning a surrogate SVM on the same
training data used to learn the random forest, but replac-
ing the (true) training labels with the classiﬁcation labels
assigned by the random forest to such data. Then, the sur-
rogate SVM can be used to ﬁnd a suitable descent direc-
tion, and run the evasion attack against the random forest.
To our knowledge, only preliminary work has attempted the
evasion of non-diﬀerentiable classiﬁers like decision trees, us-
ing black-box optimization strategies like genetic algorithms

61and greedy descent techniques [16, 35]. Learning a surrogate
(diﬀerentiable) model to solve the evasion problem should be
more computationally eﬃcient, at least in principle.

Understanding classiﬁer security. Observing the shape
of the nonlinear decision functions in Fig. 1 (third and fourth
plot), and the corresponding evasion samples, one may inter-
estingly note that, in some cases (see, e.g., the evasion sam-
ple in the third plot), to evade detection, it suﬃces to create
a sample that is far enough from the known malicious sam-
ples (learned by the classiﬁer during training), without even
mimicking any legitimate sample. In some other cases (see,
e.g., the evasion sample in the fourth plot), the attacker is in-
stead required to mimic the characteristics exhibited by the
legitimate samples, which can be a much harder task in real-
world applications. This reveals an interesting insight on
the security of nonlinear classiﬁers, as also already pointed
out in recent work [3, 4], i.e., that decision functions that
better enclose the legitimate data tend to be more secure
against evasion attacks. In practice, the main vulnerability
of learning algorithms relies upon the fact that, sometimes,
it is possible to evade detection by creating samples which
are far enough from the rest of the training data.2 In pre-
vious work [3], this vulnerability has been referred to as a
vulnerability of the classiﬁcation algorithm. Conversely, if
the classiﬁer only allows evasion if the attack sample is close
enough to the legitimate data, and the attacker can never-
theless construct evasion samples successfully, then the vul-
nerability is related to the feature representation. In fact, if
a legitimate and a malicious sample become indistinguish-
able from each other in terms of their feature values, then
the vulnerability is clearly related to the choice of the feature
representation.

Constructing real-world attack samples. The attack
strategy discussed in this section allows one to ﬁnd an eva-
sion sample in terms of a set of desired feature values. Clearly,
feature mappings in real-world security-related tasks can be
very diﬃcult to reverse-engineer and, accordingly, construct-
ing the corresponding real-world attack samples (e.g., mal-
ware samples) may not be trivial. As widely discussed in
previous work [4, 6, 7, 15], it is clear that this problem de-
mands for application-speciﬁc solutions, and it is thus out
of the scope of this work. On the other hand, in some cases,
feature mappings can be easily inverted and the correspond-
ing samples easily constructed.

3. ROBUSTNESS AND REGULARIZATION
We clarify here the connection between regularization and
input data uncertainty highlighted by the recent ﬁndings
in [17, 20, 23, 30, 34].
In particular, Xu et al. [34] have
considered the following robust optimization problem:

(cid:0)1 − yi(w(cid:62)(xi − ui) + b)(cid:1)

(cid:80)n

i=1

max

min
w,b

u1,..,un∈U

(3)
where (z)+ is equal to z ∈ R if z > 0 and 0 otherwise,
u1, ..., un ∈ U deﬁne a set of bounded perturbations of the
training data {xi, yi}n
i=1 ∈ Rn×{−1, +1}n, and the so-called
uncertainty set U is deﬁned as

,

+

U ∆= {(u1, . . . , un)|(cid:80)n

i=1 (cid:107)ui(cid:107)∗ ≤ c} ,

(4)

2They have been also referred to as blind spots in [24], and
as adversarial examples in recent work related to the evasion
of deep learning algorithms [25, 26].

being (cid:107) · (cid:107)∗ the dual norm of (cid:107) · (cid:107). Typical examples of
uncertainty sets according to the above deﬁnition include (cid:96)1
and (cid:96)2 balls [30, 34].

Problem (3) amounts to minimizing the hinge loss for a
two-class classiﬁcation problem under worst-case, bounded
perturbations of the training samples xi, i.e., a typical set-
ting in robust optimization [17, 20, 23, 30, 34]. Under some
mild assumptions easily veriﬁed in practice (including non-
separability of the training data), the authors have shown
that the above problem is equivalent to the following non-
robust, regularized optimization problem (cf. Th. 3 in [34]):

minw,b c(cid:107)w(cid:107) +(cid:80)n

i=1

(cid:0)1 − yi(w(cid:62)xi + b)(cid:1)

.

(5)

+

This means that, if the (cid:96)2 norm is chosen as the dual norm
characterizing the uncertainty set U, then w is regularized
with the (cid:96)2 norm, and the above problem is equivalent to
a standard SVM. If input data uncertainty is modeled with
the (cid:96)1 norm, instead, the optimal regularizer would be the
(cid:96)∞ regularizer, and vice versa.3

Uncertainty sets and cost-sensitive learning. The
work by Xu et al. [34] only considers uncertainty sets of
the same size, i.e., the same perturbation is applied on both
the legitimate and the malicious class. However, it is clear
that, under evasion, the malicious samples are potentially
aﬀected by a stronger worst-case perturbation than legiti-
mate data. Interestingly, in their recent work, Katsumata
and Takeda [17] have shown that diﬀerent uncertainty sets
can be accounted for on each sample (and thus, on each class
too), by simply modifying the cost of each classiﬁcation er-
ror. This means that it suﬃces to penalize diﬀerently errors
in diﬀerent classes to consider uncertainty sets of diﬀerent
sizes.
In the SVM learning algorithm, this can be simply
accounted for by setting a diﬀerent C value for legitimate
and malicious samples. This in turn suggests that classi-
ﬁer security can be improved by unbalancing the costs of
classiﬁcation errors in diﬀerent classes.

Kernelization. To conclude, note that these ﬁndings mostly
hold for linear classiﬁers. In the case of nonlinear (kernel-
ized) classiﬁers, the authors have essentially repeated their
analysis but considering perturbations directly in the fea-
ture space induced by the kernel function, instead of retain-
ing them in the input space. Although this may be useful to
understand how to regularize nonlinear functions, the result-
ing perturbation in the feature/kernel space depends on the
kernel mapping itself, and it is not trivial to understand how
it could be modiﬁed by the kernel function. For instance,
if one considers an (cid:96)1 perturbation in input space, and an
RBF kernel k(x, z) = exp(−γ(cid:107)x − z(cid:107)2
2), the corresponding
perturbation in the feature/kernel space is likely to become
dense for suﬃciently small γ values. Intuitively, this can be
explained by the fact that a sparse modiﬁcation on an input
sample x tends to aﬀect almost all kernel values computed
between x and the rest of the training data. This analysis
is thus not very helpful in the case of evasion attacks, as the
corresponding perturbations are clearly applied in the input
space. As we will see in the next section, in fact, for nonlin-
ear classiﬁers it is not the choice of the regularization term
that plays a crucial role for improving security, but rather
the selection of a proper kernel function.

3Note that the (cid:96)1 norm is the dual norm of the (cid:96)∞ norm,
and vice versa, while the (cid:96)2 norm is the dual norm of itself.

624. CLASSIFIER SECURITY

4.2 Nonlinear Kernel Machines

We discuss here diﬀerent strategies that can be exploited
to improve security of linear and nonlinear classiﬁers, respec-
tively. Our rationale is to show that the maximum variation
of a classiﬁer’s discriminant function under an evasion at-
tack can be bounded, highlighting the factors that may harm
classiﬁer security, and discussing how to limit their impact.
This will give us a set of guidelines to help designing more
secure learning algorithms against evasion. It is also worth
remarking that, very interestingly, some of the results aris-
ing from our analysis corroborate the ﬁndings discussed in
the previous section for linear classiﬁers.
4.1 Linear Classiﬁers

We start by analyzing the worst-case variation of the dis-
criminant function of a linear classiﬁer under evasion. The
discriminant function of a linear classiﬁer is simply given as
g(x) = w(cid:62)x + b. Assuming that x is an initial malicious
sample, and x(cid:48) the corresponding manipulated evasion sam-
ple, one yields:

∆g = g(x) − g(x

(cid:48)

) = w

(cid:62)

(x − x

(cid:48)

) .

(6)

Note that, from the attacker’s perspective, this variation has
to be maximized to increase chances of successfully evade the
targeted classiﬁer, as the attacker’s strategy in Problem (1)-
(2) aims to minimize g(x(cid:48)).
Sparse Attacks. Under sparse evasion attacks, it is not
diﬃcult to see that ∆g (from Eq. 6) is upper bounded by
the following quantity:

∆g ≤ (cid:107)w(cid:107)∞ × (cid:107)x − x

(cid:48)(cid:107)1 ,

(7)
where we remind the reader that (cid:107)w(cid:107)∞ = maxj=1,...,d |wj|.
In fact, for sparse attacks, the solution x(cid:48) is found by modi-
fying the features that have been assigned the highest abso-
lute weight values (see, e.g., Fig. 1, ﬁrst plot). In the worst
case, the maximum ∆g is attained by modifying the most
relevant feature of a quantity equal to (cid:107)x − x(cid:48)(cid:107)1.
Dense Attacks. Under dense evasion attacks, instead, the
worst-case increase of ∆g corresponds to a linear shift of x
towards the decision boundary (along the opposite direction
to the hyperplane normal w), i.e., x(cid:48) = x − (cid:107)x − x(cid:48)(cid:107)2
w(cid:107)w(cid:107)2
(see Fig. 1, second plot), which implies that:

∆g ≤ w(cid:62)w
(cid:107)w(cid:107)2

× (cid:107)x − x

(cid:48)(cid:107)2 = (cid:107)w(cid:107)2 × (cid:107)x − x

(cid:48)(cid:107)2 .

(8)

The analysis of the worst-case ∆g values for linear classi-
ﬁers highlights two interesting facts. The former is that the
feature values should be bounded, to bound the maximum
variation of the relevant features. This is normally not a
problem, if feature normalization is used, as normalization
techniques often map the input samples onto a compact do-
main. The latter fact is that, under sparse attacks, one
should bound the inﬁnity-norm of w, while under dense at-
tacks, it is better to penalize its (cid:96)2 norm. This means that it
is better to use (cid:96)∞ and (cid:96)2 regularization respectively against
sparse and dense evasion attacks. This novel result in the
context of adversarial learning also conﬁrms the ﬁndings by
Xu et al. [34] related to the relationship between robustness
and regularization of learning algorithms.

g(x) =(cid:80)n

Let us now analyze how to bound the maximum variation

of ∆g for decision functions of the form:

i=1 αik(x, xi) + b ,

(9)
where k : X ×X (cid:55)→ R is the kernel function, and xi’s are the
training samples. For example, for SVMs, the αi’s are not
null only for the support vectors, and positive (respectively,
negative) for malicious (legitimate) samples. The value of
∆g in these cases is simply given as:

n(cid:88)

(cid:0)k(x, xi) − k(x

(cid:48)

, xi)(cid:1) .

∆g =

αi

(10)

i=1

As we aim to obtain decision functions that can poten-
tially enclose the legitimate data (as discussed in Sect. 2),
we focus here on kernels with an exponential form, includ-
ing the RBF and the Laplacian kernel. They are respectively
given by k(x, x(cid:48)) = exp(−γ(cid:107)x − x(cid:48)(cid:107)p
p), with p = 1, 2. The
reason is that such kernels yield decision functions whose
values tend to decrease while getting farther from the train-
ing data, thus yielding enclosing decision functions around
one of the two classes.4 For these kernels, it is not diﬃcult
to see that:

(cid:48)

−γ(cid:107)x(cid:48)−xi(cid:107)p

p ≥ e

−γ(cid:107)x(cid:48)−x(cid:107)p
p e

−γ(cid:107)x−xi(cid:107)p
p ,

k(x

, xi) = e

(11)
where we use the triangle inequality (cid:107)(x(cid:48)−x)+(x−xi)(cid:107)p ≤
(cid:107)x(cid:48) − x(cid:107)p +(cid:107)x− xi(cid:107)p to upper bound the (cid:96)p norm (valid for
p = 1, 2). Substituting the above lower bound for k(x(cid:48), xi)
into Eq. (10), one yields:

∆g ≤ n(cid:88)

(cid:16)

αik(x, xi)

1 − e

−γ(cid:107)x(cid:48)−x(cid:107)p

p

.

(12)

(cid:17)

i=1

Instead, if (cid:107)x(cid:48) − x(cid:107)p

This upper bound reveals some interesting properties about
(cid:80)
the security of nonlinear kernels. First,
it is clear that,
p → ∞, ∆g =
if x(cid:48) = x, ∆g = 0.
i αik(x, xi) and, thus, g(x(cid:48)) = b. This means that, if
b ≥ 0 and x(cid:48) is far enough from the training data, the deci-
sion function encloses the legitimate class, and x(cid:48) is classi-
ﬁed as malicious (and vice versa for b < 0), conﬁrming the
class-enclosing property of such kernels.

Kernel selection. The upper bound in Eq. (12) depends
on (cid:107)x(cid:48) − x(cid:107)p
p. Since sparse and dense evasion attacks tend
to minimize the (cid:96)1 and the (cid:96)2 distance between the same
points, it should be clear that p should be chosen accord-
if the evasion attack is sparse, then one
ingly. Namely,
should select the Laplacian kernel; otherwise,
in case of
dense attacks, the RBF kernel should be preferred. The rea-
son is that such choices will minimize the value of (cid:107)x(cid:48)− x(cid:107)p
p,
i.e., they will enable one to map the evasion samples in
a region of the kernel space which is closer to the non-
manipulated malicious samples (thus yielding a lower varia-
tion of g, and requiring more modiﬁcations to evade detec-
tion). This is an important observation, and it has a similar
eﬀect to the choice of the regularization term for linear clas-
siﬁers; in fact, if one knows whether a sparse or a dense
attack is deemed more likely, then a better regularizer (for

4This has also been discussed in [28], exploiting a probabilis-
tic model deﬁned for open-set recognition, where the goal is
to ﬁnd enclosed decision functions around known training
classes, to be able to detect novel classes at test time.

63Figure 2: Decision boundaries for SVM (ﬁrst plot), I-SVM (second plot), and their cost-sensitive versions,
C-SVM (third plot) and cI-SVM (fourth plot). In the ﬁrst and the second plot, we also report (cid:96)2 and (cid:96)1 balls
over the margin support vectors, to visually clarify why the orientation of the decision hyperplane changes.

Figure 3: Decision boundaries, and g(x) values (in colors), for RBF-SVM (ﬁrst plot), cRBF-SVM (second
plot), and γRBF-SVM (third plot). Note how the classiﬁers in the second and third plot provide a better
enclosing of the legitimate data.

linear classiﬁers) or kernel function (for nonlinear classiﬁers)
can be selected.

dition in SVM learning requires(cid:80)n

Cost-sensitive Learning. Another non-trivial suggestion
coming from Eq. (12) is to set a lower value of the cost
of classiﬁcation for malicious samples. The reason is that
the (absolute) αi values obtained from SVM learning are
bounded by the corresponding value of the SVM parameter
C. Thus, if we set a lower C value for the malicious samples,
their αi values will decrease. This will in turn decrease the
value of ∆g, and thus, the impact of evasion attacks. Simi-
larly, one may think of increasing C for legitimate data, to
further decrease ∆g. Recall however that the balance con-
i=1 αi = 0 and, thus, the
ﬁnal αi values will clearly depend on the data at hand (it
is not generally the case that they will be equal to the cor-
responding C value). Moreover, if one subsequently adjusts
the value of b on a validation set to meet some speciﬁc re-
quirements (e.g., a desired false positive rate), then it may
be even convenient to unbalance costs in a very diﬀerent
way.
It is thus diﬃcult to draw general conclusions from
Eq. (12), despite the fact that cost-sensitive learning may
be useful to shape the decision boundary in a diﬀerent way,
potentially improving security.

a diﬀerent γ to each class, and reduce only that assigned
to the malicious training samples (as they are in turn as-
signed positive αi values). Despite this breaks the positive-
semideﬁniteness and symmetry of the kernel function,
it
could improve classiﬁer security by reducing the maximum
value of ∆g. Although standard SVM learning algorithms
may not converge if the kernel function is not symmetric,
we can still learn a linear SVM in the similarity space in-
duced by our kernel, by essentially using the kernel matrix
as the set of input features. This is a well-known technique
in similarity-based classiﬁcation, which amounts to learning
the SVM on the squared kernel [10, 27].

5. SECURE LINEAR CLASSIFIERS

Here we discuss how to improve security of linear classi-

ﬁers against dense and sparse attacks, respectively.
5.1 Countering Dense Attacks

Based on our discussion in Sect. 4.1, and on the ﬁndings
in [34], one should consider (cid:96)2 regularization to counter (cid:96)2
attacks. Furthermore, as suggested in [17], also using unbal-
anced classiﬁcation costs may improve classiﬁer security.

Kernel correction. Our analysis also suggests that reduc-
ing the value of γ should be beneﬁcial, as it yields smoother
functions. Furthermore, one may also think of assigning

SVM. Accordingly, the SVM learning algorithm, with dif-
ferent values of C for each class, should guarantee a higher
level of security against dense evasion attacks. It ﬁnds w and

−505−505SVM−505−505I-SVM−505−505SVMc-SVM−505−505I-SVMcI-SVM64b by solving the following quadratic programming problem:

minw,b

1

2(cid:107)w(cid:107)2

i=1 ci (1 − yig(xi))+ ,

(13)

2 +(cid:80)n

where ci = C+ (ci = C−) for malicious (legitimate) data.
5.2 Countering Sparse Attacks

For sparse attacks, our analysis, as that in [17, 34], sug-
gests the use of (cid:96)∞ regularization, potentially with unbal-
anced costs.

Inﬁnity-norm SVM (I-SVM). We thus consider the SVM
formulation, but changing the regularization term:
i=1 ci (1 − yig(xi))+ ,

(cid:107)w(cid:107)∞ +(cid:80)n

minw,b

(14)

where the ci’s are set as in the SVM case. Diﬀerently from
the SVM learning problem, this one can be solved using a
simpler linear programming approach.

Remark I. Examples of decision boundaries for the con-
sidered classiﬁers are shown in Fig. 2. Note that the eﬀect
of unbalanced classiﬁcation costs tends to shift the decision
boundary farther from the malicious class. Despite this may
yield higher security, it may increase the fraction of mis-
classiﬁed legitimate samples (i.e., the false positive rate).
Therefore, its eﬀectiveness as a valid defense strategy needs
to be empirically assessed in detail, especially in those appli-
cations where keeping the false positive rate low is crucial.

Remark II. Another interesting observation is that the de-
cision hyperplane of the I-SVM tends to yield more evenly-
distributed weight values, i.e., weights that are all equal in
absolute value. This is clear from Fig. 2, as the hyperplane
normal tends to align with a bisect line, i.e., w (cid:117) (1, 1).
This is an important property for the security of linear clas-
siﬁers, empirically validated in previous work [5, 18]. How-
ever, based on the interpretation of robustness and regu-
larization in [34], and on our analysis, we have provided
more theoretically-sound explanations behind the meaning
of “evenly-distributed weights,” and on how to enforce this
behavior with a proper regularizer (instead of exploiting
heuristic techniques).

6. SECURE KERNEL MACHINES

Similarly to the previous section, we consider here secure

kernel machines against dense and sparse attacks.
6.1 Countering Dense Attacks

Based on the discussion in Sect. 4.2, to counter (cid:96)2 at-
tacks, one may train a standard SVM with the RBF ker-
nel (RBF-SVM), potentially using unbalanced classiﬁcation
costs (cRBF-SVM). A further option could be to assign dis-
tinct values of γ for the malicious and legitimate training
samples (γRBF-SVM). Examples of decision boundaries for
these classiﬁers are shown in Fig. 3.
6.2 Countering Sparse Attacks

In the case of sparse attacks, similar considerations can
be made, despite the fact that one should use a Laplacian
kernel. We thus consider the following classiﬁers: SVM with
the Laplacian kernel (Lap-SVM), Lap-SVM with unbalanced
costs (cLap-SVM), and Lap-SVM with diﬀerent γ values for
each class (γLap-SVM).

Secure LLR with unbalanced γ (γSec-LLR). To pro-
vide an example of a secure generative classiﬁer, we consider

the well-known log-likelihood ratio rule, for which the dis-
criminant function is computed as:

g(x) = log p(x|y = +1) − log p(x|y = −1) ,

(15)

where the class-conditional probabilities are estimated through
kernel density estimation, i.e., p(x|y) = 1
i|yi=y k( x−xi
h ).
Clearly, to counter sparse attacks, we select again the Lapla-
cian kernel, and set a higher kernel variance through the
parameter h (i.e., lower γ) for the malicious (positive) class.

ny

(cid:80)

7. EXPERIMENTAL ANALYSIS

We report here our experiments on linear and nonlinear

secure learning algorithms.
7.1 Experiments on Linear Classiﬁers

For linear classiﬁers, we consider dense and sparse evasion
attacks on handwritten digit data, to visually demonstrate
their blurring and salt-and-pepper eﬀect on images. We
then consider an adversarial application example on spam
ﬁltering, against sparse evasion attacks. We ﬁrst discuss the
datasets used in this set of experiments.
Handwritten Digit Classiﬁcation. For this task, we use
the MNIST digit data [19], where each image is represented
by a vector of 784 features, corresponding to its gray-level
pixel values. As in [4], we simulate an adversarial classiﬁ-
cation problem where the digits 8 and 9 correspond to the
legitimate and malicious class, respectively.
Spam Filtering. We consider this task, as spam ﬁltering
is a well-known application subject to adversarial attacks.
Most spam ﬁlters include an automatic text classiﬁer that
analyzes the email’s body text. In the simplest case Boolean
features are used, each representing the presence or absence
of a given term. For our experiments we use the TREC 2007
spam track data, consisting of about 25000 legitimate and
50000 spam emails [11]. We extract a dictionary of terms
(features) from the ﬁrst 5000 emails (in chronological or-
der) using the same parsing mechanism of SpamAssassin,
and then select the 200 most discriminant features accord-
ing to the information gain criterion [29]. We simulate a
well-known (sparse) evasion attack in which the attacker
aims to modify only few terms. Adding or removing a term
amounts to switching the value of the corresponding Boolean
feature [4, 7, 18, 22, 36].
Experimental Setup. In these experiments, we consider
the classiﬁers described in Sect. 5, namely, SVM, cSVM,
I-SVM, cI-SVM. We randomly select 500 legitimate and
500 malicious samples from MNIST dataset, 2500 legiti-
mate and 2500 malicious samples from the Spam dataset,
and equally subdivide them to create a training and a test-
ing set. We optimize the regularization parameter C (or
the cost-sensitive parameters C+, C−) of each SVM through
3-fold cross-validation, maximizing a trade-oﬀ between the
detection rate (i.e., the fraction of correctly-classiﬁed ma-
licious samples, also referred to as true positive rate, TP)
at 1% false positive rate (FP) in the absence of attack, and
under attack (estimated by simulating the attacks on the
validation set, for diﬀerent dmax values).

After classiﬁer training, we perform sparse and dense eva-
sion attacks on all malicious digit testing samples, and sparse
evasion attacks on all malicious spam testing samples, for in-
creasing values of dmax. For the digit data, dmax represents
either the (cid:96)2 or (cid:96)1 distance between the non-manipulated

65Figure 4: Security evaluation curves (TP at FP=1% vs dmax) for the 9-vs-8 digit classiﬁcation task against
dense (ﬁrst plot) and sparse (second plot) evasion attacks, and for the spam ﬁltering data against sparse
evasion attacks (third plot).

and the manipulated image, respectively, for dense and sparse
attacks. In the case of sparse attacks, this corresponds to the
number of gray-level pixel values modiﬁed by the attack. For
spam ﬁltering, it instead represents the number of modiﬁed
words in each spam. We evaluate the corresponding per-
formance in terms of TP at FP=1%, against an increasing
value of dmax (recall that the performance in the absence of
attack corresponds to dmax = 0). We repeat this procedure
ﬁve times, and report the average results on the original and
modiﬁed testing data. The corresponding (averaged) curves
are called security evaluation curves [4, 7].
Results. Results are reported in Fig. 4. The reported se-
curity evaluation curves show that the main improvement
in classiﬁer security is due to the choice of a proper regu-
larizer. In fact, I-SVM (i.e., inﬁnity-norm regularization) is
much more secure than SVM (i.e., (cid:96)2 regularization) against
sparse attacks, and vice versa for dense attacks, conﬁrming
the discussion in Sect. 5. The use of diﬀerent classiﬁcation
costs only introduces a slight improvement in terms of secu-
rity, as it is diﬃcult to achieve an improved level of security
while retaining FP=1%. Images of manipulated digits under
dense and sparse evasion attacks are reported in Fig. 5.
7.2 Experiments on Nonlinear Classiﬁers

For nonlinear classiﬁers, we consider another adversarial
application example involving the detection of malware in
PDF ﬁles, following the detection approach proposed in [12].
PDF Malware Detection. This is another application
that is often targeted by attackers. A PDF ﬁle can host dif-
ferent kinds of contents, like Flash and JavaScript, making it
an appealing vector to convey malware. In fact, such third-
party applications can be exploited by attacker to execute
arbitrary operations. We use here the Lux0r dataset [12],
which consists of 17,782 unique PDF documents embedding
JavaScript code: 12,548 malicious samples and 5,234 benign
samples. The whole dataset is the result of an extensive
collection of PDF ﬁles from security blogs such as “Con-
tagio”, and “Malware don’t need Coﬀee”, analysis engines
such as VirusTotal, and search engines such as Google and
Yahoo. Every ﬁle is represented using 736 features that cor-
respond to the number of occurrences of a predeﬁned set of
Javascript function calls (API), where every API represents
an action performed by one of the objects that are contained
into the PDF ﬁle (e.g., opening another document that is
stored inside the ﬁle). An attacker cannot trivially remove
an API from a PDF ﬁle without corrupting its functional-

Figure 6: Security evaluation curves (TP at FP=1%
vs dmax) for PDF malware detection against sparse
evasion attacks.

ity. Conversely, she can easily add new APIs by inserting
new function calls. For this reason, we simulate this attack
by only considering feature increments (i.e., decrementing a
feature value is not allowed). Accordingly, the most conve-
nient strategy to mislead a malware detector (classiﬁer) is
to insert as many occurrences of a given API as possible,
which is a sparse attack.5
Experimental Setup. We consider the classiﬁers secure
to sparse evasion attacks described in Sect. 6, namely, Lap-
SVM, cLap-SVM, and γLap-SVM. We randomly split the
dataset into training and testing sets of 5,000 samples each,
and optimize the classiﬁers’ parameters as done in the previ-
ous experimental setup. As in [12], we select a subset of 100
features from the training data, by retaining those exhibit-
ing higher values in malicious data, such that mimicking
legitimate samples becomes more diﬃcult.

Results. The results averaged over ﬁve repetitions are re-
ported in Fig. 6.
It is easy to appreciate how the secure
variants of the Lap-SVM algorithms outperform the base-
line algorithm.

8. RELATED WORK

As mentioned in Sect. 1, several adversary-aware learn-
ing algorithms have been proposed to date, each relying on
a diﬀerent model of the attacker. As for linear classiﬁers,

5Despite no upper bound on the number of injected APIs
may be set, we set the maximum value for each API to the
corresponding one observed during training.

020004000600000.20.40.60.81Handwritten digits (sparse attack)TP at FP=1%d max  SVMcSVMI−SVMcI−SVM020040060000.20.40.60.81Handwritten digits (dense attack)TP at FP=1%d max  SVMcSVMI−SVMcI−SVM05101500.20.40.60.81Spam filtering (sparse attack)TP at FP=1%d max  SVMcSVMI−SVMcI−SVM020406080100dmax0.00.20.40.60.81.0TP at FP=1%Lap-SVMcLap-SVMg-Lap-SVMPDF Malware Detection (Sparse Attack)66Figure 5: Original and manipulated handwritten digits at dmax = 3000 by sparse attacks (top row), and at
dmax = 250 by dense attacks (bottom row), against SVM (second column), c-SVM (third column), I-SVM
(fourth column), and cI-SVM (ﬁfth column). Values of g(x) are also reported for each digit and classiﬁer,
conﬁrming that sparse attacks are less eﬀective against I-SVM and cI-SVM, and that dense attacks are less
eﬀective against SVM and cSVM. Note also how the blurring eﬀect induced by dense attacks is more diﬃcult
to spot for humans than the salt-and-pepper noise induced by sparse attacks.

the underlying rationale has been that of devising classiﬁers
with more evenly-distributed feature weights, to intuitively
enforce the attacker to manipulate more feature values to
evade detection [5, 18]. Based on this intuition, diﬀerent
heuristic techniques have been proposed. In this work, high-
lighting connections between robustness and regularization,
and thanks to our analysis in Sect. 4, we have provided a
clearer explanation and a theoretically-sound approach to
devise secure linear classiﬁers. First, we have clariﬁed the
notion of sparse and dense attacks, as done in [32]. Then, we
have shown that inﬁnity-norm regularization is the optimal
solution against worst-case sparse attacks (as the ones only
qualitatively envisioned in [5, 18]).

More complicated approaches have been exploited in [9,
14, 31], relying on game theory and robust optimization to
model interactions between the classiﬁer and the attacker.
The main drawback of these approaches is their increased
training complexity. For example, game-theoretical approaches
(as that advocated in [9]) require simulating the attacks dur-
ing training, and iteratively adjust the classiﬁcation func-
tion. Another issue is that such approaches require speciﬁc
assumptions to be met, to guarantee existence of a unique
equilibrium in the game. For instance, in [9] the objective
function of the attacker is required to be twice diﬀerentiable,
and this is clearly not the case for sparse attacks (since the
(cid:96)1 distance representing the attacker’s cost to modify data
is not diﬀerentiable). This means in turn that this approach
can not deal with sparse attacks, at least in principle. The
underling idea of the work in [14, 31] is instead to consider
a worst-case loss suited to sparse attacks in which features
can be set to their maximum/minimum values. In this case
too, training complexity becomes higher with respect to the
non-robust versions of the same algorithms.

9. CONCLUSIONS AND FUTURE WORK
In this work, we have provided several insights on how to

enforce security of linear and nonlinear classiﬁers, from the
choice of the regularization term and the kernel function, to
the selection of diﬀerent classiﬁcation costs and kernel pa-
rameters. We have developed secure kernel machines that
are not computationally more demanding than their non-
secure counterparts, to reduce the risks associated to evasion
attacks at test time. We believe that this would help over-
coming the intrinsic limitations of current secure learning
algorithms, namely, their strong theoretical requirements,
complexity of implementation, and scalability issues due to
their training complexity, to ﬁnally favor the wide adoption
of more secure learning algorithms in practical settings.

As for future work, we aim to better systematize the state
of the art in secure learning, and to extend our experimen-
tal analysis also to current secure-learning approaches.
It
may be worth considering also application settings in which
the attack can be a combination of sparse and dense attacks,
and try to mitigate their impact by exploiting a convex com-
bination of (cid:96)∞ and (cid:96)2 regularization. As another extension
of this work, we plan to investigate, in a similar manner, the
security properties of learning algorithms against poisoning
attacks. Given that in a poisoning scenario the attacker
can only inject few samples into the training set to mis-
lead learning, poisoning can be considered a sparse attack
(in terms of the number of samples). Thus, an interesting
idea may be to consider an inﬁnity-norm regularization on
the α values of nonlinear kernel machines, such that more
evenly-distributed weights are assigned to the training sam-
ples. This should indeed reduce the impact of each poisoning
(outlying) sample in the training set, making learning more
robust even in the presence of poisoning.

References
[1] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and

J. D. Tygar. Can machine learning be secure? In

original sample510152025510152025SVM g(x)= −0.216510152025510152025cSVM g(x)= −0.158510152025510152025I−SVM g(x)= 0.112510152025510152025cI−SVM g(x)= 0.148510152025510152025original sample510152025510152025SVM g(x)= 0.213510152025510152025cSVM g(x)= 0.242510152025510152025I−SVM g(x)= −0.163510152025510152025cI−SVM g(x)= −0.01851015202551015202567Proc. ACM Symp. Information, Computer and Comm.
Sec., ASIACCS ’06, pages 16–25, New York, NY,
USA, 2006. ACM.

[2] J. Bi and T. Zhang. Support vector classiﬁcation with

input data uncertainty. In Advances in Neural
Information Processing Systems 17, 2004.

[3] B. Biggio, I. Corona, Z.-M. He, P. P. K. Chan,

G. Giacinto, D. S. Yeung, and F. Roli.
One-and-a-half-class multiple classiﬁer systems for
secure learning against evasion attacks at test time. In
F. Schwenker, F. Roli, and J. Kittler, editors, Multiple
Classiﬁer Systems, volume 9132 of Lecture Notes in
Computer Science, pages 168–180. Springer
International Publishing, 2015.

[4] B. Biggio, I. Corona, D. Maiorca, B. Nelson,

N. ˇSrndi´c, P. Laskov, G. Giacinto, and F. Roli.
Evasion attacks against machine learning at test time.
In H. Blockeel, K. Kersting, S. Nijssen, and
F. ˇZelezn´y, editors, European Conference on Machine
Learning and Principles and Practice of Knowledge
Discovery in Databases (ECML PKDD), Part III,
volume 8190 of Lecture Notes in Computer Science,
pages 387–402. Springer Berlin Heidelberg, 2013.

[5] B. Biggio, G. Fumera, and F. Roli. Multiple classiﬁer

systems for robust classiﬁer design in adversarial
environments. Int’l J. Mach. Learn. and Cybernetics,
1(1):27–41, 2010.

[6] B. Biggio, G. Fumera, and F. Roli. Pattern

recognition systems under attack: Design issues and
research challenges. Int’l J. Patt. Recogn. Artif.
Intell., 28(7):1460002, 2014.

[7] B. Biggio, G. Fumera, and F. Roli. Security evaluation
of pattern classiﬁers under attack. IEEE Transactions
on Knowledge and Data Engineering, 26(4):984–996,
April 2014.

[13] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and

D. Verma. Adversarial classiﬁcation. In Tenth ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), pages 99–108,
Seattle, 2004.

[14] A. Globerson and S. T. Roweis. Nightmare at test

time: robust learning by feature deletion. In W. W.
Cohen and A. Moore, editors, Proceedings of the 23rd
International Conference on Machine Learning,
volume 148, pages 353–360. ACM, 2006.

[15] L. Huang, A. D. Joseph, B. Nelson, B. Rubinstein,

and J. D. Tygar. Adversarial machine learning. In 4th
ACM Workshop on Artiﬁcial Intelligence and Security
(AISec 2011), pages 43–57, Chicago, IL, USA, 2011.

[16] A. Kantchelian, J. D. Tygar, and A. D. Joseph.

Evasion and hardening of tree ensemble classiﬁers. In
33rd ICML, volume 48 of JMLR Workshop and
Conference Proceedings, pages 2387–2396. JMLR.org,
2016.

[17] S. Katsumata and A. Takeda. Robust cost sensitive

support vector machine. In G. Lebanon and
S. Vishwanathan, editors, 18th Int’l Conf. on Artiﬁcial
Intelligence and Statistics (AISTATS), volume 38 of
JMLR Workshop and Conference Proceedings, pages
434–443. JMLR.org, 2015.

[18] A. Kolcz and C. H. Teo. Feature weighting for

improved classiﬁer robustness. In Sixth Conference on
Email and Anti-Spam (CEAS), Mountain View, CA,
USA, 2009.

[19] Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,

J. Denker, H. Drucker, I. Guyon, U. M¨uller,
E. S¨ackinger, P. Simard, and V. Vapnik. Comparison
of learning algorithms for handwritten digit
recognition. In Int’l Conf. on Artiﬁcial Neural
Networks, pages 53–60, 1995.

[8] B. Biggio, B. Nelson, and P. Laskov. Poisoning

[20] R. Livni, K. Crammer, A. Globerson, E.-i. Edmond,

attacks against support vector machines. In
J. Langford and J. Pineau, editors, 29th Int’l Conf. on
Machine Learning, pages 1807–1814. Omnipress, 2012.

[9] M. Br¨uckner, C. Kanzow, and T. Scheﬀer. Static

prediction games for adversarial learning problems. J.
Mach. Learn. Res., 13:2617–2654, September 2012.

[10] Y. Chen, E. K. Garcia, M. R. Gupta, A. Rahimi, and
L. Cazzanti. Similarity-based classiﬁcation: Concepts
and algorithms. J. Mach. Learn. Res., 10:747–776,
June 2009.

[11] G. V. Cormack. Trec 2007 spam track overview. In
E. M. Voorhees and L. P. Buckland, editors, TREC,
volume Special Publication 500-274. National Institute
of Standards and Technology (NIST), 2007.

[12] I. Corona, D. Maiorca, D. Ariu, and G. Giacinto.

Lux0r: Detection of malicious pdf-embedded
javascript code through discriminant analysis of API
references. In Proc. 2014 Workshop on Artiﬁcial
Intelligent and Security Workshop, AISec ’14, pages
47–57, New York, NY, USA, 2014. ACM.

and L. Safra. A simple geometric interpretation of
SVM using stochastic adversaries. In JMLR W&CP -
Proc., volume 22 of AISTATS ’12, pages 722–730,
2012.

[21] D. Lowd and C. Meek. Adversarial learning. In Proc.

11th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD), pages
641–647, Chicago, IL, USA, 2005. ACM Press.

[22] D. Lowd and C. Meek. Good word attacks on

statistical spam ﬁlters. In Second Conference on
Email and Anti-Spam (CEAS), Mountain View, CA,
USA, 2005.

[23] T. Miyato, S.-i. Maeda, M. Koyama, K. Nakae, and

S. Ishii. Distributional smoothing with virtual
adversarial training. In International Conference on
Learning Representation, 2016.

[24] B. Nelson, B. I. Rubinstein, L. Huang, A. D. Joseph,
S. J. Lee, S. Rao, and J. D. Tygar. Query strategies
for evading convex-inducing classiﬁers. J. Mach.
Learn. Res., 13:1293–1332, May 2012.

68[32] F. Wang, W. Liu, and S. Chawla. On sparse feature

attacks in adversarial learning. In IEEE Int’l Conf. on
Data Mining (ICDM), pages 1013–1018. IEEE, 2014.

[33] H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert,

and F. Roli. Is feature selection secure against
training data poisoning? In F. Bach and D. Blei,
editors, JMLR W&CP - Proc. 32nd Int’l Conf. Mach.
Learning (ICML), volume 37, pages 1689–1698, 2015.

[34] H. Xu, C. Caramanis, and S. Mannor. Robustness and

regularization of support vector machines. Journal of
Machine Learning Research, 10:1485–1510, July 2009.

[35] W. Xu, Y. Qi, and D. Evans. Automatically evading

classiﬁers. In Proc. 23rd Annual Network &
Distributed System Security Symposium (NDSS). The
Internet Society, 2016.

[36] F. Zhang, P. Chan, B. Biggio, D. Yeung, and F. Roli.
Adversarial feature selection against evasion attacks.
IEEE Transactions on Cybernetics, 46(3):766–777,
2016.

[25] A. M. Nguyen, J. Yosinski, and J. Clune. Deep neural
networks are easily fooled: High conﬁdence predictions
for unrecognizable images. In IEEE Conf. Computer
Vision and Pattern Recognition (CVPR), pages
427–436. IEEE, 2015.

[26] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson,

Z. B. Celik, and A. Swami. The limitations of deep
learning in adversarial settings. In Proc. 1st IEEE
European Symposium on Security and Privacy, pages
372–387. IEEE, 2016.

[27] E. Pekalska, P. Paclik, and R. P. Duin. A generalized

kernel approach to dissimilarity based classiﬁcation.
Journal of Machine Learning Research, 2:175–211,
2001.

[28] W. Scheirer, L. Jain, and T. Boult. Probability

models for open set recognition. IEEE Trans. Patt.
An. Mach. Intell., 36(11):2317–2324, 2014.

[29] F. Sebastiani. Machine learning in automated text

categorization. ACM Comput. Surv., 34:1–47, March
2002.

[30] S. Sra, S. Nowozin, and S. J. Wright. Optimization for

Machine Learning. The MIT Press, 2011.

[31] C. H. Teo, A. Globerson, S. Roweis, and A. Smola.

Convex learning with invariances. In J. Platt,
D. Koller, Y. Singer, and S. Roweis, editors, Advances
in Neural Information Processing Systems 20, pages
1489–1496. MIT Press, Cambridge, MA, 2008.

69