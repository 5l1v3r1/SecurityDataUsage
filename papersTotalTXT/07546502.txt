2016 IEEE Symposium on Security and Privacy
2016 IEEE Symposium on Security and Privacy

A Practical Oblivious Map Data Structure

with Secure Deletion and History Independence

Daniel S. Roche, Adam Aviv, Seung Geol Choi

Computer Science Department
United States Naval Academy

{roche,aviv,choi}@usna.edu

Abstract—We present a new oblivious RAM that supports
variable-sized storage blocks (vORAM), which is the ﬁrst ORAM
to allow varying block sizes without trivial padding. We also
present a new history-independent data structure (a HIRB tree)
that can be stored within a vORAM. Together, this construction
provides an efﬁcient and practical oblivious data structure (ODS)
for a key/value map, and goes further to provide an additional
privacy guarantee as compared to prior ODS maps: even upon
client compromise, deleted data and the history of old operations
remain hidden to the attacker. We implement and measure the
performance of our system using Amazon Web Services, and the
single-operation time for a realistic database (up to 256K entries)
is less than 1 second. This represents a 100x speed-up compared
to the current best oblivious map data structure (which provides
neither secure deletion nor history independence) by Wang et
al. (CCS 14).

I. INTRODUCTION

A. Motivation

Increasingly, organizations and individuals are storing large
amounts of data in remote, shared cloud servers. For sensitive
data, it is important to protect the privacy not only of the
data itself but also of the access to the metadata that may
contain which records have been accessed and when, thereby
revealing properties of the underlying data, even if that data is
encrypted. There are multiple points of potential information
leakage in this setting: an adversary could observe network
communication between the client and server; an adversary
could compromise the cloud itself, observing the data stored
at the server, possibly including mirrored copies or backups;
an adversary could observe the computations performed by
the remote server; the adversary may compromise the locally-
stored client data; or, ﬁnally, the adversary may compromise
the data in multiple ways, e.g., a complete compromise of
both the remotely stored cloud storage and locally-stored client
storage1.

While a complete compromise will inevitably reveal private
data, we seek data storage mechanisms which maximize
privacy while maintaining reasonable, practical efﬁciency,
at any level of compromise. For generality, we assume a
computationally-limited server which may only store and
retrieve blocks of raw data, and we focus on the most basic
(and perhaps most important) data structure: a key/value map.

1We assume an honest-but-curious server throughout, and leave achieving

an ODS with malicious servers as an open problem.

2375-1207/16 $31.00 © 2016 IEEE
© 2016, Daniel S. Roche. Under license to IEEE.
DOI 10.1109/SP.2016.19
DOI 10.1109/SP.2016.19

178
178

Oblivious RAM (ORAM). With a computationally-limited
server,
the access pattern of client-server communication
reveals the entire history of the remote data store. This access
pattern, even if the actual data is encrypted, may leak sensitive
information about the underlying stored data, such as keyword
search queries or encryption keys [1]–[3].

A generic solution to protect against access pattern leakage
is oblivious RAM (ORAM) [4], which obscures the operation
being performed (read/write), the address on which it operates,
and the contents of the underlying data. Any program (with
the possible necessity of some padding) can be executed using
an ORAM to hide the access patterns to the underlying data.
A great number of ORAM schemes have been recently
proposed, most aiming to improve the efﬁciency as it relates
to the recursive index structure, which is typically required
to store the hidden locations of items within the ORAM
(for example [5]–[10] and references therein). However, an
important aspect overlooked by previous work is the size of
data items themselves. The vORAM construction we propose
provides an afﬁrmative answer to the following question:
Can an oblivious RAM hide the size of varying-sized
items, with greater efﬁciency than that achieved by
trivial padding?

it

Oblivious data structure (ODS). Recently, Wang et al. [11]
showed that
is possible to provide obliviousness more
efﬁciently if the speciﬁcs of the target program are considered.
In particular, among other results, Wang et al. achieved an
oblivious data structure (ODS) scheme for a key-value map,
by constructing an AVL tree on a non-recursive ORAM with-
out using the position map. Their scheme requires ˜O(log n)
ORAM blocks of client storage, where n is the maximum
number of allowable data items. More importantly, due to
lack of position map lookups,
the scheme requires only
n) blocks of communication bandwidth, which con-
O(log
stituted roughly an O(log n)-multiplicative improvement in
communication bandwidth over the generic ORAM solution.
We will brieﬂy explain “the pointer-based technique" they
introduced to eliminate the position map in Section I-C.

2

The practicality of oblivious data structures are challenging,
however, owing to the combination of inefﬁciencies in the data
structures compounded with that of the underlying ORAM. In
our experimental results presented in Section VI, and Table I
speciﬁcally, we found that the AVL ODS suffers greatly from

the per-operation
a high round complexity, and also that
bandwidth exceeds the total database size (and hence a trivial
alternative implementation) until the number of entries exceeds
1 million.

Similar observations for ORAMs more generally were made
recently by Bindschaedler et al. [12], who examined existing
ORAM alternatives in a realistic cloud setting, and found
many theoretical results lacking in practice. We ask a related
question for ODS, and answer it in the afﬁrmative with our
HIRB data structure stored in vORAM:

Can an oblivious map data structure be made prac-
tically useful in the cloud setting?

Catastrophic attack.
In the cloud storage scenario, obliv-
iousness will protect the client’s privacy from any observer
of network trafﬁc or from the cloud server itself. However,
if the attacker compromises the client and obtains critical
information such as the encryption keys used in the ODS,
all the sensitive information stored in the cloud will simply be
revealed to the attacker.

We call this scenario a catastrophic attack, and it is im-
portant to stress that this attack is quite realistic. The client
machine may be stolen or hacked, or it may even be legally
seized due to a subpoena.

Considering the increasing incidence of high-proﬁle catas-
trophic attacks in practice (e.g., [13], [14]), and that even
government agencies such the CIA are turning to third-party
cloud storage providers [15], it is important to provide some
level of privacy in this attack scenario. Given this reality, we
ask and answer the following additional question:

Can we provide any privacy guarantee even under
a catastrophic attack?

Speciﬁcally, our vORAM+HIRB construction will provide
strong security for deleted data, as well as a weaker (yet
optimal) security for the history of past operations, after
complete client compromise.

B. Security Requirements

Motivated by the goals outlined previously, we aim to
construct a cloud database system that provides the following
two security properties:

• Obliviousness: The system should hide both the data and
the access patterns from an observer of all client-server
communication (i.e., be an ODS).

• Secure Deletion and History Independence: The system,
in the face of a catastrophic attack, should ensure that
no previously deleted data, the fact that previous data
existed, or the order in which extant data has been
accessed, is revealed to an attacker.

Additionally, we require that the system be practically useful,
meaning it should be more efﬁcient (w.r.t. communication
cost, access time, and round complexity) than previous ODS
schemes, even those that do not necessarily provide secure
deletion nor history independence.

Each required security notion has individually been the
focus of numerous recent research efforts (see Section II). To

179179

the best of our knowledge, however, there is no previous work
that considers all the properties simultaneously. We aim at
combining the security properties from obliviousness, secure
deletion, and history independence into a new, uniﬁed system
for secure remote cloud storage. The previous ODS schemes
do not provide history-independence nor secure deletion and
are inefﬁcient for even small data stores. Previous mechanisms
providing secure deletion or history independence are more
efﬁcient, but do not hide the access pattern in remote cloud
storage (i.e., do not provide obliviousness). And unfortunately,
the speciﬁc requirements of these constructions means they
cannot trivially be combined in a straightforward way.

To better understand the necessity of each of the security

requirements, consider each in kind.
Obliviousness: The network trafﬁc to a remote server reveals
to an attacker, or to the server itself, which raw blocks
are being read and written. Even if the block contents
are encrypted, an attacker may be able to infer sensitive
information from this access pattern itself. Like previous
ODS schemes, our system will ensure this is not the case;
the server-level access pattern reveals nothing about the
underlying data operations that the user is performing.

History independence: By inspecting the internal structure of
the currently existing data in the cloud after a catastrophic
attack, the attacker may still be able to infer information
about which items were recently accessed or the likely
prior existence of a record even if that record was
previously deleted [16]. However, if an ODS scheme
provides perfect history independence, the catastrophic
attacker cannot infer which sequence of operations was
applied, among all the sequences that could have resulted
in the current set of the data items. Interestingly, we
show that it is impossible to achieve perfect history in-
dependence in our setting with a computationally-limited
server; nonetheless, providing (cid:2)-history independence is
still desirable, where only the most recent (cid:2) operations
are revealed but nothing else.

Secure deletion: Given that only bounded history indepen-
dence is possible, the privacy of deleted data must be
considered. It is desirable that the catastrophic attacker
should not be able to guess information about deleted
data. In practice, data deleted from persistent media, such
as hard disk drives, is easily recoverable through stan-
dard forensic tools. In the cloud setting, the problem is
compounded because there is normally no direct control
of how and where data is stored on physical disks, or
backed up and duplicated in servers around the globe.
We follow a similar approach as [17], where secure
deletion is accomplished by re-encrypting and deleting
the old encryption key from local, erasable memory such
as RAM.
C. Our Work
Pointer-based technique. Wang et al. [11] designed an ODS
scheme for map by storing an AVL tree on top of the non-
recursive Path ORAM [9] using the pointer-based technique, in

which the ORAM position tags act as pointers, and the pointer
to each node in the AVL tree is stored in its parent node. With
this technique, when the parent node is fetched, the position
tags of its children are immediately obtained. Therefore, the
position map lookups are no more necessary.

Similarly,

in our ODS scheme, we will overlay a data
structure on a non-recursive ORAM using a pointer-based
technique for building the data structure.

We stress that the non-recursive Path ORAM still remains
the best choice when we would like to embed our data
structure in an ORAM with the pointer-based technique, in
spite of all the recent improvements on ORAM techniques.
This is mainly because all ORAM improvement techniques
consider the setting where an ORAM runs in a stand-alone
fashion, unlike our setting where the ORAM actions,
in
particular with position map lookups, depend on the upper-
layer data structure. In particular, with the non-recursive Path
ORAM, each ORAM operation takes only a single round of
communication between the client and server, since there is
no position map lookup; moreover, each operation transfers
O(log n) blocks where the size of each block can be arbi-
trarily small up to Ω(log n). To compare the non-recursive
Path ORAM with the most recent stand-alone ORAMs, each
operation of the constant communication ORAM [18] transfers
O(1) blocks each of which should be of size Ω(log
n), and it
additionally uses computation-intensive homomorphic encryp-
tions. For Ring ORAM [19], it still refers to the position map,
and although its online stage may be comparable to the non-
recursive Path ORAM, it still has the additional ofﬂine stage.
The non-recursive version of these ORAMs has essentially the
same efﬁciency as the non-recursive Path ORAM.

4

Impracticality of existing data structures. Unfortunately,
no current data structure exists that can meet our security and
efﬁciency requirements:

• It should be a rooted tree. This is necessary, since we
would like to use the pointer-based technique. Because
the positions are randomly re-selected on any access to
that node, the tree structure is important in order to avoid
dangling references to old pointers.

• The height of the tree should be O(log n) in the worst
case. To achieve obliviousness, all operations must ex-
ecute with the same running time, which implies all
operations will be padded to some upper bound that is
dependent on the height of the tree.

• The data structure itself should be (strongly) history-
independent, meaning the organization of nodes depends
only on the current contents, and not the order of op-
erations which led to the current state. As a negative
example, consider an AVL tree, which is not history
independent. Inserting the records A, B, C, D in that
order; or B, C, D, A in that order; or A, B, C, D, E
and then deleting E; will each result in a different state of
the data structure, thereby revealing (under a catastrophic
attack) information on the insertion order and previous
deletions.

180180

To the best of our knowledge,

there is no data struc-
ture satisfying all of the above conditions. Most tree-based
solutions, including AVL trees and B-trees, are not history
independent. Treaps and B-treaps are rooted trees with history
independence, but they have linear height in the worst case.
Skip-lists and B-Skip-lists are history independent and tree-
like, but technically they are not rooted trees and thereby not
amenable to the pointer-based technique. That is, Skip-lists
and B-Skip-lists have multiple incoming links, requiring linear
updates in the ORAM to maintain the pointers and position
tags in the worst case.
HIRB. We developed a new data structure, called a HIRB tree
(history independent, randomized B-tree), that satisﬁes all the
aforementioned requirements. Conceptually, it is a ﬁxed height
B-tree such that when each item is inserted, the level in HIRB
tree is determined by logβ n trials of (pseudorandom) biased
coin ﬂipping where β is the block factor. The tree may split
or merge depending on the situation, but it never rotates. The
ﬁxed height of the tree, i.e. H = 1 + logβ n, is very beneﬁcial
for efﬁciency. In particular, every operation visits at most 2H
nodes, which greatly saves on padding costs, compared to the
ODS scheme of [11] where each AVL tree operation must be
padded up to visiting 3 · 1.44 · lg n nodes.

The HIRB is described more carefully in Section V, with

full details in the appendix.
vORAM. One challenge with HIRB trees is that number of
items that each tree node contains are variable, and in the
unlucky case, it may become too large for an ORAM bucket
to store.

This challenge is overcome by introducing vORAM
(ORAM with variable-size blocks). The design of vORAM
is based on the non-recursive version of Path ORAM where
the bucket size remains ﬁxed, but each bucket may contain as
many variable-size blocks (or parts of blocks) as the bucket
space allows. Blocks may also be stored across multiple
buckets (in the same path).

We observe that the irregularity of the HIRB node sizes can
be smoothed over O(log n) buckets from the vORAM root to
an vORAM leaf, and we prove that the stash size on the client
can still be small ˜O(log n) with high probability. We note that
vORAM is the ﬁrst ORAM that deals with variable size blocks,
and may be of independent interest.

The vORAM is described carefully in Section IV, and the

Finally,

full details are provided in the appendix.
Secure deletion.
for secure deletion, a parent
vORAM bucket contains the encryption keys of both children.
When a bucket is modiﬁed, it is encrypted with a fresh key;
then the encryption keys in the parent are accordingly modi-
ﬁed, which recursively affects all its ancestors. However, we
stress that in each vORAM operation, leaf-to-root refreshing
takes place anyway, and adding this mechanism is bandwidth-
free. Additionally, instead of using the label of each item
directly in HIRB, we use the hash of the label. This way,
we can remove the dependency between the item location in

HIRB and its label (with security proven in the random oracle
model).

Imperfect history independence. Our approach does not
provide perfect history independence. Although the data struc-
ture in the vORAM is history independent, the vORAM is
not. Indeed, in any tree-based or hierarchical ORAM, the
items near the root have been more likely recently accessed as
compared to items near the leaves. The catastrophic adversary
can observe all the ORAM structure, and such leakage breaks
perfect history independence. We show a formal lower bound
for the amount of leakage in Section III.

Experiments and efﬁciency of our scheme.
In order to
empirically measure the performance of our construction, we
ﬁrst performed an analysis to determine the smallest constant
factor overhead to achieve high performance with negligible
likelihood of failure. Following this, we implemented our
system in the cloud with Amazon Web Services as the cloud
provider and compared it to alternatives that provide some,
but not all of the desired security properties. To the best of
our knowledge, there has been no previous work that imple-
ments and tests any ODS system in the actual cloud setting.
As argued in Bindschaedler et al. [12], who independently
compared various ORAM systems in the cloud, it is important
to see how systems work in the actual intended setting. As
comparison points, we compare our system with the following
implementations:

• ORAM+AVL: We reimplemented the ODS map by Wang
et al. [11] that provides obliviousness but not secure
deletion nor history independence.

• SD-B-Tree: We implemented a remotely stored block-
level, encrypted B-Tree (as recommend by the secure
deletion community [17]) that provides secure deletion
but not history independence nor obliviousness.

• Naive approach: We implemented a naive approach that
achieves all the security properties by transferring and
re-encrypting the entire database on each access.

In all cases the remotely stored B-Tree is the fastest
as it requires the least amount of communication cost (no
obliviousness). For similar reasons, vORAM+HIRB is much
faster than the baseline as the number of items grows (starting
from 214 items), since the baseline requires communication
that is linear in the number of items. We also describe a
number of optimizations (such as concurrent connections and
caching) that enables vORAM+HIRB to be competitive with
the baseline even when storing as few as 29 items. It should be
noted, without optimizations, the access time is on the order
of a few seconds, and with optimizations, access times are less
than one second.

Surprisingly, however, the vORAM+HIRB is 20x faster than
ORAM+AVL, irrespective of the number of items, even though
ORAM+AVL does not support history independence or secure
deletion. We believe this is mainly because vORAM+HIRB
requires much smaller round complexity. Two factors drive
the round complexity improvement:

181181

Much smaller height: While each AVL tree node contains
only one item, each HIRB node contains β items on
average, and is able to take advantage of slightly larger
buckets which optimize the bandwidth to remote cloud
storage by storing the same amount of data in trees with
smaller height.

Much less padding: AVL tree operations sometimes get com-
plicated with balancing and rotations, due to which each
operation should be padded up to 3 · 1.44 lg n node
accesses. However, HIRB operations are simple, do not
require rotations, and thus, each operation accesses at
most 2 logβ n nodes.

Although the Path-ORAM bucket for ORAM+AVL is four
times smaller than the vORAM bucket in our implementation,
it affects bandwidth but not the round complexity. The fully
optimized vORAM+HIRB protocol is about 100x faster than
ORAM+AVL. We describe the details of our experiments in
Section VI.
Summary of our contributions. To summarize, the contri-
butions of this paper are:

• New security deﬁnitions of history independence and

secure deletion under a catastrophic attack.

• The design and analysis of an oblivious RAM with

variable size blocks, the vORAM;

• The design and analysis of a new history independent and

randomized data structure, the HIRB tree;

• A lower bound on history independence for any ORAM

construction with sub-linear bandwidth;

• Improvements to the performance of mapped data struc-

tures stored in ORAMs;

• An empirical measurement of the settings and perfor-

• The

mance of the vORAM in the actual cloud setting;
of
vORAM+HIRB system in the actual cloud setting.

and measurement

implementation

the

II. RELATED WORK

We discuss related work in oblivious data structures, history
independence, and secure deletion. Our system builds upon
these prior results and combines the security properties into a
uniﬁed system.
ORAM and oblivious data structures. ORAM protects the
access pattern from an observer such that it is impossible
to determine which operation is occurring, and on which
item. The seminal work on the topic is by Goldreich and
Ostrovsky [4], and since then, many works have focused on
improving efﬁciency of ORAM in both the space, time, and
communication cost complexities (for example [5]–[10] just
to name a few; see the references therein).

There have been works addressing individual oblivious
data structures to accomplish speciﬁc tasks, such as prior-
ity queues [20], stacks and queues [21], and graph algo-
rithms [22]. Recently, Wang et al. [11] achieved oblivious
data structures (ODS) for maps, priority queues, stacks, and
queues much more efﬁciently than previous works or naive
implementation of the data structures on top of ORAM.

Our vORAM construction builds upon the non-recursive
Path ORAM [11] and allows variable sized data items to be
spread across multiple ORAM buckets. Although our original
motivation was to store differing-sized B-tree nodes from the
HIRB, there may be wider applicability to any context where
the size (as well as contents and access patterns) to data needs
to be hidden.

Interestingly, based on our experimental results, we believe
the ability of vORAM to store partial blocks in each bucket
may even improve the performance of ORAM when storing
uniformly-sized items. However, we will not consider this
further in the current investigation.

the current organization of

History independence. History independence of data struc-
tures requires that
the data
within the structure reveals nothing about the prior opera-
tions thereon. Micciancio [23] ﬁrst considered history inde-
pendence in the context of 2-3 trees, and the notions of
history independence were formally developed in [24]–[26].
The notion of strong history independence [24] holds if for
any two sequences of operations,
the distributions of the
memory representations are identical at all time-points that
yield the same storage content. Moreover, a data structure is
strongly history independent if and only if it has a unique
representation [25]. There have been uniquely-represented
constructions for hash functions [27], [28] and variants of
a B-tree (a B-treap [29], and a B-skip-list [30]). We adopt
the notion of unique representation for history independence
when developing our history independent, randomized B-tree,
or HIRB tree.

We note that history independence of these data structures
considers a setting where a single party runs some algorithms
on a single storage medium, which doesn’t correctly capture
the actual cloud setting where client and server have separate
storage, execute protocols, and exchange messages to maintain
the data structures. Therefore, we extend the existing history
independence and give a new, augmented notion of history
independence for the cloud setting with a catastrophic attack.
Independently, the recent work of [31] also considers a
limited notion of history independence, called Δ-history inde-
pendence, parameterized with a function Δ that describes the
leakage. Our deﬁnition of history independence has a similar
notion, where the leakage function Δ captures the number
of recent operations which may be revealed in a catastrophic
attack.

Secure deletion. Secure deletion means that data deleted
cannot be recovered, even by the original owner. It has been
studied in many contexts [32], but here we focus on the cloud
setting, where the user has little or no control over the physical
media or redundant duplication or backup copies of data. In
particular, we build upon secure deletion techniques from the
applied cryptography community. The approach is to encrypt
all data stored in the cloud with encryption keys stored locally
in erasable memory, so that deleting the keys will securely
delete the remote data by rendering it non-decryptable.

Boneh and Lipton [33] were the ﬁrst to use encryption to
securely remove ﬁles in a system with backup tapes. The
challenge since was to more effectively manage encrypted
content and the processes of re-encryption and erasing de-
cryption keys. For example, Di Crescenzo et al. [34] showed a
more efﬁcient method for secure deletion using a tree structure
applied in the setting of a large non-erasable persistent medium
and a small erasable medium. Several works considered secure
deletion mechanisms for a versioning ﬁle system [35], an
inverted index in a write-once-read-many compliance stor-
age [36], and a B-tree (and generally a mangrove) [17].

III. PRELIMINARIES

We assume that readers are familiar with security notions
of standard cryptographic primitives [37]. Let λ denote the
security parameter.

Modeling data structures. Following the approach from the
secure deletion literature, we use two storage types: erasable
memory and persistent storage. Contents deleted from erasable
memory are non-recoverable, while the contents in persistent
storage cannot be fully erased. We assume the size of erasable
memory is small while the persistent storage has a much larger
capacity. This mimics the cloud computing setting where cloud
storage is large and persistent due to lack of user control, and
local storage is more expensive but also controlled directly.
We deﬁne a data structure D as a collection of data that
supports initialization, insertion, deletion, and lookup, using
both the erasable memory and the persistent storage. Each
operation may be parameterized by some operands (e.g.,
lookup by a label). For a data structure D stored in this model,
let D.em and D.ps denote the contents of the erasable memory
and persistent storage, respectively. For example, an encrypted
graph structure may be stored in D.ps while the decryption key
resides in D.em. For an operation op on D, let acc←D.op()
denote executing the operation op on the data structure D
where acc is the access pattern over the persistent storage
−→
during the operation. The access pattern to erasable memory
−→
is assumed to be hidden. For a sequence of operations
op =
op() denote applying the opera-
(op1, . . . , opm), let
tions on D, that is, acc1←D.op1(),
, accm←D.opm(),
−→
−→
acc = (acc1, . . . , accm). We note that the access pattern
with
acc completely determines the state of persistent storage D.ps.
Obliviousness and history independence. Obliviousness
requires that the adversary without access to erasable memory
cannot obtain any information about actual operations per-
formed on data structure D other than the number of oper-
ations. This security notion is deﬁned through an experiment
obl-hi, given in Figure 1, where D, λ, n, h, b denote a data
structure, the security parameter, the maximum number of
items D can contain, history independence, and the challenge
choice.

−→
acc←D.

. . .

In the experiment, the adversary chooses two sequences
of operations on the data structure and tries to guess which
sequence was chosen by the experiment with the help of access

182182

λ, n);

EXPobl-hiA1,A2 (D, λ, n, h, b)
acc0←D.Init(1
op(0),−→
−→
op(1), ST)←A1(1
−→
acc←D.−→
(
op(b)
return A2(ST,−→
if h = 1:
return A2(ST,−→

();
acc,D.em);

acc);

else

λ, acc0);

λ, n);

EXPsdelA1,A2,A3 (D, λ, n, b)
acc0 ← D.Init(1
d0 ← A1(1
λ, 0);
d1 ← A1(1
λ, 1);
−→
opd0,d1 , S)←A2(acc0, d0, d1);
−→
−→
(
acc←D.(
return A3(acc0,−→
opd0,d1 (cid:2)(cid:3)S db)();
acc,D.em);

Figure 1: Experiments for security deﬁnitions

patterns. The data structure provides obliviousness if every
polynomial-time adversary has only a negligible advantage.
Deﬁnition 1. For a data structure D, consider the experiment
obl-hiA (D, λ, n, 0, b) with adversary A = (A1,A2). We
EXP
call the adversary A admissible if A1 always outputs two
sequences with the same number of operations storing at most
n items. We deﬁne the advantage of the adversary A in this
experiment as:
AdvoblA (D, λ, n) =
We say that D provides obliviousness if for any sufﬁciently
large λ, any n ∈ poly(λ), and any PPT admissible adversary
A, we have AdvoblA (D, λ, n) ≤ negl(λ).

obl-hiA (D, λ, n, 0, 0) = 1]

obl-hiA (D, λ, n, 0, 1) = 1]

(cid:2)(cid:2)(cid:2)(cid:2) Pr[EXP

− Pr[EXP

(cid:2)(cid:2)(cid:2)(cid:2) .

Now we deﬁne history independence. As we will see,
perfect history independence is inherently at odds with obliv-
iousness and sub-linear communication cost. Therefore, we
deﬁne parameterized history independence instead that allows
for a relaxation of the security requirement. The parameter de-
termines the allowable leakage of recent history of operations.
One can interpret a history-independent data structure with
leakage of (cid:2) operations as follows: Although the data structure
may reveal some recent (cid:2) operations applied to itself, it does
not reveal any information about older operations, except that
the total sequence resulted in the current state of data storage.
The experiment in this case is equivalent to that for obliv-
iousness, except that (1) the two sequences must result in
the same state of the data structure at the end, (2) the last
(cid:2) operations in both sequences must be identical, and (3) the
adversary gets to view the local, erasable memory as well as
the access pattern to persistent storage.
Deﬁnition 2. For a data structure D, consider the experiment
obl-hiA (D, λ, n, 1, b) with adversary A = (A1,A2). We call
EXP
the adversary A (cid:2)-admissible if A1 always outputs sequences
−→
−→
op(0) and
op(1) which have the same number of operations and
result in the same set storing at most n data items, and the last
(cid:2) operations of both are identical. We deﬁne the advantage of
an adversary A in this experiment above as:
AdvhiA(D, λ, n) =

obl-hiA (D, λ, n, 1, 0) = 1]

(cid:2)(cid:2)(cid:2)(cid:2) Pr[EXP

(cid:2)(cid:2)(cid:2)(cid:2) .

say

We
history independence with leakage of

that

the

− Pr[EXP
data

obl-hiA (D, λ, n, 1, 1) = 1]
structure D provides
if

(cid:2) operations

for any sufﬁciently large λ, any n ∈ poly(λ), and any PPT (cid:2)-
admissible adversary A, we have AdvhiA(D, λ, n) ≤ negl(λ).

Lower bound on history independence. Unfortunately,
the history independence property is inherently at odds with
the nature of oblivious RAM. The following lower bound
demonstrates that there is a linear tradeoff between the amount
of history independence and the communication bandwidth of
any ORAM mechanism.
Theorem 1. Any oblivious RAM storage system with a
bandwidth of k bytes per access achieves at best history
independence with leakage of Ω(n/k) operations in storing
n blocks.

The intuition behind the proof2 is that, in a catastrophic
attack, an adversary can observe which persistent storage
locations were recently accessed, and furthermore can decrypt
the contents of those locations because they have the keys from
erasable memory. This will inevitably reveal information to the
attacker about the order and contents of recent accesses, up to
the point at which all n elements have been touched by the
ORAM and no further past information is recoverable.

Admittedly this lower bound limits what may be achievable
in terms of history independence. But still, leaking only a
known maximum number of prior operations is better than
(potentially) leaking all of them!

Consider, by contrast, an AVL tree implemented within a
standard ORAM as in prior work. Using the fact that AVL tree
shapes reveal information about past operations, the adversary
can come up with two sequences of operations such that (i)
the ﬁrst operations of each sequence result in a distinct AVL
tree shape but the same data items, and (ii) the same read
operations, as many as necessary, follow at the end. With
the catastrophic attack, the adversary will simply observe the
tree shape and make a correct guess. This argument holds
for any data structure whose shape reveals information about
past operations, which therefore have no upper bound on the
amount of history leakage.
Secure deletion. Perfect history independence implies se-
cure deletion. However, the above lower bound shows that
complete history independence will not be possible in our
setting. So, we consider a complementary security notion that
requires strong security for the deleted data. Secure deletion
is deﬁned through an experiment sdel, given in Figure 1.
In the experiment, A1 chooses two data items d0 and d1
−→
at random, based on which A2 outputs (
−→
opd0,d1 , S). Here,
opd0,d1 denotes a vector of operations containing neither d0
−→
nor d1, and S = (s1, s2, . . . , sm) is a monotonically increasing
sequence.
opd0,d1
according to S. In particular, “insert db" is placed at position
−→
s1; for example, if s1 is 5, this insert operation is placed right
opd0,d1. Then, “look-up db" is
before the 6th operation of
placed at positions s2, . . . , sm−1, and ﬁnally “delete db" at
sm.

(cid:2)(cid:3)S db denotes injecting db into

−→
opd0,d1

2Full proofs for the main theorems may be found in Appendix C.

183183

Deﬁnition 3. For a data structure D, consider the experiment
sdelA1,A2,A3 (D, λ, n, b) with adversary A = (A1,A2,A3).
EXP
We call the adversary A admissible if for any data item d that
A1(1λ, 0) (resp., A1(1λ, 1)) outputs, the probability that A1
outputs d is negligible in λ, i.e., the output A1 forms a high-
the sequence of operations
entropy distribution; moreover,
from A2 must store at most n items. We deﬁne the advantage
of A as:
AdvsdelA (D, λ, n) =
We say that the data structure D provides secure deletion if
for any sufﬁciently large λ, any n ∈ poly(λ), and any PPT
admissible adversary A, we have AdvsdelA (D, λ, n) ≤ negl(λ).
Note that our deﬁnition is stronger than just requiring that
the adversary cannot recover the deleted item; for any two high
entropy distributions chosen by the adversary, the adversary
cannot tell from which distribution the deleted item was drawn.

(cid:2)(cid:2)(cid:2)(cid:2) Pr[EXP

sdelA (D, λ, n, 1) = 1]

sdelA (D, λ, n, 0) = 1]

− Pr[EXP

(cid:2)(cid:2)(cid:2)(cid:2) .

IV. ORAM WITH VARIABLE-SIZE BLOCKS (VORAM)
The design of vORAM is based on the non-recursive version
of Path ORAM [9], but we are able to add more ﬂexibility by
allowing each ORAM bucket to contain as many variable-size
blocks (or parts of blocks) as the bucket space allows. We
will show that vORAM preserves obliviousness and maintains
a small stash as long as the size of variable blocks can be
bounded by a geometric probability distribution, which is the
case for the HIRB that we intend to store within the vORAM.
To support secure deletion, we also store encryption keys
within each bucket for its two children, and these keys are re-
generated on every access, similarly to other work on secure
deletion [17], [34].
Parameters. The vORAM construction is governed by the
following parameters:

• The height T of the vORAM tree: The vORAM is
represented as a complete binary tree of buckets with
height T (the levels of the tree are numbered 0 to T ), so
the total number of buckets is 2T +1 − 1. T also controls
the total number of allowable data blocks, which is 2T .
• The bucket size Z: Each bucket has Z bits, and this Z
must be at least some constant times the expected block
size B for what will be stored in the vORAM.

• The stash size parameter R: Blocks (or partial blocks)
that overﬂow from the root bucket are stored temporarily
in an additional memory bank in local storage called the
stash, which can contain up to R · B bits.
• Block collision parameter γ: Each block will be assigned
a random identiﬁer id; these identiﬁers will all be distinct
at every step with probability 1 − negl(γ).

Bucket structure. Each bucket is split into two areas: header
and data. See Figure 2 for a pictorial description. The header
area contains two encryption keys for the two child buckets.
The data area contains a sequence of (possibly partial) blocks,
each preceded by a unique identiﬁer string and the block

184184

k1 k2 id1 l1

blk1

. . . id(cid:4) l(cid:4)

blk(cid:4)

0

header
Figure 2: A single vORAM bucket with (cid:3) partial blocks.

l1 bytes

l(cid:4) bytes

Figure 3: A sample vORAM state with partial blocks with
id0, id1, id2, id3: Note that the partial blocks for id0 are
opportunistically ﬁlled up the vORAM from leaf to root and
then remaining partial blocks are placed in the stash.

data length. The end of the data area is ﬁlled with 0 bits,
if necessary, to pad up to the bucket size Z.

Each idi uniquely identiﬁes a block and also encodes the
path of buckets along which the block should reside. Partial
blocks share the same identiﬁer with each length l indicating
how many bytes of the block are stored in that bucket.
Recovering the full block is accomplished by scanning from
the stash along the path associated with id (see Figure 3). We
further require the ﬁrst bit of each identiﬁer to be always 1
in order to differentiate between zero padding and the start
of next identiﬁer. Moreover, to avoid collisions in identiﬁers,
the length of each identiﬁer is extended to 2T + γ + 1 bits,
where γ is the collision parameter mentioned above. The
most signiﬁcant T + 1 bits of the identiﬁer (including the
ﬁxed leading 1-bit) are used to match a block to a leaf, or
equivalently, a path from root to leaf in the vORAM tree.
vORAM operations. Our vORAM construction supports the
following operations.
• insert(blk) (cid:6)→ id. Inserts the given block blk of data into
the ORAM and returns a new, randomly-generated id to
be used only once at a later time to retrieve the original
contents.
• remove(id) (cid:6)→ blk. Removes the block corresponding to
id and returns the original data blk as a sequence of bytes.
• update(id, callback) (cid:6)→ id+. Given id and a user-deﬁned
function callback, perform insert(callback(remove(id)))
in a single step.

Each vORAM operation involves two phases:
1) evict(id). Decrypt and read the buckets along the path
from the root to the leaf encoded in the identiﬁer id,
and remove all the partial blocks along the path, merging
partial blocks that share an identiﬁer, and storing them in
the stash.

2) writeback(id). Encrypt all blocks along the path encoded
by id with new encryption keys and opportunistically
store any partial blocks from stash, dividing blocks as
necessary, ﬁlling from the leaf to the root.

An insert operation ﬁrst evicts a randomly-chosen path,
then inserts the new data item into the stash with a sec-
ond randomly-chosen identiﬁer, and ﬁnally writes back the
originally-evicted path. A remove operation evicts the path
speciﬁed by the identiﬁer, then removes that item from the
stash (which must have had all its partial blocks recombined
along the evicted path), and ﬁnally writes back the evicted path
without the deleted item. The update operation evicts the path
from the initial id, retrieves the block from stash, passes it to
the callback function, re-inserts the result to the stash with a
new random id+, and ﬁnally calls writeback on the original
id. A full pseudocode description of all these operations is
provided in Appendix A.
Security properties.
Theorem 2. The vORAM provides obliviousness.

For obliviousness, any insert, remove, update operation is
computationally indistinguishable based on its access pattern
because the identiﬁer of each block is used only once to re-
trieve that item and then immediately discarded. Each remove
or update trivially discards the identiﬁer after reading the
path, and each insert evicts buckets along a bogus, randomly
chosen path before returning a fresh id+ to be used as the
new identiﬁer for that block.
Theorem 3. The vORAM provides secure deletion.

Secure deletion is achieved via key management of buckets.
Every evict and writeback will result in a path’s worth of
buckets to be re-encrypted and re-keyed, including the root
bucket. Buckets containing any removed data may persist,
but
the decryption keys are erased since the root bucket
is re-encrypted, rendering the data unrecoverable. Similarly,
recovering any previously deleted data reduces to acquiring the
old-root key, which was securely deleted from local, erasable
memory.

However, each evict and writeback will disclose the
vORAM path being accessed, which must be handled care-
fully to ensure no leakage occurs. Fortunately,
identiﬁers
(and therefore vORAM paths as well) are uniformly random,
independent of the deleted data and revealing no information
about them.
Theorem 4. The vORAM provides history independence with
leakage of O(n log n + λn) operations.

Regarding history independence, although any removed
items are unrecoverable,
the height of each item in the
vORAM tree, as well as the history of accesses to each
vORAM tree bucket, may reveal some information about the
order, or timing, of when each item was inserted. Intuitively,
items appearing closer to the root level of the vORAM are
more likely to have been inserted recently, and vice versa.

185185

In fact, we can achieve asymptotically optimal

However, if an item is inserted and then later has its path
entirely evicted due to some other item’s insertion or removal,
then any history information of the older item is essentially
wiped out; it is as if that item had been removed and re-
inserted. Because the identiﬁers used in each operation are
chosen at random, after some O(n log n) operations it is likely
that every path in the vORAM has been evicted at least once.
leakage
with only a constant-factor blowup in the bandwidth. Every
vORAM operation involves reading and writing a single
path. Additionally, after each operation, we can evict and
then re-write a complete subtree of size lg n which contains
(lgn)/2 − 1 leaf buckets in a deterministicly chosen dummy
operation that simply reads the buckets into stash, then rewrites
the buckets with no change in contents but allowing the blocks
evicted from the dummy operation and those evicted from the
access to all move between levels of the vORAM as usual. The
number of nodes evicted will be less than 2 lg n, to encompass
the subtree itself as well as the path of buckets to the root of
the subtree, and hence the total bandwidth for the operation
remains O(log n).

The beneﬁt of this approach is that if these dummy subtree
evictions are performed sequentially across the vORAM tree
on each operation, any sequence of n/ lg n operations is
guaranteed to have evicted every bucket in the vORAM at least
once. Hence this would achieve history independence with
only O(n/ log n) leakage, which matches the lower bound of
Theorem 1 and is therefore optimal up to constant factors.
Stash size. Our vORAM construction maintains a small stash
as long as the size of variable blocks can be bounded by a
geometric probability distribution, which is the case for the
HIRB that we intend to store within the vORAM.
Theorem 5. Consider a vORAM with T levels, collision
parameter γ, storing at most n = 2T blocks, where the length
l of each block is chosen independently from a distribution
such that E[l] = B and Pr[l > mB] < 0.5m. Then, if the
bucket size Z satisﬁes Z ≥ 20B, for any R ≥ 1, and after
any single access to the vORAM, we have

Pr[|stash| > R B] < 28 · (0.883)

R

.

Note that the constants 28 and 0.883 are technical artifacts
of the analysis, and do not matter except to say that 0.883 < 1
and thus the failure probability decreases exponentially with
the size of stash.

As a corollary, for a vORAM storing at most n blocks, the
cloud storage requirement is 40Bn bits, and the bandwidth
for each operation amounts to 40B lg n bits. However, this is
a theoretical upper bound, and our experiments in Section VI
show a smaller constants sufﬁce. namely, setting Z = 6B
and T = (cid:8)lg n − 1(cid:9) stabilizes the stash, so that the actual
storage requirement and bandwidth per operation are 6Bn and
12B lg n bits, respectively.

Furthermore, to avoid failure due to stash overﬂow or col-
lisions, the client storage R and collision parameter γ should
both grow slightly faster than log n, i.e., R, γ ∈ ω(log n).

V. HIRB TREE DATA STRUCTURE

We now use the vORAM construction described in the
previous section to implement a data structure supporting the
operations of a dictionary that maps labels to values. In this
paper, we intentionally use the word “labels” rather than the
word “keys” to distinguish from the encryption keys that are
stored in the vORAM.
Motivating the HIRB. Before describing the construction
and properties of the history independent, randomized B-Tree
(HIRB), we ﬁrst wish to motivate the need for the HIRB as it
relates to the security and efﬁciently requirements of storing
it within the vORAM:

• The data structure must be easily partitioned into blocks
that have expected size bounded by a geometric distribu-
tion for vORAM storage.

• The data structure must be pointer-based, and the struc-
ture of blocks and pointers must form a directed graph
that is an arborescence, such that there exists at most one
pointer to each block. This is because a non-recursive
ORAM uses random identiﬁers for storage blocks, which
must change on every read or write to that block.

• The memory access pattern for an operation (e.g., get,
set, or delete) must be bounded by a ﬁx parameter to
ensure obliviousness; otherwise the number of vORAM
accesses could leak information about the data access.

• Finally, the data structure must be uniquely represented
such that the pointer structures and contents are deter-
mined only by the set of (label, value) pairs stored within,
up to some randomization performed during initialization.
Recall that strong history independence is provided via a
unique representation, a sufﬁcient and necessary condi-
tion [25] for the desired security property.

In summary, we require a uniquely-represented, tree-based
data structure with bounded height. While a variety of uniquely
represented (or strongly history independent) data structures
have been proposed in the literature [24], [29], we are not
aware of any that satisfy all of the requisite properties.

While some form of hash table might seem like an obvious
choice, we note that such a structure would violate the second
condition above; namely, it would be impossible to store a
hash table within an ORAM without having a separate position
map, incurring an extra logarithmic factor in the cost. As it
turns out, our HIRB tree does use hashing in order to support
secure deletion, but this is only to sort the labels within the
tree data structure.
Overview of HIRB tree. The closest data structure to the
HIRB is the B-Skip List [30]; unfortunately, a skip list does
not form a tree. The HIRB is essentially equivalent to a B-
Skip List after sorting labels according to a hash function and
removing pointers between skip-nodes to impose a top-down
tree structure.

Recall that a typical B-tree consists of large nodes, each
with an array of (label, value) pairs and child nodes. A B-tree
node has branching factor of k, and we call it a k-node, if the

186186

node contains k − 1 labels, k − 1 values, and k children (as
in Figure 4). In a typical B-tree, the branching factor of each
node is allowed to vary in some range [B + 1, 2B], where
B is a ﬁxed parameter of the construction that controls the
maximum size of any single node.
label1, value1 label2, value2 ···

labelk−1, valuek−1

child1

child2

childk
Figure 4: B-tree node with branching factor k

child3

···

HIRB tree nodes differ from typical B-tree nodes in two
ways. First, instead of storing the label in the node a cryp-
tographic hash3 of the label is stored. This is necessary to
support secure deletion of vORAM+HIRB even when the
nature of vORAM leaks some history of operations; namely,
revealing which HIRB node an item was deleted from should
not reveal the label that was deleted.

The second difference from a normal B-tree node is that the
branching factor of each node, rather than being limited to a
ﬁxed range, can take any value k ∈ [1,∞). This branching
factor will observe a geometric distribution for storage within
the vORAM. In particular,
it will be a random variable
X drawn independently from a geometric distribution with
expected value β, where β is a parameter of the HIRB tree
construction.

The height of a node in the HIRB tree is deﬁned as the
length of the path from that node to a leaf node; all leaf nodes
are the same distance to the root node for B-trees. The height
of a new insertion of (label, value) in the HIRB is determined
by a series of pseudorandom biased coin ﬂips based on the
hash of the label4. The distribution of selected heights for
insertions uniquely determines the structure of the HIRB tree
because the process is deterministic, and thus the HIRB is
uniquely-represented.
Parameters and preliminaries. Two parameters are ﬁxed at
initialization: the expected branching factor β, and the height
H. In addition, throughout this section we will write n as the
maximum number of distinct labels that may be stored in the
HIRB tree, and γ as a parameter that affects the length of hash
digests5.
A HIRB tree node with branching factor k consists of k− 1
label hashes, k − 1 values, and k vORAM identiﬁers which
represent pointers to the child nodes. This is described in
Figure 5 where hi indicates Hash(labeli).

Similar to the vORAM itself, the length of the hash function
should be long enough to reduce the probability of collision
−γ, so deﬁne |Hash(label)| = max(2H lg β + γ, λ),
below 2
3We need a random oracle for formal security. In practice, we used a SHA1
initialized with a random string chosen when the HIRB tree is instantiated.
4Note that this choice of heights is more or less the same as the randomly-

chosen node heights in a skip list.

5The parameter γ for HIRB and vORAM serves the same purpose in
avoiding collisions in identiﬁers so for simplicity we assume they are the
same

id0 h1

value1

id1 . . . hk

valuek

idk

Figure 5: HIRB node with branching factor k.

and deﬁne nodesizek to be the size of a HIRB tree node with
branching factor k, given as
nodesizek = (k + 1)(2T + γ + 1) + k(|Hash(label)| +|value|),
where we write |value| as an upper bound on the size of the
largest value stored in the HIRB. (Recall that the size of each
vORAM identiﬁer is 2T + γ + 1.) Each HIRB tree node will
be stored as a single block in the vORAM, so that a HIRB
node with branching factor k will ultimately be a vORAM
block with length nodesizek.

As β reﬂects the expected branching factor of a node, it
must be an integer greater than or equal to 1. This parameter
controls the efﬁciency of the tree and should be chosen
according to the size of vORAM buckets. In particular, using
the results of Theorem 5 in the previous section, and the
HIRB node size deﬁned above, one would choose β according
to the inequality 20nodesizeβ ≤ Z, where Z is the size of
each vORAM bucket. According to our experimental results
in Section VI, the constant 20 may be reduced to 6.
The height H must be set so that H ≥ logβ n; otherwise
we risk the root node growing too large. We assume that H
is ﬁxed at all times, which is easily handled when an upper
bound n is known a priori.
HIRB tree operations. As previously described, the entries
in a HIRB node are sorted by the hash of the labels, and the
search path for a label is also according to the label hashes.
A lookup operation for a label requires fetching each HIRB
node along the search path from the vORAM and returning
the matching value.

Initially, an empty HIRB tree of height H is created, as
shown in Figure 6. Each node has a branching factor of 1 and
contains only the single vORAM identiﬁer of its child.

(cid:11)
(cid:11)
...
(cid:11)

H + 1 nodes

Figure 6: Empty HIRB with height H.

Modifying the HIRB with a set or delete operation on
some label involves ﬁrst computing the height of the label.
The height
is determined by sampling from a geometric
distribution with probability (β−1)/β, which we derandomize
by using a pseudorandom sequence based on Hash(label).
The distribution guarantees that, in expectation, the number
of items at height 0 (i.e., in the leaves) is β−1
β n, the number
of items at height 1 is β−1

β2 n, and so on.

Inserting or removing an element from the HIRB involves
(respectively) splitting or merging nodes along the search path

. . .

. . .

. . .

. . .

. . .

X

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

. . .

(cid:11) . . .

. . .

. . .

Figure 7: HIRB insertion/deletion of X = (Hash(label),
value): On the left is the HIRB without item X, displaying only
the nodes along the search path for X, and on the right is the
state of the HIRB with X inserted. Observe that the insertion
operation (left to right) involves splitting the nodes below X
in the HIRB, and the deletion operation (right to left) involves
merging the nodes below X.

from the height of the item down to the leaf. This differs from
a typical B-tree in that rather than inserting items at the leaf
level and propagating up or down with splitting or merging, the
HIRB tree requires that the heights of all items are ﬁxed. As
a result, insertions and deletions occur at the selected height
within the tree according to the label hash. A demonstration
of this process is provided in Figure 7.

In a HIRB tree with height H, each get operation requires
reading exactly H +1 nodes from the vORAM, and each set or
delete operation involves reading and writing at most 2H + 1
nodes. To support obliviousness, each operation will require
exactly 2H + 1, accomplished by padding with “dummy”
accesses so that every operation has an indistinguishable
access pattern.

One way of reading and updating the nodes along the search
path would be to read all 2H + 1 HIRB nodes from the
vORAM and store them in temporary memory and then write
back the entire path after any update. However, properties of
the HIRB tree enable better performance because the height of
each HIRB tree element is uniquely determined, which means
we can perform the updates on the way down in the search
path. This only requires 2 HIRB tree nodes to be stored in
local memory at any given time.

Facilitating this extra efﬁciency requires considerable care
in the implementation due to the nature of vORAM identiﬁers;
namely, each internal node must be written back to vORAM
before its children nodes are fetched. Fetching children nodes
will change their vORAM identiﬁers and invalidate the point-
ers in the parent node. The solution is to pre-generate new
random identiﬁers of the child nodes before they are even
accessed from the vORAM.

The full details of the HIRB operations can be found in

Appendix B.

HIRB tree properties.
For our analysis of the HIRB
tree, we ﬁrst need to understand the distribution of items
among each level in the HIRB tree. We assume a subroutine
chooseheight(label) evaluates a random function on label

187187

to generate random coins, using which it samples from a
truncated geometric distribution with maximum value H and
probability (β − 1)/β.
Assumption 6. If
stored in a HIRB, then the heights

label1, . . . , labeln are any n distinct labels

chooseheight(label1), . . . , chooseheight(labeln)

are independent random samples from a truncated geometric
distribution over {0, 1, . . . , H} with probability (β − 1)/β,
where the randomness is determined entirely by the the random
oracle and the random function upon creation of the HIRB.
In practice, the random coins for chooseheight(label) are
prepared by computing coins = PRG(SHA1(seed(cid:12)label)),
where seed is a global random seed, and PRG is a pseudo-
random generator. With SHA1 modeled as a random oracle,
the coins will be pseudorandom.
Theorem 7. The HIRB tree is a dictionary data structure that
associates arbitrary labels to values. If it contains n items,
and has height H ≥ logβ n, and the nodes are stored in a
vORAM, then the following properties hold:
−γ.
• The probability of failure in any operation is at most 2
• Each operation requires exactly 2H + 1 node accesses,
only 2 of which need to be stored in temporary memory
at any given time.

• The data structure itself, not counting the pointers, is

strongly history independent.

The ﬁrst property follows from the fact that the only way
the HIRB tree can fail
to work properly is if there is a
hash collision. Based on the hash length deﬁned above, the
probability that any 2 keys collide amongst the n labels in the
−γ. The second property follows from the
HIRB is at most 2
description of the operations get, set, and delete, and is crucial
not only for the performance of the HIRB but also for the
obliviousness property. The third property is a consequence of
the fact that the HIRB is uniquely represented up to the pointer
values, after the hash function is chosen at initialization.
vORAM+HIRB properties. We are now ready for the main
theoretical results of the paper, which have to do with the
performance and security guaranteed by the vORAM+HIRB
construction.
Theorem 8. Suppose a HIRB tree with n items and height
H is stored within a vORAM with L levels, bucket size Z,
and stash size R. Given choices for Z and γ > 0, set the
parameters as follows:

The parameters follow from the discussion above. Again
note that the constants 30 and 0.883 are technical artifacts of
the analysis.
Theorem 9. Suppose a vORAM+HIRB is constructed with
parameters as above. The vORAM+HIRB provides oblivious-
ness, secure deletion, and history independence with leakage
of O(n + nλ/(log n)) operations.

The security properties follow from the previous results on
the vORAM and the HIRB. Note that the HIRB structure
itself provides history independence with no leakage, but when
combined with the vORAM, the pointers may leak information
about recent operations. The factor O(log n) difference from
the amount of leakage from vORAM in Theorem 4 arises
because each HIRB operation entails O(log n) vORAM opera-
tions. Following the discussion after Theorem 4, we could also
reduce the leakage in vORAM+HIRB to O(n/ log
n), with
constant-factor increase in bandwidth, which again is optimal
according to Theorem 1.

2

VI. EVALUATION
two
empirical

of

We

analyses

completed

the
vORAM+HIRB system. First, we sought
to determine
the most effective size for vORAM buckets with respect
to the expected block size, i.e., the ratio Z/B. Second, we
made a complete implementation of the vORAM+HIRB and
measured its performance in storing a realistic dataset of
key/value pairs of 22MB in size. The complete source code
of our implementation is available upon request.

A. Optimizing vORAM parameters

A crucial performance parameter in our vORAM construc-
tion is the ratio Z/B between the size Z of each bucket and
the expected size B of each block. (Note that B = nodesizeβ
when storing HIRB nodes within the vORAM.) This ratio is a
constant factor in the bandwidth of every vORAM operation
and has a considerable effect on performance. In the Path
ORAM, the best corresponding theoretical ratio is 5, whereas
it has been shown experimentally that a ratio of 4 will also
work, even in the worst case [9].

We performed a similar experimental analysis of the ra-
tio Z/B for the vORAM. Our best theoretical ratio from
Theorem 5 is 20, but as in related work, the experimental
performance is better. The goal is then to ﬁnd the optimal,
empirical choice for the ratio Z/B: If Z/B is too large, this
will increase the overall communication cost of the vORAM,
and if it is too small, there is a risk of stash overﬂow and loss
of data or obliviousness.

For the experiments described below, we implemented a
vORAM structure without encryption and inserted a chosen
number of variable size blocks whose sizes were randomly
sampled from a geometric distribution with expected size 68
bytes. To avoid collisions, we ensured the identiﬁer lengths
satisﬁed γ ≥ 40.
Stash size. To analyze the stash size for different Z/B ratios,
we ran a number of experiments and monitored the maximum

T ≥ lg(4n + lg n + γ)
β = max{β|Z ≥ 20 · nodesizeβ}
R ≥ γ · nodesizeβ
H ≥ logβ n
Then the probability of
collisions after each operation is at most

Pr[vORAM+HIRB failure] ≤ 30 · (0.883)

failure due to stash overﬂow or

γ

.

188188






























	













































	



Figure 8: Maximum stash size, scaled by lg n, observed across
50 simulations of a vORAM for various Z/B values.

Figure 10: Utilization at different levels of ORAM

e
z
i
S
 
h
s
a
t
S
 
l
i
t
n
U

 
.
r
e
p
O

 
f
o
 
.

m
u
N
 
n
a
e
M

220
218
216
214
212
210
28
26
24

218 items, total 2MB
219 items, total 4MB
220 items, total 8MB

 0

 2

 4

 6

 8

 10

 12

Stash Size (KB)

Figure 9: Average time until stash overﬂow, for varying
vORAM and stash sizes. Stash size is linear-scale, number of
operations in log-scale. Higher is better. For each vORAM size
n, we performed 2n operations to gather sufﬁcient experimental
data.

|

stash size observed at any point throughout the experiment.
Recall, while the stash will typically be empty after every
operation, the max stash size should grow logarithmically with
respect to the number of items inserted in the vORAM. The
primary results are presented in Figure 8.

This experiment was conducted by running 50 simulations
of a vORAM with n insertions and a height of T = lg n. The
Z/B value ranged from 1 to 50, and results in the range 1
through 12 are presented in the graph for values of n ranging
from 102 through 105. The graph plots the ratio R/ lg n, where
R is the largest max stash size at any point in any of the 50
simulations. Observe that between Z/B = 4 and Z/B = 6
the ratio stabilizes for all values of n, indicating a maximum
stash of approximately 100 lg n.

In order to measure how much stash would be needed in
practice for much larger experimental runs, we ﬁxed Z/B = 6
and for three large database sizes, n = 218, 219, 220, For
each size, we executed 2n operations, measuring the size
of stash after each. In practice, as we would assume from
the theoretical results, the stash size is almost always zero.

189189

However, the stash does occasionally become non-empty, and
it is precisely the frequency and size of these rare events that
we wish to measure.

Fig. 9 shows the result of our stash overﬂow experiment. We
divided each test run of roughly 2n operations into roughly n
overlapping windows of n operations each, and then for each
window, and each possible stash size, calculated the number of
operations before the ﬁrst time that stash size was exceeded.
The average number of operations until this occurred, over
all n windows, is plotted in the graph. The data shows a
linear trend in log-scale, meaning that the stash size neces-
sary to ensure low overﬂow probability after N operations
is O(log N ), as expected. Furthermore, in all experiments
we never witnessed a stash size larger than roughly 10KB,
whereas the theoretical bound of 100 lg n items would be
16KB for the largest test with 220 8-byte items.
Bucket utilization. Stash size is the most important parameter
of vORAM, but it provides a limited view into the optimal
bucket size ratio, in particular as the stash overﬂow is typically
zero after every operation, for sufﬁciently large buckets. We
measured the utilization of buckets at different levels of the
vORAM with varied heights and Z/B values. The results are
presented in Figure 10 and were collected by averaging the
ﬁnal bucket utilization from 10 simulations. The utilization at
each level is measured by dividing the total storage capacity
of the level by the number of bytes at the level. In all cases,
n = 215 elements were inserted, and the vORAM height
varied between 14, 15, and 16. The graph shows that with
height lg n = 15 or higher and Z/B is 6 or higher, utilization
stabilizes throughout all the levels (with only a small spike at
the leaf level).

that when Z/B = 6,

The results indicate, again,

the
utilization at the interior buckets stabilizes. With smaller ratios,
e.g., Z/B = 4, the utilization of buckets higher in the tree
dominates those lower in the tree; essentially, blocks are not
able to reach lower levels resulting in higher stash sizes (see
previous experiment). With larger ratios, which we measured
all the way to Z/B = 13, we observed consistent stabilization.
In addition, our data shows that decreasing the number of

levels from lg n to lg n − 1 (e.g., from 15 to 14 in the ﬁgure)
increases utilization at the leaf nodes as expected (as depicted
in the spike in the tail of the graphs), but when Z/B ≥ 6
the extra blocks in leaf nodes do not propagate up the tree
and affect the stash. It therefore appears that in practice, the
number of levels T could be set to lg n − 1, which will result
in a factor of 2 savings in the size of persistent (cloud) storage
due to high utilization at the leaf nodes. This follows a similar
observation about the height of the Path ORAM made by [9].

B. Measuring vORAM+HIRB Performance

We measured the performance of our vORAM+HIRB im-
plementation on a real data set of reasonable size, and
compared to some alternative methods for storing a remote
map data structure that provide varying levels of security
and efﬁciency. All of our implementations used the same
client/server setup, with a Python3 implementation and AWS
as the cloud provider, in order to give a fair comparison.
Sample dataset. We tested the performance of our imple-
mentation on a dataset of 300,000 synthetic key/value pairs
where keys were variable sizes (in the order of 10–20 bytes)
and values were ﬁxed at 16 bytes. The total unencrypted data
set is 22MB in size. In our experiments, we used some subset
of this data dependent on the size of the ORAM, and for each
size, we also assumed that the ORAM user would want to
allow the database to grow. As such, we built the ORAM to
double the size of the initialization.
Optimized vORAM+HIRB implementation. We fully
implemented our vORAM+HIRB map data structure using
Python3 and Amazon Web Services as the cloud service
provider. We used AES256 for encryption in vORAM, and
used SHA1 to generate labels for the HIRB. In our setting,
we considered a client running on the local machine that
maintained the erasable memory, and the server (the cloud)
provided the persistent storage with a simple get/set interface
to store or retrieve a given (encrypted) vORAM bucket.

For the vORAM buckets, we choose Z/B = 6 based on
the prior experiments, and a bucket size of 4K, which is the
preferred back-end transfer size for AWS, and was also the
bucket size used by [12]. One of the advantages of the vORAM
over other ORAMs is that the bucket size can be set to match
the storage requirements with high bucket utilization. The
settings for the HIRB were then selected based on Theorem 8
and based on that, we calculated a β = 12 for the sample data
(labels and values) stored within the HIRB. The label, value,
and associated vORAM identiﬁers total 56 bytes per item.

In our experiments, we found that the round complexity of
protocols dominate performance and so we made a number
of improvements and optimizations to the vORAM access
routines to compensate. The result is an optimized version
of the vORAM. In particular:

• Parallelization: The optimized vORAM transfers buckets
along a single path in parallel over simultaneous con-
nections for both the evict and writeback methods. Our
experiments used up to T threads in parallel to fetch and

send ORAM block ﬁles, and each maintained a persistent
sftp connection.

• Buffering: A local buffer storing 2T top-most ORAM
buckets was used to facilitate asynchronous path reading
and writing by our threads. Note the size of the client
storage still remains O(log n) since T = O(log n). This
had an added performance beneﬁt beyond the paralleliza-
tion because the top few levels of the ORAM generally
resided in the buffer and did not need to be transferred
back and forth to the cloud after every operation.

These optimizations had a considerable effect on the per-
formance. We did not include the cost of the ≈ 2 second
setup/teardown time for these SSL connections in our results
as these were a one-time cost incurred at initialization. Many
similar techniques to these have been used in previous work to
achieve similar performance gains (e.g., [38], [39]), although
they have not been previously applied to oblivious data struc-
tures.
Comparison baselines. We compared our optimized
vORAM+HIRB construction with four other alternative im-
plementations of a remote map data structure, with a wide
range of performance and security properties:

• Un-optimized vORAM+HIRB. This is the same as our
normal vORAM+HIRB construction, but without any
buffering of vORAM buckets and with only a single
concurrent sftp connection. This comparison allows us to
see what gains are due to the algorithmic improvements
in vORAM and HIRB, and which are due to the network
optimizations.

• Naive Baseline: We implemented a naive approach that
provides all three security properties, obliviousness, se-
cure deletion, and history independence. The method
involves maintaining a single, ﬁxed-size encrypted ﬁle
transferred back and forth between the server and client
and re-encrypted on each access. While this solution is
cumbersome for large sizes, it is the obvious solution
for small databases and thus provides a useful baseline.
Furthermore, we are not aware of any other method (other
than vORAM+HIRB) to provide obliviousness, secure-
deletion, and history independence.

• ORAM+AVL: We implemented the ODS proposed by
[11] of an AVL embedded within an non-recursive Path
ORAM. Note that ORAM+AVL does not provide secure
deletion nor history independence. We used the same
cryptographic settings as our vORAM+HIRB implemen-
tation, and used 256 byte blocks for each AVL node,
which was the smallest size we could achieve without
additional optimizations. As recommended by [9], we
stored Z = 4 ﬁxed-size blocks in each bucket, for a
total of 1K bucket size. Note that this bucket size is
less than the 4K transfer size recommended by the cloud
storage, which reﬂects the limitation of ORAM+AVL in
that it cannot effectively utilize larger buckets. We add
the observation that, when the same experiments were
run with 4K size buckets (more wasted bandwidth, but

190190








	





















	
	

























 !"#
$
 !%&"#
$
'(
)*&!&+



!













	








	






Figure 11: Median of 100 access times for different number of
entries

matching the other experiments),
the timings did not
change by more than 1 second, indicating that the 4K
bucket size is a good choice for the AWS back-end.

• SD-B-Tree: As another comparison point, we imple-
mented a remotely stored B-Tree with secure deletion
where each node is encrypted with a key stored in the
parent with re-keying for each access, much as described
by Reardon et al. [17]. While this solution provides secure
deletion, and stores all data encrypted, it does not provide
obliviousness nor history independence. Again, we used
AES256 encryption, with β = 110 for the B-tree max
internal node size in order to optimize 4K-size blocks.

In terms of asymptotic performance,

In terms of security, only our vORAM+HIRB as well as
the naive baseline provide obliviousness, secure deletion, and
history independence. The ORAM+AVL provides oblivious-
ness only, and the SD-B-Tree is most vulnerable to leaking
information in the cloud, as it provides secure deletion only.
the SD-B-Tree is
fastest, requiring only O(log n) data transfer per operation.
The vORAM+HIRB and ORAM+AVL both require O(log
n)
data transfer per operation, although as discussed previously
the vORAM+HIRB saves a considerable constant factor. The
naive baseline requires O(n) transfer per operation, albeit with
the smallest possible constant factor.
Experimental results. The primary result of the experiment
is presented in Figure 11 where we compared the time (in
seconds) for a single access. Unsurprisingly, the SD-B-Tree
implementation is fastest for sufﬁciently large database sizes.
However, our optimized vORAM+HIRB implementation was
competitive to the SD-B-Tree performance, both being less
than 1 second across our range of experiments.

2

Most striking is the access time of ORAM+AVL compared
to the vORAM+HIRB implementations. In both the optimized
and un-optimized setting, the vORAM+HIRB is orders of
magnitude faster than ORAM+AVL, 20X faster un-optimized
and 100X faster when optimized. Even for a relatively small
number of entries such as 211, a single access of ORAM+AVL
takes 35 seconds, while it only requires 1.3 seconds of un-
optimized vORAM+HIRB and 0.2 second of an optimized

191191

Naive baseline
Secure deletion B-tree
ORAM+AVL
vORAM+HIRB

Naive baseline
Secure deletion B-tree
ORAM+AVL
vORAM+HIRB

Naive baseline
Secure deletion B-tree
ORAM+AVL
vORAM+HIRB

Naive baseline
Secure deletion B-tree
ORAM+AVL
vORAM+HIRB

Naive baseline
Secure deletion B-tree
ORAM+AVL
vORAM+HIRB

Size 210

Total storage
8.2 KB
36.9 KB
8.4 MB
127.0 KB
Size 215

Total storage
262.1 KB
1.1 MB
268.4 MB
4.2 MB

Size 220

Total storage
8.4 MB
33.8 MB
8.6 GB
134.2 MB
Size 225

Total storage
268.4 MB
1.1 GB
274.9 GB
4.3 GB

Size 230

Total storage
8.6 GB
34.6 GB
8.8 TB
137.4 GB

Bandwidth
8.2 KB
12.3 KB
4.0 MB
102.4 KB

Bandwidth
262.1 KB
20.5 KB
8.6 MB
286.7 KB

Bandwidth
8.4 MB
20.5 KB
15.1 MB
553.0 KB

Bandwidth
268.4 MB
28.7 KB
23.2 MB
901.1 KB

Bandwidth
8.6 GB
36.9 KB
33.3 MB
1.5 MB

Rounds
1
2
968
3

Rounds
1
3
2096
4

Rounds
1
3
3675
5

Rounds
1
4
5668
6

Rounds
1
5
8122
8

Table I: Storage and communication cost comparisons. Total
storage is the amount of space required for the server, and the
bandwidth and rounds are counted per operation. Each stored
item consists of a 4-byte label and 4-byte value.

implementation. It is not until 219 entries that ORAM+AVL
even outperforms the naive O(n) baseline solution.

As described previously, we attribute much of the speed
to decreasing the round complexity. The HIRB tree requires
much smaller height as compared to an AVL tree because each
HIRB node contains β items on average as compared to just a
single item for an AVL tree. Additionally, the HIRB’s height
is ﬁxed and does not require padding to achieve obliviousness.
Each AVL operation entails 3· 1.44 lg N ORAM operations as
compared to just 2 logβ N vORAM operations for the HIRB.
This difference in communication cost is easily observed in
Table I. Overall, we see that the storage and communication
costs for vORAM+HIRB are not too much larger than that for
a secure deletion B-tree, which does not provide any access
pattern hiding as the oblivious alternatives do.

(The values in this table were generated by considering
the worst-case costs in all cases, for our actual implemen-
tations, but considering only a single operation. Note that, for
constructions providing obliviousness, every operation must
actually follow this worst case cost, and so the comparison is
fair.)

Put simply,

the vORAM+HIRB and SD-B-Tree are the
only implementations which can be considered practical for
real data sizes, and the beneﬁt of vORAM+HIRB is its
considerable additional security guarantees of oblivious and
bounded history independence.

VII. CONCLUSION

In this paper, we have shown a new secure cloud storage
system combining the previously disjoint security properties
of obliviousness, secure deletion, and history independence.
This was accomplished by developing a new variable block
size ORAM, or vORAM, and a new history independent,
randomized data structure (HIRB) to be stored within the
vORAM.

The theoretical performance of our vORAM+HIRB con-
struction is competitive to existing systems which provide
fewer security properties. Our implemented system is up to
100X faster (w.r.t. access time) than current best oblivious map
data structure (which provides no secure deletion or history
independence) by Wang et al. (CCS 14), bringing our single-
operation time for a reasonable-sized database (> 219) to less
than 1 second per access.

There much potential for future work in this area. For
example, one could consider data structures that support a
richer set of operations, such as range queries, while preserv-
ing obliviousness, secure deletion, and history independence.
Additionally, the vORAM construction in itself may provide
novel and exciting new analytic results for ORAMs generally
by not requiring ﬁxed bucket sizes. There is a potential
to improve the overall utilization and communication cost
compared to existing ORAM models that used ﬁxed size
blocks.

Finally, while we have demonstrated the practicality in
terms of overall per-operation speed, we did not consider some
additional practical performance measures as investigated by
[12], such as performing asynchronous operations and opti-
mizing upload vs download rates. Developing an ODS map
considering these concerns as well would be a useful direction
for future work.

ACKNOWLEDGEMENTS

This work was
through

supported by the Ofﬁce of Naval
Research
and
N0001415WX01532, and by the National Science Foundation
through awards #1406177 and #1319994.

N0001415WX01532

awards

REFERENCES

[1] X. Zhuang, T. Zhang, and S. Pande, “HIDE: an infrastructure for efﬁ-
ciently protecting information leakage on the address bus,” in ASPLOS
2004, 2004, pp. 72–84.

[2] M. S. Islam, M. Kuzu, and M. Kantarcioglu, “Access pattern disclo-
sure on searchable encryption: Ramiﬁcation, attack and mitigation,” in
NDSS 2012. The Internet Society, Feb. 2012.

[3] J. L. Dautrich Jr and C. V. Ravishankar, “Compromising privacy in
precise query protocols,” in Proceedings of
the 16th International
Conference on Extending Database Technology. ACM, 2013, pp. 155–
166.

[4] O. Goldreich and R. Ostrovsky, “Software protection and simulation on
oblivious rams,” J. ACM, vol. 43, no. 3, pp. 431–473, 1996. [Online].
Available: http://doi.acm.org/10.1145/233551.233553

[5] I. Damgård, S. Meldgaard, and J. B. Nielsen, “Perfectly secure oblivious
RAM without random oracles,” in TCC 2011, ser. LNCS, Y. Ishai, Ed.,
vol. 6597. Springer, Mar. 2011, pp. 144–163.

[6] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and R. Tamassia,
“Privacy-preserving group data access via stateless oblivious RAM
simulation,” in 23rd SODA, Y. Rabani, Ed. ACM-SIAM, Jan. 2012,
pp. 157–167.

192192

[7] E. Kushilevitz, S. Lu, and R. Ostrovsky, “On the (in)security of hash-
based oblivious RAM and a new balancing scheme,” in 23rd SODA,
Y. Rabani, Ed. ACM-SIAM, Jan. 2012, pp. 143–156.

[8] E. Stefanov, E. Shi, and D. X. Song, “Towards practical oblivious ram,”

in NDSS, 2012.

[9] E. Stefanov, M. van Dijk, E. Shi, C. W. Fletcher, L. Ren, X. Yu,
and S. Devadas, “Path ORAM: an extremely simple oblivious RAM
protocol,” in ACM CCS 13. ACM Press, Nov. 2013, pp. 299–310.

[10] J. L. D.

Jr., E.

Stefanov,

“Burst ORAM:
for bursty access patterns,”
minimizing ORAM response times
in Proceedings of
the 23rd USENIX Security Symposium, 2014,
pp. 749–764. [Online]. Available: https://www.usenix.org/conference/
usenixsecurity14/technical-sessions/presentation/dautrich

and E.

Shi,

[11] X. S. Wang, K. Nayak, C. Liu, T.-H. H. Chan, E. Shi, E. Stefanov,
and Y. Huang, “Oblivious data structures,” in ACM CCS 14, G.-J. Ahn,
M. Yung, and N. Li, Eds. ACM Press, Nov. 2014, pp. 215–226.

[12] V. Bindschaedler, M. Naveed, X. Pan, X. Wang, and Y. Huang, “Prac-
ticing oblivious access on cloud storage: the gap, the fallacy and the
new way forward,” in ACM CCS 15, 2015, to appear.

[13] M.

Isaac, “Nude photos of

front
in online privacy debate,” New York Times, Sept. 2, 2014,
http://www.nytimes.com/2014/09/03/technology/trove-of-nude-photos-
sparks-debate-over-online-behavior.html.

lawrence are latest

jennifer

[14] “Lastpass security notice,” https://blog.lastpass.com/2015/06/lastpass-

security-notice.html/.

[15] F.

Konkel,

“The

details

about

the
Jul.

CIA’s
17,

deal
2014,

Amazon,”

with
http://www.theatlantic.com/technology/archive/2014/07/the-details-
about-the-cias-deal-with-amazon/374632/.

Atlantic,

The

[16] S. Bajaj and R. Sion, “Ficklebase: Looking into the future to erase
the past,” in 29th IEEE International Conference on Data Engineering,
ICDE 2013, Brisbane, Australia, April 8-12, 2013, 2013, pp. 86–97.

[17] J. Reardon, H. Ritzdorf, D. A. Basin, and S. Capkun, “Secure data
deletion from persistent media,” in ACM CCS 13. ACM Press, Nov.
2013, pp. 271–284.

[18] T. Moataz, T. Mayberry, and E. Blass, “Constant communication ORAM
with small blocksize,” in Proceedings of
the 22nd ACM SIGSAC
Conference on Computer and Communications Security, Denver, CO,
USA, October 12-6, 2015, 2015, pp. 862–873.

[19] L. Ren, C. W. Fletcher, A. Kwon, E. Stefanov, E. Shi, M. van Dijk,
and S. Devadas, “Constants count: Practical improvements to oblivious
RAM,” in 24th USENIX Security Symposium, USENIX Security 15,
Washington, D.C., USA, August 12-14, 2015., 2015, pp. 415–430.

[20] T. Toft, “Brief announcement: Secure data structures based on multi-
party computation,” in 30th ACM PODC, C. Gavoille and P. Fraigniaud,
Eds. ACM, Jun. 2011, pp. 291–292.

[21] J. C. Mitchell and J. Zimmerman, “Data-oblivious data structures,”
in 31st International Symposium on Theoretical Aspects of Computer
Science (STACS 2014), 2014, pp. 554–565.
[Online]. Available:
http://dx.doi.org/10.4230/LIPIcs.STACS.2014.554

[22] M. Blanton, A. Steele, and M. Aliasgari, “Data-oblivious graph algo-
rithms for secure computation and outsourcing,” in ASIACCS 13. ACM
Press, May 2013, pp. 207–218.

[23] D. Micciancio, “Oblivious data structures: Applications to cryptogra-

phy,” in 29th ACM STOC. ACM Press, May 1997, pp. 456–464.

[24] M. Naor and V. Teague, “Anti-presistence: History independent data
structures,” in 33rd ACM STOC. ACM Press, Jul. 2001, pp. 492–501.
[25] J. D. Hartline, E. S. Hong, A. E. Mohr, W. R. Pentney, and E. C.
Rocke, “Characterizing history independent data structures,” Algorith-
mica, vol. 42, no. 1, pp. 57–74, 2005.

[26] N. Buchbinder and E. Petrank, “Lower and upper bounds on obtaining
history independence,” Inf. Comput., vol. 204, no. 2, pp. 291–337,
2006. [Online]. Available: http://dx.doi.org/10.1016/j.ic.2005.11.001

[27] G. E. Blelloch and D. Golovin, “Strongly history-independent hashing
IEEE Computer Society Press, Oct.

with applications,” in 48th FOCS.
2007, pp. 272–282.

[28] M. Naor, G. Segev, and U. Wieder, “History-independent cuckoo hash-
ing,” in ICALP 2008, Part II, ser. LNCS, L. Aceto, I. Damgård, L. A.
Goldberg, M. M. Halldórsson, A. Ingólfsdóttir, and I. Walukiewicz, Eds.,
vol. 5126. Springer, Jul. 2008, pp. 631–642.

[29] D. Golovin, “B-treaps: A uniquely represented alternative to B-trees,”
in ICALP 2009, Part I, ser. LNCS, vol. 5555. Springer, Jul. 2009, pp.
487–499.

[30] ——, “The B-skip-list: A simpler uniquely represented alternative to

B-trees,” CoRR, vol. abs/1005.0662, 2010.

[31] S. Bajaj, A. Chakraborti, and R. Sion, “Practical foundations of history

independence,” CoRR, vol. abs/1501.06508, 2015.

[32] J. Reardon, D. A. Basin, and S. Capkun, “Sok: Secure data deletion,”
in 2013 IEEE Symposium on Security and Privacy, SP 2013, 2013, pp.
301–315. [Online]. Available: http://dx.doi.org/10.1109/SP.2013.28

[33] D.

R.

and
J.
Proceedings

Lipton,
the
of

Boneh
in

system,”
Symposium,
conference/6th-usenix-security-symposium/revocable-backup-system

[Online]. Available:

1996.

“A
revocable
6th USENIX

backup
Security
https://www.usenix.org/

[34] G. D. Crescenzo, N. Ferguson, R. Impagliazzo, and M. Jakobsson, “How

to forget a secret,” in STACS, 1999, pp. 500–509.

[35] Z. N. J. Peterson, R. C. Burns, J. Herring, A. Stubbleﬁeld, and A. D.
Rubin, “Secure deletion for a versioning ﬁle system,” in Proceedings
of the FAST ’05 Conference on File and Storage Technologies, 2005.
[Online]. Available: http://www.usenix.org/events/fast05/tech/peterson.
html

[36] S. Mitra, M. Winslett, and N. Borisov, “Deleting index entries from
compliance storage,” in EDBT 2008, 11th International Conference
on Extending Database Technology, 2008, pp. 109–120. [Online].
Available: http://doi.acm.org/10.1145/1353343.1353361

[37] J. Katz and Y. Lindell, Introduction to Modern Cryptography. Chapman

and Hall/CRC Press, 2007.

[38] J. R. Lorch, B. Parno, J. W. Mickens, M. Raykova, and J. Schiffman,
“Shroud: ensuring private access to large-scale data in the data center.”
in FAST, vol. 2013, 2013, pp. 199–213.

[39] X. Yu, L. Ren, C. W. Fletcher, A. Kwon, M. van Dijk, and S. Devadas,
“Enhancing oblivious ram performance using dynamic prefetching,”
IACR Cryptology ePrint Archive, vol. 2014, p. 234, 2014.

[40] E. Stefanov, M. van Dijk, E. Shi, T.-H. H. Chan, C. F. L. Ren, X. Yu,
and S. Devadas, “Path ORAM: an extremely simple oblivious RAM
protocol,” CoRR, vol. abs/1202.5150v3, 2014.
[Online]. Available:
http://arxiv.org/abs/1202.5150v3

[41] W. Hoeffding, “Probability inequalities for sums of bounded random
variables,” J. Amer. Statist. Assoc., vol. 58, pp. 13–30, 1963. [Online].
Available: http://www.jstor.org/stable/2282952

[42] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction

to Algorithms, 2nd ed. The MIT Press, September 2001.

APPENDIX A

VORAM OPERATION DETAILS

We give the pseudocode of vORAM helper functions.
idgen()
1: Choose r ← {0, 1}2T +γ.
2: return 1(cid:4)r.
loc(id, t)
1: return the location of the node at level t along the path from
the root to the leaf node identiﬁed by id. This is is simply the
index indicated by the (t + 1) most signiﬁcant bits of id.

(cid:4) rootkey : enc key for root bucket

evict(id)
1: key ← rootkey
2: B ← empty list
3: for t = 0, 1, . . . T do
4:

remove bucket at loc(id, t) from persistent storage
decrypt it with key
Append all partial blocks in the bucket to the end of B
key ← child key from bucket according to loc(id, t + 1)

5:
6:
7: end for
8: for each partial block (id∗, (cid:3), blk) in B do
if (id∗, (cid:3)0, blk0) is in stash already
9:
then replace with (id∗, (cid:3)0 + (cid:3), blk0 blk)
10:
else Add (id∗, (cid:3), blk) to stash
11:
12: end for
writeback(id)
1: key ← nil
2: for t = T, T − 1, . . . , 0 do
3:

W←{(id∗, (cid:3), blk) ∈ stash : loc(id∗, t) = loc(id, t)}

(cid:4) merge

4:

5:
6:
7:

8:
9:
10:
11:

(cid:4) W is the partial blocks storable in the bucket

create empty bucket with new child key key
(other child key remains the same)
while W is not empty and bucket is not full do

in the bucket with blk = blk0 blk1 and |blk1| = (cid:3)1.

(id∗, (cid:3), blk) ← arbitrary element from W
(id∗, (cid:3)1, blk1) ← largest partial block of the above, ﬁtting
Add (id∗, (cid:3)1, blk1) to the bucket
if (cid:3)1 = (cid:3)
then remove (id∗, (cid:3), blk) from W and from stash
else replace (id∗, (cid:3), blk) in stash with (id∗, (cid:3)− (cid:3)1, blk0).
(cid:4) split a partial block

end while
key ← {0, 1}λ chosen uniformly at random
insert Enckey(bucket) at loc(id, t) in persistent storage.

12:
13:
14:
15: end for
16: rootkey ← key

Now, we give the pseudocode of the vORAM operations:
insert(blk)
1: id0 ← idgen()
2: evict(id0)
3: id+ ← idgen()
4: insert (id+,|blk|, blk) into stash
5: writeback(id0)
6: return id+

remove(id)
1: evict(id)
2: remove (id, (cid:3), blk) from stash
3: writeback(id)
4: return blk

update(id, callback)
1: evict(id)
2: remove (id, (cid:3), blk) from stash
3: id+ ← idgen()
4: blk+ ← callback(blk)
5: insert (id+,|blk+|, blk+
6: writeback(id)
7: return id+

) into stash

APPENDIX B

HIRB OPERATION DETAILS

All

the HIRB tree operations depend on a subroutine
HIRBpath, which given a label hash, HIRB root node iden-
+
1 ) corre-
tiﬁer, and vORAM, generates tuples ((cid:2), v0, v1, cid
sponding to the search path for that label in the HIRB. In
each tuple, (cid:2) is the level of node v0, which is along the search
path for the label. In the initial part of the search path, that
is, before the given label hash is found, node v1 is always nil,
a dummy access used to preserve obliviousness. The value
+
1 is the pre-generated identiﬁer of the new node that will
cid
be inserted on the next level, for possible inclusion in one
of the parent nodes as a child pointer. This pre-generation is
important, as discussed in Section V, so that only 2 nodes
need to be stored in local memory at any given time.

When the given label hash is found, the search path splits
into two below that node, and nodes v0 and v1 will be the
nodes on either side of that hash label. Note that
in the
actual implementation of HIRBpath, v0 (resp. v1, if deﬁned)
corresponds to a vORAM block, evicted with identiﬁer id0

193193

0 ) ← (rootid, M.idgen())
1 ) ← (M.idgen(), M.idgen())

(resp. id1) and taken out from vORAM stash. When each tuple
+
1 ) is returned from the generator, the two nodes
((cid:2), v0, v1, cid
can be modiﬁed by the calling function, and the modiﬁed
nodes will be written back to the HIRB. If v1 is returned
from HIRBpath as nil, but is then modiﬁed to be a normal
HIRB node, that new node is subsequently inserted into the
HIRB.
HIRBpath(h, rootid, M)
1: (id0, id+
2: rootid ← id+
3: (id1, id+
4: f ound ← false
5: for (cid:3) = 0, 1, 2, . . . , H do
6:
7:
8:
9:
10:
11:
12:
13:
14:

M.evict(id0)
M.evict(id1)
if (cid:3) = H then (cid+
else (cid+
remove (id0,|v0|, v0) from M.stash
if f ound = true then

remove (id1,|v1|, v1) from M.stash
(cid0, v0.childlast) ← (v0.childlast, cid+
0 )
(cid1, v1.child0) ← (v1.child0, cid+
1 )

1 ) ← (M.idgen(), M.idgen())

1 ) ← (nil, nil)

(cid:4) M is vORAM

(cid:4) dummy access

0 , cid+

0 , cid+

0

(cid:4) v1 is right next to v0 at level (cid:3)

else

(cid:4) only fetched after the target is found.
(cid:4) v0.hi−1 < h ≤ v0.hi

v1 ← nil
i ← index of h in v0
(cid0, v0.childi) ← (v0.childi, cid+
0 )
if v0.hi = h then
f ound ← true
(cid1, v0.childi+1) ← (v0.childi+1, cid+
1 )

(cid:4) split path: cid0 = v0.childi, cid1 = v0.childi+1

(cid:4) dummy access until found

(cid:4) Return to the caller, who may modify nodes.

0 ,|v0|, v0) into M.stash

1 ,|v1|, v1) into M.stash

15:
16:
17:
18:
19:
20:
21:

22:
23:
24:
25:
26:

cid1 ← M.idgen()

else

end if

end if
yield ((cid:3), v0, v1, cid+
1 )

insert (id+
if v1 (cid:7)= nil then insert (id+
M.writeback(id0)
M.writeback(id1)
(id0, id+
(id1, id+

0 ) ← (cid0, cid+
0 )
1 ) ← (cid1, cid+
1 )

27:
28:
29:
30:
31:
32:
33: end for

The update operation simply looks in each returned v0
along the search path for the existence of the indicated label
hash, and if found, the corresponding data value is passed to
the callback function, possibly modifying it.

As with update,

the insert operation uses subroutine
HIRBpath as a generator to traverse the HIRB tree. Inserting
an element from the HIRB involves splitting nodes along the
search path from the height of the item down to the leaf. That
is, for each tuple ((cid:2), v0, v1) with (cid:2) > (cid:2)h, where (cid:2)h is the height
of the label hash h, if v1 is nil, then a new node v1 is created,
and the items in v0 with a label greater than h are moved to
a new node v1.

The remove operation works similarly, but instead of split-
ting each v0 below the level of the found item, the values in
v0 and v1 are merged into v0, and v1 is removed by setting it
to nil.
hirbinit(H, M)
1: rootid ← nil

2: salt ← {0, 1}λ. Initialize Hash with salt.
3: for (cid:3) = H, H − 1, . . . , 0 do
node ← new 1-ary HIRB node with child id rootid
4:
rootid ← M.insert(node)
5:
6: end for
7: return rootid
chooseheight(label)
1: h ← Hash(label)
2: Choose coins (c0, c1, . . . , cH−1) ∈ {0, 1, . . . , β − 1}H by
3: return The largest integer (cid:3) ∈ {0, 1, . . . , H} such that c1 =

evaluating PRG(h).
c2 = ··· = c(cid:4) = 0.

(cid:4) v0.hi−1 < h ≤ v0.hi

1 ) ∈ HIRBpath(h, rootid, M) do

insert(label, value, rootid, M)
1: (h, (cid:3)h) ← (Hash(label), chooseheight(label))
2: for ((cid:3), v0, v1, cid+
3:
4:
5:
6:
7:

i ← index of h in v0
if v0.hi = h then
v0.valuei ← value

else if (cid:3) = (cid:3)h then

(h, value, cid+
1 )

Insert

(v0.hi, v0.valuei, v0.childi)

else if (cid:3) > (cid:3)h and v1 = nil then

v1 ← new node with v1.child0 ← cid+
Move items in v0 past index i into v1

1

8:
9:
10:
11:
12: end for

end if

before

(cid:4) Other items in v0 are shifted over

1 ) ∈ HIRBpath(label, rootid, M) do

remove(label, rootid, M)
1: (h, (cid:3)h) ← (Hash(label), chooseheight(label))
2: for ((cid:3), v0, v1, cid+
if h ∈ v0 then
3:
4:
else if (cid:3) > (cid:3)h and v1 (cid:7)= nil then
5:
6:
7:
8:
9: end for

Add all items in v1 except v1.child0 to v0
v1 ← nil

end if

Remove h and its associated value and subtree from v0

update(label, callback, rootid, M)
1: h ← Hash(label)
2: for ((cid:3), v0, v1) ∈ HIRBpath(h, rootid, M) do
3:
4:
5: end for

i ← index of h in v0
if v0.hi = h then v0.valuei ← callback(v0.valuei)

APPENDIX C

PROOFS OF IMPORTANT THEOREMS

A. Proof of Theorem 1

Let D be any system that stores blocks of data in persistent
storage and erasable memory and supports insert and remove
operations, accessing at most k bytes in persistent or local
Let n ≥ 36 and k ≤ √
storage in each insert or remove operation.
n/2. For any (cid:2) ≤ n/(4k), we
describe a PPT adversary A = (A1,A2) that breaks history
independence with leakage of (cid:2) operations.
Supposing all operations are insertions, D must access the
location where that item’s data is actually to be stored during
execution of the insert operation, which is required to correctly
store the data somehow. However, it may access some other
locations as well to “hide” the access pattern from a potential

194194

attacker. This hiding is limited of course by k, which we will
now exploit.
The “chooser”, A1, randomly chooses n items which will
these could simply be random bit strings of
be inserted;
equal
these items (and their arbitrary order)
a1, a2, . . . , an. The chooser also randomly picks an index
j ∈ {1, 2, . . . , n − (cid:2) − 1} from the beginning of the sequence.
−→
op(0) returned by A1 consists of n
The operation sequence
insertion operations for a1, . . . , an in order:

length. Call

a1, . . . , aj−1, aj, aj+1, . . . , an−(cid:4)−1, an−(cid:4), an−(cid:4)+1, . . . , an,
−→
op(1) returned by A1
whereas the second operation sequence
contains the same n insertions, with only the order of the j’th
and (n − (cid:2))’th insertions swapped:
a1, . . . , aj−1, an−(cid:4), aj+1, . . . , an−(cid:4)−1, aj, an−(cid:4)+1, . . . , an.
The adversary A1 includes the complete list of a1 up to an,
along with the distinguished index j, in the ST which is passed
to A2. As the last (cid:2) operations are identical (insertion of items
an−(cid:4)+1 up to an), A1 is (cid:2)-admissible.
The “guesser“, A2, looks back in the last ((cid:2) + 1)k entries in
−→
the access pattern history of persistent storage
acc, and tries to
opportunistically decrypt the data in each access entry using
the keys from D.em (and, recursively, any other decryption
keys which are found from decrypting data in the access
pattern history). Some of the data may be unrecoverable, but
at least the (cid:2) + 1 items which were inserted in the last (cid:2) + 1
operations must be present
in the decryptions, since their
data must be recoverable using the erasable memory. Then
the guesser simply looks to see whether aj is present in the
decryptions; if aj is present then A2 returns 1, otherwise if
aj is not present then A2 returns 0.
obl-hiA (D, λ, n, 1, 1), aj must be
among the decrypted values in the last ((cid:2) + 1)k access entries,
since aj was inserted within the last (cid:2) + 1 operations and each
operation is allowed to trigger at most k operations on the per-
obl-hiA (D, λ, n, 1, 1) = 1] = 1.
sistent storage. Therefore Pr[EXP
obl-hiA (D, λ, n, 1, 0), we know that
each item an−(cid:4), . . . , an must be present in the decryptions,
and there can be at most ((cid:2) + 1)(k − 1) other items in the
decryptions. Since the index j was chosen randomly from
among the ﬁrst n − (cid:2) − 1 items in the list, the probability
that aj is among the decrypted items in this case is at most

In the experiment EXP

In the experiment EXP

((cid:2) + 1)(k − 1)

n − (cid:2) − 1

.

From the restriction that (cid:2) ≤ n/(4k), and k ≤ √
((cid:2) + 1)(k − 1) < ((cid:2) + 1)k = (cid:2)k + k ≤ n

we have

4 + n

12 = n
3 .

n/2 ≤ n/12,

In addition, we have n − (cid:2) − 1 > n/2, so the probability
that aj is among the decrypted items is at most 2
3, and we
obl-hiA (D, λ, n, 1, 0) = 1] ≤ 2/3, and therefore
have Pr[EXP
AdvhiA(D, λ, n) ≥ 1/3. According to the deﬁnition,
this
means that D does not provide history independence with
leakage of (cid:2) operations.

B. Proof of Theorem 5

Our proofs on the distribution of block sizes in the ORAM
and on the number of HIRB nodes depend on the following
bound on the sum of geometric random variables. This is a
(cid:3)
standard type of result along the lines of Lemma 6 in [11].
1≤i≤n Xi be the sum of n ≥ 1 inde-
Lemma 10. Let X =
pendent random variables Xi, each stochastically dominated
by a geometric distribution over {0, 1, 2, . . .} with expected
value E[Xi] ≤ μ. Then there exists a constant c0 > 0 whose
value depends only on μ such that, for any a ≥ 2 and b ≥ 0,
we have

Pr[X ≥ (μ + 1)(an + b)] < exp(−c0(an + b)).

Proof. By linearity of expectation, E[X] =
nμ.

Recall that a geometric random variable with expected value
μ is equivalent to the number of independent Bernoulli trials,
each with probability p = 1/(μ + 1), before the ﬁrst success.
If X ≥ (μ+1)(an+b), this is equivalent to having fewer than
n successes over k = (μ + 1)(an + b) independent Bernoulli
trials with probability p.

Using this formulation, we can apply the Hoeffding inequal-

ity to obtain
Pr[X ≥ k] = Pr[Binomial(k, p) ≤ n − 1] < exp(−2
where  is deﬁned such that n − 1 = (p − )k; namely

2

k),

(cid:3)
i∈[n] E[Xi] ≤

μ+1 − n−1
k .

 = p − n−1
k = 1
(cid:4)
We do some manipulation:
(μ+1)2 ·
1 − (n−1)(μ+1)
(cid:5)2
(cid:4)
1 − n−1

k = 2k

2(an+b)

2

=

·

k

2

.

μ+1

an+b

(cid:5)2

Because a ≥ 2 and b ≥ 0, we have
an ≤ 1
n−1
an+b < n
2 ,
(cid:4)
− 1

and so

exp(−2

k) < exp

2

(cid:5)

.

2(μ+1) (an + b)

The stated result follows with the constant deﬁned by

c0 = 1

2(μ+1) .

(1)
(cid:2)

Outline of proof of Theorem 5. We will mostly follow the
proof of the small-stash-size theorem in Path ORAM [9]. The
proof of the theorem consists of several steps.
the deﬁnition of ∞-ORAM (ORAM with
inﬁnitely large buckets) and show that stash usage in an
∞-ORAM with post-processing is the same as that in the
actual vORAM.

1) We recall

2) We rely on results from the most recent version of [40] to
show that the stash usage after post-processing is greater

195195

than R if and only if there exists a subtree for which its
usage in ∞-ORAM is more than its capacity.

3) We bound the total size of all blocks in any such subtree
by combining two separate measure concentrations on the
number of blocks in any such subtree, and the total size
of any ﬁxed number of variable-length blocks.

4) We complete the proof by connecting the measure con-
centrations to the actual stash size, in a similar way to
[40].

Note that the ﬁrst and third steps are those that differ most
substantially from prior work, and where we must incorporate
the unique properties of the vORAM.
Proof of Theorem 5. We now give the proof.
Step 1: ∞-ORAM. The ∞-ORAM is the same as our vORAM
tree, except that each bucket has inﬁnite size. In any writeback
operation all blocks will go as far down along the path as their
identiﬁer allows.
After simulating a series of vORAM operations on the ∞-
ORAM, we perform a greedy post-processing to restore the
block size condition:

• Repeatedly select a bucket storing more than Z bytes.
Remove a partial block from the bucket, and let b be the
number of remaining bytes stored in the bucket.
• If Z − b is greater than the size of metadata per partial
block (identiﬁer and length), then there is some room left
in the bucket. In this case, split the removed block into
two parts. Place the last Z−b bytes into the current bucket
and the remainder into the parent bucket. Otherwise, if
there is insufﬁcient room in the bucket, place the entire
block into the parent bucket, or into the stash if the current
bucket is the root.

By continuing this process until there are no remaining buckets
with greater than Z bytes, we have returned to a normal
vORAM with bucket size Z. Furthermore, there is an ordering
of the accesses, with the same identiﬁers and block lengths,
that would result in the same vORAM. Since the access order
of the ∞-ORAM does not matter, this shows that the two
models are equivalent after post-processing.

Observe that we are ignoring the metadata (block identiﬁers
and length strings). This is acceptable, as the removal process
in the actual vORAM always ensures that each partial block
of a given block, except possibly for the ﬁrst (highest in the
vORAM tree), has size at least equal to the size of its metadata.
In that way, at most half the vORAM is used for metadata
storage, and so the metadata has only a constant factor effect
on the overall performance.
Step 2: Overﬂowing subtrees. Consider the size of vORAM
stash after any series of vORAM operations that result in a
total of at most n blocks being stored. Similarly to [40, Lemma
2], the stash size at this point is equal to the total overﬂow
from some subtree of the ∞-ORAM buckets that contains the
root. If we write τ for that subtree, then we have

(cid:3)
|stash| > BR

iff

node v∈τ (size of v in ∞-ORAM) ≥ Z|τ| + BR.

Step 3: Size of subtrees. We prove a bound on the total size
of all blocks in any subtree τ in the ∞-ORAM in two steps.
First we bound the number of blocks in the subtree, which
can use the same analysis as the Path ORAM; then we bound
the total size of a given number of variable-length blocks; and,
ﬁnally, we combine these with a union bound argument.
To bound the total number of blocks that occur in τ, because
the block sizes do not matter in the ∞-ORAM, we can
simply recall from [40, Lemma 5] that, for any subtree τ,
the probability that τ contains more than 5|τ| + R/4 blocks
is at most

4|τ| · (0.9332)

1

|τ| · (0.881)

.

(2)
Next we consider the total size of 5|τ|+R/4 variable-length
blocks. From the statement of the theorem, each block size
is stochastically dominated by BX, where B is the expected
block size and X is a geometric random variable with expected
value μ = 1. From Lemma 10, the total size of all 5|τ| + R/4
blocks exceeds 2(a(5|τ| + R/4))B with probability at most

R

exp (−c0a (5|τ| + R/4)) .

From (1), we can take c0 = 1/4, and by setting a = 2 >
(cid:7)
(cid:6)− 5
(4/5) ln 4, the probability that the total size of 5|τ| + R/4
2|τ| − 1
blocks exceeds (20|τ| + R)B is at most exp
,
8 R
which in turn is less than

4|τ| · (0.329)

1

|τ| · (0.883)

R

.

(3)

Finally, by the union bound, the probability that the total
size of all blocks in τ exceeds (20|τ| + R)B is at most the
sum of the probabilities in (2) and (3), which is less than

4|τ| · (0.9332)

2

|τ| · (0.883)

R

.

(4)

Step 4: Stash overﬂow probability. As in [40, Section 5.2],
the number of subtrees of size i is less than 4i, and therefore
by another application of the union bound along with (4), the
probability of any subtree τ having total block size greater
than (20|τ| + R)B is at most

4i · (0.9332)

(cid:8)
i≥1
< 28 · (0.883)

i 2
4

R

.

i · (0.883)

R

C. Proof of Theorem 8

We now utilize Lemma 10 to prove the two lemmata on the

distributions of the number and size of HIRB tree nodes.
Lemma 11. Suppose a HIRB tree with n items has height
H ≥ logβ n, and let X be the total number of nodes in the
HIRB, which is a random variable over the choice of hash
function in initializing the HIRB. Then for any m ≥ 1, we
have

Pr [X ≥ H + 4n + m] < 0.883

m

.

In other words, the number of HIRB nodes in storage at
any given time is O(n) with high probability. The proof is a
fairly standard application of the Hoeffding inequality [41].

196196

From the assumption H ≥ logβ n, nk ≤ βHk, so the bound
−k+1. Setting k = mβ, the probability
above becomes simply 2
that the root node has at least k items and hence branching
−mβ+1, which
factor greater than mβ, is seen to be at most 2
is always at most 2

−m because m ≥ 1 and β ≥ 2..

Next consider any nonempty HIRB tree node at height (cid:2),
and consider a hypothetically inﬁnite list of possible label
hashes from the HIRB which have height at least (cid:2) and could
be in this node. The actual number of items is determined
by the number of those labels whose height is exactly equal
to (cid:2) before we ﬁnd one whose height
least (cid:2) + 1.
From Assumption 6, and the memorylessness property of the
geometric distribution,
these label heights are independent
Bernoulli trials, and each height equals (cid:2) with probability
(β − 1)/β.
Therefore the size of each non-root node is a geometric
random variable over {0, 1, . . .} with parameter 1/β. The
probability that the node contains at least mβ items, and
therefore has banching factor greater than mβ, is exactly

is at

(cid:5)mβ

(cid:4)

β−1
β

< exp(−m) < 0.5

m

.

x )ax < exp(−a) for any x ≥ 1

Here we use the fact that (1− 1
and any real a.
All that remains is to say that a node with branching factor
mβ has size less than m · nodesizeβ, which follows directly
from m ≥ 1 and the deﬁnition of nodesizeβ in (5).
(cid:2)

Finally, we prove the main theorems on the vORAM+HIRB

performance and security.
Proof of Theorem 8. We step through and motivate the
choices of parameters, one by one.
The expected branching factor β must be at least 2 for the
HIRB to work, which means we must always have H ≤ lg n,
and so T = lg(4n + lg n + γ) ≤ lg(4n + H + γ). Then
Lemma 11 guarantees that the number of HIRB nodes is less
than H +4n+γ with probability at least (0.883)γ. This means
that T is an admissible height for the vORAM according to
Theorem 5 with at least that probability.
The choice of β is such that Z ≥ 20 · nodesizeβ, using the

inequality

H ≤ lg n < lg(4n) < T.

Therefore, by Lemma 12, the size of blocks in the HIRB will
be admissible for the vORAM according to Theorem 5.
This allows us to say from the choice of R and Theorem 5
that the probability of stash overﬂow is at most 28 · (0.883)γ.
Choosing H as we do is required to actually apply Lem-

mas 11 and 12 above.

Finally, the probability of two label hashes in the HIRB
−γ. The stated result follows from the

colliding is at most 2
union bound over the three failure probabilities.

Proof. The HIRB has H nodes initially. Consider the n
items label1, . . . , labeln in the HIRB. Because the tree is
uniquely represented, we can consider the number of nodes
after inserting the items in any particular order.

When inserting an item with labeli into the HIRB, its height
h = chooseheight(labeli) is computed from the label hash,
where 0 ≤ h ≤ H, and then exactly h existing HIRB nodes
are split when labeli is inserted, resulting in exactly h newly
created nodes.

Therefore the total number of nodes in the HIRB after
inserting all n items is exactly H plus the sum of the heights
of all items in the HIRB, which from Assumption 6 is the
sum of n iid geometric random variables, each with expected
value 1/(β − 1). Call this sum Y .
We are interested in bounding the probability that Y exceeds
4n + m. Writing μ = 1/(β− 1) for the expected value of each
r.v., we have μ + 1 = β/(β − 1), which is at most 2 since
β ≥ 1. This means that 4n + m ≥ (μ + 1)(2n + m/2), and
from Lemma 10,

Pr[X ≥ H + 4n + m] = Pr[Y ≥ 4n + m]

≤ Pr[Y ≥ (μ + 1)(2n + m/2)]
< exp(−c0(2n + m/2))
< exp(−c0m/2).

Because μ + 1 ≤ 2, c0 = 1/(2(μ + 1)) ≥ 1/4. Numerical
computation conﬁrms that exp(−1/8) < 0.883, which com-
(cid:2)
pletes the proof.

Along with the bound above on the number of HIRB nodes,

we also need a bound on the size of each node.
Lemma 12. Suppose a HIRB tree with n items has height
H ≥ logβ n, and let X, a random variable over the choice of
hash function, be the size of an arbitrary node in the HIRB.
Then for any m ≥ 1, we have

Pr[X ≥ m · nodesizeβ] < 0.5

m

.

The proof of this lemma works by ﬁrst bounding the
probability that the number of items in any node is at most
mβ and applies the formula for node size, i.e.,

nodesizek =
(k + 1)(2T + γ + 1) + k(|Hash(label)| + |value|).

(5)

Proof. We ﬁrst show that
the probability that any node’s
branching factor is more than mβ is at most 0.5m. This ﬁrst
part requires a special case for the root node, and a general
case for any other node. Then we show that any node with
branching factor at most mβ has size less than m· nodesizeβ.
First consider the items in the root node. These items all
have height H, which according to Assumption 6 occurs for
any given label with probability 1/βH. Therefore the number
of items in the root node follows a binomial distribution with
parameter 1/βH. It is a standard result (for example, Theorem
C.2 in [42]) that a sample from such a distribution is at least
(cid:10)
k with probability at most
1
βHk <

2k−1βHk .

(cid:9)

n
k

nk

197197

