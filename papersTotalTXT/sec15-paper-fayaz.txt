Bohatei: Flexible and Elastic DDoS Defense
Seyed K. Fayaz, Yoshiaki Tobioka, and Vyas Sekar, Carnegie Mellon University;  

Michael Bailey, University of Illinois at Urbana-Champaign

https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/fayaz

This paper is included in the Proceedings of the 24th USENIX Security SymposiumAugust 12–14, 2015 • Washington, D.C.ISBN 978-1-931971-232Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXBohatei: Flexible and Elastic DDoS Defense

Seyed K. Fayaz∗

Yoshiaki Tobioka∗

∗CMU

Vyas Sekar∗ Michael Bailey†

†UIUC

Abstract

DDoS defense today relies on expensive and propri-
etary hardware appliances deployed at ﬁxed locations.
This introduces key limitations with respect to ﬂexibil-
ity (e.g., complex routing to get trafﬁc to these “choke-
points”) and elasticity in handling changing attack pat-
terns. We observe an opportunity to address these limita-
tions using new networking paradigms such as software-
deﬁned networking (SDN) and network functions virtu-
alization (NFV). Based on this observation, we design
and implement Bohatei, a ﬂexible and elastic DDoS de-
fense system.
In designing Bohatei, we address key
challenges with respect to scalability, responsiveness,
and adversary-resilience. We have implemented de-
fenses for several DDoS attacks using Bohatei. Our
evaluations show that Bohatei is scalable (handling 500
Gbps attacks), responsive (mitigating attacks within one
minute), and resilient to dynamic adversaries.
1
In spite of extensive industrial and academic efforts
(e.g., [3, 41, 42]), distributed denial-of-service (DDoS)
attacks continue to plague the Internet. Over the last
few years, we have observed a dramatic escalation
in the number, scale, and diversity of DDoS attacks.
For instance, recent estimates suggest that over 20,000
DDoS attacks occur per day [44], with peak volumes
of 0.5 Tbps [14, 30]. At the same time, new vec-
tors [37, 55] and variations of known attacks [49] are
constantly emerging. The damage that these DDoS at-
tacks cause to organizations is well-known and include
both monetary losses (e.g., $40,000 per hour [12]) and
loss of customer trust.

Introduction

DDoS defense today is implemented using expensive
and proprietary hardware appliances (deployed in-house
or in the cloud [8, 19]) that are ﬁxed in terms of place-
ment, functionality, and capacity. First, they are typi-
cally deployed at ﬁxed network aggregation points (e.g.,
a peering edge link of an ISP). Second, they provide

ﬁxed functionality with respect to the types of DDoS at-
tacks they can handle. Third, they have a ﬁxed capacity
with respect to the maximum volume of trafﬁc they can
process. This ﬁxed nature of today’s approach leaves
network operators with two unpleasant options: (1) to
overprovision by deploying defense appliances that can
handle a high (but pre-deﬁned) volume of every known
attack type at each of the aggregation points, or (2) to
deploy a smaller number of defense appliances at a cen-
tral location (e.g., a scrubbing center) and reroute traf-
ﬁc to this location. While option (2) might be more
cost-effective, it raises two other challenges. First, op-
erators run the risk of underprovisioning. Second, traf-
ﬁc needs to be explicitly routed through a ﬁxed central
location, which introduces additional trafﬁc latency and
requires complex routing hacks (e.g., [57]). Either way,
handling larger volumes or new types of attacks typically
mandates purchasing and deploying new hardware appli-
ances.

Ideally, a DDoS defense architecture should provide
the ﬂexibility to seamlessly place defense mechanisms
where they are needed and the elasticity to launch de-
fenses as needed depending on the type and scale of the
attack. We observe that similar problems in other ar-
eas of network management have been tackled by tak-
ing advantage of two new paradigms: software-deﬁned
networking (SDN) [32, 40] and network functions vir-
tualization (NFV) [43]. SDN simpliﬁes routing by de-
coupling the control plane (i.e., routing policy) from the
data plane (i.e., switches). In parallel, the use of virtual-
ized network functions via NFV reduces cost and enables
elastic scaling and reduced time-to-deploy akin to cloud
computing [43]. These potential beneﬁts have led major
industry players (e.g., Verizon, AT&T) to embrace SDN
and NFV [4, 6, 15, 23].1

In this paper, we present Bohatei2, a ﬂexible and

1To quote the SEVP of AT&T: “To say that we are both feet in [on

SDN] would be an understatement. We are literally all in [4].”

2It means breakwater in Japanese, used to defend against tsunamis.

USENIX Association  

24th USENIX Security Symposium  817

elastic DDoS defense system that demonstrates the ben-
eﬁts of these new network management paradigms in the
context of DDoS defense. Bohatei leverages NFV ca-
pabilities to elastically vary the required scale (e.g., 10
Gbps vs. 100 Gbps attacks) and type (e.g., SYN proxy
vs. DNS reﬂector defense) of DDoS defense realized by
defense virtual machines (VMs). Using the ﬂexibility
of SDN, Bohatei steers suspicious trafﬁc through the de-
fense VMs while minimizing user-perceived latency and
network congestion.

In designing Bohatei, we address three key algorith-
mic and system design challenges. First, the resource
management problem to determine the number and loca-
tion of defense VMs is NP-hard and takes hours to solve.
Second, existing SDN solutions are fundamentally un-
suitable for DDoS defense (and even introduce new at-
tack avenues) because they rely on a per-ﬂow orchestra-
tion paradigm, where switches need to contact a network
controller each time they receive a new ﬂow. Finally,
an intelligent DDoS adversary can attempt to evade an
elastic defense, or alternatively induce provisioning inef-
ﬁciencies by dynamically changing attack patterns.

In summary, this paper

We have implemented a Bohatei controller using
OpenDaylight [17], an industry-grade SDN platform.
We have used a combination of open source tools (e.g.,
OpenvSwitch [16], Snort [48], Bro [46], iptables [13]) as
defense modules. We have developed a scalable resource
management algorithm. Our evaluation, performed on a
real testbed as well as using simulations, shows that Bo-
hatei effectively defends against several different DDoS
attack types, scales to scenarios involving 500 Gbps at-
tacks and ISPs with about 200 backbone routers, and can
effectively cope with dynamic adversaries.
Contributions and roadmap:
makes the following contributions:
• Identifying new opportunities via SDN/NFV to im-
prove the current DDoS defense practice (§2);
• Highlighting the challenges of applying existing
SDN/NFV techniques in the context of DDoS
defense(§3);
• Designing a responsive resource management algo-
rithm that is 4-5 orders of magnitude faster than the
state-of-the-art solvers (§4);
• Engineering a practical and scalable network or-
chestration mechanism using proactive tag-based for-
warding that avoids the pitfalls of existing SDN so-
lutions (§5);
• An adaptation strategy to handle dynamic adversaries
that can change the DDoS attack mix over time (§6);
• A proof-of-concept implementation to handle several
known DDoS attack types using industry-grade SD-
N/NFV platforms (§7); and

• A systematic demonstration of the scalability and ef-
fectiveness of Bohatei (§8).
We discuss related work (§9) before concluding (§10).

2 Background and Motivation
In this section, we give a brief overview of software-
deﬁned networking (SDN) and network functions virtu-
alization (NFV) and discuss new opportunities these can
enable in the context of DDoS defense.
2.1 New network management trends
Software-deﬁned networking (SDN): Traditionally,
network control tasks (e.g., routing, trafﬁc engineering,
and access control) have been tightly coupled with their
data plane implementations (e.g., distributed routing pro-
tocols, ad hoc ACLs). This practice has made net-
work management complex, brittle, and error-prone [32].
SDN simpliﬁes network management by decoupling the
network control plane (e.g., an intended routing policy)
from the network data plane (e.g., packet forwarding
by individual switches). Using SDN, a network opera-
tor can centrally program the network behavior through
APIs such as OpenFlow [40]. This ﬂexibility has mo-
tivated several real world deployments to transition to
SDN-based architectures (e.g., [34]).
Network functions virtualization (NFV): Today, net-
work functions (e.g., ﬁrewalls, IDSes) are implemented
using specialized hardware. While this practice was nec-
essary for performance reasons, it leads to high cost and
inﬂexibility. These limitations have motivated the use
of virtual network functions (e.g., a virtual ﬁrewall) on
general-purpose servers [43]. Similar to traditional vir-
tualization, NFV reduces costs and enables new opportu-
nities (e.g., elastic scaling). Indeed, leading vendors al-
ready offer virtual appliance products (e.g., [24]). Given
these beneﬁts, major ISPs have deployed (or are planning
to deploy) datacenters to run virtualized functions that re-
place existing specialized hardware [6, 15, 23]. One po-
tential concern with NFV is low packet processing per-
formance. Fortunately, several recent advances enable
line-rate (e.g., 10-40Gbps) packet processing by soft-
ware running on commodity hardware [47]. Thus, such
performance concerns are increasingly a non-issue and
will further diminish given constantly improving hard-
ware support [18].
2.2 New opportunities in DDoS defense
Next, we brieﬂy highlight new opportunities that SDN
and NFV can enable for DDoS defense.
Lower capital costs: Current DDoS defense is based
on specialized hardware appliances (e.g., [3, 20]). Net-
work operators either deploy them on-premises, or out-
source DDoS defense to a remote packet scrubbing site
(e.g., [8]).
In either case, DDoS defense is expensive.

818  24th USENIX Security Symposium 

USENIX Association

For instance, based on public estimates from the Gen-
eral Services Administration (GSA) Schedule, a 10 Gbps
DDoS defense appliance costs ≈$128,000 [11]. To put
this in context, a commodity server with a 10 Gbps Net-
work Interface Card (NIC) costs about $3,000 [10]. This
suggests roughly 1-2 orders of magnitude potential re-
duction in capital expenses (ignoring software and de-
velopment costs) by moving from specialized appliances
to commodity hardware.3
Time to market: As new and larger attacks emerge,
enterprises today need to frequently purchase more ca-
pable hardware appliances and integrate them into the
network infrastructure. This is an expensive and tedious
process [43]. In contrast, launching a VM customized for
a new type of attack, or launching more VMs to handle
larger-scale attacks, is trivial using SDN and NFV.
Elasticity with respect to attack volume:
Today,
DDoS defense appliances deployed at network choke-
points need to be provisioned to handle a predeﬁned
maximum attack volume. As an illustrative example,
consider an enterprise network where a DDoS scrubber
appliance is deployed at each ingress point. Suppose the
projected resource footprint (i.e., defense resource us-
age over time) to defend against a SYN ﬂood attack at
times t1, t2, and t3 is 40, 80, and 10 Gbps, respectively.4
The total resource footprint over this entire time period
is 3× max{40,80,10} = 240 Gbps, as we need to provi-
sion for the worst case. However, if we could elastically
scale the defense capacity, we would only introduce a re-
source footprint of 40 + 80 + 10 = 130 Gbps—a 45% re-
duction in defense resource footprint. This reduced hard-
ware footprint can yield energy savings and allow ISPs to
repurpose the hardware for other services.
Flexibility with respect to attack types: Building on
the above example, suppose in addition to the SYN ﬂood
attack, the projected resource footprint for a DNS ampli-
ﬁcation attack in time intervals t1, t2, and t3 is 20, 40,
and 80 Gbps, respectively. Launching only the required
types of defense VMs as opposed to using monolithic
appliances (which handle both attacks), drops the hard-
ware footprint by 40%; i.e., from 3× (max{40,80,10} +
max{20,40,80}) =480 to 270.
Flexibility with respect to vendors: Today, network
operators are locked-in to the defense capabilities offered
by speciﬁc vendors.
In contrast, with SDN and NFV,
they can launch appropriate best-of-breed defenses. For
example, suppose vendor 1 is better for SYN ﬂood de-
fense, but vendor 2 is better for DNS ﬂood defense. The
physical constraints today may force an ISP to pick only

3Operational expenses are harder to compare due to the lack of pub-

licly available data.

4For brevity, we use the trafﬁc volume as a proxy for the memory

consumption and CPU cycles required to handle the trafﬁc.

(cid:5)(cid:5)(cid:21)(cid:7)(cid:1)(cid:11)(cid:12)(cid:13)(cid:12)(cid:20)(cid:24)(cid:12)(cid:1)

(cid:9)(cid:22)(cid:22)(cid:19)(cid:18)(cid:9)(cid:20)(cid:10)(cid:12)(cid:1)

(cid:8)(cid:6)(cid:1)(cid:8)(cid:6)(cid:1)(cid:8)(cid:6)(cid:1)

(cid:15)(cid:21)(cid:29)(cid:31)(cid:1)

(cid:4)(cid:1)

(cid:15)(cid:21)(cid:29)(cid:31)(cid:1)

(cid:2)(cid:1)

(cid:3)(cid:1)

(cid:15)(cid:21)(cid:29)(cid:32)(cid:1)
(cid:25)(cid:23)(cid:9)(cid:14)(cid:10)(cid:1)(cid:13)(cid:21)(cid:21)(cid:25)(cid:22)(cid:23)(cid:18)(cid:20)(cid:25)(cid:1)(cid:16)(cid:18)(cid:28)(cid:12)(cid:20)(cid:1)

(cid:17)(cid:9)(cid:23)(cid:11)(cid:29)(cid:9)(cid:23)(cid:12)(cid:1)(cid:9)(cid:22)(cid:22)(cid:19)(cid:18)(cid:9)(cid:20)(cid:10)(cid:12)(cid:34)(cid:33)(cid:1)(cid:17)(cid:21)(cid:22)(cid:24)(cid:1)

(cid:2)(cid:1)

(cid:15)(cid:21)(cid:29)(cid:32)(cid:1)

(cid:4)(cid:1)
(cid:8)(cid:6)(cid:1)(cid:8)(cid:6)(cid:1)(cid:8)(cid:6)(cid:1)
(cid:25)(cid:23)(cid:9)(cid:14)(cid:10)(cid:1)(cid:13)(cid:21)(cid:21)(cid:25)(cid:22)(cid:23)(cid:18)(cid:20)(cid:25)(cid:1)(cid:16)(cid:18)(cid:28)(cid:12)(cid:20)(cid:1)(cid:1)
(cid:12)(cid:19)(cid:9)(cid:24)(cid:26)(cid:10)(cid:1)(cid:24)(cid:10)(cid:9)(cid:19)(cid:18)(cid:20)(cid:16)(cid:34)(cid:32)(cid:1)(cid:17)(cid:21)(cid:22)(cid:24)(cid:1)

(cid:3)(cid:1)

Figure 1: DDoS defense routing efﬁciency enabled by
SDN and NFV.

one hardware appliance. With SDN/NFV we can avoid
the undesirable situation of picking only one vendor and
rather have a deployment with both types of VMs each
for a certain type of attack. Looking even further, we
also envision that network operators can mix and match
capabilities from different vendors; e.g., if vendor 1 has
better detection capabilities but vendor 2’s blocking al-
gorithm is more effective, then we can ﬂexibly combine
these two to create a more powerful defense platform.
Simpliﬁed and efﬁcient routing: Network operators
today need to employ complex routing hacks (e.g., [57])
to steer trafﬁc through a ﬁxed-location DDoS hardware
appliance (deployed either on-premises or in a remote
site). As Figure 1 illustrates, this causes additional la-
tency. Consider two end-to-end ﬂows f low1 and f low2.
Way-pointing f low2 through the appliance (the left hand
side of the ﬁgure) makes the total path lengths 3 hops.
But if we could launch VMs where they are needed (the
right hand side of the ﬁgure), we could drop the total
path lengths to 2 hops—a 33% decrease in trafﬁc foot-
print. Using NFV we can launch defense VMs on the
closest location to where they are currently needed, and
using SDN we can ﬂexibly route trafﬁc through them.

In summary, we observe new opportunities to build a
ﬂexible and elastic DDoS defense mechanism via SD-
N/NFV. In the next section, we highlight the challenges
in realizing these beneﬁts.
3 System Overview
In this section, we envision the deployment model and
workﬂow of Bohatei, highlight the challenges in realiz-
ing our vision, and outline our key ideas to address these
challenges.
3.1 Problem scope
Deployment scenario: For concreteness, we focus on
an ISP-centric deployment model, where an ISP offers
DDoS-defense-as-a-service to its customers. Note that
several ISPs already have such commercial offerings
(e.g., [5]). We envision different monetization avenues.
For example, an ISP can offer a value-added security ser-
vice to its customers that can replace the customers’ in-
house DDoS defense hardware. Alternatively, the ISP
can allow its customers to use Bohatei as a cloudburst-
ing option when the attack exceeds the customers’ on-

USENIX Association  

24th USENIX Security Symposium  819

(cid:2)(cid:22)(cid:16)(cid:8)(cid:27)(cid:12)(cid:17)(cid:1)(cid:15)(cid:19)(cid:22)(cid:9)(cid:8)(cid:19)(cid:1)
(cid:6)(cid:3)(cid:5)(cid:1)(cid:10)(cid:22)(cid:21)(cid:27)(cid:25)(cid:22)(cid:19)(cid:19)(cid:12)(cid:25)(cid:1)
(cid:2)(cid:22)(cid:16)(cid:8)(cid:27)(cid:12)(cid:17)(cid:1)(cid:19)(cid:22)(cid:10)(cid:8)(cid:19)(cid:1)(cid:1)
(cid:6)(cid:3)(cid:5)(cid:1)(cid:10)(cid:22)(cid:21)(cid:27)(cid:25)(cid:22)(cid:19)(cid:19)(cid:12)(cid:25)(cid:1)

(cid:31)(cid:36)(cid:31)(cid:29)(cid:23)(cid:15)(cid:23)(cid:28)(cid:36)(cid:31)(cid:1)

(cid:32)(cid:30)(cid:13)(cid:20)(cid:15)(cid:1)

(cid:31)(cid:29)(cid:17)(cid:15)(cid:23)(cid:19)(cid:15)(cid:13)(cid:33)(cid:28)(cid:27)(cid:1)

(cid:11)(cid:32)(cid:30)(cid:13)(cid:32)(cid:17)(cid:21)(cid:37)(cid:1)(cid:1)
(cid:42)(cid:6)(cid:12)(cid:10)(cid:28)(cid:22)(cid:21)(cid:1)(cid:35)(cid:43)(cid:1)

(cid:8)(cid:30)(cid:15)(cid:22)(cid:17)(cid:31)(cid:32)(cid:30)(cid:13)(cid:33)(cid:28)(cid:27)(cid:1)

(cid:42)(cid:6)(cid:12)(cid:10)(cid:28)(cid:22)(cid:21)(cid:1)(cid:34)(cid:43)(cid:1)

(cid:12)(cid:26)(cid:28)(cid:20)(cid:8)(cid:28)(cid:22)(cid:21)(cid:1)(cid:22)(cid:13)(cid:1)(cid:31)(cid:22)(cid:19)(cid:30)(cid:20)(cid:12)(cid:1)
(cid:22)(cid:13)(cid:1)(cid:26)(cid:30)(cid:26)(cid:23)(cid:17)(cid:10)(cid:17)(cid:22)(cid:30)(cid:26)(cid:1)(cid:27)(cid:25)(cid:8)(cid:14)(cid:10)(cid:1)
(cid:22)(cid:13)(cid:1)(cid:12)(cid:8)(cid:10)(cid:16)(cid:1)(cid:8)(cid:29)(cid:8)(cid:10)(cid:18)(cid:1)(cid:27)(cid:32)(cid:23)(cid:12)(cid:1)

(cid:8)(cid:27)(cid:1)(cid:12)(cid:8)(cid:10)(cid:16)(cid:1)(cid:17)(cid:21)(cid:15)(cid:25)(cid:12)(cid:26)(cid:26)(cid:1)

(cid:24)(cid:30)(cid:8)(cid:21)(cid:28)(cid:27)(cid:32)(cid:1)(cid:8)(cid:21)(cid:11)(cid:1)
(cid:19)(cid:22)(cid:10)(cid:8)(cid:28)(cid:22)(cid:21)(cid:1)(cid:22)(cid:13)(cid:1)(cid:7)(cid:4)(cid:26)(cid:1)

(cid:10)(cid:17)(cid:31)(cid:28)(cid:36)(cid:30)(cid:15)(cid:17)(cid:1)(cid:26)(cid:13)(cid:27)(cid:13)(cid:21)(cid:17)(cid:30)(cid:1)

(cid:42)(cid:6)(cid:12)(cid:10)(cid:28)(cid:22)(cid:21)(cid:1)(cid:33)(cid:43)(cid:1)

(cid:6)(cid:5)(cid:2)(cid:4)(cid:3)(cid:1)

(cid:6)(cid:11)(cid:8)(cid:6)(cid:13)(cid:1)(cid:4)(cid:2)(cid:3)(cid:1)(cid:1)
(cid:6)(cid:17)(cid:21)(cid:16)(cid:20)(cid:1)(cid:17)(cid:9)(cid:1)(cid:19)(cid:18)(cid:6)(cid:1)

(cid:6)(cid:2)(cid:4)(cid:1)(cid:8)(cid:8)(cid:1)(cid:2)(cid:7)(cid:2)(cid:3)(cid:5)(cid:1)

(cid:14)(cid:17)(cid:10)(cid:1)

(cid:9)(cid:17)(cid:18)(cid:22)(cid:5)(cid:18)(cid:7)(cid:1)(cid:20)(cid:17)(cid:1)
(cid:6)(cid:21)(cid:19)(cid:20)(cid:17)(cid:15)(cid:8)(cid:18)(cid:1)

Figure 3: A sample defense against UDP ﬂood.

(cid:18)(cid:5)(cid:20)(cid:8)(cid:1)(cid:14)(cid:12)(cid:15)(cid:12)(cid:20)(cid:1)

nodes processing (e.g., “benign” or “attack” or “analyze
further”). Each logical node will be realized by one (or
more) virtual appliance(s) depending on the attack vol-
ume. Figure 3 shows an example strategy graph with 4
modules used for defending against a UDP ﬂood attack.
Here, the ﬁrst module tracks the number of UDP pack-
ets each source sends and performs a simple threshold-
based check to decide whether the source needs to be let
through or throttled.

Our goal here is not to develop new defense algorithms
but to develop the system orchestration capabilities to en-
able ﬂexible and elastic defense. As such, we assume the
DAGs have been provided by domain experts, DDoS de-
fense vendors, or by consulting best practices.
3.2 Bohatei workﬂow and challenges
The workﬂow of Bohatei has four steps (see Figure 2):
1. Attack detection: We assume the ISP uses some out-
of-band anomaly detection technique to ﬂag whether
a customer is under a DDoS attack [27]. The de-
sign of this detection algorithm is outside the scope
of this paper. The detection algorithm gives a coarse-
grained speciﬁcation of the suspicious trafﬁc, indi-
cating the customer under attack and some coarse
identiﬁcations of the type and sources of the attack;
e.g., “srcpreﬁx=*,dstpreﬁx=cust,type=SYN”.

2. Attack estimation: Once suspicious trafﬁc is de-
tected, the strategy module estimates the volume of
suspicious trafﬁc of each attack type arriving at each
ingress.

3. Resource management: The resource manager then
uses these estimates as well as the library of defenses
to determine the type, number, and the location of
defense VMs that need to be instantiated. The goal of
the resource manager is to efﬁciently assign available
network resources to the defense while minimizing
user-perceived latency and network congestion.

4. Network orchestration: Finally, the network orches-
tration module sets up the required network forward-
ing rules to steer suspicious trafﬁc to the defense
VMs as mandated by the resource manager.
Given this workﬂow, we highlight the three challenges

we need to address to realize our vision:
C1. Responsive resource management: We need an
efﬁcient way of assigning the ISP’s available compute
and network resources to DDoS defense. Speciﬁcally,
we need to decide how many VMs of each type to run

(cid:27)(cid:25)(cid:8)(cid:14)(cid:10)(cid:1)(cid:23)(cid:8)(cid:27)(cid:16)(cid:1)
(cid:26)(cid:12)(cid:27)(cid:1)(cid:30)(cid:23)(cid:1)

(cid:46)(cid:2)(cid:44)(cid:39)(cid:1)(cid:4)(cid:17)(cid:18)(cid:17)(cid:27)(cid:31)(cid:17)(cid:1)(cid:5)(cid:30)(cid:13)(cid:29)(cid:22)(cid:44)(cid:47)(cid:1)
(cid:46)(cid:2)(cid:27)(cid:39)(cid:1)(cid:4)(cid:17)(cid:18)(cid:17)(cid:27)(cid:31)(cid:17)(cid:1)(cid:5)(cid:30)(cid:13)(cid:29)(cid:22)(cid:27)(cid:47)(cid:1)
(cid:16)(cid:17)(cid:18)(cid:17)(cid:27)(cid:31)(cid:17)(cid:1)(cid:29)(cid:28)(cid:25)(cid:23)(cid:15)(cid:37)(cid:1)(cid:25)(cid:23)(cid:14)(cid:30)(cid:13)(cid:30)(cid:37)(cid:1)

(cid:41)(cid:1)

(cid:25)(cid:17)(cid:21)(cid:23)(cid:32)(cid:40)(cid:1)(cid:32)(cid:30)(cid:13)(cid:20)(cid:15)(cid:1)

(cid:15)(cid:36)(cid:31)(cid:32)(cid:28)(cid:26)(cid:17)(cid:30)(cid:1)

(cid:12)(cid:7)(cid:1)(cid:12)(cid:7)(cid:1)(cid:12)(cid:7)(cid:1)
(cid:4)(cid:3)(cid:45)(cid:1)

(cid:25)(cid:17)(cid:21)(cid:23)(cid:32)(cid:40)(cid:1)(cid:32)(cid:30)(cid:13)(cid:20)(cid:15)(cid:1)
(cid:30) (cid:13) (cid:20) (cid:15) (cid:1)

(cid:32)

(cid:13) (cid:35) (cid:13) (cid:15)

(cid:24) (cid:1)

(cid:12)(cid:7)(cid:1)(cid:12)(cid:7)(cid:1)(cid:12)(cid:7)(cid:1)

(cid:4)(cid:3)(cid:44)(cid:1)

Figure 2: Bohatei system overview and workﬂow.

(cid:6)(cid:11)(cid:9)

premise hardware. While we describe our work in an ISP
setting, our ideas are general and can be applied to other
deployment models; e.g., CDN-based DDoS defense or
deployments inside cloud providers [19].

In addition to traditional backbone routers and inter-
connecting links, we envision the ISP has deployed mul-
tiple datacenters as shown in Figure 2. Note that this
is not a new requirement; ISPs already have several in-
network datacenters and are planning additional rollouts
in the near future [15,23]. Each datacenter has commod-
ity hardware servers and can run standard virtualized net-
work functions [45].
Threat model: We focus on a general DDoS threat
against the victim, who is a customer of the ISP. The
adversary’s aim is to exhaust the network bandwidth of
the victim. The adversary can ﬂexibly choose from a set
of candidate attacks AttackSet = {Aa}a. As a concrete
starting point, we consider the following types of DDoS
attacks: TCP SYN ﬂood, UDP ﬂood, DNS ampliﬁcation,
and elephant ﬂow. We assume the adversary controls a
large number of bots, but the total budget in terms of the
maximum volume of attack trafﬁc it can launch at any
given time is ﬁxed. Given the budget, the adversary has
a complete control over the choice of (1) type and mix
of attacks from the AttackSet (e.g., 60% SYN and 40%
DNS) and (2) the set of ISP ingress locations at which
the attack trafﬁc enters the ISP. For instance, a simple ad-
versary may launch a single ﬁxed attack Aa arriving at a
single ingress, while an advanced adversary may choose
a mix of various attack types and multiple ingresses. For
clarity, we restrict our presentation to focus on a single
customer noting that it is straightforward to extend our
design to support multiple customers.
Defenses: We assume the ISP has a pre-deﬁned library
of defenses specifying a defense strategy for each attack
type. For each attack type Aa, the defense strategy is
speciﬁed as a directed acyclic graph DAGa representing a
typical multi-stage attack analysis and mitigation proce-
dure. Each node of the graph represents a logical module
and the edges are tagged with the result of the previous

820  24th USENIX Security Symposium 

USENIX Association

on each server of each datacenter location so that attack
trafﬁc is handled properly while minimizing the latency
experienced by legitimate trafﬁc. Doing so in a respon-
sive manner (e.g., within tens of seconds), however, is
challenging. Speciﬁcally, this entails solving a large NP-
hard optimization problem, which can take several hours
to solve even with state-of-the-art solvers.
C2. Scalable network orchestration: The canonical
view in SDN is to set up switch forwarding rules in a
per-ﬂow and reactive manner [40]. That is, every time
a switch receives a ﬂow for which it does not have a
forwarding entry, the switch queries the SDN controller
to get the forwarding rule. Unfortunately, this per-ﬂow
and reactive paradigm is fundamentally unsuitable for
DDoS defense. First, an adversary can easily saturate the
control plane bandwidth as well as the controller com-
pute resources [54]. Second, installing per-ﬂow rules on
the switches will quickly exhaust the limited rule space
(≈4K TCAM rules). Note that unlike trafﬁc engineering
applications of SDN [34], coarse-grained IP preﬁx-based
forwarding policies would not sufﬁce in the context of
DDoS defense, as we cannot predict the IP preﬁxes of
future attack trafﬁc.
C3. Dynamic adversaries: Consider a dynamic ad-
versary who can rapidly change the attack mix (i.e., at-
tack type, volume, and ingress point). This behavior can
make the ISP choose between two undesirable choices:
(1) wasting compute resources by overprovisioning for
attack scenarios that may not ever arrive, (2) not instan-
tiating the required defenses (to save resources), which
will let attack trafﬁc reach the customer.
3.3 High-level approach
Next we highlight our key ideas to address C1–C3:
• Hierarchical optimization decomposition (§4): To
address C1, we use a hierarchical decomposition of
the resource optimization problem into two stages.
First, the Bohatei global (i.e., ISP-wide) controller
uses coarse-grained information (e.g., total spare ca-
pacity of each datacenter) to determine how many
and what types of VMs to run in each datacen-
ter. Then, each local (i.e., per-datacenter) controller
uses more ﬁne-grained information (e.g., location of
available servers) to determine the speciﬁc server on
which each defense VM will run.

• Proactive tag-based forwarding (§5): To address
C2, we design a scalable orchestration mechanism
using two key ideas. First, switch forwarding rules
are based on per-VM tags rather than per-ﬂow to dra-
matically reduce the size of the forwarding tables.
Second, we proactively conﬁgure the switches to
eliminate frequent interactions between the switches
and the control plane [40].

(cid:2)(cid:28)(cid:1)

(cid:2)(cid:29)(cid:1)

(cid:5)(cid:28)(cid:1)

(cid:5)(cid:29)(cid:1)

(cid:28)(cid:1)

(cid:2)(cid:28)(cid:1)
(cid:27)(cid:23)(cid:29)(cid:1)

(cid:2)(cid:29)(cid:1)

(cid:27)(cid:23)(cid:34)(cid:1)
(cid:27)(cid:23)(cid:30)(cid:29)(cid:1)
(cid:5)(cid:28)(cid:1)
(cid:27)(cid:23)(cid:29)(cid:32)(cid:1)

(cid:27)(cid:23)(cid:31)(cid:34)(cid:1)

(cid:27)(cid:23)(cid:29)(cid:32)(cid:1)

(cid:27)(cid:23)(cid:33)(cid:31)(cid:1)

(cid:5)(cid:29)(cid:1)

(cid:6)(cid:20)(cid:18)(cid:7)(cid:20)(cid:10)(cid:11)(cid:21)(cid:1)(cid:1)
(cid:3)(cid:18)(cid:7)(cid:17)(cid:12)(cid:1)

(cid:2)(cid:15)(cid:15)(cid:16)(cid:20)(cid:7)(cid:20)(cid:10)(cid:9)(cid:1)

(cid:3)(cid:18)(cid:7)(cid:17)(cid:12)(cid:1)

(cid:2)(cid:28)(cid:22)(cid:28)(cid:1)

(cid:2)(cid:28)(cid:22)(cid:29)(cid:1)

(cid:23)(cid:23)(cid:23)(cid:1)

(cid:23)(cid:23)(cid:23)(cid:1)

(cid:2)(cid:29)(cid:22)(cid:28)(cid:1)
(cid:23)(cid:23)(cid:1)
(cid:2)(cid:29)(cid:22)(cid:29)(cid:1)
(cid:23)(cid:23)(cid:1)
(cid:5)(cid:28)(cid:22)(cid:28)(cid:1)

(cid:5)(cid:28)(cid:22)(cid:29)(cid:1)
(cid:23)(cid:23)(cid:23)(cid:1)

(cid:24)(cid:1)

(cid:23)(cid:23)(cid:23)(cid:1)

(cid:5)(cid:29)(cid:22)(cid:28)(cid:1)

(cid:4)(cid:12)(cid:21)(cid:19)(cid:13)(cid:8)(cid:7)(cid:14)(cid:1)(cid:1)
(cid:3)(cid:18)(cid:7)(cid:17)(cid:12)(cid:1)

(cid:5)(cid:29)(cid:22)(cid:29)(cid:1)

Figure 4: An illustration of strategy vs. annotated vs.
physical graphs. Given annotated graphs and suspi-
cious trafﬁc volumes, the resource manager computes
physical graphs.

• Online adaptation (§6): To handle a dynamic adver-
sary that changes the attack mix (C3), we design a de-
fense strategy adaptation approach inspired by clas-
sical online algorithms for regret minimization [36].

4 Resource Manager
The goal of the resource management module is to efﬁ-
ciently determine network and compute resources to ana-
lyze and take action on suspicious trafﬁc. The key here is
responsiveness—a slow algorithm enables adversaries to
nullify the defense by rapidly changing their attack char-
acteristics. In this section, we describe the optimization
problem that Bohatei needs to solve and then present a
scalable heuristic that achieves near optimal results.
4.1 Problem inputs
Before we describe the resource management problem,
we establish the main input parameters: the ISP’s com-
pute and network parameters and the defense processing
requirements of trafﬁc of different attack types. We con-
sider an ISP composed of a set of edge PoPs5 E = {Ee}e
and a set of datacenters D = {Dd}d.
ISP constraints: Each datacenter’s trafﬁc processing
capacity is determined by a pre-provisioned uplink ca-
pacity Clink
. The com-
pute capacity is speciﬁed in terms of the number of VM
slots, where each VM slot has a given capacity speciﬁca-
tion (e.g., instance sizes in EC2 [2]).
Processing requirements: As discussed earlier in §3.1,
different attacks require different strategy graphs. How-
ever, the notion of a strategy graph by itself will not suf-

and compute capacity Ccompute

d

d

5We use the terms “edge PoP” and “ingress” interchangeably.

USENIX Association  

24th USENIX Security Symposium  821

ﬁce for resource management, as it is does not specify
the trafﬁc volume that at each module should process.

.

a

a

The input to the resource manager is in form of an-
notated graphs as shown in Figure 4. An annotated
graph DAGannotated
is a strategy graph annotated with
edge weights, where each weight represents the fraction
of the total input trafﬁc to the graph that is expected to
traverse the corresponding edge. These weights are pre-
computed based on prior network monitoring data (e.g.,
using NetFlow) and from our adaptation module (§6).
Te,a denotes the volume of suspicious trafﬁc of type a
arriving at edge PoP e. For example, in Figure 4, weight
0.48 from node A2 to node R2 means 48% of the total in-
put trafﬁc to the graph (i.e., to A1) is expected to traverse
edge A2 → R2.
Since modules may vary in terms of compute com-
plexity and the trafﬁc rate that can be handled per VM-
slot, we need to account for the parameter Pa,i that is
the trafﬁc processing capacity of a VM (e.g., in terms of
compute requirements) for the logical module va,i, where
va,i is node i of graph DAGannotated
Network footprint: We denote the network-level cost
of transferring the unit of trafﬁc from ingress e to data-
center d by Le,d; e.g., this can represent the path latency
per byte of trafﬁc. Similarly, within a datacenter, the
units of intra-rack and inter-rack trafﬁc costs are denoted
by IntraUnitCost and InterUnitCost, respectively (e.g.,
they may represent latency such that IntraUnitCost <
InterUnitCost).
4.2 Problem statement
Our resource management problem is to translate the an-
notated graph into a physical graph (see Figure 4); i.e.,
each node i of the annotated graph DAGannotated
will be
realized by one or more VMs each of which implement
the logical module va,i.
Fine-grained scaling:
To generate physical graphs
given annotated graphs in a resource-efﬁcient manner,
we adopt a ﬁne-grained scaling approach, where each
logical module is scaled independently. We illustrate this
idea in Figure 5. Figure 5a shows an annotated graph
with three logical modules A, B, and C, receiving differ-
ent amounts of trafﬁc and consuming different amounts
of compute resources. Once implemented as a physical
graph, suppose module C becomes the bottleneck due to
its processing capacity and input trafﬁc volume. Using
a monolithic approach (e.g., running A, B, and C within
a single VM), we will need to scale the entire graph as
shown in Figure 5b. Instead, we decouple the modules
to enable scaling out individual VMs; this yields higher
resource efﬁciency as shown in Figure 5c.
Goals: Our objective here is to (a) instantiate the VMs
across the compute servers throughout the ISP, and (b)

a

(cid:3)(cid:14)(cid:10)(cid:10)(cid:5)(cid:9)(cid:12)(cid:8)(cid:16)(cid:1)(cid:10)(cid:14)(cid:9)(cid:9)(cid:7)(cid:9)(cid:6)(cid:1)
(cid:9)(cid:5)(cid:15)(cid:8)(cid:16)(cid:1)(cid:7)(cid:9)(cid:11)(cid:12)(cid:2)(cid:9)(cid:13)(cid:2)(cid:12)(cid:5)(cid:4)(cid:1)
(cid:3)(cid:1)

(cid:8)(cid:7)(cid:10)(cid:1)

(cid:2)(cid:1)

(cid:8)(cid:7)(cid:11)(cid:1)

(cid:4)(cid:1)

(cid:6)(cid:5)(cid:2)(cid:4)(cid:3)(cid:1)

(cid:9)(cid:1)

(cid:11)(cid:10)(cid:2)(cid:6)(cid:3)(cid:1)(cid:1)

(cid:14)(cid:9)(cid:4)(cid:5)(cid:10)(cid:14)(cid:12)(cid:8)(cid:7)(cid:15)(cid:5)(cid:4)(cid:1)

(cid:2)(cid:1)

(cid:2)(cid:1)

(cid:3)(cid:1)

(cid:4)(cid:1)

(cid:3)(cid:1)

(cid:4)(cid:1)

(cid:6)(cid:5)(cid:2)(cid:4)(cid:3)(cid:1)

(cid:2)(cid:1)

(cid:3)(cid:1)

(cid:4)(cid:1)

(cid:4)(cid:1)

(a) Annotated graph.
(c) Fine-grained.
Figure 5: An illustration of ﬁne-grained elastic scal-
ing when module C becomes the bottleneck.

(b) Monolithic.

distribute the processing load across these servers to min-
imize the expected latency for legitimate trafﬁc. Further,
we want to achieve (a) and (b) while minimizing the foot-
print of suspicious trafﬁc.6

To this end, we need to assign values to two key sets
of decision variables: (1) the fraction of trafﬁc Te,a to
send to each datacenter Dd (denoted by fe,a,d), and (2)
the number of VMs of type va,i to run on server s of dat-
acenter Dd. Naturally, these decisions must respect the
datacenters’ bandwidth and compute constraints.

Theoretically, we can formulate this resource manage-
ment problem as a constrained optimization via an In-
teger Linear Program (ILP). For completeness, we de-
scribe the full ILP in Appendix A. Solving the ILP for-
mulation gives an optimal solution to the resource man-
agement problem. However, if the ILP-based solution is
incorporated into Bohatei, an adversary can easily over-
whelm the system. This is because the ILP approach
takes several hours (see Table 2). By the time it computes
a solution, the adversary may have radically changed the
attack mix.
4.3 Hierarchical decomposition
To solve the resource management problem, we decom-
pose the optimization problem into two subproblems: (1)
the Bohatei global controller solves a Datacenter Selec-
tion Problem (DSP) to choose datacenters responsible for
processing suspicious trafﬁc, and (2) given the solution
to the DSP, each local controller solves a Server Selec-
tion Problem (SSP) to assign servers inside each selected
datacenter to run the required VMs. This decomposition
is naturally scalable as the individual SSP problems can
be solved independently by datacenter controllers. Next,
we describe practical greedy heuristics for the DSP and
SSP problems that yield close-to-optimal solutions (see
Table 2).
Datacenter selection problem (DSP): We design a
greedy algorithm to solve DSP with the goal of reduc-
ing ISP-wide suspicious trafﬁc footprint. To this end,
the algorithm ﬁrst sorts suspicious trafﬁc volumes (i.e.,

6While it

is possible to explicitly minimize network conges-
tion [33], minimizing suspicious trafﬁc footprint naturally helps reduce
network congestion as well.

822  24th USENIX Security Symposium 

USENIX Association

Te,a values) in a decreasing order. Then, for each sus-
picious trafﬁc volume Te,a from the sorted list, the algo-
rithm tries to assign the trafﬁc volume to the datacenter
with the least cost based on Le,d values. The algorithm
has two outputs: (1) fe,a,d values denoting what fraction
of suspicious trafﬁc from each ingress should be steered
to each datacenter (as we will see in §5, these values will
be used by network orchestration to steer trafﬁc corre-
spondingly), (2) the physical graph corresponding to at-
tack type a to be deployed by datacenter d. For complete-
ness, we show the pseudocode for the DSP algorithm in
Figure 16 in Appendix B.
Server selection problem (SSP):
Intuitively, the SSP
algorithm attempts to preserve trafﬁc locality by instan-
tiating nodes adjacent in the physical graph as close as
possible within the datacenter. Speciﬁcally, given the
physical graph, the SSP algorithm greedily tries to assign
nodes with higher capacities (based on Pa,i values) along
with its predecessors to the same server, or the same rack.
For completeness we show the pseudocode for the SSP
algorithm in Figure 17 in Appendix B.
5 Network Orchestration
Given the outputs of the resource manager module (i.e.,
assignment of datacenters to incoming suspicious traf-
ﬁc and assignment of servers to defense VMs), the role
of the network orchestration module is to conﬁgure the
network to implement these decisions. This includes set-
ting up forwarding rules in the ISP backbone and inside
the datacenters. The main requirement is scalability in
the presence of attack trafﬁc. In this section, we present
our tag-based and proactive forwarding approach to ad-
dress the limitations of the per-ﬂow and reactive SDN
approach.
5.1 High-level idea
As discussed earlier in §3.2, the canonical SDN view of
setting up switch forwarding rules in a per-ﬂow and re-
active manner is not suitable in the presence of DDoS
attacks. Furthermore, there are practical scalability and
deployability concerns with using SDN in ISP back-
bones [21,29]. There are two main ideas in our approach
to address these limitations:
• Following the hierarchical decomposition in re-
source management, we also decompose the net-
work orchestration problem into two-sub-problems:
(1) Wide-area routing to get trafﬁc to datacenters,
and (2) Intra-datacenter routing to get trafﬁc to the
right VM instances. This decomposition allows us
to use different network-layer techniques; e.g., SDN
is more suitable inside the datacenter while tradi-
tional MPLS-style routing is better suited for wide-
area routing.

• Instead of the controller reacting to each ﬂow arrival,
we proactively install forwarding rules before trafﬁc
arrives. Since we do not know the speciﬁc IP-level
suspicious ﬂows that will arrive in the future, we use
logical tag-based forwarding rules with per-VM tags
instead of per-ﬂow rules.

5.2 Wide-area orchestration
The Bohatei global controller sets up forwarding rules
on backbone routers so that trafﬁc detected as suspicious
is steered from edge PoPs to datacenters according to
the resource management decisions speciﬁed by the fe,a,d
values (see §4.3).7
To avoid a forklift upgrade of the ISP backbone and
enable an immediate adoption of Bohatei, we use tra-
ditional tunneling mechanisms in the backbone (e.g.,
MPLS or IP tunneling). We proactively set up static
tunnels from each edge PoP to each datacenter. Once
the global controller has solved the DSP problem, the
controller conﬁgures backbone routers to split the traf-
ﬁc according to the fe,a,d values. While our design is
not tied to any speciﬁc tunneling scheme, the widespread
use of MPLS and IP tunneling make them natural candi-
dates [34].
5.3
Inside each datacenter, the trafﬁc needs to be steered
through the intended sequence of VMs. There are two
main considerations here:
1. The next VM a packet needs to be sent to depends on
the context of the current VM. For example, the node
check UDP count of src in the graph shown in Fig-
ure 3 may send trafﬁc to either forward to customer
or log depending on its analysis outcome.

Intra-datacenter orchestration

2. With elastic scaling, we may instantiate several phys-
ical VMs for each logical node depending on the de-
mand. Conceptually, we need a “load balancer” at
every level of our annotated graph to distribute traf-
ﬁc across different VM instances of a given logical
node.
Note that we can trivially address both requirements
using a per-ﬂow and reactive solution. Speciﬁcally, a lo-
cal controller can track a packet as it traverses the phys-
ical graph, obtain the relevant context information from
each VM, and determine the next VM to route the traf-
ﬁc to. However, this approach is clearly not scalable and
can introduce avenues for new attacks. The challenge
here is to meet these requirements without incurring the
overhead of this per-ﬂow and reactive approach.
Encoding processing context:
Instead of having the
controller track the context, our high-level idea is to en-
7We assume the ISP uses legacy mechanisms for forwarding non-
attack trafﬁc and trafﬁc to non-Bohatei customers, so these are not the
focus of our work.

USENIX Association  

24th USENIX Security Symposium  823

(cid:4)(cid:15)(cid:17)(cid:22)(cid:26)(cid:1)
(cid:4)(cid:15)(cid:17)(cid:22)(cid:27)(cid:1)
(cid:4)(cid:15)(cid:17)(cid:22)(cid:26)(cid:25)(cid:25)(cid:25)(cid:1)

(cid:24)

(cid:1)

(cid:2)(cid:26)(cid:23)(cid:26)(cid:1)

(cid:18)(cid:17)(cid:19)(cid:20)(cid:1)(cid:26)(cid:1)

(cid:7)(cid:8)(cid:1)

(cid:2)(cid:27)(cid:23)(cid:26)(cid:1)

(cid:6)(cid:26)(cid:23)(cid:26)(cid:1)

(cid:5)(cid:8)(cid:1)(cid:3)(cid:18)(cid:19)(cid:23)(cid:9)(cid:19)(cid:11)(cid:15)(cid:17)(cid:13)(cid:1)(cid:6)(cid:9)(cid:10)(cid:16)(cid:12)(cid:1)
(cid:10)(cid:13)(cid:12)(cid:3)(cid:10)(cid:11)(cid:12)(cid:1)
(cid:2)(cid:8)(cid:10)(cid:14)(cid:1)
(cid:4)(cid:15)(cid:17)(cid:22)(cid:26)(cid:1)
(cid:5)(cid:17)(cid:19)(cid:20)(cid:1)(cid:27)(cid:1)
(cid:4)(cid:15)(cid:17)(cid:22)(cid:27)(cid:1)
(cid:5)(cid:17)(cid:19)(cid:20)(cid:1)(cid:27)(cid:1)
(cid:24)(cid:1)
(cid:4)(cid:15)(cid:17)(cid:22)(cid:26)(cid:25)(cid:25)(cid:25)(cid:1)

(cid:5)(cid:17)(cid:19)(cid:20)(cid:1)(cid:28)(cid:1)

(cid:7)(cid:4)(cid:1)(cid:2)(cid:25)(cid:24)(cid:25)(cid:1)(cid:6)(cid:9)(cid:13)(cid:1)(cid:6)(cid:9)(cid:10)(cid:16)(cid:12)(cid:1)
(cid:4)(cid:5)(cid:7)(cid:1)
(cid:1)(cid:10)(cid:9)(cid:12)(cid:6)(cid:15)(cid:12)(cid:1)
(cid:26)(cid:1)
(cid:3)(cid:11)(cid:16)(cid:13)(cid:12)(cid:16)(cid:1)
(cid:2)(cid:21)(cid:9)(cid:10)(cid:14)(cid:1)
(cid:27)(cid:1)

(cid:5)(cid:8)(cid:1)(cid:3)(cid:18)(cid:19)(cid:23)(cid:9)(cid:19)(cid:11)(cid:15)(cid:17)(cid:13)(cid:1)(cid:6)(cid:9)(cid:10)(cid:16)(cid:12)

(cid:4)(cid:5)(cid:7)(cid:1) (cid:10)(cid:13)(cid:12)(cid:3)(cid:10)(cid:11)(cid:12)(cid:1)
(cid:26)(cid:1)
(cid:27)(cid:1)

(cid:5)(cid:17)(cid:19)(cid:20)(cid:1)(cid:27)(cid:1)
(cid:5)(cid:17)(cid:19)(cid:20)(cid:1)(cid:28)(cid:1)

(cid:22)(cid:20)(cid:15)(cid:17)(cid:13)(cid:1)(cid:21)(cid:9)(cid:13)(cid:13)(cid:15)(cid:17)(cid:13)(cid:1)

(cid:23)(cid:15)(cid:21)(cid:14)(cid:18)(cid:22)(cid:21)(cid:1)(cid:21)(cid:9)(cid:13)(cid:13)(cid:15)(cid:17)(cid:13)(cid:1)

Figure 6: Context-dependent forwarding using tags.

code the necessary context as tags inside packet head-
ers [31]. Consider the example shown in Figure 6 com-
posed of VMs A1,1, A2,1, and R1,1. A1,1 encodes the pro-
cessing context of outgoing trafﬁc as tag values embed-
ded in its outgoing packets (i.e., tag values 1 and 2 denote
benign and attack trafﬁc, respectively). The switch then
uses this tag value to forward each packet to the correct
next VM.

Tag-based forwarding addresses the control channel
bottleneck and switch rule explosion. First, the tag gen-
eration and tag-based forwarding behavior of each VM
and switch is conﬁgured proactively once the local con-
troller has solved the SSP. We proactively assign a tag
for each VM and populate forwarding rules before ﬂows
arrive; e.g., in Figure 6, the tag table of A1,1 and the for-
warding table of the router have been already populated
as shown. Second, this reduces router forwarding rules
as illustrated in Figure 6. Without tagging, there will be
one rule for each of the 1000 ﬂows. Using tag-based for-
warding, we achieve the same forwarding behavior using
only two forwarding rules.
Scale-out load balancing: One could interconnect VMs
of the same physical graph as shown in Figure 7a us-
ing a dedicated load balancer (load balancer). However,
such a load balancer may itself become a bottleneck, as
it is on the path of every packet from any VM in the set
{A1,1,A1,2} to any VM in the set {R1,1,R1,2, ,R 1,3}. To
circumvent this problem, we implement the distribution
strategy inside each VM so that the load balancer capa-
bility scales proportional to the current number of VMs.
Consider the example shown in Figure 7b where due to
an increase in attack trafﬁc volume we have added one
more VM of type A1 (denoted by A1,2) and one more
VM of type R1 (denoted by R1,2). To load balance trafﬁc
between the two VMs of type R1, the load balancer of
A1 VMs (shown as LB1,1 and LB1,2 in the ﬁgure) pick a
tag value from a tag pool (shown by {2,3} in the ﬁgure)
based on the processing context of the outgoing packet
and the intended load balancing scheme (e.g., uniformly
at random to distribute load equally). Note that this tag
pool is pre-populated by the local controller (given the
defense library and the output of the resource manager

(cid:2)(cid:32)(cid:29)(cid:32)(cid:1)

(cid:5)(cid:3)(cid:32)(cid:29)(cid:32)(cid:1)

(cid:2)(cid:32)(cid:29)(cid:33)(cid:1)

(cid:5)(cid:3)(cid:32)(cid:29)(cid:33)(cid:1)

(cid:24)(cid:23)(cid:25)(cid:26)(cid:1)(cid:34)(cid:1)

(cid:9)(cid:12)(cid:1)

(cid:2)(cid:33)(cid:29)(cid:32)(cid:1)
(cid:8)(cid:32)(cid:29)(cid:32)(cid:1)
(cid:8)(cid:32)(cid:29)(cid:33)(cid:1)

(cid:11)(cid:6)(cid:1)(cid:2)(cid:32)(cid:29)(cid:32)(cid:1)(cid:10)(cid:13)(cid:18)(cid:1)(cid:10)(cid:13)(cid:14)(cid:21)(cid:17)(cid:1)

(cid:1)(cid:8)(cid:7)(cid:10)(cid:5)(cid:12)(cid:10)(cid:1)(cid:3)(cid:4)(cid:6)(cid:1)
(cid:3)(cid:17)(cid:22)(cid:19)(cid:18)(cid:22)(cid:1) (cid:32)(cid:1)
(cid:2)(cid:27)(cid:13)(cid:15)(cid:20)(cid:1) (cid:30)(cid:33)(cid:29)(cid:34)(cid:31)(cid:1)

(cid:9)(cid:12)(cid:1)(cid:4)(cid:23)(cid:25)(cid:28)(cid:13)(cid:25)(cid:16)(cid:19)(cid:22)(cid:18)(cid:1)(cid:10)(cid:13)(cid:14)(cid:21)(cid:17)(cid:1)

(cid:11)(cid:6)(cid:1)(cid:2)(cid:32)(cid:29)(cid:33)(cid:1)(cid:10)(cid:13)(cid:18)(cid:1)(cid:10)(cid:13)(cid:14)(cid:21)(cid:17)(cid:1)
(cid:1)(cid:8)(cid:7)(cid:10)(cid:5)(cid:12)(cid:10)(cid:1)(cid:3)(cid:4)(cid:6)(cid:1)
(cid:3)(cid:17)(cid:22)(cid:19)(cid:18)(cid:22)(cid:1) (cid:32)(cid:1)
(cid:2)(cid:27)(cid:13)(cid:15)(cid:20)(cid:1) (cid:30)(cid:33)(cid:29)(cid:1)(cid:34)(cid:31)(cid:1)

(cid:3)(cid:4)(cid:6)(cid:1)(cid:8)(cid:11)(cid:10)(cid:2)(cid:8)(cid:9)(cid:10)(cid:1)
(cid:32)(cid:1) (cid:7)(cid:23)(cid:25)(cid:26)(cid:1)(cid:33)(cid:1)
(cid:33)(cid:1) (cid:7)(cid:23)(cid:25)(cid:26)(cid:1)(cid:34)(cid:1)
(cid:34)(cid:1) (cid:7)(cid:23)(cid:25)(cid:26)(cid:1)(cid:35)(cid:1)

(b) A distributed load balancer design.

(cid:1)(cid:6)(cid:5)(cid:6)(cid:1)

(cid:1)(cid:6)(cid:5)(cid:7)(cid:1)

(cid:3)(cid:2)(cid:1)

(cid:4)(cid:6)(cid:5)(cid:6)(cid:1)
(cid:4)(cid:6)(cid:5)(cid:7)(cid:1)
(cid:4)(cid:6)(cid:5)(cid:8)(cid:1)

(a) A naive load
balancer design.

Figure 7: Different load balancer design points.

module). This scheme, thus, satisﬁes the load balancing
requirement in a scalable manner.
Other issues: There are two remaining practical issues:
• Number of tag bits: We give a simple upper bound on
the required number of bits to encode tags. First, to
support context-dependent forwarding out of a VM
with k relevant contexts, we need k distinct tag val-
ues. Second. to support load balancing among l VMs
of the same logical type, each VM needs to be popu-
lated with a tag pool including l tags. Thus, at each
VM we need at most k× l distinct tag values. There-
fore, an upper bound on the total number of unique
tag values is kmax × lmax × ∑
|, where kmax
and lmax are the maximum number of contexts and
VMs of the same type in a graph, and Vannotated
is
the set of vertices of annotated graph for attack type
a. To make this concrete, across the evaluation ex-
periments §8, the maximum value required tags was
800, that can be encoded in �log2(800)� = 10 bits.
In practice, this tag space requirement of Bohatei
can be easily satisﬁed given that datacenter grade
networking platforms already have extensible header
ﬁelds [56].

a | Vannotated

a

a

• Bidirectional processing: Some logical modules may
have bidirectional semantics. For example, in case
of a DNS ampliﬁcation attack, request and response
trafﬁc must be processed by the same VM. (In other
cases, such as the UDP ﬂood attack, bidirectional-
ity is not required.). To enforce bidirectionality, ISP
edge switches use tag values of outgoing trafﬁc so
that when the corresponding incoming trafﬁc comes
back, edge switches sends it to the datacenter within
which the VM that processed the outgoing trafﬁc is
located. Within the datacenter, using this tag value,
the trafﬁc is steered to the VM.

6 Strategy Layer
As we saw in §4, a key input to the resource manager
module is the set of Te,a values, which represents the vol-
ume of suspicious trafﬁc of each attack type a arriving at
each edge PoP e. This means we need to estimate the fu-

824  24th USENIX Security Symposium 

USENIX Association

ture attack mix based on observed measurements of the
network and then instantiate the required defenses. We
begin by describing an adversary that intends to thwart
a Bohatei-like system. Then, we discuss limitations of
strawman solutions before describing our online adapta-
tion mechanism.
Interaction model: We model the interaction between
the ISP running Bohatei and the adversary as a repeated
interaction over several epochs. The ISP’s “move” is
one epoch behind the adversary; i.e., it takes Bohatei an
epoch to react to a new attack scenario due to implemen-
tation delays in Bohatei operations. The epoch duration
is simply the sum of the time to detect the attack, run the
resource manager, and execute the network orchestration
logic. While we can engineer the system to minimize this
lag, there will still be non-zero delays in practice and thus
we need an adaptation strategy.
Objectives: Given this interaction model, the ISP has to
pre-allocate VMs and hardware resources for a speciﬁc
attack mix. An intelligent and dynamic adversary can
change its attack mix to meet two goals:
G1 Increase hardware resource consumption: The ad-
versary can cause ISP to overprovision defense VMs.
This may impact the ISP’s ability to accommodate
other attack types or reduce proﬁts from other ser-
vices that could have used the infrastructure.

G2 Succeed in delivering attack trafﬁc: If the ISP’s de-
tection and estimation logic is sub-optimal and does
not have the required defenses installed, then the ad-
versary can maximize the volume of attack trafﬁc de-
livered to the target.
The adversary’s goal is to maximize these objectives,
while the ISPs goal is to minimize these to the extent pos-
sible. One could also consider a third objective of collat-
eral damage on legitimate trafﬁc; e.g., introduce need-
less delays. We do not discuss this dimension because
our optimization algorithm from §4 will naturally push
the defense as close to the ISP edge (i.e., trafﬁc ingress
points) as possible to minimize the impact on legitimate
trafﬁc.
Threat model: We consider an adversary with a ﬁxed
budget in terms of the total volume of attack trafﬁc it can
launch at any given time. Note that the adversary can
apportion this budget across the types of attacks and the
ingress locations from which the attacks are launched.
Formally, we have ∑
Te,a ≤ B, but there are no con-
e
straints on the speciﬁc Te,a values.
Limitations of strawman solutions: For simplicity, let
us consider a single ingress point. Let us consider a
strawman solution called PrevEpoch where we measure
the attack observed in the previous epoch and use it as the
estimate for the next epoch. Unfortunately, this can have

∑
a

serious issues w.r.t. goals G1 and G2. To see why, con-
sider a simple scenario where we have two attack types
with a budget of 30 units and three epochs with the attack
volumes as follows: T1: A1= 10, A2=0; T2: A1=20,
A2=0; T3: A1=0; A2=30. Now consider the PrevEpoch
strategy starting at the 0,0 conﬁguration. It has a total
wastage of 0,0,20 units and a total evasion of 10,10,30
units because it has overﬁt to the previous measurement.
We can also consider other strategies; e.g., a Uniform
strategy that provisions 15 units each for A1 and A2 or
extensions of these to overprovision where we multiply
the number of VMs given by the resource manager in the
last epoch by a ﬁxed value γ > 1. However, these suffer
from the same problems and are not competitive.
Online adaptation: Our metric of success here is to
have low regret measured with respect to the best static
solution computed in hindsight [36]. Note that in gen-
eral, it is not possible to be competitive w.r.t. the best
dynamic solution since that presumes oracle knowledge
of the adversary, which is not practical.

Intuitively, if we have a non-adaptive adversary, using
the observed empirical average is the best possible static
hindsight estimation strategy; i.e., T∗e,a = ∑t Te,a,t
would
|t|
be the optimal solution (|t| denotes the total number of
epochs). However, an attacker who knows that we are
using this strategy can game the system by changing the
attack mix. To address this, we use a follow the per-
turbed leader (FPL) strategy [36] where our estimation
uses a combination of the past observed behavior of the
adversary and a randomized component. Intuitively, the
random component makes it impossible for the attacker
to predict the ISP’s estimates. This is a well-known ap-
proach in online algorithms to minimize the regret [36].
Speciﬁcally, the trafﬁc estimates for the next epoch t +1,
denoted by (cid:31)Te,a,t+1 values, are calculated based on the
average of the past values plus a random component:
(cid:31)Te,a,t+1 =

+ randperturb.

∑t
t�=1 Te,a,t�

|t|

2×B

nextE poch×|E|×|A|

Implementation

Here, Te,a,t� is the empirically observed value of the
attack trafﬁc and randperturb is a random value drawn
]. (This is assuming a
uniformly from [0,
total defense of budget of 2× B.) It can be shown that
this is indeed a provably good regret minimization strat-
egy [36]; we do not show the proof for brevity.
7
In this section, we brieﬂy describe how we implemented
the key functions described in the previous sections. We
have made the source code available [1].
7.1 DDoS defense modules
The design of the Bohatei strategy layer is inspired by
the prior modular efforts in Click [7] and Bro [46]. This
modularity has two advantages. First, it allows us to

USENIX Association  

24th USENIX Security Symposium  825

adopt best of breed solutions and compose them for dif-
ferent attacks. Second, it enables more ﬁne-grained scal-
ing. At a high level, there are two types of logical build-
ing blocks in our defense library:
1. Analysis (A): Each analysis module processes a sus-
picious ﬂow and determines appropriate action (e.g.,
more analysis or speciﬁc response).
It receives a
packet and outputs a tagged packet, and the tags are
used to steer trafﬁc to subsequent analysis and re-
sponse module instances as discussed earlier.

2. Response (R): The input to an R module is a tagged
packet from some A module. Typical responses in-
clude forward to customer (for benign trafﬁc), log,
drop, and rate limit. Response functions will depend
on the type of attack; e.g., sending RST packets in
case of a TCP SYN attack.
Next, we describe defenses we have implemented for
different DDoS attacks. Our goal here is to illustrate the
ﬂexibility Bohatei provides in dealing with a diverse set
of known attacks rather than develop new defenses.
1. SYN ﬂood (Figure 8): We track the number of open
TCP sessions for each source IP; if a source IP has
no asymmetry between SYNs and ACKs, then mark
its packets as benign. If a source IP never completes
a connection, then we can mark its future packets as
known attack packets. If we see a gray area where the
source IP has completed some connections but not
others, in which case we use a SYN-Proxy defense
(e.g., [9, 28]).

2. DNS ampliﬁcation (Figure 9): We check if the DNS
server has been queried by some customer IP. This
example highlights another advantage—we can de-
couple fast (e.g., the header-based A LIGHTCHECK
module) and slow path analyses (e.g., the second A
module needs to look into payloads). The responses
are quite simple and implement logging, dropping, or
basic forwarding to the destination. We do not show
the code for brevity.

3. UDP ﬂood: The analysis node A UDP identiﬁes
source IPs that send an anomalously higher num-
ber of UDP packets and uses this to categorize each
packet as either attack or benign. The function
forward will direct the packet to the next node in the
defense strategy; i.e., R OK if benign, or R LOG if
attack.

4. Elephant ﬂow: Here, the attacker launches legiti-
mate but very large ﬂows. The A module detects ab-
normally large ﬂows and ﬂags them as attack ﬂows.
The response is to randomly drop packets from these
large ﬂows (not shown).

Attack detection: We use simple time series anomaly
detection using nfdump, a tool that provides NetFlow-

(cid:2)(cid:15)(cid:12)(cid:14)(cid:8)(cid:4)(cid:7)(cid:9)(cid:9)(cid:3)(cid:1)
(cid:41)(cid:41)(cid:4)(cid:24)(cid:20)(cid:3)(cid:29)(cid:35)(cid:28)(cid:33)(cid:1)(cid:33)(cid:31)(cid:14)(cid:16)(cid:25)(cid:32)(cid:1)(cid:17)(cid:24)(cid:20)(cid:1)(cid:15)(cid:41)(cid:37)(cid:1)(cid:32)(cid:38)(cid:28)(cid:1)(cid:14)(cid:28)(cid:17)(cid:1)(cid:14)(cid:16)(cid:25)(cid:1)
(cid:41)(cid:41)(cid:14)(cid:22)(cid:22)(cid:31)(cid:18)(cid:22)(cid:14)(cid:33)(cid:18)(cid:17)(cid:1)(cid:29)(cid:36)(cid:18)(cid:31)(cid:1)(cid:32)(cid:29)(cid:27)(cid:18)(cid:1)(cid:37)(cid:24)(cid:28)(cid:17)(cid:29)(cid:37)(cid:1)
(cid:32)(cid:31)(cid:16)(cid:6)(cid:9)(cid:1)(cid:48)(cid:1)(cid:30)(cid:25)(cid:33)(cid:40)(cid:32)(cid:31)(cid:16)(cid:6)(cid:9)(cid:1)
(cid:24)(cid:19)(cid:1)(cid:42)(cid:11)(cid:3)(cid:9)(cid:1)(cid:30)(cid:25)(cid:33)(cid:1)(cid:37)(cid:24)(cid:33)(cid:23)(cid:1)(cid:10)(cid:13)(cid:8)(cid:1)(cid:32)(cid:18)(cid:33)(cid:1)(cid:14)(cid:28)(cid:17)(cid:1)(cid:2)(cid:3)(cid:7)(cid:1)(cid:35)(cid:28)(cid:32)(cid:18)(cid:33)(cid:43)(cid:1)

(cid:1)(cid:10)(cid:38)(cid:28)(cid:3)(cid:29)(cid:35)(cid:28)(cid:33)(cid:44)(cid:32)(cid:31)(cid:16)(cid:6)(cid:9)(cid:45)(cid:47)(cid:47)(cid:39)(cid:1)

(cid:24)(cid:19)(cid:1)(cid:42)(cid:30)(cid:25)(cid:33)(cid:1)(cid:24)(cid:32)(cid:1)(cid:11)(cid:3)(cid:9)(cid:1)(cid:14)(cid:28)(cid:17)(cid:1)(cid:21)(cid:31)(cid:32)(cid:33)(cid:1)(cid:2)(cid:3)(cid:7)(cid:43)(cid:1)

(cid:1)(cid:5)(cid:24)(cid:31)(cid:32)(cid:33)(cid:2)(cid:16)(cid:25)(cid:3)(cid:29)(cid:35)(cid:28)(cid:33)(cid:44)(cid:32)(cid:31)(cid:16)(cid:6)(cid:9)(cid:45)(cid:47)(cid:47)(cid:39)(cid:1)

(cid:12)(cid:30)(cid:17)(cid:14)(cid:33)(cid:18)(cid:4)(cid:24)(cid:20)(cid:3)(cid:29)(cid:35)(cid:28)(cid:33)(cid:42)(cid:32)(cid:31)(cid:16)(cid:6)(cid:9)(cid:43)(cid:1)(cid:1)
(cid:24)(cid:19)(cid:1)(cid:42)(cid:4)(cid:24)(cid:20)(cid:3)(cid:29)(cid:35)(cid:28)(cid:33)(cid:42)(cid:32)(cid:31)(cid:16)(cid:6)(cid:9)(cid:43)(cid:1)(cid:48)(cid:1)(cid:1)(cid:10)(cid:38)(cid:28)(cid:3)(cid:29)(cid:35)(cid:28)(cid:33)(cid:44)(cid:32)(cid:31)(cid:16)(cid:6)(cid:9)(cid:45)(cid:43)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:13)(cid:5)(cid:16)(cid:12)(cid:2)(cid:17)(cid:2)(cid:18)(cid:5)(cid:6)(cid:12)(cid:42)(cid:30)(cid:25)(cid:33)(cid:43)(cid:39)(cid:1)
(cid:18)(cid:26)(cid:32)(cid:18)(cid:1)(cid:1)(cid:24)(cid:19)(cid:1)(cid:42)(cid:4)(cid:24)(cid:20)(cid:3)(cid:29)(cid:35)(cid:28)(cid:33)(cid:42)(cid:32)(cid:31)(cid:16)(cid:6)(cid:9)(cid:43)(cid:1)(cid:49)(cid:1)(cid:46)(cid:43)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:13)(cid:5)(cid:16)(cid:12)(cid:2)(cid:17)(cid:4)(cid:14)(cid:12)(cid:14)(cid:15)(cid:19)(cid:14)(cid:42)(cid:30)(cid:25)(cid:33)(cid:43)(cid:39)(cid:1)
(cid:18)(cid:26)(cid:32)(cid:18)(cid:1)(cid:1)(cid:24)(cid:19)(cid:1)(cid:42)(cid:4)(cid:24)(cid:20)(cid:3)(cid:29)(cid:35)(cid:28)(cid:33)(cid:42)(cid:32)(cid:31)(cid:16)(cid:6)(cid:9)(cid:43)(cid:1)(cid:48)(cid:1)(cid:46)(cid:1)(cid:43)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:13)(cid:5)(cid:16)(cid:12)(cid:2)(cid:17)(cid:3)(cid:8)(cid:14)(cid:11)(cid:10)(cid:14)(cid:42)(cid:30)(cid:25)(cid:33)(cid:43)(cid:39)(cid:1)
(cid:9)(cid:15)(cid:16)(cid:19)(cid:5)(cid:16)(cid:7)(cid:42)(cid:30)(cid:25)(cid:33)(cid:43)(cid:39)(cid:1)

(cid:11)(cid:15)(cid:12)(cid:14)(cid:8)(cid:10)(cid:11)(cid:9)(cid:13)(cid:14)(cid:1)

(cid:44)(cid:12)(cid:28)(cid:25)(cid:28)(cid:29)(cid:37)(cid:28)(cid:45)(cid:1)

(cid:44)(cid:2)(cid:34)(cid:14)(cid:16)(cid:25)(cid:45)(cid:1)

(cid:44)(cid:2)(cid:34)(cid:14)(cid:16)(cid:25)(cid:45)(cid:1)

(cid:11)(cid:15)(cid:7)(cid:9)(cid:5)(cid:1)

(cid:11)(cid:15)(cid:3)(cid:11)(cid:9)(cid:10)(cid:1)

(cid:44)(cid:15)(cid:18)(cid:28)(cid:24)(cid:22)(cid:28)(cid:45)(cid:1)

(cid:11)(cid:15)(cid:9)(cid:6)(cid:1)
Figure 8: SYN Flood defense strategy graph.

(cid:2)(cid:18)(cid:10)(cid:8)(cid:6)(cid:7)(cid:17)(cid:3)(cid:7)(cid:5)(cid:3)(cid:9)(cid:1)
(cid:21)(cid:20)(cid:1)(cid:37)(cid:26)(cid:22)(cid:30)(cid:1)(cid:21)(cid:29)(cid:1)(cid:5)(cid:11)(cid:13)(cid:1)(cid:28)(cid:19)(cid:27)(cid:31)(cid:19)(cid:29)(cid:30)(cid:38)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:27)(cid:31)(cid:19)(cid:28)(cid:21)(cid:19)(cid:18)(cid:13)(cid:19)(cid:28)(cid:32)(cid:19)(cid:28)(cid:39)(cid:26)(cid:22)(cid:30)(cid:36)(cid:18)(cid:29)(cid:30)(cid:9)(cid:12)(cid:40)(cid:1)(cid:41)(cid:1)(cid:30)(cid:28)(cid:31)(cid:19)(cid:35)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:15)(cid:6)(cid:19)(cid:14)(cid:2)(cid:20)(cid:3)(cid:9)(cid:16)(cid:13)(cid:11)(cid:16)(cid:37)(cid:26)(cid:22)(cid:30)(cid:38)(cid:35)(cid:1)
(cid:21)(cid:20)(cid:1)(cid:37)(cid:26)(cid:22)(cid:30)(cid:1)(cid:21)(cid:29)(cid:1)(cid:5)(cid:11)(cid:13)(cid:1)(cid:28)(cid:19)(cid:29)(cid:26)(cid:25)(cid:24)(cid:29)(cid:19)(cid:38)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:21)(cid:20)(cid:1)(cid:37)(cid:34)(cid:27)(cid:31)(cid:19)(cid:28)(cid:21)(cid:19)(cid:18)(cid:13)(cid:19)(cid:28)(cid:32)(cid:19)(cid:28)(cid:39)(cid:26)(cid:22)(cid:30)(cid:36)(cid:29)(cid:28)(cid:17)(cid:9)(cid:12)(cid:40)(cid:38)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:15)(cid:6)(cid:19)(cid:14)(cid:2)(cid:20)(cid:2)(cid:22)(cid:6)(cid:7)(cid:14)(cid:37)(cid:26)(cid:22)(cid:30)(cid:38)(cid:35)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:19)(cid:23)(cid:29)(cid:19)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:15)(cid:6)(cid:19)(cid:14)(cid:2)(cid:20)(cid:5)(cid:16)(cid:14)(cid:16)(cid:17)(cid:24)(cid:16)(cid:37)(cid:26)(cid:22)(cid:30)(cid:38)(cid:35)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:10)(cid:17)(cid:19)(cid:24)(cid:6)(cid:19)(cid:8)(cid:37)(cid:26)(cid:22)(cid:30)(cid:38)(cid:35)(cid:1)

(cid:39)(cid:15)(cid:11)(cid:10)(cid:40)(cid:1)

(cid:2)(cid:18)(cid:11)(cid:2)(cid:17)(cid:3)(cid:7)(cid:15)(cid:14)(cid:16)(cid:17)(cid:1)
(cid:21)(cid:20)(cid:1)(cid:37)(cid:26)(cid:22)(cid:30)(cid:1)(cid:21)(cid:29)(cid:1)(cid:5)(cid:11)(cid:13)(cid:1)(cid:28)(cid:19)(cid:27)(cid:31)(cid:19)(cid:29)(cid:30)(cid:38)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:28)(cid:19)(cid:27)(cid:31)(cid:19)(cid:29)(cid:30)(cid:8)(cid:21)(cid:29)(cid:30)(cid:25)(cid:28)(cid:33)(cid:39)(cid:26)(cid:22)(cid:30)(cid:40)(cid:1)(cid:41)(cid:1)(cid:30)(cid:28)(cid:31)(cid:19)(cid:35)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:15)(cid:6)(cid:19)(cid:14)(cid:2)(cid:20)(cid:3)(cid:9)(cid:16)(cid:13)(cid:11)(cid:16)(cid:37)(cid:26)(cid:22)(cid:30)(cid:38)(cid:35)(cid:1)
(cid:21)(cid:20)(cid:1)(cid:37)(cid:26)(cid:16)(cid:17)(cid:22)(cid:19)(cid:30)(cid:1)(cid:21)(cid:29)(cid:1)(cid:5)(cid:11)(cid:13)(cid:1)(cid:28)(cid:19)(cid:29)(cid:26)(cid:25)(cid:24)(cid:29)(cid:19)(cid:38)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:21)(cid:20)(cid:1)(cid:26)(cid:25)(cid:15)(cid:6)(cid:21)(cid:7)(cid:12)(cid:4)(cid:9)(cid:18)(cid:23)(cid:9)(cid:20)(cid:21)(cid:37)(cid:26)(cid:22)(cid:30)(cid:38)(cid:38)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:15)(cid:6)(cid:19)(cid:14)(cid:2)(cid:20)(cid:2)(cid:22)(cid:6)(cid:7)(cid:14)(cid:37)(cid:26)(cid:22)(cid:30)(cid:38)(cid:35)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:19)(cid:23)(cid:29)(cid:19)(cid:1)(cid:1)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:15)(cid:6)(cid:19)(cid:14)(cid:2)(cid:20)(cid:3)(cid:9)(cid:16)(cid:13)(cid:11)(cid:16)(cid:37)(cid:26)(cid:22)(cid:30)(cid:38)(cid:35)(cid:1)

(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)

(cid:1)(cid:10)(cid:17)(cid:19)(cid:24)(cid:6)(cid:19)(cid:8)(cid:37)(cid:26)(cid:22)(cid:30)(cid:38)(cid:1)

(cid:39)(cid:3)(cid:6)(cid:11)(cid:9)(cid:7)(cid:11)(cid:40)(cid:1)

(cid:39)(cid:2)(cid:14)(cid:14)(cid:2)(cid:4)(cid:10)(cid:40)(cid:1)

(cid:39)(cid:3)(cid:6)(cid:11)(cid:9)(cid:7)(cid:11)(cid:40)(cid:1)

(cid:39)(cid:2)(cid:14)(cid:14)(cid:2)(cid:4)(cid:10)(cid:40)(cid:1)

(cid:15)(cid:18)(cid:12)(cid:9)(cid:1)

(cid:15)(cid:18)(cid:4)(cid:15)(cid:12)(cid:13)(cid:1)
Figure 9: DNS ampliﬁcation defense strategy graph.

(cid:15)(cid:18)(cid:10)(cid:12)(cid:6)(cid:1)

like capabilities, and custom code [27]. The output of the
detection module is sent to the Bohatei global controller
as a 3-tuple (cid:31)Type,FlowSpec,Volume(cid:30), where Type indi-
cates the type of DDoS attack (e.g., SYN ﬂood, DNS am-
pliﬁcation), FlowSpec provides a generic description of
the ﬂow space of suspicious trafﬁc (involving wildcards),
and Volume indicates the suspicious trafﬁc volume based
on the ﬂow records. Note that this FlowSpec does not
pinpoint speciﬁc attack ﬂows; rather,
it is a coarse-
grained hint on characteristics of suspicious trafﬁc that
need further processing through the defense graphs.
7.2 SDN/NFV platform
Control plane: We use the OpenDayLight network
control platform, as it has gained signiﬁcant traction
from key industry players [17]. We implemented the
Bohatei global and local control plane modules (i.e.,
strategy, resource management, and network orchestra-
tion) as separate OpenDayLight plugins. Bohatei uses
OpenFlow [40] for conﬁguring switches; this is purely
for ease of prototyping, and it is easy to integrate other
network control APIs (e.g., YANG/NetCONF).
Data plane: Each physical node is realized using a
VM running on KVM. We use open source tools (e.g.,
Snort, Bro) to implement the different Analysis (A) and

826  24th USENIX Security Symposium 

USENIX Association

Analysis

Response

Topology

#Nodes

Run time (secs)

Optimality

Attack
type
UDP
ﬂood

DNS
amp.

SYN
ﬂood

A UDP using Snort (inline
mode)

both LIGHTCHECK and
MATCHRQST using net-
ﬁlter library, iptables, cus-
tom code
A SYNFLOOD using Bro

R LOG using iptables and
R RATELIMIT using tc li-
brary
R LOG and R DROP us-
ing iptables

R SYNPROXY
using
PF ﬁrewall, R LOG and
R DROP using iptables
R DROP using iptables

Heanet
OTEGlobe
Cogent

6
92
196

Baseline Bohatei Gap
205
2234
> 1 hr

0.002
0.007
0.01

0.0003
0.0004
0.0005

Table 2: Run time and optimality gap of Bohatei vs.
ILP formulation across different topologies.

)
s
m

(
 
y
c
n
e
a

t

w
o

l
f
-
r
e
p

 

h
c
t
i

w
S

l
 

p
u
-
t

e
s

Bohatei
Reactive control

600
500
400
300
200
100
0

 0

 50

 100

 150

 200

Number of attack flows per second (∗1000)
Figure 10: Bohatei control plane scalability.

pairs [50]. The total volume is scaled linearly with
the size of the network such that the average link load
on the topology backbone is 24Gbps with a maxi-
mum bottleneck link load of 55Gbps. We use iperf
and custom code to generate benign trafﬁc.

• Attack trafﬁc: We implemented custom modules to
generate attack trafﬁc: (1) SYN ﬂood attack by send-
ing only SYN packets with spoofed IP addresses at
a high rate; (2) DNS ampliﬁcation using OpenDNS
server with BIND (version 9.8) and emulating an at-
tacker sending DNS requests with spoofed source
IPs; (3) We use iperf to create some ﬁxed band-
width trafﬁc to generate elephant ﬂows, and (4) UDP
ﬂood attacks. We randomly pick one edge PoP as
the target and vary the target across runs. We ramp
up the attack volume until it induces maximum re-
duction in throughput of benign ﬂows to the target.
On our testbed, we can ramp up the volume up to
10 Gbps. For larger attacks, we use simulations.

8.1 Bohatei scalability
Resource management: Table 2 compares the run time
and optimality of the ILP-based algorithm and Bohatei
(i.e., DSP and SSP) for 3 ISP topologies of various sizes.
(We have results for several other topologies but do not
show it for brevity.) The ILP approach takes from sev-
eral tens of minutes to hours, whereas Bohatei takes only
a few milliseconds enabling rapid response to changing
trafﬁc patterns. The optimality gap is ≤ 0.04%.
Control plane responsiveness: Figure 10 shows the
per-ﬂow setup latency comparing Bohatei to the SDN
per-ﬂow and reactive paradigm as the number of attack
ﬂows in a DNS ampliﬁcation attack increases. (The re-
sults are consistent for other types of attacks and are not
shown for brevity.) In both cases, we have a dedicated
machine for the controller with 8 2.8GHz cores and 64

Elephant
ﬂow

A ELEPHANT using net-
ﬁlter library, iptables, cus-
tom code

Table 1: Implementation of Bohatei modules.

Response (R) modules. Table 1 summarizes the speciﬁc
platforms we have used. These tools are instrumented us-
ing FlowTags [31] to add tags to outgoing packets to pro-
vide contextual information. We used OpenvSwitch [16]
to emulate switches in both datacenters and ISP back-
bone. The choice of OpenvSwitch is for ease of proto-
typing on our testbed.
Resource management algorithms: We implement the
DSP and SSP algorithms using custom Go code.
8 Evaluation
In this section, we show that:
1. Bohatei is scalable and handles attacks of hundreds
of Gbps in large ISPs and that our design decisions
are crucial for its scale and responsiveness (§8.1)
several canonical DDoS attack scenarios (§8.2)
attack strategies (§8.3)

2. Bohatei enables a rapid (≤ 1 minute) response for
3. Bohatei can successfully cope with several dynamic

Setup and methodology: We use a combination of real
testbed and trace-driven evaluations to demonstrate the
above beneﬁts. Here we brieﬂy describe our testbed,
topologies, and attack conﬁgurations:
• SDN Testbed: Our testbed has 13 Dell R720 ma-
chines (20-core 2.8 GHz Xeon CPUs, 128GB RAM).
Each machine runs KVM on CentOS 6.5 (Linux ker-
nel v2.6.32). On each machine, we assigned equal
amount of resources to each VM: 1 vCPU (virtual
CPU) and 512MB of memory.

• Network topologies: We emulate several router-
level ISP topologies (6–196 nodes) from the Internet
Topology Zoo [22]. We set the bandwidth of each
core link to be 100Gbps and link latency to be 10ms.
The number of datacenters, which are located ran-
domly, is 5% of the number of backbone switches
with a capacity of 4,000 VMs per datacenter.

• Benign trafﬁc demands: We assume a gravity model
of trafﬁc demands between ingress-egress switch

USENIX Association  

24th USENIX Security Symposium  827

Bohatei
per-flow rules

10e+06
100,000
1,000
10

Attack type

DNS Ampliﬁcation
SYN Flood
Elephant ﬂows
UDP ﬂood

# VMs needed

Monolithic
5,422
3,167
1,948
3,642

Fine-grained scaling
1,005
856
910
1,253

r
e
b
m
u
n

 

d
e
r
i
u
q
e
r
 
x
a
M

h
c
t
i

w
s
 

a

 

n
o

l

 
s
e
u
r
 
f

o

Table 3: Total hardware provisioning cost needed to
handle a 100 Gbps attack for different attacks.

some small differences across attacks, the overall reac-
tion time is short.

c
i
f
f

a
r
t
 

i

n
g
n
e
B

)
s
p
b
G

(
 
t

u
p
h
g
u
o
r
h

t

10
6
2

SYN flood
DNS amp.
attack starts

Elephant flow
UDP flood

 0

 20

 40

 60

 80
Time (s)

 100  120  140

Figure 12: Bohatei enables rapid response and re-
stores throughput of legitimate trafﬁc.

The key takeaway is that Bohatei can help networks
respond rapidly (within one minute) to diverse attacks
and restore the performance of legitimate ﬂows. We re-
peated the experiments with UDP as the benign trafﬁc.
In this case, the recovery time was even shorter, as the
throughput does not suffer from the congestion control
mechanism of TCP.
Hardware cost: We measure the total number of VMs
needed to handle a given attack volume and compare two
cases: (1) monolithic VMs embedding the entire defense
logic for an attack, and (2) using Bohatei’s ﬁne-grained
modular scaling. Table 3 shows the number of VMs
required to handle different types of 100 Gbps attacks.
Fine-grained scaling gives a 2.1–5.4× reduction in hard-
ware cost vs. monolithic VMs. Assuming a commodity
server costs $3,000 and can run 40VMs in Bohatei (as
we did), we see that it takes a total hardware cost of less
than about $32,000 to handle a 100 Gbps attack across
Table 3. This is in contrast to the total server cost of
about $160,000 for the same scenario if we use mono-
lithic VMs. Moreover, since Bohatei is horizontally scal-
able by construction, dealing with larger attacks simply
entails a linearly scale up of the number of VMs.
Routing efﬁciency: To quantify how Bohatei addresses
the routing inefﬁciency of existing solutions (§2.2), we
ran the following experiment. For each topology, we
measured the end-to-end latency in two equivalently pro-
visioned scenarios:
(1) the location of the DDoS de-
fense appliance is the node with the highest between-
ness value8, and (2) Bohatei. As a baseline, we consider

8Betweenness is a measure of a node’s centrality, which is the frac-
tion of the network’s all-pairs shortest paths that pass through that node.

200

100
400
Attack traffic volume (Gbps)

300

500

Figure 11: Number of switch forwarding rules in Bo-
hatei vs. today’s ﬂow-based forwarding.

GB RAM. To put the number of ﬂows in context, 200K
ﬂows roughly corresponds to a 1 Gbps attack. Note that a
typical upper bound for switch ﬂow set-up time is on the
order of a few milliseconds [59]. We see that Bohatei in-
curs zero rule setup latency, while the reactive approach
deteriorates rapidly as the attack volume increases.
Number of forwarding rules: Figure 11 shows the
maximum number of rules required on a switch across
different topologies for the SYN ﬂood attack. Using to-
day’s ﬂow-based forwarding, each new ﬂow will require
a rule. Using tag-based forwarding, the number of rules
depends on the number of VM instances, which reduces
the switch rule space by four orders of magnitude. For
other attack types, we observed consistent results (not
shown). To put this in context, the typical capacity of an
SDN switch is 3K-4K rules (shared across various net-
work management tasks). This means that per-ﬂow rules
will not sufﬁce for attacks beyond 10Gbps. In contrast,
Bohatei can handle hundreds of Gbps of attack trafﬁc;
e.g., a 1 Tbps attack will require < 1K rules on a switch.
Beneﬁt of scale-out load balancing: We measured the
resources that would be consumed by a dedicated load
balancing solution. Across different types of attacks with
a ﬁxed rate of 10Gbps, we observed that a dedicated load
balancer design requires between 220–300 VMs for load
balancing alone. By delegating the load balancing task
to the VMs, our design obviates the need for these extra
load balancers (not shown).
8.2 Bohatei end-to-end effectiveness
We evaluated the effectiveness of Bohatei under four dif-
ferent types of DDoS attacks. We launch the attack traf-
ﬁc of the corresponding type at 10th second; the attack
is sustained for the duration of the experiment. In each
scenario, we choose the attack volume such that it is ca-
pable of bringing the throughput of the benign trafﬁc to
zero. Figure 12 shows the impact of attack trafﬁc on the
throughput of benign trafﬁc. The Y axis for each sce-
nario shows the network-wide throughput for TCP traf-
ﬁc (a total of 10Gbps if there is no attack). The results
shown in this ﬁgure are based on Cogent, the largest
topology with 196 switches; the results for other topolo-
gies were consistent and are not shown. While we do see

828  24th USENIX Security Symposium 

USENIX Association

)

%

e
s
a
e
r
c
n
i
 
h
t
g
n
e
l
 
h
t
a
P

(
 
h
t
a
p
 
t
s
e
t
r
o
h
s
 
.
t
.
r
.

w

 100
 80
 60
 40
 20
 0

Fixed defense facility
Bohatei

)

6

e t (

n

a

H e

)

5

2

E G l o
U n i C (
Topology (#nodes)

O T

9

(

e

b

2

)
C o

e

g

)

6

9

1

n t (

Figure 13: Routing efﬁciency in Bohatei.

r
e
b
m
u
n

 
.
t
.
r
.

w

 
t

e
r
g
e
R

)

%

(
 
s
M
V

 
f

o

 100
 80
 60
 40
 20
 0

Uniform
PrevEpoch
Bohatei

r

g

d I n

n

s

n

s
e
R a

A tt a

d

k
c
R a

d H y

n

b

ri d
S t e

y

d

a

R a

F li p
(a) Regret w.r.t. defense resource consumption.

h

c

o

p

E

v

e

P r

l

e
m
u
o
v
 
.
t
.
r
.

w

 
t
e
r
g
e
R

)

%

(
 
s
k
c
a
t
t
a
 
l
u
f
s
s
e
c
c
u
s
 
f
o

 60
 50
 40
 30
 20
 10
 0

Uniform
PrevEpoch
Bohatei

r

g

d I n

n

s

e

s
R a

A tt a

d

n

c

k
R a

d H y

n

b

ri d
S t e

y

d

a

R a
(b) Regret w.r.t. successful attacks.

h

c

o

p

E

v

e

P r

F li p

Figure 14: Effect of different adaptation strategies
(bars) vs. different attacker strategies (X axis).
shortest path routing without attacks. The main conclu-
sion in Figure 13 is that Bohatei reduces trafﬁc latency
by 20% to 65% across different scenarios.
8.3 Dynamic DDoS attacks
We consider the following dynamic DDoS attack strate-
gies: (1) RandIngress: In each epoch, pick a random
subset of attack ingresses and distribute the attack bud-
get evenly across attack types; (2) RandAttack: In each
epoch, pick a random subset of attack types and dis-
tribute the budget evenly across all ingresses; (3) Rand-
Hybrid: In each epoch, pick a random subset of ingresses
and attack types independently and distribute the attack
budget evenly across selected pairs; (4) Steady: The ad-
versary picks a random attack type and a subset of in-
gresses and sustains it during all epochs; and (5) Flip-
PrevEpoch: This is conceptually equivalent to conduct-
ing two Steady attacks A1 and A2 with each being active
during odd and even epochs, respectively.

Given the typical DDoS attack duration (≈ 6
hours [12]), we consider an attack lasting for 5000 5-
second epochs (i.e., ≈7 hours). Bohatei is initialized
with a zero starting point of attack estimates. The met-

ric of interest we report is the normalized regret with re-
spect to the best static decision in hindsight; i.e., if we
had to pick a single static strategy for the entire duration.
Figure 14a and Figure 14b show the regret w.r.t. the two
goals G1 (the number of VMs) and G2 (volume of suc-
cessful attack) for a 24-node topology. The results are
similar using other topologies and are not shown here.
Overall, Bohatei’s online adaptation achieves low regret
across the adversarial strategies compared to two straw-
man solutions: (1) uniform estimates, and (2) estimates
given the previous measurements.
9 Related Work
DDoS has a long history; we refer readers to surveys for a
taxonomy of DDoS attacks and defenses (e.g., [41]). We
have already discussed relevant SDN/NFV work in the
previous sections. Here, we brieﬂy review other related
topics.
Attack detection: There are several algorithms for de-
tecting and ﬁltering DDoS attacks. These include time
series detection techniques (e.g., [27]), use of backscat-
ter analysis (e.g., [42]), exploiting attack-speciﬁc fea-
tures (e.g., [35]), and network-wide analysis (e.g., [38]).
These are orthogonal to the focus of this paper.
DDoS-resilient Internet architectures: These include
the use of capabilities [58], better inter-domain routing
(e.g., [60]), inter-AS collaboration (e.g., [39]), packet
marking and unforgeable identiﬁers (e.g., [26]), and
traceback (e.g., [51]). However, they do not provide an
immediate deployment path or resolution for current net-
works. In contrast, Bohatei focuses on a more practical,
single-ISP context, and is aligned with economic incen-
tives for ISPs and their customers.
Overlay-based solutions: There are overlay-based so-
lutions (e.g., [25,52]) that act as a “buffer zone” between
attack sources and targets. The design contributions in
Bohatei can be applied to these as well.
SDN/NFV-based security: There are few efforts in
this space such as FRESCO [53] and AvantGuard [54].
As we saw earlier, these SDN solutions will introduce
new DDoS avenues because of the per-ﬂow and reac-
tive model [54]. Solving this control bottleneck requires
hardware modiﬁcations to SDN switches to add “state-
ful” components, which is unlikely to be supported by
switch vendors soon [54]. In contrast, Bohatei chooses
a proactive approach of setting up tag-based forwarding
rules that is immune to these pitfalls.
10 Conclusions
Bohatei brings the ﬂexibility and elasticity beneﬁts of
recent networking trends, such as SDN and NFV, to
DDoS defense. We addressed practical challenges in
the design of Bohatei’s resource management algorithms

USENIX Association  

24th USENIX Security Symposium  829

and control/data plane mechanisms to ensure that these
do not become bottlenecks for DDoS defense. We
implemented a full-featured Bohatei prototype built on
industry-standard SDN control platforms and commod-
ity network appliances. Our evaluations on a real testbed
show that Bohatei (1) is scalable and responds rapidly to
attacks, (2) outperforms naive SDN implementations that
do not address the control/data plane bottlenecks, and (3)
enables resilient defenses against dynamic adversaries.
Looking forward, we believe that these design principles
can also be applied to other aspects of network security.

Acknowledgments
This work was supported in part by grant number
N00014-13-1-0048 from the Ofﬁce of Naval Research,
and NSF awards 1409758, 1111699, 1440056, and
1440065. Seyed K. Fayaz was supported in part by the
CMU Bertucci Fellowship. We thank Limin Jia, Min
Suk Kang, the anonymous reviewers, and our shepherd
Patrick Traynor for their helpful suggestions.

References
[1] Bohatei. https://github.com/ddos-defense/bohatei.
[2] Amazon EC2. http://aws.amazon.com/ec2/.
[3] Arbor Networks, worldwide infrastructure security report, volume IX,

2014. http://bit.ly/1R0NDRi.

[4] AT&T and Intel: Transforming the Network with NFV and SDN. https:

//www.youtube.com/watch?v=F55pHxTeJLc#t=76.

[5] AT&T Denial of Service Protection.

[6] AT&T Domain 2.0 Vision White Paper.

http://soc.att.com/

http://soc.att.com/

1999.

[7] Click Modular Router. http://www.read.cs.ucla.edu/click/

SIGCOMM CCR, 2005.

[30] J. Czyz, M. Kallitsis, M. Gharaibeh, C. Papadopoulos, M. Bailey, and
M. Karir. Taming the 800 pound gorilla: The rise and decline of ntp ddos
attacks. In Proc. IMC, 2014.

[31] S. K. Fayazbakhsh, L. Chiang, V. Sekar, M. Yu, and J. C. Mogul. Enforcing
network-wide policies in the presence of dynamic middlebox actions using
FlowTags. In Proc. NSDI, 2014.

[32] A. Greenberg, G. Hjalmtysson, D. A. Maltz, A. Myers, J. Rexford, G. Xie,
H. Yan, J. Zhan, and H. Zhang. A clean slate 4D approach to network
control and management. ACM CCR, 2005.

[33] V. Heorhiadi, S. K. Fayaz, M. Reiter, and V. Sekar. Frenetic: A network

programming language. Information Systems Security, 2014.

[34] Jain et al. B4: Experience with a globally-deployed software deﬁned wan.

In Proc. SIGCOMM, 2013.

[35] C. Jin, H. Wang, and K. G. Shin. Hop-count ﬁltering: An effective defense

against spoofed ddos trafﬁc. In Proc. CCS, 2003.

[36] A. Kalai and S. Vempala. Efﬁcient algorithms for online decision problems.

[37] M. S. Kang, S. B. Lee, and V. Gligor. The crossﬁre attack. In Proc. IEEE

J. Comput. Syst. Sci., 2005.

Security and Privacy, 2013.

[38] A. Lakhina, M. Crovella, and C. Diot. Mining Anomalies Using Trafﬁc

Feature Distributions. In Proc. SIGCOMM, 2005.

[39] R. Mahajan et al. Controlling high bandwidth aggregates in the network.

CCR, 2001.

CCR, March 2008.

[40] N. McKeown et al. OpenFlow: enabling innovation in campus networks.

[41] J. Mirkovic and P. Reiher. A taxonomy of ddos attack and ddos defense

mechanisms. In CCR, 2004.

[42] D. Moore, C. Shannon, D. J. Brown, G. M. Voelker, and S. Savage. Infer-

ring internet denial-of-service activity. ACM Trans. Comput. Syst., 2006.

[43] Network functions virtualisation – introductory white paper. http://

portal.etsi.org/NFV/NFV_White_Paper.pdf.

[44] A. Networks. ATLAS Summary Report: Global Denial of Service. http:

//atlas.arbor.net/summary/dos.

[45] P. Patel et al. Ananta: cloud scale load balancing.

In Proc. ACM SIG-

COMM, 2013.

Computer Networks, 1999.

[46] V. Paxson. Bro: A system for detecting network intruders in real-time. In

[47] S. Peter, J. Li, I. Zhang, D. R. K. Ports, D. Woos, A. Krishnamurthy, T. An-
derson, and T. Roscoe. Arrakis: The operating system is the control plane.
In Proc. OSDI, 2014.

[48] M. Roesch. Snort - Lightweight Intrusion Detection for Networks. In LISA,

[49] C. Rossow. Ampliﬁcation hell: Revisiting network protocols for ddos

abuse. In Proc. USENIX Security, 2014.

[50] M. Roughan. Simplifying the Synthesis of Internet Trafﬁc Matrices. ACM

[51] S. Savage, D. Wetherall, A. Karlin, and T. Anderson. Practical network

support for ip traceback. In Proc. SIGCOMM, 2000.

[52] E. Shi, I. Stoica, D. Andersen, and A. Perrig. OverDoSe: A generic DDoS
protection service using an overlay network. Technical Report CMU-CS-
06-114, School of Computer Science, Carnegie Mellon University, 2006.

[53] S. Shin, P. Porras, V. Yegneswaran, M. Fong, G. Gu, and M. Tyson.
FRESCO: Modular composable security services for software-deﬁned net-
works. In Proc. NDSS, 2013.

[54] S. Shin, V. Yegneswaran, P. Porras, and G. Gu. AVANT-GUARD: Scal-
able and vigilant switch ﬂow management in software-deﬁned networks.
In Proc. CCS, 2013.

[55] A. Studer and A. Perrig. The coremelt attack. In Proc. ESORICS, 2009.
[56] T. Koponen et al. Network virtualization in multi-tenant datacenters. In

Proc. NSDI, 2014.

[57] P. Verkaik, D. Pei, T. Schollf, A. Shaikh, A. C. Snoeren, and J. E. van der
Merwe. Wresting Control from BGP: Scalable Fine-grained Route Control.
In Proc. USENIX ATC, 2007.

[58] X. Yang, D. Wetherall, and T. Anderson. A dos-limiting network architec-

ture. In Proc. SIGCOMM, 2005.

[59] S. Yeganeh, A. Tootoonchian, and Y. Ganjali. On scalability of software-

deﬁned networking. Communications Magazine, IEEE, 2013.

[60] X. Zhang, H.-C. Hsiao, G. Hasker, H. Chan, A. Perrig, and D. G. Andersen.
Scion: Scalability, control, and isolation on next-generation networks. In
Proc. IEEE Security and Privacy, 2011.

A ILP Formulation

The ILP formulation for an optimal resource manage-

ment (mentioned in §4.2) is shown in Figure 15.
Vairables:
In addition to the parameters and variables
that we have deﬁned earlier in §4, we deﬁne the binary
variable qd,a,i,vm,s,i�,vm�,s�,l as follows: if it is 1, VM vm of

1IIlUec.

1kAw1Kp.

click.

[8] CloudFlare. https://www.cloudflare.com/ddos.
[9] DDoS protection using Netﬁlter/iptables. http://bit.ly/1IImM2F.
[10] Dell PowerEdge Rack Servers. http://dell.to/1dtP5Jk.
[11] GSA Advantage. http://1.usa.gov/1ggEgFN.
[12]

Incapsula Survey : What DDoS Attacks Really Cost Businesses, 2014.
http://bit.ly/1CFZyIr.
[13]
iptables. http://www.netfilter.org/projects/iptables/.
[14] NTP attacks: Welcome to the hockey stick era. http://bit.ly/

1ROlwQe.

[15] ONS 2014 Keynote: John Donovan, Senior EVP, AT&T Technology &

Network Operations. http://bit.ly/1RQFMko.

[16] Open vSwitch. http://openvswitch.org/.
[17] OpenDaylight project. http://www.opendaylight.org/.
[18] Packet processing on Intel architecture. http://intel.ly/1efIEu6.
[19] Prolexic. http://www.prolexic.com/.
[20] Radware. http://www.radware.com/Solutions/Security/.
[21] Time for an SDN Sequel? http://bit.ly/1BSpdma.
[22] Topology Zoo. www.topology-zoo.org.
[23] Verizon-Carrier Adoption of Software-deﬁned Networking. https://

www.youtube.com/watch?v=WVczl03edi4.

[24] ZScaler Cloud Security. http://www.zscaler.com.
[25] D. G. Andersen. Mayday: Distributed ﬁltering for internet services.

In

Proc. USITS, 2003.

[26] D. G. Andersen, H. Balakrishnan, N. Feamster, T. Koponen, D. Moon, and
In Proc. SIGCOMM,

S. Shenker. Accountable internet protocol (AIP).
2008.

[27] P. Barford, J. Kline, D. Plonka, and A. Ron. A signal analysis of network
trafﬁc anomalies. In Proc. ACM SIGCOMM Workshop on Internet Mea-
surement, 2002.

[28] R. C´aceres, F. Douglis, A. Feldmann, G. Glass, and M. Rabinovich. Web
proxy caching: The devil is in the details. SIGMETRICS Perform. Eval.
Rev., 26(3):11–15, Dec. 1998.

[29] M. Casado, T. Koponen, S. Shenker, and A. Tootoonchian. Fabric: A ret-

rospective on evolving sdn. In Proc. HotSDN, 2012.

830  24th USENIX Security Symposium 

USENIX Association

1 Minimize α × ∑

e

s.t.

∑
a

∑
d

fe,a,d × Te,a × Le,d + ∑

d

dscd

fe,a,d = 1  all suspicious trafﬁc should be served

d

2 ∀e,a : ∑
3 ∀a,d : ta,d = ∑
4 ∀d : ∑

e

a

fe,a,d × Te,a  trafﬁc of each type to each datacenter
d  datacenter link capacity

ta,d ≤ Clink
nd,s
a,i ≥ ta,d ×
a,i ≤ Ccompute
nd,s
∑
i

d,s

i�:(i�,i)=eannotated

Wa,i�→i

∑
a,i�→i
Pa,i
 server compute capacity

 provisioning sufﬁcient VMs (Sd is the set of d’s servers.)

5 ∀d,a,i : ∑
s∈Sd
6 ∀d,s ∈ Sd : ∑
a
7 ∀d : dscd = intraRd × IntraUnitCost + interRd × InterUnitCost  total cost within each datacenter
8 ∀d : intraRd = ∑
qd,a,i,vm,s,i�,vm�,s�,l  intra-rack cost

MaxVM

MaxVM

MaxVol

∑

∑

a

∑
l=1

9 ∀d : interRd = ∑

a

MaxVol

∑
l=1

qd,a,i,vm,s,i�,vm�,s�,l  inter-rack cost

∑
vm=1
MaxVM

∑
vm=1

vm�=1
MaxVM

∑

vm�=1

(i,i�)=eannotated

(i,i�)=eannotated

∑
a,i→i�
∑
a,i→i�
∑
i:(i,i�)=eannotated
a,i→i�
a,i� × Pa,i� ≥

(s,s�)∈sameRack

∑

(s,s�) /∈sameRack
MaxVM
MaxVol
∑
l=1
MaxVM

∑
vm=1
MaxVM

s

∑
s�

qd,a,i,vm,s,i�,vm�,s�,l ≤ Pa,i�  enforcing VMs capacities

10 ∀d,a,i�,vm� : ∑
11 ∀d,s ∈ Sd,a,i� : nd,s
12 ∀d,s ∈ Sd,a,i� : nd,s
13  ﬂow conservation for VM vm of type logical node k that has both predecessor(s) and successor(s)

∑
a,i→i�
∑
a,i→i�

a,i� × Pa,i� ≤

∑
vm=1
MaxVM

i:(i,i�)=eannotated

i:(i,i�)=eannotated

vm�=1
MaxVM

∑
vm=1

∑
l=1

∑
l=1

MaxVol

MaxVol

vm�=1

∑
s�

∑
s�

∑

∑

qd,a,i,vm,s,i�,vm�,s�,l  bound trafﬁc volumes

qd,a,i,vm,s,i�,vm�,s�,l + 1  bound trafﬁc volumes

∀d,a,k,vm :

MaxVM

∑

vm�=1

g:(g,k)=eannotated

MaxVol

∑
l=1

∑

a,g→k
∑

∑
s

∑
s�
∑
a

qd,a,g,vm�,s�,k,vm,s,l =

MaxVM

∑

∑

vm�=1

h:(k,h)=eannotated

a,k→h

MaxVol

∑
s

∑
s�

∑
l=1

qd,a,k,vm,s,h,vm�,s�,l

14 ∀link ∈ ISP backbone :
15

fe,a,d ∈ [0,1],qd,a,i,vm,s,i�,vm�,s�,l ∈ {0,1},nd

fe,a,d × Te,a ≤ β × MaxLinkCapacity  per-link trafﬁc load control
a,i ∈ {0,1, . . .}, ta,d,interRd,intraRd,dscd ∈ R  variables

a,i,nd,s

link∈Pathe→d

Figure 15: ILP formulation for an optimal resource management.

a

∈ Eannotated

type va,i runs on server s and sends 1 unit of trafﬁc (e.g., 1
Gbps) to VM vm� of type va,i� that runs on server s�, where
eannotated
, and servers s and s� are located in
a,i→i�
datacenter d; otherwise, qd,a,i,vm,s,i�,vm�,s�,l = 0. Here l is
an auxiliary subscript indicating that the one unit of traf-
ﬁc associated with q is the lth one out of MaxVol possible
units of trafﬁc. The maximum required number of VMs
of any type is denoted by MaxVM.

.

a

The ILP involves two key decision variables: (1) fe,a,d
is the fraction of trafﬁc Te,a to send to datacenter Dd, and
(2) nd,s
a,i is the number of VMs of type va,i on server s of
datacenter d, hence physical graphs DAGphysical
Objective function:
The objective function (1) is
composed of inter-datacenter and intra-datacenter costs,
where constant α > 0 reﬂects the relative importance of
inter-datacenter cost to intra datacenter cost.
Constraints: Equation (2) ensures all suspicious traf-
ﬁc will be sent to data centers for processing. Equation
(3) computes the amount of trafﬁc of each attack type
going to each datacenter, which is ensured to be within
datacenters bandwidth capacity using (4). Equation (5) is

intended to ensure sufﬁcient numbers of VMs of the re-
quired types in each datacenter. Servers compute capaci-
ties are enforced using (6). Equation (7) sums up the cost
associated with each datacenter, which is composed of
two components: intra-rack cost, given by (8), and inter-
rack component, given by (9). Equation (10) ensures the
trafﬁc processing capacity of each VM is not exceeded.
Equations (11) and (12) tie the variables for number
of VMs (i.e., nd,s
a,i ) and trafﬁc (i.e., qd,a,i,vm,s,i�,vm�,s�,l) to
each other. Flow conservation of nodes is guaranteed
by (13). Inequality (14) ensures no ISP backbone link
gets congested (i.e., by getting a trafﬁc volume of more
than a ﬁxed fraction β of its maximum capacity), while
Pathe→d is a path from a precomputed set of paths from
e to d. The ILP decision variables are shown in (15).
B DSP and SSP Algorithms
As described in §4.3, due to the impractically long time
needed to solve the ILP formulation, we design the DSP
and SSP heuristics for resource management. The ISP
global controller solves the DSP problem to assign sus-
picious incoming trafﬁc to data centers. Then each lo-
cal controller solves an SSP problem to assign servers to

USENIX Association  

24th USENIX Security Symposium  831

VMs. Figure 16 and 17 show the detailed pseudocode
for the DSP and SSP heuristics, respectively.

d

d

a,d

, Clink

, and Ccompute

and fe,a,d values

1  Inputs: L, T, DAGannotated
a
2  Outputs: DAGphysical
3
4 Build max-heap TmaxHeap of attack volumes T
5 while !Empty(TmaxHeap)
6
7
8
9
10
11

d ← datacenter with min. Lt.e,t.d and cap.> 0
 enforcing datacenter link capacity
t1 ← min(t,Clink
 compute capacity of d for trafﬁc type a
t2 ←

do t ← ExtractMax(TmaxHeap)

)

d

CCompute
d
∑
Wa,i�→i
i�
Pa,i

∑
i

 enforcing datacenter compute capacity
tassigned ← min(t1,t2)
fe,a,d ←
for each module type i

tassigned
Tt.e,t.a

do  update nd

a,i given new assignment

∑
i�

Wa,i�→i
Pa,i

assigned

a,i = nd
a,i + td
nd
d − tassigned
← Ccompute

Clink
d ← Clink
Ccompute
d
 leftover trafﬁc
tunassigned = t− tassigned
if (tunassigned > 0)
then Insert(TmaxHeap,tunassigned )

− tassigned∑

∑
i�

d

i

Wa,i�→i
Pa,i

12
13
14
15
16

17
18

19
20
21
22
23
24
25

for each datacenter d and attack type a

do Given nd

a,i and DAGannotated

a

, compute DAGphysical

a,d

Figure 16: Heuristic for datacenter selection prob-
lem (DSP).

is not assigned to d’s servers

whose all predecessors are assigned

corresponding to N)

, IntraUnitCost, InterUnitCost,

1  Inputs: DAGphysical
values
a,i values

and Ccompute

a,d

d,s

a,d

2  Outputs: nd,s
3
4 while entire DAGphysical
do N ← vannotated
5
if (N == NIL)
6
then N ← vannotated
7
a
localize(nodes of DAGphysical
8
9
10  function localize tries to assign all of its

a,d

a,i

with max Pa,i

input physical nodes to the same server or rack
localize(inNodes){
assign all inNodes to emptiest server
if failed

then assign all inNodes to emptiest rack

if failed

update nd,s

then split inNodes Vphysical
a,i values

a

across racks

11
12
13
14
15
16
17
18 }

Figure 17: Heuristic for server selection problem
(SSP) at datacenter d.

832  24th USENIX Security Symposium 

USENIX Association

