rBridge: User Reputation based Tor Bridge Distribution with Privacy

Preservation

Qiyan Wang

Department of Computer Science

University of Illinois at Urbana-Champaign

qwang26@illinois.edu

Nikita Borisov

Department of Electrical & Computer Engineering

University of Illinois at Urbana-Champaign

nikita@illinois.edu

Zi Lin

Department of Computer Science

University of Minnesota

lin@cs.umn.edu

Nicholas J. Hopper

Department of Computer Science

University of Minnesota

hopper@cs.umn.edu

Abstract

Tor is one of the most popular censorship circumven-
tion systems; it uses bridges run by volunteers as proxies to
evade censorship. A key challenge to the Tor circumvention
system is to distribute bridges to a large number of users
while avoiding having the bridges fall into the hands of
corrupt users. We propose rBridge—a user reputation sys-
tem for bridge distribution; it assigns bridges according to
the past history of users to limit corrupt users from repeat-
edly blocking bridges, and employs an introduction-based
mechanism to invite new users while resisting Sybil attacks.
Our evaluation results show that rBridge provides much
stronger protection for bridges than any existing scheme.
We also address another important challenge to the bridge
distribution—preserving the privacy of users’ bridge as-
signment information, which can be exploited by malicious
parties to degrade users’ anonymity in anonymous commu-
nication.

1

Introduction

Censoring the Internet is a means adopted by many re-
pressive regimes to control the information that their citi-
zens can access on the Internet. Many websites that allow
people to exchange political ideas (e.g., Facebook, Twitter,
and Flickr) or may provide political information contrary
to the state’s agenda (e.g., YouTube, Wikipedia, and CNN)
have been blocked by the repressive governments [23]. As
of April 2012, 7 out of the top 10 non-Chinese websites1
(70%) were blocked or partially blocked, and in total, 3716

out of 14521 websites (26%) were blocked, by the “Great
Firewall of China”2. To further tighten the Internet censor-
ship, the Chinese government employs an Internet police
force of over 30 000 people to constantly monitor the citi-
zens’ Internet usage [13].

A typical approach to skirting censorship is to deploy
circumvention proxies outside the censored network, which
can provide indirect access to blocked websites. Tor [10]
is one of the most popular proxy-based circumvention sys-
tems; it uses bridges run by volunteers as proxies to evade
censorship. Censors are, however, eager to discover such
bridges and block them as well. A particularly powerful ap-
proach to enumerating bridges is the insider attack, wherein
the censor colludes with corrupt users to discover and shut
down bridges; the censor can further amplify the attack by
deploying a large number of Sybils to accelerate the discov-
ery of bridges.

To make enumeration more difﬁcult, the bridge distribu-
tor gives a limited number of bridges to each user. But this
creates a new privacy problem, as this information could be
used to ﬁngerprint the user: an adversary who can monitor
the bridges used for a set of anonymous tunnels can poten-
tially link them back to the user. As a result, the bridge dis-
tributor becomes a fully trusted component, whereas other
components of Tor must be at most honest-but-curious.

In this work, we make the following contributions to

both bridge protection and user privacy preservation:

1) We propose rBridge—a user reputation system for
bridge distribution.
rBridge computes users’ reputation
based on the uptime of their assigned bridges, and allows a
user to replace a blocked bridge by paying a certain amount
of reputation credits; this prevents corrupt users from re-

1http://www.alexa.com/topsites

2https://zh.greatfire.org

In addition, high-reputation
peatedly blocking bridges.
users are granted opportunities to invite friends into the sys-
tem. The introduction-based approach ensures the system
can steadily grow the user base as recruiting new bridges,
while preventing adversaries from inserting a large number
of corrupt users or Sybils into the system. We performed
extensive evaluation to show that rBridge provides much
stronger protection for bridges than any existing scheme;
for instance, the number of user-hours served by bridges in
rBridge is at least one order of magnitude more than that of
the state-of-the-art proxy distribution scheme [15].

2) For privacy-preserving bridge distribution, the bridge-
related information on users’ reputation proﬁles must be
managed by the users themselves to avoid leaking the in-
formation to the bridge distributor. This raises the prob-
lem that malicious users could cheat the reputation system
by manipulating their records. In this work, we propose a
novel privacy-preserving user reputation scheme for bridge
distribution, which can not only ensure the bridge distribu-
tor learns nothing about users’ bridge assignment, but also
prevent corrupt users from cheating. To our best knowledge,
rBridge is the ﬁrst scheme that is able to perfectly preserve
users’ privacy in bridge distribution. We implemented the
privacy-preserving scheme, and experimental results show
that rBridge has reasonable performance.

The rest of this paper is organized as follows. We in-
troduce related work in Section 2. Section 3 presents the
basic concepts, including design goals, threat model, and
the scope of this work. In Section 4, we elaborate the basic
rBridge scheme without privacy preservation and provide
evaluation results. Section 5 presents the privacy-preserving
scheme and performance evaluation results. We analyze po-
tential attacks in Section 6, and conclude in Section 7.

2 Background and Related Work

2.1 Tor and Bridge Distribution

Tor [10] is primarily an anonymous communication sys-
tem. A Tor user randomly selects 3 relays to build an onion
encryption tunnel to communicate with a target anony-
mously. As of May 4 2012, there are about 3 000 relays
in the Tor network [1]. The selection of relays must be
kept private from any entity, because otherwise an adver-
sary could likely link the user to his communicating target.
To ensure this, each user is required to download all the re-
lays’ descriptors (from a directory authority or a Tor relay)
even though he only needs 3 relays, and make his selection
locally.

Recently, Tor has been increasingly used as a censorship
circumvention tool. Users in a censored country can use Tor
relays as proxies to access blocked sites. However, since all
of the Tor relays are publicly listed, many countries (e.g.,

China) have blocked the public Tor relays altogether. In re-
sponse, Tor turned to private relays run by volunteers, called
bridges, to circumvent censorship. A key challenge though
is to distribute the addresses of bridges to a large number of
users without exposing them to the censor.

The bridge distribution strategies that have been de-
ployed by Tor are to give a small subset of bridges to each
user, as identiﬁed by a unique IP address or a Gmail ac-
count. Unfortunately, these cannot survive a powerful ad-
versary who can access a large number of IP addresses and
Gmail accounts to create a large number of Sybils; the Chi-
nese government were able to enumerate all the bridges dis-
tributed using these strategies in under a month [2]. The al-
ternative approaches adopted by Tor employ more stringent
distribution strategies: the bridges are given to a few trusted
people in censored countries in an ad hoc manner, who fur-
ther disseminate the bridges to their social networks; or, in-
dividuals deploy their private bridges and give the bridges’
addresses only to trusted contacts. However, the stringent
bridge distribution can only reach a very limited fraction of
potential bridge users and restrict the openness of the sys-
tem.

2.2 Proxy Distribution Strategies

Researchers have tried to design better proxy distribution
strategies [11, 14, 15, 20]. Feamster et al. [11] proposed a
keyspace-hopping mechanism for proxy distribution, which
employs computational puzzles to prevent a corrupt user
from learning a large number of proxies. However, this
mechanism is not likely to withstand an adversary who has
strong computational power; the results of [11] show that
95% of 100 000 proxies would be discovered if the adver-
sary can solve about 300 000 puzzles. In the scheme pro-
posed by Sovran et al. [20], the address of a proxy is given
to a few highly trusted people who play as internal prox-
ies to relay other users’ trafﬁc to the external proxy; the
addresses of these forwarders are advertised by performing
random walks on social networks. However, this scheme
is unable to provide users reliable circumvention service as
forwarders may go ofﬂine from time to time; besides, the
forwarders (residing in the censored country) could receive
a severe penalty for facilitating circumvention, which may
make people hesitate to serve as forwarders.

Mahdian [14] studied the proxy distribution problem
from an algorithmic point of view, and theoretically ana-
lyzed the lower bound of the number of proxies required
to survive a certain number of malicious insiders. Never-
theless, their scheme is not practical, as it assumes that the
number of corrupt users is known in advance and there is
no limit on the capacity of each proxy. Recently, McCoy
et al. [15] proposed Proximax, which leverages social net-
works for proxy distribution and distributes proxies based

on the efﬁciency of each distribution channel to maximize
the overall usage of all proxies. In this work, we explicitly
compare rBridge with Proximax and show that rBridge is
able to provide much stronger protection for bridges than
Proximax.

We note that none of the existing proxy distribution
strategies is able to preserve users’ privacy. They assume
the proxy distributor is fully trusted and authorized to know
which proxies are given to a particular user. Applying
these distribution strategies to Tor would degrade users’
anonymity in anonymous communication.

2.3 Anonymous Authentication and Anonymous

Reputation Systems

Researchers have put forward several designs for anony-
mous authentication and anonymous reputation systems [7,
8, 21, 22] that are similar to what we are seeking. Au et
al. [8] proposed a k-times anonymous authentication (k-
TAA) scheme that allows a user to be authenticated anony-
mously for a bounded number of times. The work in [21,22]
extended this scheme to allow revocation without trusted
third parties. Later, Au et al. [7] further extended the anony-
mous authentication schemes to support users’ reputation
management. We note that, however, none of these schemes
is applicable to bridge distribution due to inability to limit
misbehavior of malicious users.
In bridge distribution, a
user’s reputation that is calculated based on the user’s bridge
assignment records should be managed by the user himself
to avoid leaking the bridge information to the bridge dis-
tributor, which raises the risk that a malicious user could
manipulate his reputation records, e.g., increasing his credit
balance. Whereas, in the aforementioned schemes, users’
reputation is calculated by servers that are trusted to per-
form the reputation calculation, and thus they do not need
to consider potential cheating of malicious users.

3 Concept

2. Minimized thirsty-hours of users: Another important
aspect, which is overlooked by prior work, is thirsti-
ness of honest users. We use thirsty-hours to measure
the time that an honest user has no bridge to use. We
aim to minimize it to ensure high quality of service.

3. Healthy growth of the user base: We assume the bridge
distributor can recruit new bridges from time to time,
and each bridge can support up to a certain num-
ber of users due to limited capacity. The consump-
tion of bridges is due to either new user joining or
bridge blocking. By “healthy growth of the user base”,
we mean the user base can grow correspondingly as
new bridges are added to the system, without causing
thirstiness of existing users. For an ineffective bridge
distribution strategy, corrupt users can drain out the
bridge resource, leaving little ability to grow the user
base.

4. Privacy preservation of bridge assignment: We aim
to prevent any entity (e.g., a curious bridge distrib-
utor) from learning any information about bridge as-
signment of a particular user; such information can be
exploited to degrade the user’s anonymity.

We note that 4) distinguishes rBridge from prior work,
as none of the existing approaches preserves users’ bridge
assignment information. For 1), 2), and 3), we shall show
that rBridge can achieve much higher performance than any
existing approach.

It is important to note that similar to prior work, we are
not interested in ensuring a single or a few important indi-
viduals can access unblocked bridges. Instead, we aim to
provide the circumvention service to the majority of users;
in other words, it is possible that a few honest users could
lose all their bridges before boosting their reputation to re-
ceive new bridges. Providing guaranteed circumvention ser-
vice to a few special users can be easily achieved by de-
ploying a few exclusive circumvention proxies; however,
we believe it is more valuable to provide the circumvention
service to a large number of ordinary users.

In this section, we present the design goals, threat model,

and scope of this work.

3.2 Threat Model

3.1 Goals

rBridge aims to achieve the following goals:

1. Maximized user-hours of bridges: McCoy et al. [15]
proposed the metric user-hours to evaluate the robust-
ness of a proxy distribution strategy. It represents the
sum of hours that a bridge can serve for all of its users
before being blocked.

We consider a state-level adversary (i.e., the censor),
who has access to rich human resource, i.e., controlling a
substantial number of potential bridge users.
In rBridge,
a new user needs an invitation ticket (which is probabilis-
tically distributed to high-reputation users) to register and
join the system. A registered malicious user can block his
assigned bridges by revealing them to the censor who can
later block the bridges (referred to as the insider attack).
Typically, the censor would like to block as many bridges
as quickly as possible, but in some instances she can adopt

other strategies, such as keeping known bridges unblocked
for some period of time to boost the number of insiders and
later performing a massive blocking attempt in a crisis. In
general, we assume the set of malicious users is a Byzan-
tine adversary, and can deviate from the protocol in arbi-
trary ways to maximize their chance of blocking bridges.
The adversary could also launch the Sybil attack by creat-
ing a large number of fake accounts in the population of
potential bridge users. We note that, however, the Sybils
can help the adversary discover bridges only if they can get
registered. In addition, we assume that the adversary can
access substantial network resources, e.g., a large number
of IP addresses and Email accounts, but she has bounded
computational power and is unable to subvert widely used
cryptographic systems.

Unlike the existing schemes [2, 11, 14, 15, 20] that as-
sume the bridge distributor is fully trusted, we consider an
honest-but-curious model for the bridge distributor, which
is within the threat model of Tor [10]. More speciﬁcally, we
assume the bridge distributor honestly follows the protocol,
but is interested in learning any private information about
users, such as which bridges are assigned to a particular
user. For ease of presentation, we assume there is a single
bridge distributor, but it is straightforward to duplicate the
bridge distributor by creating multiple mirrored servers.

3.3 Scope

For clarity, we do not attempt to address network-level
bridge discovery. We assume the censor is able to learn
bridges only from the distribution channels (i.e., based on
the knowledge of registered corrupt users and Sybils).
It
is possible that the censor employs other techniques to dis-
cover bridges. For instance, the censor could try to probe all
IP addresses on the Internet to ﬁnd hosts that run Tor hand-
shake protocols, ﬁngerprint Tor trafﬁc to identify bridges, or
monitor the users who connect to a discovered bridge to see
what other TLS connections these users establish and try to
further verify whether the connected hosts are bridges [2].
We note that if the censor were able to identify bridges using
such network-level bridge discovery techniques, any bridge
distribution strategy would not be able to work. Defending
against such attacks is an active research area; researchers
have been proposing various defense mechanisms, such as
obfsproxy [3], BridgeSPA [19], and client password autho-
rization [4]. We acknowledge that effective mechanisms for
resisting the network-level bridge discovery are important
research problems, but they are orthogonal to this work.

In rBridge, users’ reputation is calculated based on the
uptime of their assigned bridges, which requires a mecha-
nism to test reachability of bridges from censored countries.
Recently, the Tor project has proposed several methods to
accurately test bridges’ availability [6], and we expect these

mechanisms to be deployed soon. To clarify, we assume the
availability information of bridges can be provided by the
Tor network, and how to reliably check the bridges’ reach-
ability is out of the scope of this work.

4 The Basic rBridge Scheme

In this section, we present the basic rBridge scheme that
does not provide privacy preservation, i.e., the bridge dis-
tributor knows the bridge assignment details of each user.
We elaborate the privacy-preserving scheme in Section 5.

4.1 Overview

The openness of a proxy-based censorship circumven-
tion system and its robustness to the insider attack seem
to be in conﬂict. On the one hand, allowing anyone to
join the system and get bridges allows malicious users to
quickly enumerate all of the bridges [2]. On the other hand,
applying highly stringent restrictions on user registration
and bridge distribution (e.g., giving bridges only to highly
trusted people using social networks) enhances robustness,
but makes it hard for the majority of potential bridge users
to get bridges.

Our key insight is that it is possible to bridge the gap
between the openness and robustness of bridge distribution
by building a user reputation system. Instead of trying to
keep all malicious users outside the system, we adopt a less
restrictive user invitation mechanism to ensure the bridge
distribution can reach a large number of potential users; in
particular, we use a loosely trusted social network for user
invitation, and a well-behaving user can invite his less close
friends into the system. Meanwhile, we leverage a user
reputation system to punish blockers and limit them from
repeatedly blocking bridges; more speciﬁcally, each user
earns credits based on the uptime of his bridges, needs to
pay credits to get a new bridge, and is provided opportuni-
ties to invite new users only if his credit balance is above a
certain threshold.

It is important to note that our goal is not to keep bridges
unblocked forever; instead, we try to achieve a more practi-
cal goal—having bridges serve a sufﬁciently long period of
time so that the overall rate of recruiting new bridges out-
paces the rate of losing bridges. We also note that this is still
a very challenging problem; as will be shown by the com-
parison results (Section 4.3.3), the existing schemes have
a difﬁcult time protecting bridges from being blocked even
for a very limited period of time.

4.2 Scheme

When joining the system, a new user U receives k bridges
B1,··· , Bk as well as a credential, which is used to verify

U as a legitimate registered user. The credential is signed by
the bridge distributor D and includes the following informa-
tion:

U∥(cid:8)∥{Bi, τi, ϕi}k

i=1

wherein (cid:8) denotes the total credits owned by U, τi denotes
the time when Bi is given to U, and ϕi denotes the credits
that U has earned from Bi. (At the initialization, (cid:8) = 0,
ϕi = 0, and τi is the joining time). The selection of the
bridges B1,··· , Bk is at random. D keeps counting the
number of users assigned to each bridge and stops giving
a bridge’s address to new users once the number of users
assigned to the bridge reaches an upper limit (denoted by
g).

4.2.1 Earning Credits

U is given credits based on the uptime of his bridges. The
credit assignment policy should have the following proper-
ties. First, it should provide incentives for corrupt users to
keep the bridge alive for at least a certain period of time, say
T0 days, which should be long enough to make sure enough
new bridges can be recruited in time to maintain the over-
all bridge resources in the system. Second, the total credits
that a user earns from a bridge should be upper-bounded, to
prevent corrupt users from keeping one bridge alive to con-
tinuously earn credits and using the earned credits to request
and block other bridges.
Now, we deﬁne the credit assignment function Credit(·).
Let Tcur denote the current time, and βi denote the time
when Bi gets blocked (if Bi is not blocked yet, βi = ∞).
We deﬁne t as the length of the time period from the time
when U knows Bi to the time when Bi gets blocked or the
current time if Bi is not blocked, i.e., t = min{βi, Tcur} −
τi. We let ρ denote the rate of earning credits from a bridge
(credits/day) and T1 denote the upper-bound time by which
U can earn credits from the bridge. Then, the amount of
credits ϕi earned from Bi is deﬁned as:

 0

ϕi = Credit(t) =

(t − T0) · ρ
(T1 − T0) · ρ

t < T0
T0 ≤ t ≤ T1
t > T1

Without loss of generality, we deﬁne ρ = 1 credit/day; then,
the maximum credits that a user can earn from a bridge are
(T1 − T0).

From time to time (e.g., before requesting a new bridge),
U requests D to update his credit balance (cid:8) with his recently
earned credits, say, from Bi. D ﬁrst validates U’s credential
(verifying the tagged signature), and then re-calculates the
credits ~ϕi according to the uptime of Bi, adds the difference
~ϕi − ϕi to (cid:8), updates ϕi with ~ϕi, and ﬁnally re-signs the
updated credential.

4.2.2 Getting a New Bridge

To limit the number of bridges that a corrupt user knows,
we allow each user to have k or fewer alive bridges at any
time. This is enforced by granting a new bridge to a user
U only if one of his bridges (say Bb) has been blocked. In
particular, upon a request for a new bridge in replace of
Bb, D ﬁrst veriﬁes that Bb is in U’s credential and has been
blocked. D also checks whether U has enough credits to pay
− is the price for a
for a new bridge, i.e., (cid:8) > ϕ
new bridge.
After giving out a new bridge ~Bb, D updates U’s creden-
tial by replacing the record {Bb, τb, ϕb} with { ~Bb, Tcur, 0}
and updating the total credits with ~(cid:8) = (cid:8)− ϕ
−. To prevent
a malicious user from re-using his old credentials that has
more credits, D keeps a list of expired credentials (e.g., stor-
ing the hash value of the credential); once U’s credential is
updated, the old credential is added to the expired credential
list and cannot be used again.

−, where ϕ

We note that temporarily blocking a bridge just to create
an open spot for a new bridge does not help a corrupt user,
because he still needs to pay the same amount of credits to
get a new bridge and the availability loss of a temporarily
blocked bridge is strictly smaller than that of a permanently
blocked bridge.

4.2.3

Inviting New Users

D periodically sends out invitation tickets to high-reputation
users whose credit balances are higher than the threshold
(cid:8)θ. Since the censor may let some corrupt users behave
legitimately to simply accumulate credits and obtain invita-
tion tickets in order to deploy more corrupt users or Sybils
in the system, we let D randomly select the recipients of
invitation tickets from qualiﬁed users. A user who has re-
ceived an invitation ticket can give it to any of his friends,
who can later use the ticket to join the system.

Note that the system needs to reserve a certain fraction
(e.g., 50%) of bridge resource (i.e., the sum of the remain-
ing capacity of unblocked bridges) for potential replace-
ment of blocked bridges for existing users, while using the
rest bridge resource to invite new users. The amount of
reserved resource can be dynamically adjusted according
to the amount of current bridge resource and the plans for
growing the user base and recruiting new bridges.

4.3 Evaluation and Comparison

We now analyze the robustness of rBridge against the
following blocking strategies, and compare it with Proxi-
max [15]. We discuss other potential attacks in Section 6.
• Aggressive blocking: The censor is eager to block dis-
covered bridges, i.e., shutting down the bridge once it
is known to a corrupt user.

(a) Number of initial bridges (N = 1000)

(b) Number of users per bridge (p is attack
probability, f = 5%)

(c) Probability distribution of malicious users
(f = 5%)

Figure 1: Parameter selection

• Conservative blocking: A sophisticated censor may
keep some bridges alive for a certain period of time
to accumulate credits, and use the credits to discover
new bridges and/or invite more corrupt users.

• Event-driven blocking: The censor may dramatically
tighten the control of the Internet access when certain
events (e.g., crisis) take place. We consider such at-
tacks by assuming that malicious users do not block
any bridges until a certain time, when suddenly all the
discovered bridges get blocked.

To evaluate rBridge under these attacks, we imple-
mented an event-based simulator using a timing-based pri-
ority queue, by treating each state change of the system as
an event, such as inviting a new user, getting a new bridge,
blocking a bridge, recruiting a new bridge, etc. Each event
contains a time stamp indicating when the event occurs as
well as an ID of the subject indicating who will carry out
the event. We start with choosing the parameters for our
simulation.

4.3.1 Parameter Selection

We employ probabilistic analysis to select appropriate pa-
rameters. To simplify the parameter calculation, we con-
sider a static user group (i.e., no new users join the sys-
tem); later, we validate our parameter selection in a dy-
namic setting using the event-based simulator. In practice,
the bridge distributor can periodically re-calculate the pa-
rameters (e.g., every 30 days) using the current size of the
user group.

Initial setup. Let f denote the fraction of malicious users
among all potential bridge users (note that f is not the ac-
tual ratio of malicious users in the system). We expect a
typical value of f between 1% and 5%, but we also eval-
uate rBridge with much higher f to see its robustness in
extreme cases. The system starts with N = 1000 users,
which are randomly selected from the pool of all potential

bridge users; for instance, D could randomly select a num-
ber of Chinese users on Twitter (based on their proﬁles) as
the initial bridge users, and very likely these users are will-
ing to use the bridge based circumvention service because
they already used some circumvention tools to access Twit-
ter (which is blocked in China).

Each user is initially provided k = 3 bridges3. Suppose
there are m0 initial bridges in the system; the number of
users per bridge is g0 = N·k
on average. Assuming a cor-
rupt user blocks all of his bridges, the probability that an
honest user has no alive bridge is (1− (1− f )g0 )k. Accord-
ing to Figure 1a, we choose m0 = 200 to make sure the
majority of users can survive the initial blocking.

m0

g—the maximum number of users per bridge. In rBridge,
when a bridge gets blocked, all the g users sharing this
bridge will be “punished” (i.e., receive no more credits from
the bridge and need to pay credits to get a new bridge); in-
tuitively, with a smaller g, it is easier to precisely punish
the real blocker, as fewer honest users would be punished
by mistake. On the other hand, we should make sure g is
sufﬁciently large to avoid underusing the bridges.

Here, we calculate the probability that a user has a cer-
tain number of blocked bridges; this probability depends on
g and determines the punishment on the user. Let p denote
the probability that a corrupt user blocks a bridge he knows,
and λ denote the probability that a bridge is blocked. Then,
we have λ = 1− (1− f · p)g (here we use f to approximate
the ratio of corrupt users in the system). We deﬁne Xh (or
Xm) as the number of blocked bridges of an honest (or cor-
rupt) user. Assuming a user obtains l bridges since joining
the system, we get:

· (1 − λ)x · λl−x

(1)
· (1 − λ)x−p·lλl−x(2)

(
(

)
l
x
l − p · l
x − p · l

)

P r(Xh = x) =

P r(Xm = x) =

3In the current bridge distribution strategy deployed by Tor, each re-

questing user is given 3 different bridges.

010020030040050000.20.40.60.81m0 −− num. of initial bridgesPr(no alive bridge)  f=5%f=2%f=1%02040608010000.20.40.60.81g −− num. of users per bridgePr(Xm > Xh)  p=80%p=50%p=20%00.20.40.60.8100.050.10.150.20.25indexPr(mal | index)  uniformstagedlinear(a) User-hours

(b) % of thirsty-hours

(c) User base

Figure 2: Aggressive blocking

We are interested in calculating P r(Xm > Xh) —the prob-
ability that a corrupt user has more blocked bridges than an
honest user (i.e., the likelihood that a corrupt user receives
more punishment than an honest user); ideally, this proba-
bility should be maximized.

l∑

P r(Xm > Xh) =

x−1∑

P r(Xm = x)

P r(Xh = y)

y=0

x=p·l

(3)

Figure 1b depicts P r(Xm > Xh) with l = 10 and f =
5%. While P r(Xm > Xh) is maximal when g is small, we
choose a fairly large value g = 40 to make sure bridges are
not underutilized.

Credit(t)—the credit assignment function. Recall that
T0 and T1 represent the expected lower and upper bounds
of a bridge’s life time, respectively. We let Tlf denote the
expected life time of a bridge, T0 ≤ Tlf ≤ T1, and s denote
the speed of recruiting new bridges. To maintain the overall
bridge resource, we should have:

Tlf · g · s · time = N · k · time

From this, we get:

T0 =

N · k
g · smax

, T1 =

N · k
g · smin

(4)

(5)

where smax and smin denote the maximum and minimum
rate of recruiting new bridges, respectively.
(From May
2011 to May 2012, the Tor project recruited about 400 new
bridges [1].) In our evaluation, we set smax = 1 bridge/day
and smin = 0.2 bridge/day, which implies that 70 ∼ 360
bridges need to be recruited per year. With N = 1000, we
get T0 = 75 days and T1 = 375 days according to (5). Note
that with a larger number of users, the overall bridge con-
sumption will become higher and the system needs to re-
cruit more bridges. However, T0 and T1 we have calculated
are the worst-case expectations; as will be shown in the sim-
ulation, the lifetime of bridges is actually much longer than

the worst-case T0 and T1, and hence the pressure of recruit-
ing new bridges is smaller in practice.
−—the price for getting a new bridge. The credits
ϕ
earned from unblocked bridges should be roughly equal to
the credits paid to replace blocked bridges. Therefore, ap-
proximately we have:

P r(Xh = x) · x · ϕ

−

=

(6)

P r(Xh = x) · (k − x) · (T1 − T0)

k∑
k∑

x=0

x=0

From Equation (1) (6), we get ϕ

= 45.

−

(cid:8)θ—the threshold of credits for invitation. To decide the
value of (cid:8)θ, we assume that a user with at least half of his
bridges unblocked can be considered to invite new users.
Then, we have:

⌈ k

⌉∑
·(k − x) · (T1 − T0)
From Equation (1) (7), we get (cid:8)θ = 236.

(cid:8)θ =

x=0

2

P r(Xh = x|Xh ≤ ⌈ k

2

⌉)

(7)

User invitation.

In our simulation, we set the rate of
recruiting new bridges as s = 1 bridge/day; we reserve
50% of bridge resource for potential replacement of blocked
bridges. Every 7 days, the bridge distributor calculates the
number of new users to be invited based on the current
bridge resource, and distributes the corresponding number
of invitation tickets to randomly selected users whose credit
balance is higher than (cid:8)θ.

Probability distribution of malicious users. Now we con-
sider the probability that an invited user is malicious. We
suppose each corrupt user always gives his invitation tick-
ets to malicious users or Sybils. For an honest user, if he
randomly selects a new user to invite, the probability that
the new user is malicious is approximately f. However,

10210410600.20.40.60.81Use hours of bridgesCDF  f=5%, linearf=5%, stagedf=10%, stagedf=30%, stagedf=50%, staged00.20.40.60.810.50.60.70.80.91% thirsty hourCDF  f=5%, linearf=5%, stagedf=10%, stagedf=30%, stagedf=50%, staged01002003004000100020003000400050006000Time (day)Num. of users  f=5%, linearf=5%, stagedf=10%, stagedf=30%, stagedf=50%, staged(a) User-hours

(b) % of thirsty-hours

(c) User base

Figure 3: Conservative blocking (f = 5%, staged distribution)

in practice, a user is inclined to ﬁrst invite the friends he
trusts most; as receiving more and more invitation tick-
ets, the user will start to invite less trusted friends. To
model this, we assume each user ranks his friends based
on trustworthiness: each friend is assigned an index rang-
ing from 0 to 1 according to the trustworthiness (e.g., the
most trusted one out of 100 friends has the index 1/100 =
0.01). We consider two speciﬁc models to assign probabili-
ties of malicious users. One is staged distribution, wherein
the friends are divided into two groups (i.e., more trusted
and less trusted) and all users within a group have the same
probability of being malicious. We assume 80% friends be-
long to the “more trusted” group and the remaining 20% are
in the “less trusted” group. The other is linear distribution,
for which the probability of being a malicious user is a lin-
ear function of the index. We suppose the probability that
the most trusted friend is malicious is 1%. For both distri-
butions, the overall ratio of malicious users is f. Figure 1c
depicts the probability distributions of these two models.

4.3.2 Evaluation Results

Using the event-based simulator, we measured the user-
hours, thirsty-hours, and growth of user base under differ-
ent blocking strategies.

Aggressive blocking. The simulation results for the ag-
gressive blocking are provided in Figure 2. We can see from
Figure 2a that when f = 5%, 80% of bridges can pro-
duce over 1000 user-hours, and 70% of bridges can serve
more than 10 000 user-hours, before being blocked; about
50% of bridges are never blocked. Figure 2b shows that
with f = 5%, over 95% of users are never thirsty for
bridges; a small fraction (about 2%) of users are unable
to get new bridges, because all of their initially assigned
bridges get blocked before they earn enough credits to re-
quest new bridges. Figure 2c shows that users need some
time (150 ∼ 200 days) to accumulate enough credits to be-
come qualiﬁed for inviting friends. After the accumulation

phase, the user base starts to steadily grow almost linearly
with the number of newly recruited bridges. We also see
that rBridge performs relatively better with the staged dis-
tribution of malicious users than with the linear distribution;
this is because for the staged distribution most invited users
belong to the “more trusted” group, for which the probabil-
ity of being a malicious user is lower than that for the linear
distribution.

In addition, we evaluate rBridge with a much higher f
(using the same system conﬁguration). We can see that
rBridge can easily tolerate 10% malicious users; even when
f = 30%, the performance of rBridge is still acceptable; for
f ≥ 50%, rBridge fails to provide reasonable protection for
bridges.

Conservative blocking. There are two factors related to
the conservative blocking:
the probability of blocking a
bridge (p), and the waiting time to block a bridge (wait).
Since the time to earn credits from a bridge is upper-
bounded by T1, we assume a corrupt user always blocks
his bridges by time T1 (i.e., wait ≤ T1). In the simula-
tion, we consider the following cases for the waiting time:
0 day (i.e., aggressive blocking), 120 days (by which the
earned credits are sufﬁcient to get a new bridge), and 225
days (i.e., the middle point between T0 and T1).

We can see from Figure 3 that compared with the aggres-
sive blocking, the conservative blocking causes less damage
to the user-hours and the growth of user base; this is be-
cause the bridges under the conservative blocking can serve
a longer time and more users can accumulate enough credits
to invite new users. We also notice that when wait = 225
days and p = 100%, about 10% of users are thirsty for 15%
of their time, which is worse than the aggressive blocking;
the reason for this is that after waiting 225 days, malicious
users earn enough credits to be considered for inviting new
(malicious) users (i.e., (225 − 75) × 3 = 450 > 236), and
overall they can block more bridges, which causes more re-
cently joined users to become thirsty.

Event-driven blocking. For event-driven blocking, we let

10210410600.20.40.60.81Use hours of bridgesCDF  p=100%,wait=225p=100%,wait=120p=100%,wait=0p=50%,wait=225p=50%,wait=120p=50%,wait=000.20.40.60.810.50.60.70.80.91% thirsty hourCDF  p=100%,wait=225p=100%,wait=120p=100%,wait=0p=50%,wait=225p=50%,wait=120p=50%,wait=0010020030040050001000200030004000500060007000Time (day)Num. of users  p=100%,wait=225p=100%,wait=120p=100%,wait=0p=50%,wait=225p=50%,wait=120p=50%,wait=0(a) Unblocked bridges

(b) Thirsty users

(c) Thirsty users with backup bridges

Figure 4: Event-driven blocking (f = 5%)

all of the corrupt users are synchronized to block all their
bridges simultaneously on the 300-th day. Figures 4a and 4b
show that right after the massive blocking, the number of
available bridges drops from 500 to 150, and the percent-
age of thirsty users rises to 25%. We note that the damage
of the event-driven blocking can be effectively mitigated by
keeping a small number of backup bridges (that are never
seen by any user). We can see from Figure 4c that with 50
backup bridges (about 10% of deployed bridges), the num-
ber of thirsty users can be reduced by half; with 100 backup
bridges, the number of thirsty users is minimized. We also
notice that keeping backup bridges cannot entirely eliminate
thirsty users; this is because there are a small fraction (about
10%) of users who join the system not long before the mas-
sive blocking and have not accumulated enough credits to
request new bridges.

4.3.3 Comparison with Proximax

W now compare rBridge with Proximax [15]—the state-of-
the-art proxy distribution scheme. Using the same method-
ology, we developed an event-based simulator for Proxi-
max. Since the authors of Proximax did not provide suf-
ﬁcient details about how to invite new users, we only con-
sider a static set of users for the comparison. We evaluate
both rBridge and Proximax under the aggressive blocking
using the same system conﬁguration as before; for Proxi-
max, we set the maximum delay of distributing bridges at
each hop (i.e., from when a user receives a bridge to when
he distributes the bridge to his friends) to 1 day4.

Figure 5a shows that in Proximax less than 5% bridges
are able to produce more than 20 user-hours, and none of the
bridges can serve over 126 user-hours. In comparison, in
rBridge, over 99% bridges can produce over 20 user-hours,
and 57% bridges are not ever blocked and are able to contin-
uously generate user-hours. In addition, in Proximax 99%

4In the simulation of Proximax, we found that higher propagation delay

leads to higher user-hours; hence, we chose a fairly large value (1 day).

of users are always thirsty for bridges, and this number is
only 10% for rBridge. Since our simulation for the compari-
son only considers a static user group, we are unable to eval-
uate Proximax in terms of the growth of the user base over
time; instead, we measure how many bridges are required to
support different-sized user bases for 30 days, while mak-
ing sure that each existing user has at least one unblocked
bridge at any time. We can see from Figure 5c that Proxi-
max requires a substantially larger number of bridges than
rBridge; for instance, to support 200 users, Proximax re-
quires at least 2400 bridges, while rBridge only needs 108
bridges. We note that for all these metrics (user-hours of
bridges, thirsty-hours of users, and bridge consumption),
the performance of rBridge is at least one order of mag-
nitude higher than that of Proximax.

Discussion. It is possible to improve Proximax by adopt-
ing a more restrictive distribution strategy, e.g., limiting
how many people a bridge recipient can share the bridge
with (i.e., the width of the distribution tree) as well as how
many hops a bridge can be distributed (i.e., the depth of
the distribution tree). Figure 5 also provides the results for
limiting the maximum width and depth of the distribution
tree to 5. While the restrictive distribution strategy can im-
prove the robustness of Proximax, it is still much worse
than rBridge. More importantly, the restrictive approach de-
grades the openness of the system.

The main reason that rBridge outperforms Proximax is
that Proximax calculates “reputation” at the granularity of
distribution trees (or called distribution channels) rather
than individual users, and the formation of each distribution
tree is ﬁxed and thus an honest user would be permanently
“infected” if he resides in a tree that contains a corrupt user.
Whereas, rBridge allows a user to join a different user group
when receiving a new bridge, and keeps track of each in-
dividual user’s records, based on which the bridge distrib-
utor can reward well-behaving users and punish blockers
individually. We note that recording each individual user’
bridge assignment leads to greater risks of violating users’

01002003004005000100200300400500Time (day)Num. of unblocked bridges  linearstaged010020030040050000.050.10.150.20.25Time (day)Perc. of thirsty users  linearstaged010020030040050000.050.10.150.20.25Time (day)Perc. of thirsty users  linear, bkup=20linear, bkup=50linear, bkup=100staged, bkup=20staged, bkup=50staged, bkup=100(a) User-hours

(b) Thirsty-hours

(c) Required bridges to support a certain # of
users for 30 days

Figure 5: Comparison with Proximax (f = 5%)

privacy; we describe how to perfectly protect users’ bridge
assignment information in the next section.

Finally, we note that computing reputation merely based
on the bridges’ uptime is not the only way to design the rep-
utation system. For instance, it is possible to extend rBridge
by including the reputations of the “introducees” as a factor
to calculate the reputation of the “introducer”. However, the
increased complexity of the reputation system makes it even
harder (if possible) to design a practical privacy-preserving
mechanism to perfectly protect users’ bridge assignment in-
formation. Therefore, our design philosophy is to make the
reputation system as simple as possible, while ensuring its
robustness against various blocking strategies.

5 rBridge with Privacy Preservation

In the basic rBridge scheme, D knows all the bridge as-
signment details of each user. A malicious entity equipped
with such information can degrade the users’ anonymity in
the anonymous communication. For instance, the current
Tor network ensures that each user can conceal his identity
as long as the entry relay (i.e., the bridge) is not compro-
mised. Whereas, with the bridge assignment information,
an adversary is able to narrow the anonymity set of the user
down to a small group of people who are given this bridge,
even though the bridge is not compromised. Unfortunately,
this issue is overlooked by all the prior bridge distribution
schemes. Our goal is to preserve the bridge information of
each user.

5.1 Challenges and Requirements

In rBridge, a user (U) can get a bridge only if he can
authenticate himself to D by presenting a valid credential
(recall that in the basic scheme, a credential includes the
following information U∥(cid:8)∥{Bi, τi, ϕi}k
i=1). Firstly, in or-
der to unlink the user from his assigned bridges, we should

conceal the user’s identity by replacing U’s real identity
with a pseudonym on his credential and letting him build a
Tor circuit to communicate with D to hide his IP address.

However, the above measures are not sufﬁcient. Suppose
U has received 10 bridges from D (who knows the bridges
but not U’s identity), and 2 of them are malicious and know
U’s identity due to direct contact and collude with D; then
it is highly likely that D can link U to all of his bridges,
since very few users happen to know both of the malicious
bridges. A natural solution to this is using Oblivious Trans-
fer (OT) for privacy-preserving bridge retrieval — prevent-
ing D from learning which bridge is retrieved when U re-
quests a new bridge (called a transaction). However, since
a user is very likely to request a new bridge right after one
of his bridges gets blocked, D can infer the blocked bridge
by checking which bridge was recently blocked. As a re-
sult, D can learn all of U’s (blocked) bridges as long as D can
link different transactions of U. Therefore, unlinkability of
transactions is required to avoid such information leaks.

Thirdly, since we intend to hide the bridge assignment
from D, the bridge related information in U’s credential, such
as {Bi, τi, ϕi}, should be written and updated by U, rather
than by D. This raises the risk that a malicious user could put
incorrect information on his credential, e.g., by changing
′
i so that
the credits ϕi or replacing Bi with another bridge B
he can block Bi without being punished. Therefore, we also
need to protect the integrity of credentials.

Although researchers have proposed several designs for
anonymous authentication/reputation systems [7, 8, 21, 22],
none of them is able to ensure integrity of credentials.
In this work, we propose a novel privacy-preserving user
reputation scheme that is specially designed for bridge
distribution and satisﬁes all the three aforementioned re-
quirements. Our design integrates OT with several other
cryptographic primitives (such as commitments and zero-
knowledge proofs) to both preserve users’ privacy and pre-
vent misbehavior of corrupt users. We start with introducing

10010200.51Use hours of bridgesCDF  rBridge: staged/linearProximax: staged, no limitProximax: linear, no limitProximax: staged, width<=5, depth<=5Proximax: linear, width<=5, depth<=500.20.40.60.8100.51% of thirsty hourCDF  rBridge: staged/linearProximax: staged, no limitProximax: linear, no limitProximax: staged, width<=5, depth<=5Proximax: linear, width<=5, depth<=502004006008001000100105Num. of usersNum. of bridges  rBridge: staged/linearProximax: staged, no limitProximax: linear, no limitProximax: staged, width<=5, depth<=5Proximax: linear, width<=5, depth<=5the cryptographic primitives used in our construction.

5.2 Cryptographic Building Blocks

5.2.1

1-out-of-m Oblivious Transfer

m
1

-OT) is a se-
1-out-of-m Oblivious Transfer (denoted by
cure two-party computation protocol, where the sender has
m secrets and the chooser can get 1 and only 1 secret from
the sender without revealing any information about which
-OT scheme
secret is selected. We use the two-round
proposed in [17] to construct rBridge due to its simplicity
of implementation.

m
1

(
(

)
)

non-interactive version in the random oracle model via Fiat-
Shamir heuristic [12]. We follow the notation introduced by
Camenisch and Stadler [9], e.g., NIPK{(x) : y = gx} de-
notes a “non-interactive zero-knowledge proof of knowledge
of integer x, s.t., y = gx, and g is public and x is secret.”.
Our main use of zero-knowledge proofs is to prove
knowledge of commitments and possession of signatures.
For instance, U can construct the following proof:

 ((cid:8), C(cid:8), O(cid:8), σ(cid:8)) :

(cid:8) > (cid:8)θ∧
(C(cid:8), O(cid:8)) = CMT((cid:8))∧
Verify(P KD, σ(cid:8), C(cid:8)) = Accept



π = NIPK

To make it hard for corrupt users to collaboratively enu-
merate bridges, we let D (i.e., the sender) randomly shufﬂe
the list of available bridges (i.e., the secrets) before running
the OT protocol with each user (i.e., the chooser), so that
the user will randomly “choose” which bridge to get. Be-
cause of the randomized OT, it is possible that a user gets
a bridge that is already assigned to him even though the
chance is very small. We show how to deal with duplicate
bridges in Appendix A.

5.2.2 Commitment

A commitment scheme enables a party to create the digital
equivalent of an envelope for a secret. It supports two im-
portant properties: hiding protects the secrecy of the com-
mitted message, and binding ensures it can only be opened
to the committed message. Pedersen commitments [18] are
information-theoretically hiding and binding under the dis-
crete logarithm assumption. We use (C, O) = CMT(M ) to
denote a Pedersen commitment to a secret M, where C is
the commitment and O is the opening to the commitment.
In rBridge, we use commitments to conceal the content
on a user’s credential. For instance, to hide the amount
of credits (cid:8), U can compute a commitment of (cid:8),
i.e.,
(C(cid:8), O(cid:8)) = CMT((cid:8)), and put C(cid:8) in his credential. To
prevent U from manipulating his credential (e.g., increas-
ing (cid:8)), we let D sign C(cid:8) using his private key SKD, i.e.,
σ(cid:8) = Sign(SKD, C(cid:8)) and tag the signature σ(cid:8) to the cre-
dential, and U needs to prove to D that both the commit-
ment and the signature are valid. To prevent D from linking
U’s transactions based on the values of commitments and
signatures, we need another cryptographic primitive—zero-
knowledge proof.

5.2.3 Zero-Knowledge Proof

In a zero-knowledge proof scheme, a prover convinces
a veriﬁer that some statement is true while the veriﬁer
learns nothing except the validity of the statement. A zero-
knowledge proof can be converted into a corresponding

to prove that his credit balance (cid:8) is above the threshold (cid:8)θ
and is not tampered (i.e., correctly signed), without reveal-
ing the credit balance (cid:8), the commitment C(cid:8), the opening
O(cid:8), or the signature σ(cid:8) (where P KD is the public key of D,
and Verify is the function to verify the signature σ(cid:8)). In our
construction, we employ the k-TAA blind signature scheme
proposed by Au et al. [8] because of its compatibility with
zero-knowledge proofs.

5.3 Scheme

We now present the rBridge scheme with privacy preser-
vation. We refer interested readers to Appendix B for de-
tailed cryptographic constructions of the scheme.

5.3.1 Anonymous Credential

A key concept in rBridge is the anonymous credential,
which anonymously records the user’s bridges and reputa-
tion and allows the user to anonymously authenticate him-
self to D. Each part of the credential is signed by D individ-
ually, so that they can be veriﬁed and updated separately. In
particular, an anonymous credential contains the following
information:

i=1

x∥{(cid:8), C(cid:8), O(cid:8), σ(cid:8)}∥{ω, Cω, Oω, σω}∥{Bi, τi, ϕi, Ci, Oi, σi}k
where x is the secret key that is selected by U when reg-
istering the credential, σ♣ is the signature on C♣, and ω
denotes the latest time when the user requests an invitation
ticket (ω is used to prevent corrupt users from repeatedly
requesting tickets; we discuss this later). We note that all
the information in the credential must be kept secret from
D.

To prevent a corrupt user from replacing some part
of his credential with that of others’ credentials (e.g., a
higher (cid:8) from a well-behaving colluding user), we use x
to link different parts of the credential by including x in
each of their commitments. To be speciﬁc, (C(cid:8), O(cid:8)) =
CMT((cid:8), x), (Cω, Oω) = CMT(ω, x), and (Ci, Oi) =

CMT(Bi, τi, ϕi, x). To get a credential, a new user runs
the following registration protocol.

5.3.2 Registration

π1 = NIPK

∗

∗

m
1

)

(

, HMACscrtD(r

)}, where r

A new user U ﬁrst presents an invitation ticket to D. An
invitation ticket is an one-time token formed as tk =
{r
∗ is a random number and
scrtD is a secret only known to D; the ticket is veriﬁable to
D, and cannot be forged by anyone else. Then U runs
-
OT with D to get k initial bridges B1,··· , Bk
5. After that,
U randomly picks a secret key x and performs the follow-
ing computations: set (cid:8) = 0 and compute (C(cid:8), O(cid:8)) =
CMT((cid:8), x); set ω = Tcur (recall that Tcur denotes the
current time) and compute (Cω, Oω) = CMT(ω, x); for
each i ∈ [1, k], set τi = Tcur, ϕi = 0, and compute
(Ci, Oi) = CMT(Bi, τi, ϕi, x). To prevent multiple collud-
ing users from using the same x to share some parts of their
credentials, U is required to provide an indicator of his se-
lected x, formed as κx = OWF(x), to prove that x has not
been used by other users while hiding x from D. (wherein
OWF(·) is simply a discrete-log based one-way function.)
Note that since D does not know the bridges received by
U (i.e., {Bi}k
i=1), U could try to put other bridges on his cre-
∗
i in CMT(Bi, τi, ϕi, x), so
dential, i.e., replacing Bi with B
that he can block all of {Bi}k
i=1 instantly without worry-
ing about potential loss of credits. To prevent this attack,
D needs to verify that the bridges to be written in the cre-
dential are actually the bridges that U received in OT. To
achieve this, we let D (before running OT) generate a pair of
one-time public/private keys (denoted by P K o
D ), give
D , SK o
D to sign each available bridge Bj, and
D to U, use SK o
P K o
}k
j to Bj. After OT, U gets {Bi∥σo
tag the signature σo
i=1,
and he needs to prove the possession of a valid signature σo
i
on Bi. Intuitively, U is no longer able to replace Bi with any
∗
i ) that is not one of the k received bridges,
other bridge (B
∗
because he does not have a signature on B
i . In addition,
we let U provide D a random nonce nonj for each available
bridge Bj; nonj is included in the computation of σo
j to pre-
vent D from later ﬁnding out which bridges are retrieved by
U using these signatures (refer to Appendix B for details).

i

To get the credential, U constructs the following proof:

5U needs to run k rounds of

m
1
is possible to invoke one round of
higher when k is much smaller than m (typically k = 3).

-OT to get k bridges. While it
-OT, but its complexity is
m
k

)

(

)
(



∧
(x, (cid:8), O(cid:8), ω, Oω,{Bi, τi, ϕi, σo

k

i=1) :
i=1 ((Ci, Oi) = CMT(Bi, τi, ϕi, x)∧
i , Bi) = Accept∧

Verify(P K o
τi = Tcur ∧ ϕi = 0)∧

D , σo

i , Oi}k

(C(cid:8), O(cid:8)) = CMT((cid:8), x)∧
κx = OWF(x)∧
(cid:8) = 0∧
(Cω, Oω) = CMT(ω, x)∧
ω = Tcur



i=1

∥π1 to D.

and sends κx∥C(cid:8)∥Cω∥{Ci}k
After verifying the validity of π1 and the freshness of κx,
D signs C(cid:8), Cω, and Ci (1 ≤ i ≤ k), respectively, and sends
the signatures σ(cid:8)∥σω∥{σi}k
i=1 to U. Finally, D adds κx to
the list of indicators of used secret keys (denoted by elistx)
to prevent other users from re-using it.

Note that in the privacy-preserving scheme, it is infeasi-
ble for D to count the number of users who ever connected
to each bridge; instead, we let each bridge notify D once the
number of users that ever connect to it exceeds the thresh-
old g, and then D will exclude the bridge from the list when
running OT. (The bridge could let each new user register
himself upon the ﬁrst connection, e.g., by setting up a pass-
word [4], in order to count the ever connected users.)

5.3.3 Updating Credit Balance

U can update his credit balance (cid:8) with recently earned cred-
its from time to time. Suppose the credits are from Bu. The
new credit balance is calculated as ~(cid:8) = (cid:8)+ ~ϕu−ϕu, where
~ϕu = Credit(Tcur − τu). U needs to show that Bu is not
blocked. To do so, we let D compute bj = OWF( (cid:22)Bj) for
each of the blocked bridges { (cid:22)Bj} (cid:22)m
j=1 (where (cid:22)m is the total
number of blocked bridges) and publish {bj} (cid:22)m
j=1; U needs
to prove that OWF(Bu) is not equal to any of {bj} (cid:22)m
j=1.

D must record expired credentials to prevent re-use of old
credentials (e.g., those with more credits). For this, we let
U provide an indicator of (cid:8) to show that (cid:8) is up-to-date.
Note that we cannot use κ(cid:8) = OWF(σ(cid:8)) as the indicator,
since D could try all of the signatures he has generated to
ﬁnd a match between κ(cid:8) and σ(cid:8) to link U’s transactions.
To address this, we craft a special indicator function κ(cid:8) =
Indic(σ(cid:8)) based on the feature of k-TAA blind signature [8]
(Essentially, this indicator function ﬁrst converts σ(cid:8) into
′
(cid:8) using a random factor and then applies an
another form σ
′
(cid:8) to get κ(cid:8). See Appendix B for more
one-way function to σ
details.)

In particular, U constructs the following proof:

5.3.4 Getting a New Bridge

5.3.5

Inviting New Users



π2 = NIPK

(x, (cid:8), C(cid:8), O(cid:8), σ(cid:8), Bu, τu, ϕu, Cu, Ou, σu,
~ϕu, ~Ou, ~(cid:8), ~O(cid:8)) :

(cid:22)m

∧
j=1 (bj ̸= OWF(Bu))∧
(Cu, Ou) = CMT(Bu, τu, ϕu, x)∧
Verify(P KD, σu, Cu) = Accept∧
(C(cid:8), O(cid:8)) = CMT((cid:8), x)∧
Verify(P KD, σ(cid:8), C(cid:8)) = Accept∧
κ(cid:8) = Indic(σ(cid:8))∧
~ϕu = Credit(Tcur − τu)∧
~(cid:8) = (cid:8) + ~ϕu − ϕu∧
( ~Cu, ~Ou) = CMT(Bu, τu, ~ϕu, x)∧
( ~C(cid:8), ~O(cid:8)) = CMT( ~(cid:8), x)∧



U builds a Tor circuit (using one of his bridges as the entry
relay) to send κ(cid:8)∥ ~C(cid:8)∥ ~Cu∥π2 to D.

D veriﬁes π2 and checks that κ(cid:8) is not on the list
of seen indicators (denoted by elist(cid:8)); then, D signs ~C(cid:8)
and ~Cu, sends the signatures ~σ(cid:8) and ~σu to U, and adds
κ(cid:8) to elist(cid:8).
Finally, U updates his credential with
~(cid:8), ~C(cid:8), ~O(cid:8), ~σ(cid:8), ~ϕu, ~Cu, ~Ou, and ~σu.

To get a new bridge, U ﬁrst needs to prove that one of his
bridges (say Bb) on his credential has been blocked and
−. Since a user usu-
his credit balance is higher than ϕ
ally requests a new bridge right after one of his bridges got
blocked which allows D to ﬁgure out what Bb is by check-
ing which bridge was recently blocked, we do not intend to
hide Bb from U. We note that revealing Bb will not degrade
the anonymity, as long as D is unable to link the transaction
of replacing Bb with other transactions of U.

U ﬁrst sends Bb to D through a Tor circuit. After verifying
Bb is blocked, D replies with βb (i.e., the time when Bb got
blocked). The new credit balance of U is ~(cid:8) = (cid:8) + ( ~ϕb −
−, where ~ϕb = Credit(βb − τb), by considering
ϕb) − ϕ
the credits earned from Bb and the cost for getting a new
bridge. U constructs the following proof:



(x, (cid:8), C(cid:8), O(cid:8), σ(cid:8), τb, ϕb, Cb, Ob, σb,
~ϕb, ~(cid:8), ~O(cid:8)) :
(Cb, Ob) = CMT(Bb, τb, ϕb, x)∧
Verify(P KD, σb, Cb) = Accept∧
κb = Indic(σb)∧
(C(cid:8), O(cid:8)) = CMT((cid:8), x)∧
Verify(P KD, σ(cid:8), C(cid:8)) = Accept∧
κ(cid:8) = Indic(σ(cid:8))∧
~ϕb = Credit(βb − τb)∧
~(cid:8) = (cid:8) + ~ϕb − ϕb − ϕ
~(cid:8) > 0∧
( ~C(cid:8), ~O(cid:8)) = CMT( ~(cid:8), x)

−∧



π3 = NIPK

and sends κ(cid:8)∥κb∥ ~C(cid:8)∥π3 to D. Note that we use κb to make
sure each blocked bridge can be used only once to request a
new bridge.





)

(

After verifying κ(cid:8) /∈ elist(cid:8), κb /∈ elistBb, and π3
(where elistBb denotes the list of used indicators of Bb), D
adds κ(cid:8) to elist(cid:8) and κb to elistBb. Similar to the registra-
-OT with D to obtain a new bridge ~Bb with
tion, U runs
b . Then, U sets ~τb = Tcur and ~ϕb = 0,
a tagged signature ~σo
computes ( ~Cb, ~Ob) = CMT( ~Bb, ~τb, ~ϕb, x), constructs the
following proof:

m
1

π4 = NIPK

(x, ~(cid:8), ~O(cid:8), ~Bb, ~τb, ~ϕb, ~σo
b , ~Ob) :
( ~C(cid:8), ~O(cid:8)) = CMT( ~(cid:8), x)∧
~τb = Tcur ∧ ~ϕb = 0∧
( ~Cb, ~Ob) = CMT( ~Bb, ~τb, ~ϕb, x)∧
b , ~Bb) = Accept
Verify(P K o

D , ~σo

and sends ~Cb∥π4 to D.
D veriﬁes π4,

signs ~C(cid:8) and ~Cb, and sends ~σ(cid:8)
Finally, U updates his credential with

and ~σb to U.
~(cid:8), ~C(cid:8), ~O(cid:8), ~σ(cid:8), ~Bb, ~τb, ~ϕb, ~Cb, ~Ob, and ~σb.

U can request D for an invitation ticket as long as his credit
balance is higher than (cid:8)θ. D grants the request with certain
probability. Recall that ω in the credential represents the
latest time when U requested an invitation ticket. To prevent
a corrupt user from repeatedly requesting invitation tickets
to increase his chance of getting one, we let each requesting
user prove that his last time requesting a ticket is at least ωθ
days ago, i.e., Tcur − ω > ωθ. In particular, to request a
ticket, U constructs the proof:

π5 = NIPK

(x, (cid:8), C(cid:8), O(cid:8), σ(cid:8), ω, Cω, Oω, σω, ~ω,
~Oω, ~O(cid:8)) :

(C(cid:8), O(cid:8)) = CMT((cid:8), x)∧
Verify(P KD, σ(cid:8), C(cid:8)) = Accept∧
κ(cid:8) = Indic(σ(cid:8))∧
(Cω, Oω) = CMT(ω, x)∧
Verify(P KD, σω, Cω) = Accept∧
κω = Indic(σω)∧
(cid:8) > (cid:8)θ∧
Tcur − ω > ωθ∧
~ω = Tcur∧
( ~Cω, ~Oω) = CMT(~ω, x)∧
( ~C(cid:8), ~O(cid:8)) = CMT((cid:8), x)

and sends κ(cid:8)∥κω∥ ~C(cid:8)∥ ~Cω∥π5 to D through a Tor circuit.
After verifying κ(cid:8) /∈ elist(cid:8), κω /∈ elistω and π5, D
signs ~C(cid:8) and ~Cω, sends ~σ(cid:8) and ~σω to U, and adds κ(cid:8) to
elist(cid:8) and κω to elistω. Then, D ﬂips a coin to decide
whether to grant the request: if yes, D generates a ticket for
U; otherwise, U needs to wait at least ωθ days to try again.





Table 1: Performance (averaged over 100 runs)

6 Security Analysis

Operation

Registration

Updating credit balance
Getting a new bridge
Inviting new users

Comp. (s)
U
D

5.15
0.51
5.35
0.27

17.44
0.47
17.62
0.16

Comm. (KB)

388.1
34.7
340.1
2.0

5.4 Performance Evaluation

We implemented rBridge using Paring-Based Crytogra-
phy (PBC) Library6 and GNU Multiple Precision library7
in C++, and used OpenSSL for hash related routines. The
credential system, built upon k-TAA signature, was imple-
mented with Type-A curves, which is deﬁned in the PBC
Library in the form of E : y2 = x3 + x over the ﬁeld Fq
for some prime q. Bilinear groups G1 and G2, both formed
by points over E(Fq), are of order p for some prime p, such
that p is a factor of q + 1. Using the default setting, p and
q are 160-bit and 512-bit in length respectively. We imple-
-OT protocol in [17] using G1 as the under-
mented the
lying group. We consider there are 1000 available bridges
and 100 blocked bridges, and set the size of each bridge
descriptor the same as that of the current bridge descriptor
(208 bits including 32-bit IP, 16-bit port, and 160-bit ﬁnger-
print).

(

)

m
1

We measured the computational times for U and D on
a Dell Precision T3500 workstation with a quad-core 2.67
GHz Intel Xeon W3550 CPU and 12 GB RAM, running
Ubuntu 10.04. Table 1 shows that it takes only less than 0.5
seconds for U and D to update credit balance or process an
invitation ticket request. It takes longer to perform the ini-
tial registration or request a new bridge, which are about 5
seconds and 17 seconds for U and D, respectively. The ma-
jority of computational times are spent on the retrieval of
new bridges. We note that these operations are quite infre-
quent; each user only needs to register once, and according
to our simulation results, the averaged time interval to re-
quest new bridges is 133 days. Hence, we believe these
occasional computations can be handled by both U and D
with fairly strong computational power. We also measured
the communication costs. The operations of registration and
getting a new bridge incur 388 KB and 340 KB communi-
cation overheads respectively, and the data transmission for
any other operation is less than 35 KB. In comparison, with
2000 Tor relays, each Tor client needs to download a 120
KB “network status document” every 3 hours, and 1.25 MB
of relay “descriptors” spread over 18 hours [16].

6http://crypto.stanford.edu/pbc/
7http://gmplib.org/

In this section, we discuss the potential attacks that are

not covered by the evaluation (Section 4.3).

6.1 Curious Bridge Distributor in Privacy Preser(cid:173)

vation

One of the major security requirements in the privacy-
preserving scheme is to ensure different transactions of a
particular user are unlinkable. A typical way to link a user’s
transactions is based on the credential content. For example,
suppose in the x-th transaction U received a new bridge BX
and updated his credential with some signed information of
BX (e.g., BX, τX, and ϕX); later, when BX gets blocked
and U requests a new bridge to replace it in the y-th trans-
action, U needs to use the signed information to prove that,
e.g., BX was assigned to him and a certain amount of cred-
its should be earned from BX; however, such information
can be utilized by D to link the two transactions. (Similar
attacks are applicable in the cases when U updates his credit
balance or requests invitation tickets.) To ensure the unlink-
ability of transactions, rBridge uses commitments and zero-
knowledge proofs to conceal the content of the credential,
so that D only knows the validity of the credential but learns
nothing about the content of the credential.

Although BX is revealed to D in the y-th transaction (i.e.,
after BX gets blocked), D is unable to link BX to U due
to the use of a Tor circuit; more importantly, since BX is
perfectly hidden in any other transactions, D cannot use BX
to link any two transactions of U.

Similarly, D can learn U’s identity (i.e., his IP address, but
nothing else) in the registration, since at that moment U has
no bridge to build a Tor circuit to hide his IP address (if he
does not use any other circumvention tool or ask an existing
user to perform the registration on his behalf). Nevertheless,
D is unable to learn which bridges are retrieved because of
U in OT; moreover, since U’s IP address will be hidden in
all the later transactions of U, D cannot use U’s IP address to
link his transactions.

6.2 Users’ Misbehaviors in Privacy Preservation

In the privacy-preserving scheme, a corrupt user could
try to manipulate the information on his credential, e.g., in-
creasing the credit balance. rBridge uses zero-knowledge
proofs with the help of blind signatures to verify the cor-
rectness of the credential without revealing any information
in the credential.

In addition, since D does not know what bridge is re-
trieved by U in OT, U could try to replace the received bridge
with another one when updating his credential so that he can
block the assigned bridge instantly without worrying about

potential loss of credits. To address this, we let D employ
one-time signatures to make sure the bridge written in the
credential is indeed the bridge that the user received in OT.
Furthermore, malicious users could try to re-use old cre-
dentials that have more credits or use a blocked bridge to
request more than one new bridges. To prevent these, we
let D record all the used credentials as well as the claimed
blocked bridges, and ask U to provide indicators to prove
that the presented credential or the claimed blocked bridge
has not been used before.

6.3 Sybil Attacks

The adversary could launch Sybil attacks by creating a
large number of Sybils in the population of potential bridge
users, so that the ratio f of compromised entities in the po-
tential users can be dramatically increased. However, we
note that deploying a large number of Sybils does not nec-
essarily lead to increase in corrupt users in the system. For
honest users, their invitation tickets are given to people they
know. While corrupt users give all their received tickets
to colluding entities, the number of malicious entities they
can invite is bottlenecked by the number of invitation tickets
they have received, rather than by the number of malicious
entities the adversary can create in the population of poten-
tial users.

Alternatively, the adversary could try to deploy Sybils di-
rectly in the system, which requires the adversary to provide
each Sybil with a valid credential; however, this is infea-
sible without knowing the bridge distributor’s private key.
We also note that it is infeasible to let corrupt users share
their credentials with the Sybils either, because the bridge
distributor recycles used credentials and the total number of
bridges that can be learnt by the adversary does not increase.

6.4 Blocking the Bridge Distributor

We suppose the IP address of the bridge distributor is
publicly known. Hence, the censor could simply block the
bridge distributor to either prevent new users from join-
ing the system or stop existing users from receiving new
bridges. For an existing user who has at least one unblocked
bridge, he can use the bridge to build a Tor circuit to access
the bridge distributor. For a user without any usable bridge
(e.g., a new user), he can use a high-latency but more robust
circumvention tool (e.g., Email based circumvention [5]) to
communicate with the bridge distributor to get the initial
bridges or a replacement bridge. Besides, a new user could
ask his inviter (i.e., the existing user who gave him the in-
vitation ticket) to perform the initial bootstrapping on his
behalf to get the initial bridges.

6.5 Well Behaving of Corrupt Users

In order to increase the number of corrupt users in the
system, the adversary could let the corrupt users behave
legitimately (i.e., keeping their bridges alive) for a certain
period of time to accumulate credits in order to receive
invitation tickets. However, we note that since the invi-
tation tickets are randomly distributed to qualiﬁed users,
corrupt users may not necessarily receive invitation tickets
even if they have saved up sufﬁcient credits. In addition,
keeping bridges alive also allows honest users to accumu-
late enough credits to become qualiﬁed to receive invitation
tickets; therefore, overall, the chance of receiving invita-
tion tickets by corrupt users is no better than that of honest
users. Our simulation results in Section 4.3 (where the cor-
rupt users do not block bridges until the 225-th day) show
that this attack strategy cannot help the adversary increase
of ratio of corrupt users in the system. In addition, rBridge
does not allow users to transfer credits to others, and hence
it is infeasible to deploy a few well-behaving corrupt users
to help other corrupt users by sharing their credits.

7 Conclusion

We proposed rBridge, a user reputation system for Tor
bridge distribution. rBridge addresses two key challenges to
bridge distribution: protecting bridges from being blocked
by corrupt users, and preserving bridge assignment infor-
mation of each user. rBridge makes use of users’ reputation
to punish blockers and limit them from repeatedly blocking
bridges, and adopts an introduction-based mechanism to in-
vite new users while resisting Sybil attacks. Our simulation
results show that rBridge is able to provide much stronger
protection for bridges than any existing scheme. In addition,
we addressed privacy preservation in rBridge by conceal-
ing users’ bridge assignment information. Such information
can be explored to degrade users’ anonymity in anonymous
communication. We designed a novel privacy-preserving
reputation system for bridge distribution using several cryp-
tographic primitives. To our best knowledge, rBridge is the
ﬁrst scheme that is able to perfectly preserve users’ pri-
vacy in bridge distribution. We implemented a prototype
of rBridge, and the experiments showed that rBridge has
reasonable performance.

8 Acknowledgement

We thank anonymous reviewers for invaluable comments
on the paper. Qiyan Wang was supported in part by NSF
CNS 09-53655 and Zi Lin was supported in part by NSF
CNS 09-17154.

References

[1] https://metrics.torproject.org/network.

html#networksize.

[2] Ten ways

to

discover

Tor

bridges.

https:

//blog.torproject.org/blog/research-
problems-ten-ways-discover-tor-bridges.

[3] https://www.torproject.org/projects/

obfsproxy.html.en.

[4] Proposal 190: Password-based Bridge Client Authorization.
https://lists.torproject.org/pipermail/
tor-dev/2011-November/003042.html.

[5] Feed Over Email

(F.O.E). http://code.google.

com/p/foe-project/.

[6] Research problem: Five ways to test bridge reachability,
Dec, 1, 2011. https://blog.torproject.org/
blog/research-problem-five-ways-test-
bridge-reachability.

[7] M. H. Au, A. Kapadia, and W. Susilo. Blacr: Ttp-free black-
listable anonymous credentials with reputation. In NDSS’12,
2012.

[8] M. H. Au, W. Susilo, and Y. Mu. Constant-size dynamic
k-taa. SCN, Lecture Notes in Computer Science, 4116:111–
125, 2006.

[9] J. Camenisch and M. Stadler. Proof system for general state-
In Technical Report TR

ments about discrete logarithms.
260, Institute for Theoretical Computer Science, 1997.

[10] R. Dingledine, N. Mathewson, and P. Syverson. Tor: The
second-generation onion router. In USENIX Security Sym-
posium, August 2004.

[11] N. Feamster, M. Balazinska, W. Wang, H. Balakrishnan,
and D. Karger. Thwarting web censorship with untrusted
In Privacy Enhancing Technologies
messenger discovery.
(PETS), 2003.

[12] A. Fiat and A. Shamir. How to prove yourself: Practi-
In

cal solutions to identiﬁcation and signature problems.
CRYPTO, 1986.

How

internet

censor-
2011.

[13] J.

Jacob.
works

in

china,

ship
http://www.ibtimes.com/articles/113590/20110217/china-
internet-censorship
-great-ﬁrewall-us-hillary-clinton-communist.htm .

Feb.

17,

[14] M. Mahdian. Fighting censorship with algorithms. In Pro-

ceedings of FUN’10, 2010.

[15] D. McCoy, J. A. Morales, and K. Levchenko. Proximax:
A measurement based system for proxies dissemination. In
FC’11, Feb 2011.

[16] J. McLachlan, A. Tran, N. Hopper, and Y. Kim. Scalable

onion routing with torsk. In ACM CCS’09, 2009.

[17] M. Naor and B. Pinkas. Efﬁcient oblivious transfer proto-

cols. In SODA’01, 2001.

[18] T. Pedersen. Non-interactive and information-theoretic se-
In Advances in Cryptology,

cure veriﬁable secret sharing.
1992.

[19] R. Smits, D. Jain, S. Pidcock, I. Goldberg, and U. Hengart-
ner. Bridgespa: Improving tor bridges with single packet
authorization. In WPES’11, 2011.

[20] Y. Sovran, A. Libonati, and J. Li. Pass it on: Social networks

stymie censors. In IPTPS’08, 2008.

(

)

[21] P. P. Tsang, M. H. Au, A. Kapadia, and S. W. Smith.
Blacklistable anonymous credentials: Blocking misbehav-
ing users without ttps. In CCS’07, 2007.

[22] P. P. Tsang, M. H. Au, A. Kapadia, and S. W. Smith. Perea:
Towards practical ttp-free revocation in anonymous authen-
tication. In CCS’08, 2008.

[23] J. Zittrain and B. Edelman.

Internet Filtering in
IEEE Internet Computing, 7(2):70–77, 2003.

China.
http://csdl.computer.org/comp/mags/ic/
2003/02/w2070abs.htm.

A Dealing with Duplicate Bridges

m
1

)

(

Suppose U receives a duplicate bridge Bd in the ran-
-OT, which is identical to one of his existing
domized
bridges Be. We allow U to get a new bridge (by running
-OT again) to replace Bd as long as he can prove that
m
1
he has valid signatures for both Bd and Be.

We note that, however, a sophisticated D may try to in-
fer U’s bridges by constructing the list of available bridges
∗), and see if later U
used in OT with a single bridge (say B
requests replacement of a duplicate bridge; if yes, D can be
∗ is one of U’s existing bridges. To prevent this,
sure that B
we let D compute (Cj, Oj) = CMT(Bj) for each available
-OT,
bridge Bj, and publish all the Cj’s; before running
U randomly picks Cp, Cq, p ̸= q, and asks D to prove that
Bp and Bq are different. D constructs the following proof:

)

m
1

 (Bp, Op, Bq, Oq) :

(Cp, Op) = CMT(Bp)∧
(Cq, Oq) = CMT(Bq)∧
Bp ̸= Bq

(


π6 = NIPK

(

)

Then, U runs
Bd from Cd.

-OT to get Od; using Od, U is able to open

If Bd is duplicate, U constructs the following proof:

m
1





π7 = NIPK

(x, Bd, τd, ϕd, Cd, Od, σd, Be, τe, ϕe,
Ce, Oe, σe) :
(Cd, Od) = CMT(Bd, τd, ϕd, x)∧
Verify(P KD, σd, Cd) = Accept∧
κd = Indic(σd)∧
(Ce, Oe) = CMT(Be, τe, ϕe, x)∧
Verify(P KD, σe, Ce) = Accept∧
Bd = Be

(

)

and sends κd∥π7 to D though an established Tor tunnel.
D veriﬁes κd /∈ elistBd and π7, runs

-OT to provide
a new bridge ~Bd, and adds κd to elistBd. After receiving
Bd, U constructs the proof:

m
1

π8 = NIPK

(x, Bd, τd, ϕd, Cd, Od, σd, ~Bd, ~τd, ~ϕd, ~Od) :

(Cd, Od) = CMT(Bd, τd, ϕd, x)∧
Verify(P KD, σd, Cd) = Accept∧
κd = Indic(σd)∧
~τd = Tcur ∧ ~ϕd = 0∧
( ~Cd, ~Od) = CMT( ~Bd, ~τd, ~ϕd, x)





is the opening to C♠). Then, U computes the proof:



∧

(x, (cid:8), s
r(1)
, r(2)
i
k
i=1

i

[

′
(cid:8), ω, s
, δ(1)
i
A(1)

ω,{Bi, τi, ϕi, eo
′
, δ(2)

}k
i=1) :
r
g
2

(1)
i

(2)
i

i

r
i = g
1

i , so

i , s

′
i,

∧
∧

−eo

i ^e(g2, y)r

, h)
]
i ^e(g3, h)Bi∧

∧

(1)
i

(2)
i

δ
g
2

(A(1)
i
(2)
^e(A
i

δ
i = g
1

)eo
,pko)

i

(1)

^e(g0,h) = ^e(A(2)
i ^e(g1, h)so
^e(g2, h)δ
τi = Tcur ∧ ϕi = 0∧
′
Ci = gs
2 gBi
4 gϕi
3 gτi
1 gx
i
′
3 ∧
C(cid:8) = gs
2 g(cid:8)
1 gx
(cid:8)
κx = zx∧
(cid:8) = 0∧
Cω = gs
ω = Tcur

3 ∧
2 gω

′
1 gx
!

5

(1)
i



and sends ~Cd∥π8 to D. Note that we include the commit-
ment and signature of the duplicate bridge Bd in π8 to pre-
vent U from giving this opportunity of receiving a replace-
ment bridge to another (colluding) user. Finally, D veriﬁes
π8, signs ~Cd, and sends ~σd to U.

B Construction Details

Let (G1, G2) be a bilinear group pair and Gp be a group
of order p where DDH is intractable with ^e : G1 × G2 →
Gp, s.t., ^e(P a, Qb) = ^e(P, Q)ab, for all P ∈ G1, Q ∈ G2,
a, b ∈ Zp.

π1 = NIPK

B.1 Key Generation

D chooses sk

Let g0, g1, g2, g3, g4, g5 be generators of G1, and h
∗
p as the
Let z de-
The public key is
In addition,
∗
p to generate invi-

be a generator of G2.
private key, and computes pk = hsk.
note a random element
(g0, g1, g2, g3, g4, g5, z, h, G1, G2, Gp, ^e, pk).
D chooses a random secret scrtD from Z
tation tickets.

R←− Z

in G1.

B.2 Registration

U ﬁrst picks a list of nonces y

computes Y

′
j = g

′
y
1 , constructs the following proof:
j

R←− Z

p, 1 ≤ j ≤ m,
∗

′
j

)
[



({y
∧
j}m
′
∥π0 to D.

m
j=1

π0 = NIPK

j=1

:
′
j = g

′
y
j
1

Y

] 

1

y

eo
j

′
j

j=1

′′
j

j , y

}m

and sends {Y
D veriﬁes π0, and then chooses a pair of one-time keys
sko R←− Z
∗
p, pko = hsko. For each available bridge
Bj, D randomly selects eo
j =

R←− Z
∗
p, computes Ao
′′
′′
1 YjgBj
+sko , and tags (Ao
j ) to Bj.
j , eo
j
3 )
j , y
After OT, U receives {Bi∥(Ao
i )}k
′′
i , eo
i , y
′
′′
i , and sets σo
i = y
i + y
i , eo

(g0g
i=1. For each
i ∈ [1, k], U computes so
i =
i ). To prove possession of these signatures, U
(Ao
i , so
gr(2)
picks r(1)
, r(2)
,
1
i gr(1)
A(2)
i . To get
i = Ao
the initial credential, U sets (cid:8) = 0 and ω = Tcur, picks
R←− Z
∗
′
′
p, and computes C(cid:8) = gs
3 , Cω =
(cid:8), s
s
ω
3 ; for each i ∈ [1, k], U sets τi = Tcur, ϕi = 0, picks
′
gs
1 gx
2 gω
!
R←− Z
∗
′
′
p, and computes Ci = gs
5 (where s
♠
s
i

∗
p, and computes A(1)

R←− Z
, δ(1)

i = gr(1)

i = r(2)

i = r(1)

′
1 gx
(cid:8)

i , δ(2)

′
1 gx
i

3 gτi

4 gϕi

2 gBi

i eo

i eo

2 g(cid:8)

2

2

i

i

i

i

i

1

i

i

i=1

′′
ω

, A(2)

, Ci}k

′′
1 Cω)
!

′′
1 C(cid:8))
(cid:8)

and sends κx∥C(cid:8)∥Cω∥{A(1)
∥π1 to D.
R←−
′′
After verifying π1 and κx, D picks e(cid:8), s
(cid:8), eω, s
∗
p, and computes A(cid:8) = (g0gs
e(cid:8)+sk , Aω =
Z
R←−
e! +sk . For each i ∈ [1, k], D picks ei, s
′′
(g0gs
i
′′
∗
p, and computes Ai = (g0gs
ei+sk . Then D sends
1 Ci)
Z
i
(cid:8))∥(Aω, eω, s
i )}k
ω)∥{(Ai, ei, s
′′
′′
′′
i=1 to U.
(A(cid:8), e(cid:8), s
′′
′
′′
′
ω, and si =
(cid:8), sω = s
U computes s(cid:8) = s
ω + s
(cid:8) + s
i , 1 ≤ i ≤ k, and sets σ(cid:8) = (A(cid:8), e(cid:8), s(cid:8)), σω =
′′
′
i + s
s
(Aω, eω, sω), and σi = (Ai, ei, si), 1 ≤ i ≤ k.

1

1

B.3 Updating Credit Balance

Suppose U wants to update his credit balance with the
credits earned from Bu. U needs to prove possession of
R←− Z
∗
σu and σ(cid:8). For that, U picks r(1)
p,
u = gr(1)
, δ(1)
and computes A(1)
u =
(cid:8) = gr(1)
(cid:8) = A(cid:8)gr(1)
r(1)
u eu, δ(2)
u = r(2)
u eu, A(1)
,
δ(1)
(cid:8) = r(1)
(cid:8) e(cid:8), δ(2)
(cid:8) = r(2)
(cid:8) e(cid:8). In addition, U needs to show
that Bu is not blocked by proving that bj ̸= zBu for each
bj = z (cid:22)Bj , where { (cid:22)Bj} (cid:22)m

u , r(1)
(cid:8) , r(2)
u , r(2)
u = Augr(1)
, A(2)
gr(2)

, A(2)

gr(2)

(cid:8)

1

2

2

1

2

2

(cid:8)

(cid:8)

(cid:8)

u

u

u

j=1 is the list of blocked bridges.
credential,

the

U calculates

~ϕu =
To update
Credit(Tcur − τu) and ~(cid:8) = (cid:8) + ~ϕu − ϕu; then, he picks
R←− Z
′
∗
′
′
~ϕu
p, and computes ~Cu = g~s
5 ,
1 gx
u, ~s
~s
u
(cid:8)
′
~C(cid:8) = g~s
2 g ~(cid:8)
3 . After that, U constructs the following
1 gx
(cid:8)
proof:

3 gτu
4 g

2 gBu

(cid:8) , δ(2)
(cid:8) ,
u , δ(2)
u , δ(1)
u ,

[

(cid:22)m
j=1

∧

]∧

(cid:8) , δ(1)
u , r(2)

′
(cid:8) , r(2)
(cid:8), r(1)
(x, (cid:8), C(cid:8), e(cid:8), s(cid:8), s
′
u, r(1)
Bu, τu, ϕu, Cu, eu, su, s
′
′
(cid:8), ~ϕu, ~s
~(cid:8), ~s
u) :
bj ̸= zBu
′
Cu = gs
4 gϕu
3 gτu
2 gBu
1 gx
u
∧
A(1)
u = gr
gr
2
1
∧
(1)
(A(1)
gδ
u )eu = gδ
u
2
1
−eu ^e(g2, y)r
(2)
^e(g0,h) = ^e(A(2)
u ,pk)
^e(A
u , h)
^e(g2, h)δ
^e(g3, h)Bu ^e(g4, h)τu ^e(g5, h)ϕu∧

u ^e(g1, h)su ^e(g2, h)x

∧

(2)
u

(1)
u

(2)
u

(1)

5

(1)
u



(1)

r
1

∨

∧

(1)
(cid:8)

(1)
(cid:8)

(2)
(cid:8)

(2)
(cid:8)

)

π2 = NIPK

∧
−e(cid:8) ^e(g2, y)r

′
3 ∧
C(cid:8) = gs
2 g(cid:8)
1 gx
(cid:8)
r
A(1)
g
(cid:8) = g
2
(1)
δ
δ
(A(1)
(cid:8) )e(cid:8) = g
g
(cid:8)
2
1
(2)
^e(A
(cid:8) ,pk)
^e(g0,h) = ^e(A(2)
(cid:8) , h)
(cid:8) ^e(g1, h)s(cid:8) ^e(g2, h)x^e(g3, h)(cid:8)∧
^e(g2, h)δ
[(
κ(cid:8) = zs(cid:8)∧
tu = Tcur − τu∧
(
)
tu < T0 ∧ ~ϕu = 0
(
tu ≥ T0 ∧ tu ≤ T1 ∧ ~ϕu = ρ(t − T0)
tu > T1 ∧ ~ϕu = ρ(T1 − T0)
~(cid:8) = (cid:8) + ~ϕu − ϕu∧
′
~Cu = g ~s
2 gBu
3 gτu
1 gx
4 g
u
′
3 ∧
2 g ~(cid:8)
~C(cid:8) = g ~s
1 gx
(cid:8)
u ∥ ~C(cid:8)∥ ~Cu∥π2 to D.
u ∥A(2)
and sends κ(cid:8)∥A(1)
∥A(1)
∥A(2)
R←− Z
∗
′′
′′
p, and
D veriﬁes π2 and κ(cid:8), picks ~e(cid:8), ~s
(cid:8), ~eu, ~s
u
′′
′′
computes ~A(cid:8) = (g0g~s
~e(cid:8)+sk , ~Au = (g0g~s
~C(cid:8))
~Cu)
~eu+sk .
(cid:8)
u
(cid:8))∥( ~Au, ~eu, ~s
′′
′′
1
1
Then D sends ( ~A(cid:8), ~e(cid:8), ~s
u) to U.
′′
′
′′
′
(cid:8), ~su = ~s
U computes ~s(cid:8) = ~s
u, sets ~σ(cid:8) =
(cid:8) + ~s
u + ~s
( ~A(cid:8), ~e(cid:8), ~s(cid:8)), ~σu = ( ~Au, ~eu, ~su), and updates his credential
′
′
(cid:8), ~σ(cid:8), ~ϕu, ~Cu, ~s
with ~(cid:8), ~C(cid:8), ~s
u, and ~σu.

)]

~ϕu
5

∨

∧

∧

(cid:8)

(cid:8)

1

1

B.3.1 Getting a New Bridge

Suppose U wants to replace a blocked bridge Bb with a new
bridge. U sends Bb to D through an established Tor tunnel,
and D veriﬁes Bb is indeed blocked and then replies with the
blocking time βb of Bb.

, r(1)

gr(2)

, r(2)
(cid:8) , r(2)
b
b
b = Abgr(1)
, A(2)
gr(2)

R←− Z
b = r(1)
, δ(1)
(cid:8) = A(cid:8)gr(1)
, δ(1)

U needs to prove possession of the signatures σb and σ(cid:8).
∗
For this, U picks r(1)
p, and computes
b = gr(1)
A(1)
b =
1
r(2)
u eb, A(1)
(cid:8) e(cid:8),
δ(2)
(cid:8) = r(2)
U can earn ~ϕb = Credit(βb − τb) credits in total from
Bb, and the resulting credit balance after paying for the new
R←− Z
bridge is ~(cid:8) = (cid:8) + ~ϕb − ϕb − ϕ
∗
p,

(cid:8) = gr(1)
(cid:8) e(cid:8).

b eb, δ(2)
(cid:8) = r(1)

−. U picks ~s
′
(cid:8)

, A(2)

(cid:8)

2

1

2

2

2

(cid:8)

(cid:8)

(cid:8)

b

b

b



′
computes ~C(cid:8) = g~s
1 gx
(cid:8)

2 g ~(cid:8)

3 , constructs the following proof:



π3 = NIPK

b

b

b

b

b

5

(1)

r
1

∧

(1)
(cid:8)

(1)
b

(2)
b

(1)
b

(2)
b

, h)

2 gBb

(cid:8) , δ(1)
, δ(2)

−eb ^e(g2, y)r

(cid:8) , δ(2)
(cid:8) , τb,
′
, ~(cid:8), ~s
(cid:8)) :

′
(cid:8) , r(2)
(cid:8), r(1)
(x, (cid:8), C(cid:8), e(cid:8), s(cid:8), s
′
, δ(1)
, r(2)
b, r(1)
ϕb, Cb, eb, sb, s
′
∧
Cb = gs
3 gτb
4 gϕb
1 gx
b
∧
r
A(1)
b = g
g
2
(1)
δ
δ
(A(1)
b )eb = g
g
b
1
2
(2)
^e(A
b ,pk)
^e(g0,h) = ^e(A(2)
^e(g2, h)δ
b ^e(g1, h)sb ^e(g2, h)x
^e(g3, h)Bb ^e(g4, h)τb ^e(g5, h)ϕb∧
κb = zsb∧
′
3 ∧
C(cid:8) = gs
2 g(cid:8)
1 gx
(cid:8)
r
A(1)
(cid:8) = g
g
2
(1)
δ
δ
(A(1)
(cid:8) )e(cid:8) = g
g
(cid:8)
2
1
(2)
^e(A
(cid:8) ,pk)
^e(g0,h) = ^e(A(2)
(cid:8) , h)
(cid:8) ^e(g1, h)s(cid:8) ^e(g2, h)x^e(g3, h)(cid:8)∧
^e(g2, h)δ
[(
κ(cid:8) = zs(cid:8)∧
tb = βb − τb∧
(
)
tb < T0 ∧ ~ϕb = 0
(
)]
tb ≥ T0 ∧ tb ≤ T1 ∧ ~ϕb = ρ(tb − T0)
tb > T1 ∧ ~ϕb = ρ(T1 − T0)
~(cid:8) = (cid:8) + ~ϕb − ϕb − ϕ
~(cid:8) > 0
′
~C(cid:8) = g ~s
1 gx
(cid:8)

∧
−e(cid:8) ^e(g2, y)r

3 ∧
2 g ~(cid:8)

−∧

)

(2)
(cid:8)

(2)
(cid:8)

(1)
(cid:8)

∧

∨

∧

r
1

(1)

∨



and sends κ(cid:8)∥κb∥A(1)

(cid:8)

∥A(2)

(cid:8)

∥A(1)

b

∥A(2)

b

∥ ~C(cid:8)∥π3 to D.

D veriﬁes π3, κb, and κ(cid:8). Similar to the OT in the reg-
istration, U sends D a list of nonces, and D chooses a pair of
one-time keys to sign each available bridge using the cor-
responding nonce. Running OT, U obtains ~Bb∥~σo
b , where
b = ( ~Ao
~σo

b).
b, ~so

b, ~eo

R←− Z

To update the credential with the new bridge ~Bb, U sets
∗
p, and computes
b , U picks
, ~A(2)
b =
b. Then, U constructs

′
~τb = Tcur and ~ϕb = 0, picks ~s
b
′
~Cb = g~s
1 gx
2 g
b
R←− Z
~r(1)
, ~r(2)
b
bg ~r(1)
b = ~r(1)
, ~δ(1)
~Ao
the following proof:

~Bb
3 g ~τb
4 g
∗
b = g ~r(1)
p, and computes ~A(1)
b = ~r(2)
b ~eo

~ϕb
5 . To prove possession of ~σo
g ~r(2)

b, ~δ(2)

b ~eo

1

2

2

b

b

b

b



b

b

3

,

∧

∧

~ϕb
5

b, ~so

2 g ~(cid:8)

, ~r(2)

(2)
~δ
b
2

(1)
~r
b
1

π4 = NIPK

′
b, ~r(1)
b, ~s

′
(cid:8), ~Bb, ~τb, ~ϕb, ~eo
(x, ~(cid:8), ~s
, ~δ(2)
~δ(1)
b ) :
′
b
~C(cid:8) = g ~s
1 gx
(cid:8)
~τb = Tcur ∧ ~ϕb = 0∧
′
~Bb
~Cb = g ~s
3 g ~τb
1 gx
4 g
2 g
b
∧
(2)
~r
~A(1)
g
b = g
b
2
(1)
~δ
( ~A(1)
b )~eb = g
g
b
1
(2)
^e( ~A
b ,pk)
^e(g0,h) = ^e( ~A(2)
^e(g2, h)~δ
∥ ~Cb∥π4 to D.
R←− Z
′′
′′
D veriﬁes π4, picks ~e(cid:8), ~s
(cid:8), ~eb, ~s
b
′′
′′
~A(cid:8) = (g0g~s
~e(cid:8)+sk , ~Ab = (g0g~s
~C(cid:8))
~Cb)
(cid:8)
b
(cid:8))∥( ~Ab, ~eb, ~s
′′
′′
1
1
sends ( ~A(cid:8), ~e(cid:8), ~s
b ) to U.
′′
′
′′
′
b , sets ~σ(cid:8) =
(cid:8), ~sb = ~s
U computes ~s(cid:8) = ~s
b + ~s
(cid:8) + ~s
( ~A(cid:8), ~e(cid:8), ~s(cid:8)), ~σb = ( ~Ab, ~eb, ~sb), and updates his credential
′
′
(cid:8), ~σ(cid:8), ~ϕb, ~Cb, ~s
with ~(cid:8), ~C(cid:8), ~s
b, and ~σb.

∗
p, and computes
~eb+sk . Then D

, h)
b ^e(g1, h)~so

and sends ~A(1)

b ^e(g3, h) ~Bb

∥ ~A(2)

b ^e(g2, h)~r

−~eo

(1)
b

(1)

b

b

b

1

1

B.4

Inviting New Users

A user U who requests an invitation ticket needs to prove
that his credit balance is higher than the threshold, i.e., (cid:8) >
(cid:8)θ, and the last time he applied for an invitation ticket is at
least ωθ days ago, i.e., Tcur − ω ≥ ωθ. In addition, U needs
to prove possession of the signatures σ(cid:8) and σω. Hence, U
constructs the following proof:





π5 = NIPK

(cid:8) , δ(2)
(cid:8) , ω,
′
′
ω , ~ω, ~s
ω, ~s
(cid:8)) :

(1)

(cid:8) ^e(g1, h)s(cid:8) ^e(g2, h)x^e(g3, h)(cid:8)∧

′
(cid:8), r(1)

r
1

∧

(1)
(cid:8)

(2)
(cid:8)

(2)
(cid:8)

(cid:8) , r(2)
ω , δ(1)

(cid:8) , δ(1)
ω , δ(2)

∧
−e(cid:8) ^e(g2, y)r

(x, (cid:8), C(cid:8), e(cid:8), s(cid:8), s
′
ω, r(1)
ω , r(2)
Cω, eω, sω, s
′
3 ∧
C(cid:8) = gs
2 g(cid:8)
1 gx
(cid:8)
r
A(1)
(cid:8) = g
g
2
(1)
δ
δ
(A(1)
(cid:8) )e(cid:8) = g
g
(cid:8)
1
2
(2)
^e(A
(cid:8) ,pk)
^e(g0,h) = ^e(A(2)
(cid:8) , h)
^e(g2, h)δ
κ(cid:8) = zs(cid:8)∧
′
3 ∧
Cω = gs
2 gω
1 gx
!
∧
A(1)
ω = gr
gr
2
1
∧
(1)
(A(1)
gδ
ω )e! = gδ
!
2
1
−e! ^e(g2, y)r
(2)
^e(g0,h) = ^e(A(2)
! ,pk)
^e(A
ω , h)
^e(g2, h)δ
κω = zs!∧
(cid:8) > (cid:8)θ∧
Tcur − ω > ωθ∧
~ω = Tcur∧
′
3 ∧
~Cω = g ~s
2 g ~ω
1 gx
!
′
~C(cid:8) = g ~s
2 g(cid:8)
1 gx
(cid:8)
3

(2)
!

(1)
!

(2)
!

(1)
(cid:8)

(1)
!

(1)

! ^e(g1, h)s! ^e(g2, h)x^e(g3, h)ω∧

1

1

(cid:8)

(cid:8)

∥A(1)

∥A(2)

ω ∥A(2)

′′
~A(cid:8) = (g0g~s
(cid:8)
1

and sends kappa(cid:8)∥κω∥A(1)
to D.

ω ∥ ~C(cid:8)∥ ~Cω∥π5
R←−
′′
′′
After verifying π5, κ(cid:8) and κω, D picks ~e(cid:8), ~s
(cid:8), ~eω, ~s
ω
∗
~C(cid:8))
~Aω =
~e(cid:8)+sk ,
computes
p,
Z
(cid:8))∥( ~Aω, ~eω, ~s
′′
′′
′′
(g0g~s
~Cω)
~e! +sk , and sends ( ~A(cid:8), ~e(cid:8), ~s
ω) to
!
1
U. . Then, D ﬂips a biased coin to decide whether to give an
invitation ticket to U; if so, D generates an one-time ticket
tk = {r
∗
p, and sends it
to U.

)}, where r

, HMACscrtD (r

∗ R←− Z

∗

∗

Regardless of receiving an invitation ticket, U com-
′
′′
′
′′
putes ~s(cid:8) = ~s
(cid:8), ~sω = ~s
ω, sets ~σ(cid:8) =
ω + ~s
(cid:8) + ~s
( ~A(cid:8), ~e(cid:8), ~s(cid:8)), ~σω = ( ~Aω, ~eω, ~sω), and updates his creden-
′
′
(cid:8), ~σ(cid:8), ~ϕω, ~Cω, ~s
tial with ~C(cid:8), ~s
ω, and ~σω.



