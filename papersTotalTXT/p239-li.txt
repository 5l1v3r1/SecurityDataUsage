Classiﬁers Unclassiﬁed: An Efﬁcient Approach

to Revealing IP Trafﬁc Classiﬁcation Rules

Fangfan Li∗ Arash Molavi Kakhki∗ David Choffnes∗ Phillipa Gill† Alan Mislove∗

∗Northeastern University

†Stony Brook University

ABSTRACT
A variety of network management practices, from band-
width management to zero-rating, use policies that ap-
ply selectively to diﬀerent categories of Internet traf-
ﬁc (e.g., video, P2P, VoIP). These policies are imple-
mented by middleboxes that must, in real time, assign
traﬃc to a category using a classiﬁer. Despite their im-
portant implications for network management, billing,
and net neutrality, little is known about classiﬁer imple-
mentations because middlebox vendors use proprietary,
closed-source hardware and software.

In this paper, we develop a general, eﬃcient method-
ology for revealing classiﬁers’ matching rules without
needing to explore all permutations of ﬂow sizes and
contents in our testbed environment. We then use it
to explore implementations of two other carrier-grade
middleboxes (one of which is currently deployed in T-
Mobile). Using packet traces from more than 1,000,000
requests from 300 users, we ﬁnd that all the devices we
test use simple keyword-based matching rules on the
ﬁrst two packets of HTTP/S traﬃc and small fractions
of payload contents instead of stateful matching rules
during an entire ﬂow. Our analysis shows that diﬀerent
vendors use diﬀerent matching rules, but all generally
focus on a small number of HTTP, TLS, or content
headers. Last, we explore the potential for misclassiﬁ-
cation based on observed matching rules and discuss im-
plications for subversion and net neutrality violations.
1.

INTRODUCTION

Today’s IP networks commonly use middleboxes to
perform management tasks that include bandwidth
management [6], protecting users from malicious traf-
ﬁc [11], performance optimization [18, 20], and zero-
rating [3]. While previous work has revealed the ex-
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
IMC 2016, November 14 - 16, 2016, Santa Monica, CA, USA
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to
ACM. ISBN 978-1-4503-4526-2/16/11. . . $15.00
DOI: http://dx.doi.org/10.1145/2987443.2987464

istence of middleboxes and their policies using black-
box methods [2, 5, 7, 10, 12–14, 16, 17, 19, 21, 22], there is
little work that investigates how these middleboxes de-
termine which traﬃc is subject to a policy. In this work,
we present the ﬁrst general approach for doing so, and
use this to characterize three carrier-grade middleboxes.
To facilitate network management, numerous ven-
dors provide middleboxes that allow operators to spec-
ify high-level policies for traﬃc management (e.g., block
malicious traﬃc, prioritize VoIP) without needing to
know the details of how to implement them.
In gen-
eral, these policies include a match rule (or classiﬁca-
tion rule) that identiﬁes a class of traﬃc, and an action
that speciﬁes what should be done to this class of traf-
ﬁc. An important challenge for any middlebox is how
to develop matching rules that reliably identify a traﬃc
class—often in real time so that it can apply a policy
to it. While certain types of classiﬁcation are straight-
forward (e.g., identifying DNS traﬃc), accurately clas-
sifying traﬃc into classes such as video, voice, and Web
is challenging due to confounding factors such as SSL
encryption, ubiquitous HTTP transport, and the ab-
sence of standard content encodings. Because these de-
vices are expensive, sold under restrictive license agree-
ments, and deployed in ways that are not transparent
to users or researchers, little is known about how mid-
dlebox classiﬁers work and their implications.

In this paper, we are the ﬁrst to identify and char-
acterize the classiﬁcation rules for HTTP(S) traﬃc im-
plemented in today’s carrier-grade middleboxes. This
allows us to understand how rules are deployed, their
impact on topics such as network neutrality, and how
they can be subverted. Further, our general algorithm
for identifying classiﬁcation rules can facilitate auditing
and analysis of future middleboxes and their policies by
users, policymakers, and regulators. Our key contribu-
tions follow.

First, we develop a general methodology for identify-
ing the matching rules used by a classiﬁer. To address
the potential combinatorial explosion of tests required
to uncover them, we propose the notion of Franken-
Flows, i.e., ﬂows combining features of multiple, dif-
ferent packet traces generated by applications that are

239subject to classiﬁcation. Doing so allows us to focus
only on traﬃc that is likely to trigger matching rules.

Second, we conduct a detailed study of the classiﬁca-
tion rules used by devices in a controlled setting and in
the wild. These include a carrier-grade packet shaper
and an IPS device in our lab, as well as a third sys-
tem that enforces T-Mobile’s BingeOn service. Through
traﬃc-replay and analysis, we ﬁnd that the devices all
analyze a small number of TCP, HTTP, and TLS ﬁelds
(e.g., Host and SNI) to classify network traﬃc in our
test suite, and do not use a ﬁxed set of ports and IP
addresses.

Third, we ﬁnd that the devices use simple text-based
matching in HTTP and TLS ﬁelds,
indicating that
their accuracy is limited by the speciﬁcity of the string-
matching patterns that they use to match in HTTP
headers or TLS handshakes. We show that these strings
can lead to misclassiﬁcation, both in terms of false pos-
itives and false negatives.

Fourth, we ﬁnd that the devices exhibit a “match-
and-forget” policy where an entire ﬂow is classiﬁed by
the ﬁrst rule that matches, even if later packets match a
diﬀerent rule that would lead to more accurate classiﬁ-
cation. Further, when keywords for multiple classes ap-
pear in the same ﬁeld in the same packet (e.g., the Host
header contains facebook.youtube.com), the devices
assign it to a single class using deterministic matching-
rule priorities. These simple matching priorities are eas-
ily exploited to allow one class of traﬃc to masquerade
as another and thus evade or subvert network policies.

2. METHODOLOGY

Our approach for identifying middlebox classiﬁcation
rules is to use a device under our control as ground truth
for developing and validating our detection methodol-
ogy, then to use our methodology to study other middle-
boxes. Similar to our previous work [9], we use an air-
gapped testbed consisting of a client, server, and a mid-
dlebox between them (Fig. 1). In contrast to our prior
work, which focused on identifying when diﬀerentiation
occurs (e.g., shaping), this study focuses on identifying
precisely what content in network traﬃc triggers clas-
siﬁcation that may lead to diﬀerentiation. The server
also spoofs as an Internet gateway, allowing us to use
arbitrary IP addresses in our traﬃc and capture all com-
munication from all devices in the testbed.

The middleboxes in our possession allow us to log
in, but do not reveal the exact classiﬁcation rules that
they use. However, we can access a user interface that
indicates in real time the class of traﬃc for each ﬂow
that traverses it. This allows us to test hypotheses for
classiﬁcation rules, by sending carefully crafted traﬃc
through the device to see how it is classiﬁed.

A na¨ıve approach to hypothesis testing is to try ev-
ery combination of packet sizes and packet payloads to
determine which ones trigger classiﬁcation. However,

Figure 1: Testbed for controlled experiments with classi-
ﬁers.

this is infeasible due to combinatorial explosion in the
number of tests required to complete the analysis.

Instead, we leverage the fact that 1) a set of tar-
geted applications potentially detected by the device
is known a priori,1 and 2) application-generated traf-
ﬁc already contains content that triggers rules. Using
the ﬁrst observation, we focus only on traﬃc from ap-
plications that are likely to be classiﬁed. The second
observation allows us to use application-generated traf-
ﬁc as baseline that we then modify to eﬃciently search
for the exact properties that trigger classiﬁcation. Note
that this approach extends to devices in our testbed and
those in the wild, so long as we have both the network
traﬃc trace to test, and a way to tell that traﬃc has
been classiﬁed (e.g., rate limiting or zero-rating).
2.1 Dataset

Our approach requires us

to send application-
generated traﬃc through our testbed. While this may
sound trivial, most interesting applications are closed-
source and often require interaction with third-party
servers to run; as a result, they cannot run in our air-
gapped testbed. Instead, we use the record/replay sys-
tem developed by Molavi et al. [9], which allows us to
replay arbitrary network traces gathered from applica-
tions outside our testbed.

To obtain a set of applications representative of those
users interact with, we leverage data from the ReCon
project [15], which boasts more than 300 users of iOS
and Android devices.2 This provides us with 1,179,618
HTTP GET requests, in which there are 20,129 unique
host headers, 8,701 unique User-Agent strings, and 685
distinct Content-Type headers [1]. We also extracted
1,727 unique Server Name Indication (SNI) ﬁelds from
51,985 HTTPS TLS handshakes.

Note that we only collect TCP traﬃc in our dataset,
as the vast majority of ﬂows and applications in our
dataset use HTTP/S. Understanding UDP-based clas-
siﬁcation is part of ongoing work.
2.2 Identifying Matching Fields

The ﬁrst step in understanding matching rules is de-
termining which portions of network ﬂows contain con-
tent that matches. Instead of permuting each bit in a

1In our testbed, we have ground truth information about all appli-
cations the device claims to identify. Outside of our testbed, the
applications may be ones that an ISP publicly admits to target-
ing (e.g., T-Mobile’s Binge On) or those that an observer simply
suspects are targeted.
2This data is collected as part of ReCon’s IRB-approved study.

Replay clientReplay serverTrafﬁc classiﬁerRouterGatewayClassiﬁcation results240network ﬂow to observe its eﬀect on classiﬁcation, we
take a more eﬃcient approach that exploits the fact that
our recorded traﬃc already matches rules.

In the base case (nothing is known about matching
rules), we conduct a binary search where we replace half
of the ﬂow with random bytes and observe its eﬀect on
classiﬁcation. Our assumption is that random bytes
are very unlikely to match any classiﬁcation rules.3 If
the traﬃc is no longer classiﬁed as the recorded appli-
cation, we then identify a more speciﬁc region in which
the method will be applied on—namely, the half of bytes
that triggered the change. To do this, we ﬁrst revert the
bytes in that region back to their original content, then
repeat the process of changing one half of the bytes at
a time in that region. If both halves triggered a change,
then we identify both halves as triggering the matching
rules. Once we identify portions of network ﬂows that
trigger matching rules, we conduct more extensive tests
by modifying each byte of a packet, one at a time, until
we ﬁnd the set of bytes that aﬀect classiﬁcation. These
bytes comprise the ﬁeld(s) used for matching. We
intentionally avoid exhaustively evaluating all combi-
nations of bits, as this is combinatorially infeasible to
test at scale and is not the goal of our study.

Using this approach, we found that our packet-
shaping device uses HTTP and TLS-handshake ﬁelds
in their matching rules, and only for the ﬁrst packet in
each direction.4 With this observation, we can more ef-
ﬁciently identify matching ﬁelds by using the known
structure of HTTP and TLS packets and permuting
only the corresponding ﬁelds. For example, when run-
ning this test on Netﬂix traﬃc, we ﬁnd that the Host
header triggers classiﬁcation. For Pandora, we ﬁnd that
the User-Agent ﬁeld is used. In addition, we omit any
portion of network traﬃc that does not trigger classiﬁ-
cation, leading to substantially shorter replays.

2.3 Revealing Classiﬁcation Rules

After identifying regions of network traﬃc that trig-
ger matching rules, our next step is identifying the spe-
ciﬁc matching rule. For this work, we assume that
matching rules take the form of regular expressions.
While matching rules in general could be arbitrary and
not based on text, our extensive analysis of three devices
found no counterexamples to our assumption. Without
loss of generality, we make an additional simplifying as-
sumption that matching rules take the form of one or
more basic regular expressions. While we could adapt
our methodology to reveal more complex and diverse
matching rules, we found no need to based on the three
devices we tested with HTTP/S traﬃc.

3We validated this assumption by running 1,000 ﬂows, each with
diﬀerent random bytes (from 100 to 1000 bytes) on port 80, all
of which were classiﬁed as the same generic “HTTP” class.
4To validate this, we tried splitting the ﬁrst packet into two pack-
ets, each with diﬀerent subsets of bytes, and found that only the
ﬁrst packet was ever used for classiﬁcation.

Algorithm 1 Isolating ﬁelds used in matching rules.

Inputs:

T : the original trace
R: classiﬁcation
L: list of potential matching ﬁelds

Output:

M : set of combinations of matching ﬁelds

function LocateMatchingFields(T, R, L)

Initialize number of ﬁelds k to 1
M = Set()
while k ≤ Length(L) do

for F in combination(k,L) do

T (cid:48) = copy(T )
for f ield in F do

T (cid:48) = permute(f ield)
R(cid:48) = classiﬁcation(T (cid:48))
if R(cid:48) (cid:54)= R then
M .add(F )

k += 1
Return M

We assume that a matching rule applies to a ﬁeld,
i.e., a region of a packet identiﬁed in the previous step.
However, this alone does not tell us the matching rule
because it is not speciﬁc. For example, if the ﬁeld con-
tains Host: prefix.netflix.com, it does not neces-
sarily imply that the matching rule is “First ﬁnd the
Host ﬁeld, then ﬁnd any string that contains netﬂix ”
(e.g., the rule could also be “ﬁnd any Host ﬁeld that
ends with netﬂix.com”).

To reveal the precise matching rule, we conduct tests
that randomize and otherwise alter subsets of content in
each ﬁeld. More generally, our approach is to permute
content in matching ﬁelds to test hypotheses about the
minimal set of content in matching ﬁelds that reliably
triggers classiﬁcation for a speciﬁc application (see Al-
gorithm 1). This entails replacing content with random
bytes, as well as adding and removing content. Be-
cause these generated ﬂows include traﬃc from multiple
sources of content, we refer to them as FrankenFlows.
2.3.1 Building a FrankenFlow
To build a FrankenFlow, we start with a base ﬂow
that is associated only with the application-layer pro-
tocol, i.e., HTTP or HTTPS. To do so, we build a ﬂow
that contains a dummy value for every known match-
ing ﬁeld for a classiﬁer (revealed by the analysis in §2.2),
where each ﬁeld’s value contains random content. For
example, a base ﬂow for HTTP traﬃc is a valid GET
request and response pair where all the HTTP header
values are replaced with random content:

GET {random text} HTTP/1.1
Host: {random text}
User-Agent: {random text}
...

241Likewise, a base ﬂow for HTTPS content is a TLS
handshake with any SNI and server-certiﬁcate ﬁelds re-
placed with random content. We then automatically
generate new FrankenFlows by replacing dummy values
with payloads from matching ﬁelds in observed network
traces.
2.3.2 Extracting Matching Rules
After isolating ﬁelds used for classiﬁcation, we further
randomize substrings of matching ﬁelds to isolate the
portions of ﬁeld triggering matching rules. Based on
the resulting tests, we construct a minimal multi-level
matching rule (i.e., a rule that contains the minimum
number of ﬁelds to trigger a match) that is consistent
with our observations.

For example, consider qq.com downloads. First, we
observe that it is an HTTP GET request, a feature shared
by many applications. Second, we ﬁnd that the Host
ﬁeld is used for matching. Next, we modify the Host
ﬁeld by adding, removing, and replacing ﬁeld values
with random strings. During this process, we ﬁnd that if
dl.xyz.qq.com is replaced with random.qq.random, it
is classiﬁed as QQ, and if replaced with dl.xyz.random,
then it is classiﬁed as generic “HTTP”. Last, if we re-
place the string with random.dl.random.qq.random or
random.qq.random.dl.random, it is classiﬁed as QQ
Download (a diﬀerent class from “QQ”). We therefore
infer that classiﬁer rule is: 1) HTTP GET request, 2) Host
header contains *dl*.qq.* or *.qq.*dl*.

To determine the impact of keyword location on clas-
siﬁcation, for each line in the HTTP header we ran-
domly added, moved, removed, and replaced bytes both
inside and outside of the identiﬁed keyword region.
These tests reveal rules that match on simple keywords
anywhere in a ﬁeld, only at the beginning of a ﬁeld, or
only at the end of a ﬁeld.
2.3.3 Rule Prioritization
Our methodology also reveals how matching rules
are prioritized when a ﬂow matches multiple rules si-
multaneously. To determine the priority of matching
rules for diﬀerent matching ﬁelds, we simply build all
combinations of FrankenFlows with diﬀerent matching
ﬁeld content, and determine the relative priority of each
rule by inspecting the classiﬁcation result. For exam-
ple, if a FrankenFlow contains Host: netflix.com and
User-Agent: Pandora... and is classiﬁed as Netﬂix
regardless of the order the ﬁelds appear in the ﬂow, then
we conclude that Netﬂix’s rule has a higher matching
priority than Pandora’s. Otherwise we infer that order
determines matching priority.

For the case of multiple matches in the same ﬁeld
(e.g., Host: netflix.youtube.com), we explore all
combinations of matching strings, by concatenating
each matching string into one ﬁeld value in a Franken-
Flow. When the classiﬁer selects one application that
matches, we then determine whether the position of the
corresponding matching string matters (e.g., was the

application selected because the matching ﬁeld was the
ﬁrst to appear in the FrankenFlow?).

Speciﬁcally, we move the matching string to the end
of the concatenated string.
If the classiﬁcation result
changes, then order also impacted rule matching (e.g.,
there is a tie-breaker based on position); otherwise, we
know that order is not a factor. Once we determine the
impact of position on classiﬁcation for an application,
we remove its string from the FrankenFlow and repeat
these steps for the application that is classiﬁed next.
2.4 Efﬁciency

An important property for our methodology is eﬃ-
ciency, both in terms of time and data usage required
to learn matching rules. We now evaluate this, and dis-
cuss an optimization that improves eﬃciency.

In addition to FrankenFlow optimizations, we lever-
age the observation that classiﬁcation occurs using only
the ﬁrst two packets exchanged between client and
server for HTTP/S traﬃc for the three devices we study.
As a result, we need only conduct tests using only the
ﬁrst packet sent by a client and server. Note that if our
assumption is incorrect (which is not the case for any
devices we tested), we simply continue to replay larger
fractions of ﬂows until we are able to identify classiﬁca-
tion rules.

This approach signiﬁcantly reduces the data and time
needed for each test. For example, the size of a typical
streaming video traﬃc replay is reduced from 30 MB
to just 2 KB in our testbed (where we have immediate,
ground-truth classiﬁcation results). In our testbed, we
tested all combinations of FrankenFlows using a single
client and server in 14 hours.

In contrast, the na¨ıve approach of permuting every
bit of two 1460 B packets (one for client request and
one for server response) would require 2n tests, where
n is the number of bits (23,200). Suﬃce it to say this
number is enormous. Even if we were to permute only
80 bits of a ﬁeld value, it would require 1.2x1024 tests,
requiring ≈ 1016 years if each test takes one second.

Outside our testbed, we detect T-Mobile’s Binge On
by checking for zero-rated traﬃc against our data plan,
as done previously [8]. Here, we use 10 KB Franken-
Flows, to avoid attributing data charges to background
traﬃc. Identifying the matching rules for an application
in Binge On required on average 400 KB of data.

3. EXPERIMENTAL RESULTS

We now present results from our detailed look into
the traﬃc-shaping device in our testbed, as it contained
a large number of matching rules and provided us with
ground-truth classiﬁcation results.5 In the next section,
we summarize results from two other devices.

Table 1 lists the application categories our device de-
tected and the number of matching rules we triggered in

5This device vendor is consistently identiﬁed as one of the key
companies in the global DPI market [4].

242Application category Number of examples
Streaming Applications

Web Applications

File Transfer

VoIP

Instant Messaging

Games

Mail

Security

P2P

33
32
10
9
7
5
5
2
1

Table 1: Categories of applications detected by our de-
vice using test traﬃc gathered from user traces. Our traces
provide good coverage across a variety of application types.

Header

URI
Host

User-Agent

Content-Type

Example Value

site.js?h={...}-nbcsports-com
User-Agent: Pandora 5.0.1 {...}
Content-Type: video/quicktime

Host: www.netﬂix.com

Application
NBC Sports

Netﬂix
Pandora

QuickTime

Table 2: HTTP matching ﬁelds and examples of applica-
tions classiﬁed by them. Matching keywords are in bold
font.

each category. The device identiﬁed 104 diﬀerent classes
of traﬃc, covering 9 of the 13 categories it supports.6
3.1 Matching Fields

In this section, we identify the ﬁelds used for match-
ing rules in the packet shaper. Recall that our Franken-
Flows are generated using HTTP/S traﬃc, so our re-
sults only apply to these protocols.

HTTP.
We ﬁnd that this device ﬁrst identiﬁes
HTTP traﬃc by looking for a request that starts with
GET, then checks for application-speciﬁc content in the
following headers: URI, Host, User-Agent (in the GET
request) and Content-Type (in the GET response). Ta-
ble 2 shows examples of matching ﬁelds. We note that in
many cases, it is trivial to modify these ﬁelds to avoid
classiﬁcation, indicating that matching ﬁelds are not
particularly resilient to adversarial behavior. Further,
we will show in the next subsection that many of these
rules can lead to false positives.

HTTPS. When applications use HTTPS, the en-
crypted TLS connection prevents the device from in-
specting HTTP headers. In this case, the device iden-
tiﬁes the corresponding application by matching text
anywhere in the ﬁrst two packets of the TLS hand-
shake. These include strings in the TLS Server Name
Indication (SNI) extension, and in the server-supplied
SSL certiﬁcate, which contains ﬁelds such as the Common
Name and Subject Alternate Names (SAN) list.

Interestingly, the classiﬁer does not parse ﬁelds in the
TLS handshake: our FrankenFlows were classiﬁed as an
application even if the TLS handshake contained invalid
data, so long as a keyword in a matching rule appeared
in the packet payload. Similar to the case of HTTP, this

6The remaining 4 categories are for intranet traﬃc.

would allow misclassiﬁcation, e.g., by putting a keyword
in the SAN list that matches a diﬀerent application.
3.2 Matching Rules

Our analysis revealed that the HTTP matching rules
used by the shaper can be described by a series of key-
word matches on text in matching ﬁelds.7 Examples
of diﬀerent matching rules include: facebook (Face-
book), .spotify. (Spotify), music.qq (QQ Music),
instagram.com (Instagram), storage.live.com (Sky-
Drive), and itunes.apple (iTunes). For HTTPS traf-
ﬁc, the matching rules are simply text strings. Exam-
ples include netflix, facebook and cloudfront.

None of the Host-header-based matching rules spec-
ify where in the matching ﬁeld they occur. For exam-
ple, netflix.youtube.com and youtube.netflix.com
would both be classiﬁed as Netﬂix.
In the case of
matches of the User-Agent string, we ﬁnd that 28 cases
match only at the beginning of the string (e.g., Viber,
Pandora, Pinterest, WhatsApp) and 4 cases can match
anywhere in the string (e.g., YouTube, Twitter). We
could ﬁnd no general pattern for when location of key-
words played a role in matching rules.

Interestingly, these matching rules are surprisingly
brittle. For example, we already identiﬁed how Net-
ﬂix and YouTube can be misidentiﬁed. Further, we did
not ﬁnd any restriction on whether a string appears
at the end of a line, something that would avoid this
case. As another example, we found that traﬃc from
the “Galaxy Wars Multiplayer” app is mistakenly clas-
siﬁed as BBC’s iPlayer because the keyword iplayer
appears in the HTTP header.
3.3 Priority Rules

We characterized how the device classiﬁes a ﬂow when
multiple rules match a single ﬂow. Such “tie-breaker”
cases (which we found to be consistent over time) pro-
vide additional insight into the accuracy of a classiﬁer
and its resilience to subversion.

We tested all combinations of text in matching ﬁelds
to identify how the shaper prioritizes them. Figure 2 de-
picts a decision process that captures our observations
for HTTP traﬃc. The device ﬁrst examines the content
in the GET request, then it checks the Content-Type
headers from the server response only if the request does
not match any rule.

Importantly, the device exhibits a “match-and-forget”
policy where an entire ﬂow is classiﬁed by the highest
priority matching rule even if later packets match a dif-
ferent rule that would lead to more accurate classiﬁca-
tion. For example, if the URI contains /user/youtube
and the Host contains facebook.com, the ﬂow is clas-
siﬁed as YouTube instead of Facebook. Further, the
priority of diﬀerent matching rules in the GET re-
quest depends both on the ﬁeld and the content. For

7Note that we cannot conﬁrm whether this is the actual matching
implementation; rather, we can only say that our observations
suggest this is the case.

243Figure 2: Observed decision process for applying matching rules to HTTP traﬃc.

example, the FrankenFlow with Host: www.hulu.com
and User-Agent: Pandora is classiﬁed depending on
which header appears ﬁrst in the GET request (i.e., if
Hulu appears ﬁrst, it is classiﬁed as Hulu).
In con-
trast, the FrankenFlow with Host: facebook.com and
User-Agent: Pandora is classiﬁed as Pandora regard-
less of the order of headers in the GET request.

Our results for classiﬁcation-rule priorities for all 87
matching keywords in HTTP traﬃc are summarized in
Table 3. We ﬁnd six distinct priority groups, with only
two groups containing more than one service. For exam-
ple, if the URI matches NBC Sports, the classiﬁer will
label the entire ﬂow as NBC Sports, even if the strings
for YouTube, Netﬂix, or iPlayer also appear in the ﬂow.
When multiple matching strings from the third prior-
ity group appear in the same ﬂow, the device uses yet
another set of tie-breaking priorities, listed in Table 4.
For example, if Netﬂix and Facebook appear in the Host
header, then the ﬂow is classiﬁed as Netﬂix regardless
of where in the host ﬁeld the string netﬂix appears (be-
cause Netﬂix is in a higher priority group). For match-
ing rules in the same tie-breaking group, e.g., Netﬂix
and LinkedIn, the ﬂow is classiﬁed as the ﬁrst keyword
to appear in the Host string. We found no particular
reason for the priority order found on this device.

The device classiﬁes HTTPS traﬃc with multiple
matching rules similarly to HTTP. For example, after
matching on a TLS Client Hello packet, the device
ignores matching rules in the server response.

4. ADDITIONAL CASE STUDIES

We also tested our methodology against two other
traﬃc shaping devices: a carrier-grade IPS device in our
possession and a deployed device in T-Mobile’s network.

Priority Matching Field(s)

Example Apps #cases

1
2

3

4
5

6

URI
URI

Host header,
User-Agent
Host header

URI

User-agent

NBC Sports

YouTube

Netﬂix, ESPN,

Pandora
Facebook

iPlayer

Apple Service,

Android Content,
Mobile browsing

1
1

80

1
1

3

Table 3: When a ﬂow triggers multiple matching rules, the
packet shaper classiﬁes the ﬂow according to the rule with
highest priority. Tie-breaking priorities are listed in Table 4.

Carrier-grade IPS Device (Testbed).
Our IPS
device uses coarse-grained classiﬁcation, e.g., it identi-
ﬁes “streaming video” traﬃc, but does not identify spe-
ciﬁc applications. We tested all our recorded streaming
video traces against the device, and found that it does
not classify any of them as video traﬃc except for a Net-
ﬂix trace from 2014 (newer Netﬂix traces are not clas-
siﬁed). In this case, the device matched on the string
ftypmp4 (presumably matching on “ﬁle type mp4”) in
the HTTP payload. Interestingly, this is the only case
we found so far that inspect packet content beyond
HTTP headers. We suspect that the poor matching
results are due to the device using outdated rules, in
addition to the fact that the device’s primary purpose
is detecting security threats (and not applications).

T-Mobile’s Binge On Service
T-Mobile’s “Binge
On” service allows opt-in subscribers to stream video
content from participating providers without counting
those bytes against their data plan (i.e., such content is
zero-rated). In previous work, Molavi et al. [8] used ad-
hoc techniques to show that T-Mobile identiﬁes Binge-
On-eligible content by inspecting the values in HTTP
Content-Type and Host headers in addition to some
ﬁelds of the TLS handshake. The ground truth signal
that traﬃc is classiﬁed as Binge On is that its data is
zero-rated (based on our account’s data-usage counter).
We use this information, along with the methodology
in Section 2, to revisit Binge On—where we have no a
priori knowledge about the vendor or their rules, and
describe new ﬁndings below.

As an example of our ﬁndings, our analysis re-
vealed that Netﬂix, which was previously identiﬁed us-
ing HTTP traﬃc, has since moved to HTTPS connec-
tions. Our analysis shows that the T-Mobile classiﬁer
checks the SNI ﬁeld for the string nflx and the contents
of the Common Name in the server certiﬁcate in the

Tiebreaking

Priority

Example Apps

1

2

3

NetFlix, LinkedIn, Skype, SkyDrive,

Symantec, Yahoo Video, Gmail

Youtube, Instagram, Tango, AmazonCloud,

Kaspersky, EA Games, Wechat
Facebook, Yahoo Mail, Zynga

Table 4: When a ﬂow matches multiple rules in the same
priority group in Table 3, the classiﬁer picks the one with
the highest tie-breaking priority. If the ﬂow matches mul-
tiple rules of the same priority, the classiﬁer picks the ﬁrst
matching string to appear in the ﬂow.

GET request?Keyword inURI, Host, User-Agent?Keyword inContent-Type?HTTP inGET request?Select rule based on matching priorityThe ﬁrst two packetsContent-TypeBased ClassiﬁcationHTTP BrowsingApplicationorMobile BrowsingHTTPyesnononoyesyesyes244TLS handshake for the value nflxvideo.net. These
highly speciﬁc matching ﬁelds are surprising, since it
seems likely that such domain names change often and
require signiﬁcant manual maintenance to ensure reli-
able classiﬁcation.
Interestingly, we further ﬁnd that
unlike the packet-shaping device in our testbed, T-
Mobile’s classiﬁer parses the TLS handshake and will
not properly detect HTTPS traﬃc if the TLS packets
are malformed or the value is not in a matching ﬁeld
(SNI or Common Name).

We investigated priorities when a packet matches
multiple applications and found that T-Mobile matches
only on the last Host header if multiple are present.
Interestingly, this leads to zero-rated traﬃc if the last
header matches a BingeOn participant, even if other
headers do not.
If the ﬁrst header is, for example, a
Google App Engine domain, Web site content is re-
turned without error even though there are multiple
Host headers. This provides a way to zero-rate arbi-
trary HTTP traﬃc beyond was was found previously [8].
5. DISCUSSION AND CONCLUSION

In this paper, we presented an eﬃcient approach for
identifying the matching rules used in traﬃc shapers
for applying policies such as packet shaping, security,
and zero-rating. We showed that using existing appli-
cation traﬃc facilitates eﬃcient and reliable discovery
of matching rules. We applied this approach to several
devices and found that their approach to classifying the
applications that we tested was surprisingly simple and
generally based on matching text in a small number of
HTTP and HTTPS ﬁelds.

Our work on revealing matching rules provides a solid
framework for researchers and regulators to audit im-
plementations of policies in today’s middlebox deploy-
ments, understand their impact on issues such as net-
work performance and net neutrality, and understand
security implications. As part of our future work, we
are investigating other deployments and extending our
analysis to UDP traﬃc. We expect to further reﬁne our
methodology as we encounter new policies and classiﬁer
implementations not covered by our current approach.
Acknowledgements
We thank the anonymous reviewers and our shepherd
Renata Teixeira for their helpful feedback. This work is
supported in part by a Google Research Award, and
NSF awards CNS-1617728, CNS-1350720, and CNS-
1518845.

6. REFERENCES

[1] Complete MIME types list.

http://www.freeformatter.com/mime-types-list.html.

[2] Neubot – the network neutrality bot.

http://www.neubot.org.

[3] T-Mobile BingeOn. http://www.t-mobile.com/offer/

binge-on-streaming-video.html.

[4] Global DPI market 2014-2018: Key vendors are allot

communications, cisco, procera networks and sandvine.
http://www.prnewswire.com/news-releases/global-dpi-
market-2014-2018-key-vendors-are-allot-
communications-cisco-procera-networks-and-sandvine-
275106991.html, September 2014.

[5] V. Bashko, N. Melnikov, A. Sehgal, and J. Schonwalder.

Bonaﬁde: A traﬃc shaping detection tool for mobile
networks. In IFIP/IEEE International Symposium on
Integrated Network Management (IM2013), 2013.

[6] M. Dischinger, M. Marcon, S. Guha, K. P. Gummadi,

R. Mahajan, and S. Saroiu. Glasnost: Enabling end users
to detect traﬃc diﬀerentiation. In Proc. of USENIX NSDI,
2010.

[7] FCC announces ”Measuring Mobile America” program.

http://www.fcc.gov/document/fcc-announces-
measuring-mobile-america-program.

[8] A. M. Kakhki, F. Li, D. R. Choﬀnes, E. Katz-Bassett, and
A. Mislove. BingeOn under the microscope: Understanding
t-mobile’s zero-rating implementation. In Proc. of
SIGCOMM Workshop on Internet QoE, 2016.

[9] A. M. Kakhki, A. Razaghpanah, A. Li, H. Koo, R. Golani,
D. R. Choﬀnes, P. Gill, and A. Mislove. Identifying traﬃc
diﬀerentiation in mobile networks. In Proc. of IMC, 2015.
[10] P. Kanuparthy and C. Dovrolis. ShaperProbe: end-to-end

detection of ISP traﬃc shaping using active methods. In
Proc. of IMC, 2011.

[11] C. Kreibich, N. Weaver, B. Nechaev, and V. Paxson.

Netalyzr: Illuminating the edge network. In Proc. of IMC,
2010.

[12] R. Mahajan, M. Zhang, L. Poole, and V. Pai. Uncovering

performance diﬀerences among backbone ISPs with Netdiﬀ.
In Proc. of USENIX NSDI, 2008.

[13] Measurement Lab Consortium. ISP interconnection and its

impact on consumer internet performance. http://www.
measurementlab.net/blog/2014_interconnection_report,
October 2014.

[14] A. Nikravesh, H. Yao, S. Xu, D. R. Choﬀnes, and Z. M.

Mao. Mobilyzer: An open platform for controllable mobile
network measurements. In Proc. of MobiSys, 2015.
[15] J. Ren, A. Rao, M. Lindorfer, A. Legout, and D. R.

Choﬀnes. ReCon: Revealing and controlling privacy leaks
in mobile network traﬃc. In Proc. of MobiSys, 2016.

[16] Switzerland network testing tool. https://www.eff.org/

pages/switzerland-network-testing-tool.

[17] M. B. Tariq, M. Motiwala, N. Feamster, and M. Ammar.

Detecting network neutrality violations with causal
inference. In CoNEXT, 2009.

[18] N. Weaver, C. Kreibich, M. Dam, and V. Paxson. Here Be

Web Proxies. In Proc. PAM, 2014.

[19] N. Weaver, R. Sommer, and V. Paxson. Detecting forged

TCP reset packets. In Proc. of NDSS, 2009.

[20] X. Xu, Y. Jiang, T. Flach, E. Katz-Bassett, D. Choﬀnes,

and R. Govindan. Investigating transparent web proxies in
cellular networks. In Proc. PAM, 2015.

[21] Y. Zhang, Z. M. Mao, and M. Zhang. Detecting Traﬃc

Diﬀerentiation in Backbone ISPs with NetPolice. In Proc.
of IMC, 2009.

[22] Z. Zhang, O. Mara, and K. Argyraki. Network neutrality

inference. In Proc. of ACM SIGCOMM, 2014.

245