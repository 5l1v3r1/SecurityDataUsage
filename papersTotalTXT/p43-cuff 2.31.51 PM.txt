Differential Privacy as a Mutual Information Constraint

Paul Cuff

Princeton University

Lanqing Yu

Princeton University

ABSTRACT
Diﬀerential privacy is a precise mathematical constraint meant
to ensure privacy of individual pieces of information in a
database even while queries are being answered about the
aggregate. Intuitively, one must come to terms with what
diﬀerential privacy does and does not guarantee. For exam-
ple, the deﬁnition prevents a strong adversary who knows all
but one entry in the database from further inferring about
the last one. This strong adversary assumption can be over-
looked, resulting in misinterpretation of the privacy guaran-
tee of diﬀerential privacy.

Herein we give an equivalent deﬁnition of privacy using
mutual information that makes plain some of the subtleties
of diﬀerential privacy. The mutual-information diﬀerential
privacy is in fact sandwiched between -diﬀerential privacy
and (, δ)-diﬀerential privacy in terms of its strength. In con-
trast to previous works using unconditional mutual informa-
tion, diﬀerential privacy is fundamentally related to condi-
tional mutual information, accompanied by a maximization
over the database distribution. The conceptual advantage of
using mutual information, aside from yielding a simpler and
more intuitive deﬁnition of diﬀerential privacy, is that its
properties are well understood. Several properties of diﬀer-
ential privacy are easily veriﬁed for the mutual information
alternative, such as composition theorems.

Keywords
Diﬀerential privacy, information theory.

1.

INTRODUCTION

Diﬀerential privacy is a concept proposed in [12] for database

privacy. It allows queries to be answered about aggregate
quantities of data while protecting the privacy of individual
entries in the database. In the absence of a precise mathe-
matical framework such as diﬀerential privacy, practitioners
have been tempted to use various rules-of-thumb to pro-
tect privacy (e.g. “don’t answer a query that averages fewer
than k entries together”—see the query restriction approach
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978308

in [1]).
Instead, diﬀerential privacy directly addresses the
statistical distinguishability of the database and has led to
algorithms for answering general queries with just the right
amount of randomness used in order to preserve privacy.1

Diﬀerential privacy requires that two adjacent databases,
which diﬀer in only one entry, are statistically indistinguish-
able, as measured by a probabilistic metric deﬁned in Sec-
tion 2. This guarantee is particularly eﬀective for making in-
dividuals feel comfortable contributing personal information
to a dataset. For instance, if a person decides to participate
in a survey, his answers only constitute one response out of
the entire collection, and the responses of other people re-
main unchanged. Diﬀerential privacy is meant to assure the
one participant that his answers are concealed.

This privacy metric has gained a lot of traction in recent
years. The main contribution of this work is to cast diﬀeren-
tial privacy as a mutual information constraint. There have
been many attempts in the literature to connect diﬀeren-
tial privacy to mutual information. Here we give not only a
connection but an equivalence.

To brieﬂy summarize the main result, consider a database
X n = (X1, . . . , Xn) that returns a query response Y accord-
ing to a random mechanism PY |Xn . Let X−i denote the set
of database entries excluding Xi.

Definition 1

((, δ)-Differential Privacy [13]). A ran-

domized mechanism PY |Xn satisﬁes (, δ)-diﬀerential privacy
if for all neighboring database instances xn and ˜xn

PY |Xn=xn

(,δ)≈ PY |Xn=˜xn ,

(1)

where the approximation in (1) is deﬁned later in Deﬁni-
tion 4, and neighboring database instances are deﬁned in
Deﬁnition 3 as any pair of database vectors that diﬀer in
only one entry (i.e. Hamming distance one).2

Definition 2

(Mutual-Information Diff. Priv.).

A randomized mechanism PY |Xn satisﬁes -mutual-information
diﬀerential privacy if

I(Xi; Y |X

−i) ≤  nats.

(2)

sup
i,PXn

Note that nats are the information units that result from
using the natural logarithm instead of the logarithm base two,
which would give bits.

1Diﬀerential privacy does not assume the adversary has any
computational limitation.
2Another similar deﬁnition for “neighbor” exists in the liter-
ature, involving the removal of one entry of the database.

43The main claim of this paper, which appears in Section 3,
is an equivalence between mutual-information diﬀerential
privacy (MI-DP) and the standard deﬁnition of (, δ)-diﬀerential
privacy ((, δ)-DP). The original deﬁnition of diﬀerential pri-
vacy [12], deﬁned formally in Section 2, parameterized pri-
vacy with a single positive number . For various reasons
it has since been relaxed [13] to have two parameters  and
δ playing multiplicative and additive roles in the likelihood
constraint. We refer to the original DP as -DP and the
relaxed form as (, δ)-DP. In this notation, -DP is simply
(, 0)-DP.

The claim herein is that MI-DP is sandwiched between
these two deﬁnitions in the following sense: It is weaker than
-DP but stronger than (, δ)-DP. That is, a mechanism that
satisﬁes -DP also satisﬁes -MI-DP.3 Similarly, if -MI-DP
holds then ((cid:48), δ)-DP also must hold, where (cid:48) and δ vanish
as  goes to zero. In fact, the connection between MI-DP
and (, δ)-DP is an equivalence if either the domain or range
of the query mechanism is a ﬁnite set.

The advantage of this alternative but equally strong deﬁ-
nition of diﬀerential privacy is that mutual information is a
well-understood quantity. It provides a clear picture of what
diﬀerential privacy does and does not guarantee. Further-
more, several properties of diﬀerential privacy are immediate
to prove in this form.

While the mathematics of diﬀerential privacy, in its stan-
dard form, are straightforward, an intuitive understanding
can be elusive. The deﬁnition of (, δ)-DP involves a notion
of neighboring database instances. Upon examination one
realizes that this has the aﬀect of assuming that the ad-
versary has already learned about all but one entry in the
database and is only trying to gather additional information
about the remaining entry. We refer to this as the strong
adversary assumption, which is implicit in the deﬁnition of
diﬀerential privacy. Notice that MI-DP needs no deﬁnition
of neighborhood. The strong adversary assumption is made
explicit in the conditioning within the conditional mutual
information term.

The strong adversary assumption is both a feature and a
vulnerability of the deﬁnition of diﬀerential privacy. It is a
feature when recruiting individual participants for a survey.
The individual can decide whether or not to participate but
cannot do anything about the information contributed by
others (which may inform on them indirectly). Diﬀerential
privacy assures them that even with access to everyone else’s
responses, the survey reports will not further reveal anything
about their individual response. However, DP also has its
shortcomings as a privacy guarantee. Among all adversaries
with diﬀerent prior knowledge of the database, the strong
adversary may not be the one which beneﬁts the most from
the query output. Indeed, it is shown in [18] that a weaker
adversary can compromise privacy severely if the entries in
the database are correlated, which is quite typical in certain
applications such as social networks.

As an equivalent privacy metric, MI-DP beneﬁts and suf-
fers in the same way. Fortunately, the deﬁnition of MI-
DP puts this potential weakness in plain sight.
It shows
explicitly that information leakage is only being restricted
conditioned on the remainder of the database being known.
Clearly, this does not bound the unconditional mutual in-
formation when correlations are present.

3The other direction is not true in general, as there exist
mechanisms which satisfy -MI-DP but not (cid:48)-DP for any (cid:48).

Somewhat paradoxically, mutual information can serve si-
multaneously as both a measure of privacy, as in MI-DP, and
as a quantiﬁcation of utility—for example, the mutual infor-
mation between the entire database and the query response,
I(X n; Y ). The close connection between mutual information
and estimation and detection further captures the privacy-
utility trade-oﬀ.

Several works [20, 9, 4, 3, 10] relate mutual information to
diﬀerential privacy by upper bounding mutual information
given a diﬀerential privacy achieving mechanism. One com-
mon point of these works is that they all use unconditional
mutual information rather than conditional. Often, the con-
clusion is a bound on the unconditional mutual information
between the whole database and the private output. The use
of conditional mutual information in Deﬁnition 2 captures
the prior knowledge of the database possessed by a potential
adversary (i.e. the strong adversary assumption implicit in
DP). This is crucial in developing an equivalence with the
standard DP deﬁnition.

Another crucial ingredient that some of the literature fails
to properly incorporate (e.g. [25]) when applying mutual in-
formation to diﬀerential privacy is that diﬀerential privacy is
a property of the query mechanism and assumes no speciﬁc
prior distribution on the database. Mutual information, on
the other hand, is not well deﬁned without a joint distri-
bution, which must include a distribution for the database.
The remedy is to maximize the mutual information over all
possible distributions on the database, as seen in the deﬁni-
tion of MI-DP in Deﬁnition 2.4 Had we deﬁned MI-DP with
respect to any particular database distribution (e.g. with
independent and identically distributed entries), we would
have signiﬁcantly reduced its strength as a privacy metric.
The formula in Deﬁnition 2 in fact looks like a channel ca-
pacity formula one would encounter in expressing the fun-
damental limit of communication through a noisy channel.
This maximization removes any distributional assumption
and makes MI-DP a property of the mechanism itself.

2. PRELIMINARIES
2.1 Notation
The set {1, 2,··· , m} is denoted as [m]. An index set I is a
subset of [n] whose elements are enumerated as (i1,··· , i|I|),
where | · | denotes the cardinality of a set.
We use X n as shorthand notation for the sequence of ran-
dom variables (X1,··· , Xn). The symbol X−i denotes the
sequence of n−1 random variables (X1,··· , Xi−1, Xi+1,··· , Xn),
in other words, all of X n except Xi. The lower case symbol
x−i is an instance of X−i. For any index set I, we use XI
to denote the sequence of random variables (Xi1 ,··· , Xi|I| )
speciﬁed by I. Similarly the lower case xI = (xi1 ,··· , xi|I| )
is an instance of XI.
A database X n consists of n entries, where the i-th entry
takes values from Xi.

Definition 3

(Neighbor). Two database instances xn
and ˜xn are neighbors if they diﬀer in only one entry. In
other words,

dH (xn, ˜xn) = 1,
where dH (·,·) is Hamming distance.
4This approach, using worst-case database distribution, ap-
pears in various works throughout the literature, e.g. [11].

(3)

44The output of a privacy mechanism is a random variable

represented as Y and takes values from Y.
2.2 Statistical Indistinguishability

Two probability distributions can be considered statisti-
cally indistinguishable if they are close under an appropri-
ate metric. The criterion for indistinguishability used in the
standard deﬁnition of diﬀerential privacy is the following.

Definition 4

((, δ)-Closeness). Two probability dis-
tributions P and Q over the same measurable space (Ω,F)
are (, δ)-close, denoted as

(,δ)≈ Q

P

if

P (A) ≤ eQ(A) + δ,
Q(A) ≤ eP (A) + δ,

∀A ∈ F ,
∀A ∈ F .

(4)

(5)

(6)

Consider two special cases, δ = 0 and  = 0. If δ = 0,
then P and Q are mutually absolutely continuous, denoted
as P (cid:28)(cid:29) Q, and (, 0)-closeness is a statement about the
Radon-Nikodym derivative dP
dQ :

(cid:12)(cid:12)(cid:12)(cid:12)ln

(cid:12)(cid:12)(cid:12)(cid:12) ≤  ∀a ∈ Ω.

dP
dQ

(a)

(7)

(,0)≈ Q ⇐⇒

P

On the other hand, with  = 0, (0, δ)-closeness is a statement
about the total variation distance:

(0,δ)≈ Q ⇐⇒ (cid:107)P − Q(cid:107)T V ≤ δ.

P

(8)

We can also relate (, δ)-closeness to Kullback-Leibler di-
vergence, denoted as D(·(cid:107)·). For example, by relaxing the
right side of (7) to be an expected value rather than a state-
ment about all a ∈ Ω, we immediately get the following
implication:

(,0)≈ Q =⇒ D(P(cid:107)Q) ≤  nats,
D(Q(cid:107)P ) ≤  nats.

P

(9)

Tighter expressions of the relationship to Kullback-Leibler
divergence are given next, in Properties 1 and 2. We give a
proof of Property 1 in Appendix A.

Property 1.

P

(,0)≈ Q =⇒ D(P(cid:107)Q) ≤ min(cid:8), 2(cid:9) nats,
D(Q(cid:107)P ) ≤ min(cid:8), 2(cid:9) nats.
(e − 1)(cid:0)1 − e−(cid:1)
(e − 1)(cid:0)1 − e−(cid:1)

D(P(cid:107)Q) ≤ 

(e − 1) + (1 − e−)

D(Q(cid:107)P ) ≤ 

(e − 1) + (1 − e−)

(,0)≈ Q =⇒

P

In fact, the tightest possible statement of this form is

nats,

nats.

(11)
Equality on the right side of (11) can be achieved with bi-
nary distributions. For small , the right side of (11) is
asymptotically 1

2 2 nats.

Property 1 and Property 2 are strict in the sense that the
reverse implications are not true in any form (i.e. closeness
bounds on the right, no matter how small the parameters,
do not even imply ﬁniteness of the parameters on the left).
Also, we already mentioned that Property 1 is tight. Prop-
erty 2 is known to be tight up to a multiplicative constant.
The quantities arising in the above deﬁnitions and prop-
erties have concrete connections to inference. Total varia-
tion distance precisely captures the error probability in a
binary hypothesis test. That is, one minus the total varia-
tion distance is the minimum sum of the two types of binary
error probability. Kullback-Leibler divergence precisely cap-
tures the asymptotic hypothesis testing error upon observ-
ing many independent observations [8, Chapter 11]. The
strongest of these metrics, (, 0)-closeness, has an interpre-
tation in the Bayesian setting as a bound on the Bayes fac-
tor. That is, the log-posterior-odds-ratio cannot change by
more than  due to the observation. Finally, (, δ)-closeness
is shown in [17] to be precisely a piecewise linear constraint
on the error region in a binary hypothesis test. By inspec-
tion of that relationship, the following property is apparent
(proven in Appendix B).

(cid:16)
Property 3. For any non-negative (cid:48) < , let δ(cid:48) = 1 −
e(cid:48)

(1−δ)

(cid:17)

+1
e+1

.

(,δ)≈ Q =⇒ P

P

((cid:48),δ(cid:48))≈ Q.

(13)

Property 3 is the tightest possible trade-oﬀ between  and
δ with respect to (, δ)-closeness. Notice that δ(cid:48) > δ. It is
not possible for a larger δ to imply a smaller one, for any
ﬁnite  and (cid:48).
2.3 Differential Privacy

The deﬁnition of (, δ)-DP in Deﬁnition 1 has been now
made precise with Deﬁnition 3 (neighbor) and Deﬁnition 4
((, δ)-closeness).

We deﬁne -DP and (δ)-DP by setting either of the two

parameters to zero.

Definition 5

(-Differential Privacy [12]). A ran-
domized mechanism PY |Xn satisﬁes -DP if it satisﬁes (, 0)-
DP.

(10)

Definition 6

((δ)-Differential Privacy). A random-

ized mechanism PY |Xn satisﬁes (δ)-DP if it satisﬁes (0, δ)-
DP.

Mutual-information diﬀerential privacy was deﬁned in the

introduction in Deﬁnition 2.

Finally, let us deﬁne one additional privacy metric based
on Kullback-Leibler divergence, which we will call KL-DP.

Definition 7

(KL Differential Privacy). A random-

ized mechanism PY |Xn satisﬁes -KL-DP if for all neighbor-
ing database instances xn and ˜xn

D(cid:0)PY |Xn=xn

(cid:13)(cid:13)PY |Xn=˜xn

(cid:1) ≤  nats.

(14)

Property 2. By Pinsker’s inequality,

D(P(cid:107)Q) ≤  nats =⇒ P

(cid:16)

√
≈

0,

/2

(cid:17)

2.4 Ordering of Privacy Metrics

Q.

(12)

This work is about showing equivalence of privacy metrics.

In order to do so, we must deﬁne an ordering.

45Definition 8

(Stronger Privacy Metric). As a place-

holder, take α-DP and β-DP to represent two generic pri-
vacy guarantees with positive parameters α and β. We say
that α-DP is stronger than β-DP, denoted as

α-DP (cid:23) β-DP,

(15)

if for all β(cid:48) > 0 there exists an α(cid:48) > 0 such that

α’-DP =⇒ β’-DP.

(16)
If the parameters are vectors, then β(cid:48) > 0 and α(cid:48) > 0 should
be interpreted as inequalities on each coordinate.

Example 1. It is clear that -DP (cid:23) (, δ)-DP and (δ)-
DP (cid:23) (, δ)-DP, since -DP implies (, δ)-DP for any non-
negative δ, and likewise for (δ)-DP, by deﬁnition.
Also, (, δ)-DP = (δ)-DP by Property 3. Notice that even
if we set (cid:48) = 0, the quantity δ(cid:48), as deﬁned in the property,
goes to zero as  and δ go to zero.

and a mutual information constraint. As in this work, there
is a maximization over distributions of inputs to the random-
ized mechanism; however, conditional mutual information is
not a part of that result, while it is a necessary ingredient
here. Also, the notion of (, δ)-closeness goes by the name
of Eγ distance in some of the information theory literature,
(,δ)≈ Q is equivalent to

such as [22] and [19]. Speciﬁcally, P
the pair of statements Ee (P(cid:107)Q) ≤ δ and Ee (Q(cid:107)P ) ≤ δ.
3.3 Proof of Theorem 1

We prove (17) of Theorem 1 by proving a stronger chain

of inequalities:

-DP

(A)(cid:23) KL-DP

(B)(cid:23) MI-DP

(C)(cid:23) (δ)-DP

(D)
= (, δ)-DP. (20)

It is worth noting both (A) and (B) are in fact strict or-
derings ((cid:31))—the reverse implications do not hold, even if
cardinality bounds are assumed.

3. MAIN RESULT
3.1 Equivalence

The emphasis of this work is the equivalence of mutual-
information diﬀerential privacy with classical diﬀerential pri-
vacy.

Theorem 1

(Main Result).
-DP (cid:23) MI-DP (cid:23) (, δ)-DP.

(17)

Therefore,

We now state the components of the proof in separate
lemmas. Orderings (A) and (B) are the subject of Lemma 1,
and ordering (C) is handled by Lemma 2. Equality (D)
comes from Property 3, as discussed in Example 1.

Lemma 1

(Orderings (A) and (B)).

-DP =⇒ min(cid:8), 2(cid:9)-KL-DP,
-DP =⇒ min(cid:8), 2(cid:9)-MI-DP.

-KL-DP =⇒ -MI-DP.

(21)

(22)

(23)
Proof. The ﬁrst statement, (21), is established by Prop-
erty 1. Both -DP and KL-DP are deﬁned the same way in
terms of neighboring database instances.

I(Xi; Y |X

The second statement, (22), is best understood through
the geometric interpretation of capacity as the radius of the
information ball [8, Theorem 13.1.1]. The radius cannot be
more than the maximum of pairwise distances. However, we
will not directly use that machinery here. Instead, consider
the following direct proof.
Start by assuming that the randomized mechanism PY |Xn
satisﬁes -KL-DP. Let i ∈ {1, . . . , n} and PXn be arbitrary.
For notational clarity, let ¯X n ∼ PXn , and begin with a rep-
resentation of conditional mutual information for a general
distribution in terms of Kullback-Leibler divergence:

(cid:13)(cid:13)PY |X−i= ¯X−i
(cid:13)(cid:13)PY |X−i=x−i
(cid:1) for each in-
(cid:105)

−i) = E(cid:2)D(cid:0)PY |Xn= ¯Xn
Now we bound D(cid:0)PY |Xn=xn
PY |X−i=x−i = E(cid:104)
(cid:13)(cid:13)PY |X−i=x−i
D(cid:0)PY |Xn=xn
(cid:13)(cid:13)(cid:13)E(cid:104)
(cid:16)
(cid:13)(cid:13)(cid:13)PY |Xi= ˜X,X−i=x−i
(cid:16)
≤ E(cid:104)

stance xn. Fix xn arbitrarily, and let ˜X ∼ PXi|X−i=x−i .

PY |Xi= ˜X,X−i=x−i

Therefore, by Jensen’s inequality, and using the fact that

D(·(cid:107)·) is convex in the second argument, we conclude,

PY |Xi= ˜X,X−i=x−i

= D

PY |Xn=xn

(cid:105)(cid:17)
(cid:17)(cid:105)

.

(25)

Consider,

(cid:1)

(cid:1)(cid:3)

(24)

PY |Xn=xn

D
≤  nats,

(26)

where the last inequality is due to the fact that any two
databases that agree on X−i are neighbors.

Furthermore, if the cardinality of the database entries or the
query response is bounded, then

MI-DP = (, δ)-DP,

(18)
where the relationship (, δ)-DP (cid:23) MI-DP is dependent on
the cardinality bound

(cid:110)|Y|, max

|Xi|(cid:111)

i

min

.

(19)

Precise bounds for the privacy parameters are given in the

three lemmas in Section 3.3.
3.2 Related Work

Using information theoretic measures to quantify the pri-
vacy guarantee of diﬀerential privacy is not a new idea. An
upper bound of mutual information is shown in [20] in a two-
party diﬀerential privacy setting. Later this upper bound is
used in [9] to get I(X n; Y ) ≤ 3n. In [3] and [4], min-entropy
is considered rather than the usual Shannon entropy, and up-
per bounds are proven. In fact, [4, Corollary 1] implies an
ordering relationship similar to the ﬁrst inequality of (17)
but for min-entropy based information leakage with only a
single database entry.
In [25], a “mutual information pri-
vacy” metric is deﬁned and studied.

These works have in common that they all consider the
use of unconditional mutual information. This doesn’t cap-
ture the structure in the deﬁnition of diﬀerential privacy and
the bounds are limited to the mutual information between
the whole database and the sanitized query output, with no
focus on individual entries. Needless to say, an equivalence
is not established.

Some of the information theory literature bares resem-
blance to this work. In [5], similar proof steps to this work
are used to show an equivalence between semantic security

46y ∈ A,
y /∈ A.

1,
0,

(29)

PY |X1=x1 =

(27)

Lemma 2

(Ordering (C)).

-MI-DP =⇒ (cid:16)
-MI-DP =⇒ (cid:0)0, δ

0,

(cid:17)
(cid:48)(cid:1)-DP,

√
2

-DP.

In fact, the tightest possible statement of this form is

(28)
with δ(cid:48) = 1 − 2h−1(ln 2 − ), where h−1 is the inverse of
the increasing part of the binary entropy function in units of
nats. This formula holds for  ∈ [0, ln 2]. For  > ln 2, the
implication becomes (1)-DP, which is vacuous.

The claim in (27) is looser than that in (28) but asymp-

totically tight for small .

Proof. The essence of this claim is found in the binary
case, with a binary database and a binary query response.
We show this reduction ﬁrst.

Start by assuming that the randomized mechanism PY |Xn
satisﬁes -MI-DP. Consider an arbitrary pair of neighbor-
ing database instances xn and ˜xn, and let i be the location
where they diﬀer. Denote by ∆xn,˜xn the subset of proba-
bility distributions over the space of databases D that only
put positive mass on xn and ˜xn. Therefore, all distributions
in ∆xn,˜xn are binary, and X−i is deterministic with respect
to any of them.
Also, let A be an arbitrary measurable subset of Y. Con-

sider the indicator function

B(y) =

(cid:40)

The random variable B is the binary function B(Y ).

max

PXn∈∆xn,˜xn

I(Xi; B)

(a)≤

max

PXn∈∆xn ,˜xn

I(Xi; Y )

I(Xi; Y |X

(b)
=
≤ sup

PXn∈∆xn,˜xn

max
I(Xi; Y |X

−i)

PXn

(c)≤  nats,

−i)

(30)

where (a) is due to the data processing inequality, (b) comes
from the fact that X−i is deterministic for all distributions
in ∆xn,˜xn , and (c) is by assumption of -MI-DP.
To summarize, we have arrived at a binary input and
binary output randomized mechanism PB|Xi , where Xi ∈
{xi, ˜xi}, deﬁned by

PB|Xi=xi ({1}) = PY |Xn=xn (A),
PB|Xi=˜xi ({1}) = PY |Xn=˜xn (A).

(31)

(32)

This mechanism is shown in (30) to satisfy -MI-DP. Also,
since A, xn, and ˜xn were chosen arbitrarily, any (δ)-DP
claim that can be made about PB|Xi must also hold for
PY |Xn .

In Appendix C, we complete the proof by showing that
Lemma 2 holds for all randomized mechanisms with a binary
input and binary output, and that the characterization is
tight.

A more complete characterization is also possible, of the

-MI-DP =⇒ (cid:0)

(cid:48)

, δ

(cid:48)(cid:1)-DP,

form

for a particular set of values ((cid:48), δ(cid:48)) which, among other
things, has the property that δ(cid:48) must be greater than some
positive threshold which depends on , and as δ(cid:48) approaches
this threshold, (cid:48) must go to inﬁnity. This characterization
is also arrived at by ﬁrst reducing to the binary case as we
have done above. However, a description of the trade-oﬀ is
too unwieldy for this discussion.

We prove (18) of Theorem 1 with the following claim. The

proof is given in Appendix D.

Lemma 3

(Reverse direction). If |Xi| is ﬁnite for all

i ∈ {X1, . . . , Xn}, or if |Y| is ﬁnite, then

where, for any δ ∈ [0, 1],

(δ)-DP =⇒ 
(cid:48)

-MI-DP,

(cid:16)

(cid:110)|Y|, max

i

(cid:48)



= 2h(δ) + 2δ ln

min

|Xi| + 1

(cid:111)(cid:17)

(34)

.

(35)

Slightly tighter bounds can be found in (105) and (126) of
the proof. Although these bounds may have some looseness,
the following example shows that they get roughly within a
factor of two of the correct scaling for large cardinalities.

Example 2

(Erasure channel). Consider a database
with only one entry, X1. Let X1 = [N ] and Y = [N ] ∪ {0}.
Deﬁne

1 − δ,

δ,
0,

y = 0,
y = x1,
otherwise.

(36)

This randomized mechanism is usually referred to as an
erasure channel, where the output Y = 0 is considered an
erasure. It is known that the capacity of this channel is

C = δ log N,

(37)
where N = |X1| = |Y| − 1. This implies that there exists a
distribution of the database (in this case, the uniform distri-
bution for X1 ∈ X1) such that

I(X1; Y |X

−1) = I(X1; Y )
= δ log |X1|
= δ log(|Y| − 1).

(38)

4. PROPERTIES OF DIFF. PRIVACY

Now that we have MI-DP as an equivalent metric of pri-
vacy, we explore the insights that this brings and simple
proofs of properties about privacy.

The following are three basic and well-known properties

of mutual information:

Property 4. If U is independent of W , then

I(U ; V |W ) ≥ I(U ; V ).

(39)
Property 5. If U , V , and W form a Markov chain U −
V −W , meaning that U and W are conditionally independent
given V , then

I(U ; V |W ) ≤ I(U ; V ).
(Chain rule).

Property 6

I(U ; V, W ) = I(U ; V ) + I(U ; W|V ).

(40)

(41)

We will use these three properties (sometimes conditioned

(33)

on other random variables) to make claims about MI-DP.

474.1 The Strong Adversary Assumption

We refer to a strong adversary as one who knows the entire
database except for any one entry Xi. Diﬀerential privacy
is implicitly designed as a protection against further infor-
mation leakage to this adversary. The deﬁnition of MI-DP,
now shown to be equivalent, makes this attribute explicit by
conditioning on the remainder of the database and bounding
I(Xi; Y |X−i). But how much information does the sanitized
output Y leak to an adversary with no prior knowledge?

In [18], this is referred to as evidence of participation. In
the mutual information context, this may be measured by
the unconditional mutual information I(Xi; Y ). It is pointed
out in [18] that if the entries of the database are independent,
the evidence of participation can be protected properly by
diﬀerential privacy. This claim is straightforward using MI-
DP in light of Property 4.

Corollary 1

(Independent Data). If {Xi}n
tually independent and PY |Xn satisﬁes -MI-DP, then

i=1 are mu-

sup
P

i,PXi

X−i

I(Xi; Y ) ≤

sup
X−i
P

i,PXi

I(Xi; Y |X

−i) ≤  nats.

(42)

On the other hand, it is often the case that entries of a
database are correlated. Diﬀerential privacy does not pro-
vide a strong guarantee about the evidence of participation
in general. Consider the following familiar example:

Example 3

(Correlated database). Consider a database

with n binary entries. A data curator decides to release
the mean of all entries and chooses the Laplace mechanism.
Noise with distribution Lap( 1
n ) is added to the sample mean
to ensure -DP (also -MI-DP by Lemma 1).
Now suppose all database entries are in fact equal to each
other (maximally correlated). Let X ∼ Bern(0.5) and Xi =
X for all i ∈ {1, . . . , n}. For large enough n, the noise added
is negligible, and the binary value of the sample mean can
be estimated with high accuracy, revealing each individual
entry. In terms of mutual information, I(Xi; Y ) ≈ 1 bit for
each i even while I(Xi; Y |X−i) = 0 because H(Xi|X−i) = 0.

4.2 Composition

Among the most important properties of diﬀerential pri-
vacy is composability. This states that a collection of queries,
each satisfying diﬀerential privacy, collectively satisﬁes dif-
ferential privacy with a parameter scaled proportional to the
number of queries.

A great deal of eﬀort has been made in deriving tight
composition theorems for diﬀerential privacy. A straight-
forward composition theorem can be found in [13]. More
intricate trade-oﬀs can be found in [14] and [17], with the
latter establishing a tight characterization.

The following claims for MI-DP mirror those found in [21]

for (, 0)-DP and are in fact tight.

Corollary 2

(Conditionally independent queries).

If several query responses {Y1, . . . , Yk} are produced condi-
tionally independently given the database, and each mecha-
nism PYj|Xn satisﬁes j-MI-DP individually, then as a col-
lection PY k|Xn satisﬁes

(cid:16)(cid:80)

-MI-DP.

(cid:17)

j j

Proof. For any i and PXn , the chain rule of mutual in-
formation (Property 6) gives (a), and Property 5 gives (b):

I(Xi; Y k|X

−1)

I(Xi; Yj|X

−i, Y j−1)

I(Xi; Yj|X

−i)

j=1

(a)
=

k(cid:88)
(b)≤ k(cid:88)
≤ k(cid:88)

j=1

j nats.

(43)

j=1

Corollary 2 states that the eﬀect of releasing multiple con-
ditionally independent query responses has no more than
an additive eﬀect on the parameter of privacy. It is worth
noting two important points. First, query responses that
are not conditionally independent (i.e. the noise from one
query response is somehow reused in the next) have no such
guarantee, as the following example illustrates.

Example 4

(Correlated query responses). Consider

a database where each entry has a ﬁnite alphabet |Xi| ≤ ∞.
Consider two outputs of a query mechanism, Y1 = X1 ⊕ U
and Y2 = U , where U is a uniformly distributed random
variable on the set {1, . . . ,|X1|}, independent of the database
instance, and ⊕ is addition modulo |X1|. In other words, the
ﬁrst output Y1 is X1 encrypted by a one-time pad, and the
second output Y2 is the key to the one-time pad. Clearly, the
combination of Y1 and Y2 reveals X1 and violates diﬀerential
privacy.

On the other hand, Example 4 does not imply that corre-
lated query responses should not be considered. Quite to the
contrary, query responses that are carefully constructed to
be correlated with each other have the potential to achieve
signiﬁcantly better privacy after multiple queries, as demon-
strated in [15] and [6].

In general, the same composition claim of Corollary 2
holds even if the query responses are correlated as long as
each response in sequence is speciﬁcally designed to sat-
isfy diﬀerential privacy even with respect to the previous
responses. The following corollary states this claim, and the
proof follows directly from the proof of Corollary 2 simply
by skipping (43).

Corollary 3

(Sequential queries). If several query
responses {Y1, . . . , Yk} are produced in sequence, and each
mechanism PYj|Xn,Y j−1 satisﬁes j-MI-DP individually, then
as a collection PY k|Xn satisﬁes

(cid:16)(cid:80)

-MI-DP.

(cid:17)

j j

The next claim is about query responses that each depend

on diﬀerent subsets of the database.

Corollary 4

(Partial queries). If several query re-
sponses {Y1, . . . , Yk} are produced conditionally independently
of each other from disjoint subsets of the database entries,
denoted as XI1 , . . . , XIk , with each mechanism PYj|XIj
sat-
isfying -MI-DP individually, then as a collection PY k|Xn
also satisﬁes -MI-DP.

Proof. Let f (i) be the index j such that i ∈ Ij. For any
i and PXn , the chain rule of mutual information (Property 6)

48gives:
I(Xi; Y k|X

−1) = I(Xi; Yf (i)|X
= I(Xi; Yf (i)|X
≤  nats.

−i) + I(Xi; Y
−i)

−f (i)|X

−i, Yf (i))

(44)

5. A DISCREPANCY

While most properties of -DP or (, δ)-DP are also prop-
erties of MI-DP, it turns out that one basic property does
not carry over.

Diﬀerential privacy is deﬁned with respect to neighboring
database instances. What privacy can be guaranteed if some
bounded number of entries are changed in the database?
Similar to the composition properties, the closeness of the
output distribution scales proportionally with the number
of database changes. The following properties are obtained
by repeated application (5) and (6) from Deﬁnition 4.

Property 7

(Epsilon). Suppose xn and ˜xn are instances

of the database that diﬀer in at most k entries, and that the
randomized mechanism PY |Xn is -DP. Then

PY |Xn=xn

(k,0)≈ PY |Xn=˜xn .

(45)

Property 8

(Delta). Suppose xn and ˜xn are instances
of the database that diﬀer in at most k entries, and that the
randomized mechanism PY |Xn is (δ)-DP. Then

PY |Xn=xn

(0,kδ)≈ PY |Xn=˜xn .

(46)

Property 9

(General). Suppose xn and ˜xn are in-
stances of the database that diﬀer in at most k entries, and
that the randomized mechanism PY |Xn is (, δ)-DP. Then

(cid:19)

(cid:18)

k, ek−1
e−1 δ
≈

PY |Xn=xn

PY |Xn=˜xn .

(47)

On the other hand, MI-DP does not have an analogous
property. Even if a mechanism satisﬁes -MI-DP, there may
not be a bound on I(XI; Y |XIc ), where I represents a sub-
set of |I| = k indices. Consider the following example.

Example 5. Consider a database with two entries, X1
and X2, which are real valued. The randomized mechanism
PY |X1,X2 produces an output Y which can be a real number
or one of two special values e1 or e2. The behavior of the
mechanism is best described in two cases:

(cid:40)
(cid:40)

If X1 = X2:

If X1 (cid:54)= X2:

Y =

Y =

X1, with probability ,
e1,

with probability 1 − .

e2, with probability ,
e1, with probability 1 − .

(48)

(49)

This mechanism satisﬁes ( ln 2)-MI-DP. Notice that for
any value of X2 = x2, we have a binary erasure channel from
X1 to Y , with binary input determined by whether X1 = x2
or not. The symbol e1 serves as the erasure. The symbol e2
represents the unerased indicator that X1 (cid:54)= x2. This binary

erasure channel with erasure probability 1 −  has mutual
information bounded above by  ln 2 nats (the capacity of the
erasure channel).
On the other hand, the mutual information I(X1, X2; Y )
is unbounded if there are no constraints on X1 and X2. In-
deed, if we let X1 be a continuous random variable, and we
set X2 = X1, then

I(X1, X2; Y ) = ∞.

(50)
More generally, if the domains X1 and X2 are equal, then
the capacity of the erasure channel gives the achievable mu-
tual information (where e2 represents an additional input
symbol selected by any choice of X1 (cid:54)= X2):

max
PX1,X2

I(X1, X2; Y ) =  log(|X1| + 1)
=  log(|Y| − 1).

(51)

In fact, Example 5 might be best interpreted as a fortunate
advantage of MI-DP. With any query mechanism, there is a
trade-oﬀ between privacy and the informational utility to be
gained from the output. If we apply Property 7 with k = n,
the conclusion is that

PY |Xn=xn

(n,0)≈ PY |Xn=˜xn

(52)

for any two databases xn and ˜xn. By revisiting the proof of
Lemma 1, we obtain

I(X n; Y ) ≤ min(cid:8)n, (n)2(cid:9) nats.

(53)

One way to view this is as a crude bound on the utility of
the query output. The bound is detrimental if n is not large.
On the other hand, MI-DP does not imply such a constraint.
If, however, we take into account a cardinality bound on
the database entries or the query output, then there is indeed
an upper bound on the information leaked from a group of
database entries. This is obtained by using Property 8 in
combination with Lemma 2, followed by repeating the proof
of Lemma 3 for a group rather than an individual entry.

Corollary 5. Suppose the randomized mechanism PY |Xn
satisﬁes -MI-DP. Then for any subset of indices I, with
|I| = k, and with I c = [n] \ I,

I(XI; Y |XIc ) ≤ 2h

sup
PXn

√

(cid:16)

(cid:17)
(cid:110)|Y|, (maxi |Xi|)k + 1

k

2

+ 2k

(cid:111)

.

where M = min

√
2 log M,

(54)

6. VARIATIONS OF DIFF. PRIVACY

Many variations of diﬀerential privacy have been proposed
in the literature to provide diﬀerent assurances. Here we
demonstrate how mutual-information diﬀerential-privacy can
be adapted to correspond to these various deﬁnitions.
6.1 Personalized Differential Privacy

Personalized diﬀerential privacy [16] addresses the situa-
tion where participants of the database may have diﬀerent
concerns about the level of privacy. This is handled by as-
signing a diﬀerent i for each database entry Xi. That is,
for any database instances xn and ˜xn which diﬀer in only
the ith place,

PY |Xn=xn

(i,0)≈ PY |Xn=˜xn .

(55)

49The modiﬁcation to MI-DP would be to require that for

6.4 Adversarial Privacy

each i,

I(Xi; Y |X

−i) ≤ i nats.

(56)

sup
PXn

6.2 Free-Lunch Privacy

Free-lunch privacy was both deﬁned and refuted in [18]
as a stronger privacy deﬁnition which puts no restriction
on which database instances must be indistinguishable. A
mechanism PY |Xn is -free-lunch private if every pair of
database instances xn and ˜xn satisﬁes

PY |Xn=xn

(,0)≈ PY |Xn=˜xn .
The MI-DP equivalent of this would be
I(X n; Y ) ≤  nats.

sup
PXn

(57)

(58)

We can easily see the strength of this deﬁnition by applying
the chain rule of mutual information (Property 6) to (58).
The result is that for any pair of disjoint index sets I and
J ,

I(XI; Y |XJ ) ≤  nats.

sup
PXn

(59)

On the other hand, (58) and (59) illustrate the poor utility
provided by the -free-lunch privacy mechanism, as the in-
formation contained in the output is always upper bounded
by  regardless of distribution and prior knowledge.
6.3 Bayesian Differential Privacy

Bayesian diﬀerential privacy [26] deals with the possible
privacy degradation of diﬀerential privacy if the entries in
the database are correlated. As was discussed in Section 4.1,
a weak adversary who has less background knowledge of the
database may stand to gain much more information than the
adversary who knows all but one entry. Bayesian diﬀerential
privacy is meant to protect simultaneously against all adver-
saries, but in order to do so it assumes a prior distribution
on the database.

Given a prior distribution PXn , a mechanism PY |Xn is -
Bayesian diﬀerentially private if, for any index i and subset
of indices I,

PY |Xi=xi,XI =xI

(,0)≈ PY |Xi=˜xi,XI =xI .

(60)

Notice that the conditional distributions are not necessarily
conditioned on the entire database.
indices I,

The MI-DP equivalent is, for any index i and subset of

I(Xi; Y |XI) ≤  nats,

(61)

which is in fact implied by (60). Furthermore, this notion of
privacy can be strengthened by maximizing over database
distributions, making it a stronger notion of privacy than
diﬀerential privacy.

In spite of the additional strength of this privacy metric
(especially when removing the Bayesian prior assumption
by maximizing over the database distribution), this is not
nearly as pessimistic as free-lunch privacy. As a comparison,
the chain rule of mutual information (Property 6) in this case
implies that for any two disjoint index sets I and J ,

(62)

(63)

I(XI; Y |XJ ) ≤ |I| nats.

Consequently,

I(X n; Y ) ≤ n nats.

Adversarial privacy [23] does three things diﬀerently from
diﬀerential privacy. First, it assumes a prior distribution
PXn on the database (like Bayesian diﬀerential privacy).
Second, it does not restrict attention to neighboring database
instances (like free-lunch privacy). Third, it asymmetrically
requires

ln

dPXn|Y =y

dPXn

≤  ∀y ∈ Y.

(64)

The idea is that the adversary can not increase certainty
about a particular database value by much, even while other
database values may be eliminated.

Since mutual information is the expected value of the
quantity on the left of (64), it is clear that adversarial pri-
vacy implies

I(X n; Y ) ≤  nats.

(65)

Thus, adversarial privacy has similarity to free-lunch pri-
vacy, though in the Bayesian setting. The subtleties of the
asymmetric constraint are not captured in this MI-DP vari-
ant.
7. RÉNYI ENTROPY GENERALIZATION
The notion of α-mutual-information is the generalization
of mutual information using R´enyi information measures.
There are many proposed ways to accomplish such a gener-
alization. Here we adopt Sibson’s proposal (see [24]):

I s
α(X; Y ) = min
QY

Dα(PY |X(cid:107)QY |PX ),

(66)

where Dα is the conditional R´enyi divergence of order α and
the minimization is over all distributions QY on Y. Shan-
non’s mutual information corresponds to α = 1.
all α ≥ 0, given in the following lemma.

A simple upper bound holds for α-mutual information for

Lemma 4

α(Xi; Y |X
I s

(α-mutual-information upper bound). If
a randomized mechanism PY |Xn satisﬁes -DP, then for all
α ≥ 0, i, and instances of the remainder of the database x−i,
(67)
Proof. Let α ≥ 0, i, and PXn be arbitrary. In order to
abbreviate notation, denote the event {X−i = x−i} as U .
Pick an arbitrary xi ∈ Xi,
α(Xi; Y |U ) = min
I s

Dα(PY |Xi,U(cid:107)QY |PXi|U )

−i) ≤  nats.

−i = x

sup
PXi

QY

≤ Dα(PY |Xi,U(cid:107)PY |Xi=xi,U|PXi|U )
= Dα(PY |Xi,U PXi|U(cid:107)PY |Xi=xi,U PXi|U )
∗
(cid:21)α−1

(cid:20) dPY |Xi,U PXi|U
(cid:20) dPY |Xi,U

dPY |Xi=xi,U PXi|U

α − 1

log E

, X

(Y

=

1

∗

∗

∗

(Y

, X

)

=

1

α − 1

log E

dPY |Xi=xi,U

(cid:21)α−1

)

,

(68)

where (Y ∗, X∗) ∼ PY |Xi,U PXi|U .

For α (cid:54)= 1, the -DP constraint implies that

e for all values of Xi, thus

dPY |Xi U

dPY |Xi=xi ,U

≤

α(Xi; Y |U ) ≤ 1
I s

log E[e]α−1

α − 1
=  nats.

(69)

50For the case α = 1, the α-mutual-information reduces to
α(Xi; Y |U ) =
Shannon’s mutual information, and we have I s
I(Xi; Y |U ) ≤  from Lemma 1.

[9] A. De. Lower bounds in diﬀerential privacy. In Theory

of Cryptography, pages 321–338. Springer, 2012.

[10] J. C. Duchi, M. I. Jordan, and M. J. Wainwright.

Combining Property 7 with the proof of Lemma 4 gives

the following corollary:

Corollary 6. If the mechanism PY |Xn satisﬁes -DP,

then

α(X n; Y ) ≤ n nats.
I s

sup
PXn

(70)

Furthermore, for α > 0, when maximizing over database
distributions PXn , all three notions of α-mutual-information
discussed in [24] are equivalent. Thus,
I a
α(X n; Y ) = sup
PXn

I s
α(X n; Y ) = sup
PXn

sup
PXn

α(X n; Y ) ≤ n nats.
I c
(71)

In [3] and [4], the information leakage is deﬁned as

I∞(X n; Y ) = H∞(X n) − H∞(X n|Y )

where

H∞(X n|Y ) = − log E(cid:104)

(cid:105)

(72)

.

(73)

PXn|Y (xn|Y )

max
xn

This deﬁnition matches Arimoto’s proposal I a∞(X n; Y ), so
it is a special case of (71).

8. ACKNOWLEDGEMENTS

This work was supported by the Air Force Oﬃce of Sci-
entiﬁc Research (grant FA9550-15-1-0180) and the National
Science Foundation (grant CCF-1350595).

9. REFERENCES
[1] N. R. Adam and J. C. Worthmann. Security-control

methods for statistical databases: A comparative
study. ACM Comput. Surv., 21(4):515–556, Dec. 1989.

[2] R. Alicki and M. Fannes. Continuity of quantum

conditional information. Journal of Physics A:
Mathematical and General, 37(5):L55, 2004.

[3] M. S. Alvim, M. E. Andr´es, K. Chatzikokolakis,

P. Degano, and C. Palamidessi. Diﬀerential privacy:
on the trade-oﬀ between utility and information
leakage. In Formal Aspects of Security and Trust,
pages 39–54. Springer Berlin Heidelberg, 2012.

[4] G. Barthe and B. K¨opf. Information-theoretic bounds

for diﬀerentially private mechanisms. In 24th
Computer Security Foundations Symposium (CSF),
pages 191–204. IEEE, 2011.

[5] M. Bellare, S. Tessaro, and A. Vardy. Semantic
security for the wiretap channel. In Advances in
Cryptology–CRYPTO, pages 294–311. Springer, 2012.

Local privacy and statistical minimax rates. In 54th
Annual Symposium on Foundations of Computer
Science (FOCS), pages 429–438. IEEE, 2013.

[11] J. C. Duchi, M. I. Jordan, and M. J. Wainwright.
Privacy aware learning. J. ACM, 61(6):38:1–38:57,
Dec. 2014.

[12] C. Dwork. Diﬀerential privacy. Automata, Languages

and Programming, pages 1–12, 2006.

[13] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov,

and M. Naor. Our data, ourselves: Privacy via
distributed noise generation. In Advances in
Cryptology-EUROCRYPT, pages 486–503. Springer,
2006.

[14] C. Dwork, G. N. Rothblum, and S. Vadhan. Boosting
and diﬀerential privacy. In 51st Annual Symposium on
Foundations of Computer Science (FOCS), pages
51–60. IEEE, Oct 2010.

[15] M. Hardt and G. N. Rothblum. A multiplicative

weights mechanism for privacy-preserving data
analysis. In 51st Annual Symposium on Foundations
of Computer Science (FOCS), pages 61–70. IEEE, Oct
2010.

[16] Z. Jorgensen, T. Yu, and G. Cormode. Conservative or

liberal? personalized diﬀerential privacy. In 31st
International Conference on Data Engineering, pages
1023–1034. IEEE, April 2015.

[17] P. Kairouz, S. Oh, and P. Viswanath. The composition
theorem for diﬀerential privacy. In 32nd International
Conference on Machine Learning, 2015.

[18] D. Kifer and A. Machanavajjhala. No free lunch in

data privacy. In SIGMOD Int’l. Conference on
Management of data, pages 193–204. ACM, 2011.

[19] J. Liu, P. Cuﬀ, and S. Verd´u. Resolvability in eγ with

applications to lossy compression and wiretap
channels. In Int’l. Symp. on Information Theory
(ISIT), pages 755–759. IEEE, 2015.

[20] A. McGregor, I. Mironov, T. Pitassi, O. Reingold,
K. Talwar, and S. Vadhan. The limits of two-party
diﬀerential privacy. In 51st Annual Symposium on
Foundations of Computer Science (FOCS), pages
81–90. IEEE, 2010.

[21] F. D. McSherry. Privacy integrated queries: an
extensible platform for privacy-preserving data
analysis. In SIGMOD International Conference on
Management of data, pages 19–30. ACM, 2009.

[22] Y. Polyanskiy, H. V. Poor, and S. Verd´u. Channel
coding rate in the ﬁnite blocklength regime. IEEE
Trans. on Information Theory, 56(5):2307–2359, 2010.

[6] A. Blum, K. Ligett, and A. Roth. A learning theory

[23] V. Rastogi, M. Hay, G. Miklau, and D. Suciu.

approach to noninteractive database privacy. J. ACM,
60(2):12:1–12:25, May 2013.

[7] H. Boche, R. F. Schaefer, and H. V. Poor. On the

continuity of the secrecy capacity of compound and
arbitrarily varying wiretap channels. IEEE
Transactions on Information Forensics and Security,
10(12):2531–2546, Dec 2015.

[8] T. Cover and J. A. Thomas. Elements of information

theory. Hoboken, NJ: Wiley-Interscience, 2 edition,
2006.

Relationship privacy: output perturbation for queries
with joins. In 28th SIGMOD-SIGACT-SIGART
Symposium on Principles of Database Systems, pages
107–116. ACM, 2009.

[24] S. Verd´u. α-mutual information. In Information

Theory and Applications Workshop, 2015.

[25] W. Wang, L. Ying, and J. Zhang. On the relation

between identiﬁability, diﬀerential privacy, and
mutual-information privacy. In 52nd Annual Allerton

51Conference on Communication, Control, and
Computing (Allerton), pages 1086–1092, Sept 2014.

[26] B. Yang, I. Sato, and H. Nakagawa. Bayesian

diﬀerential privacy on correlated data. In SIGMOD
International Conference on Management of Data,
pages 747–762. ACM, 2015.

[27] Z. Zhang. Estimating mutual information via

kolmogorov distance. IEEE Transactions on
Information Theory, 53(9):3280–3282, 2007.

APPENDIX
A. PROOF OF PROPERTY 1

Assume that

As stated in (7), this gives

which is equivalent to

(,0)≈ Q.

P

(cid:12)(cid:12)(cid:12)(cid:12) ≤  ∀a ∈ Ω,
−, e(cid:3)

∀a ∈ Ω.

(a)

dP
dQ

(cid:12)(cid:12)(cid:12)(cid:12)ln
(a) ∈(cid:2)e
(cid:90)
(cid:90)

dP
dQ

Consider that

D(P(cid:107)Q) =

dP (a) ln

dP
dQ

(a)

=

dQ(a)

dP
dQ

(a) ln

dP
dQ

(a)

(cid:21)

(cid:20) dP

dQ

(X) ln

dP
dQ

(X)

,

(77)

= E

where X ∼ Q.

Let us deﬁne the random variable Z = dP

dQ (X). We know

the following facts:

Z ∈(cid:2)e

−, e(cid:3) w.p. 1,

E[Z] = 1,

D(P(cid:107)Q) = E[Z ln Z].

Since the function f (x) = x ln x for x > 0 is convex, we
know that a distribution of Z that maximizes D(P(cid:107)Q) un-
der these constraints places all mass at the endpoints of the
allowed support interval. Therefore, maximum D(P(cid:107)Q) oc-
curs with the following choice of distribution for Z:

(cid:40)

Z =

e,
e−, w.p.

w.p. 1−e−
e−e− ,
e−1
e−e− .

(81)

A computation of E[Z ln Z] gives the desired result.

This extreme is achieved by a symmetric pair of binary
distributions, consistent with the distribution of Z derived
above. Thus, coincidentally, for this choice of extreme distri-
butions that maximize D(P(cid:107)Q), it turns out that D(P(cid:107)Q) =
D(Q(cid:107)P ).

The relaxation in Property 1 can be arrived at by making

the following observation:

(e − 1)(cid:0)1 − e−(cid:1)



(e − 1) + (1 − e−)

−(cid:1)
≤ (cid:0)1 − e
≤ min(cid:8), 2(cid:9) .

(74)

(75)

(76)

(78)

(79)

(80)

Other bounds in the literature (Lemma III.2 of [14] and
Theorem 1 of [10]), while slightly loose, establish that (, 0)-
closeness implies an upper bound of roughly 2 nats of Kullback-
Leibler divergence for small , which is only oﬀ by a factor
of two.

B. PROOF OF PROPERTY 3
are ((cid:48), δ(cid:48))-close, we must show that for any A ∈ F

Assume the P and Q are (, δ)-close. To show that they

P (A) ≤ δ
Q(A) ≤ δ

(cid:48)

(cid:48)

+ e(cid:48)
+ e(cid:48)

Q(A),

P (A).

(83)

(84)

By symmetry, we need only argue (83).

We will build the proof from two inequalities. The ﬁrst is

a direction application of (5):

P (A) ≤ δ + eQ(A).

(85)

The second is an application of (6) to the complement of A,
denoted as Ac:

Q(Ac) ≤ δ + eP (Ac).

(86)
By substituting P (Ac) = 1 − P (A) and Q(Ac) = 1 − Q(A)
and rearranging, this implies

P (A) ≤ 1 − e

−(1 − δ) + e

−Q(A).

(87)

Now we complete the proof with some simple manipula-
(1−δ)

tions and by substituting the value δ(cid:48) = 1 −
stated in Property 3. From (85) we can conclude

+1
e+1

(cid:16)
e(cid:48)

(cid:17)

P (A) ≤ δ + eQ(A)

Q(A) + (δ − δ

(cid:48)

(cid:48)

+ e(cid:48)
+ e(cid:48)

= δ

= δ

Q(A) +

e − e(cid:48)(cid:17)
(cid:16)
e − e(cid:48)(cid:17)(cid:18)
(cid:16)
Q(A) − 1 − δ

Q(A)

) +

(cid:48)

e + 1

(cid:19)

.

(88)

From (87) we have
P (A) ≤ 1 − e
+ e(cid:48)
+ e(cid:48)

= δ

= δ

(cid:48)

(cid:48)

−(1 − δ) + e

Q(A) +(cid:0)1 − e

−Q(A)

Q(A) +

e(cid:48) − e

(cid:16)

(cid:48)(cid:1) +
−(cid:17)(cid:18) 1 − δ

−(1 − δ) − δ

(cid:16)

e

− − e(cid:48)(cid:17)
(cid:19)

− Q(A)

e + 1

.

(89)

Q(A)

If Q(A) ≤ 1−δ
Otherwise, (89) establishes (85).

e+1 then (88) establishes (85), since  ≥ (cid:48) ≥ 0.

C. PROOF OF LEMMA 2

According to the arguments immediately following Lemma 2,

we only need to show that the claim holds for randomized
mechanisms PY |X that have binary input and binary output.
That is, |X| = |Y| = 2.

Start by assuming that the randomized mechanism PY |X
satisﬁes -MI-DP. Since X is a database with only one entry,
-MI-DP simply means

(82)

I(X; Y ) ≤  nats.

max
PX

(90)

52Notice that the left side is the expression for channel ca-
pacity from information theory, where PY |X would be in-
terpreted as a communication channel. With this interpre-
tation, what we are trying to show is that a bound on the
channel capacity for binary channels implies a total varia-
tion bound between the conditional output distributions. It
has already been argued that binary channels contain the
extreme cases, since total variation can be expressed as an
inequality relating probabilities of a single arbitrary set (in
general, the form of (5) and (6) gives this conclusion). The
next step is to show, speciﬁcally for total variation, that
binary symmetric channels are the extreme cases.

Because X and Y are binary, the channel PY |X can be

parametrized with two parameters:

a (cid:44) P[Y = 1|X = 0],
b (cid:44) P[Y = 1|X = 1].

(91)

(92)

Now consider the complementary channel P ˜Y | ˜X , where ˜X =
X ⊕ 1 and ˜Y = Y ⊕ 1, with ⊕ representing addition modulo
2. This gives

= 1 − b,
= 1 − a.

(93)

(94)

Finally, deﬁne a new binary channel, denoted as P ˆY | ˆX , which
is a convex combination of the original channel and the com-
plementary channel. Then,

(cid:105)
(cid:105)

(cid:12)(cid:12)(cid:12) ˜X = 0
P(cid:104) ˜Y = 1
(cid:12)(cid:12)(cid:12) ˜X = 1
P(cid:104) ˜Y = 1
(cid:12)(cid:12)(cid:12) ˆX = 0
(cid:105)
(cid:12)(cid:12)(cid:12) ˆX = 1
(cid:105)
(cid:13)(cid:13)T V

=

P(cid:104) ˆY = 1
P(cid:104) ˆY = 1
(cid:13)(cid:13)PY |X=0 − PY |X=1

Notice that for all three channels, PY |X , P ˜Y | ˜X , and P ˆY | ˆX ,
the total variation between the two conditional output dis-
tributions is the same:

+

a − b
2
− a − b

2

1
2
1
2

=

=

,

.

(95)

(96)

(cid:13)(cid:13)(cid:13)P ˜Y | ˜X=0 − P ˜Y | ˜X=1
(cid:13)(cid:13)(cid:13)P ˆY | ˆX=0 − P ˆY | ˆX=1

(cid:13)(cid:13)(cid:13)T V
(cid:13)(cid:13)(cid:13)T V

=
= |a − b|.

fact an upper bound:

h(x) ≤ ln 2 − 2

(cid:18)

x − 1
2

(cid:19)2

.

(99)

An alternative simple argument directly arrives at the
looser bound in (27), without even reducing to the binary
case. We again refer to the geometric interpretation of
capacity as the radius of the information ball [8, Theo-
rem 13.1.1]. By Pinsker’s inequality (Property 2), each con-
ditional output distribution is within total variation distance
2 of the center of the information ball. The triangle in-

(cid:112) 

equality gives (27).

Assume that the randomized mechanism PY |Xn is (δ)-DP,

D. PROOF OF LEMMA 3
and let i ∈ {1, . . . , n} and PXn be arbitrary.
Two proof arguments are needed, one based on the database
entries {Xi} having a ﬁnite set of possible values, and the
other based on the same for the query response Y . In both
cases, however, we ﬁrst note that the conditional mutual in-
formation I(Xi; Y |X−i) is an expected value over instances
of X−i. We provide bounds that uniformly hold for each
instance of x−i. To that end, ﬁx x−i arbitrarily, and let
( ˜X, ˜Y ) ∼ PXi,Y |X−i=x−i . This gives,

I(Xi; Y |X

−i = x

−i) = I( ˜X; ˜Y ).

(100)

Notice further that any two databases in the set {˜xn
:
˜x−i = x−i} are neighbors according to Deﬁnition 3. There-
fore, by assumption,

P ˜Y | ˜X=˜x1

(0,δ)≈ P ˜Y | ˜X=˜x2

(101)

We now aim to bound I( ˜X; ˜Y ). Consider ﬁrst the case

for any two values ˜x1 and ˜x2.
where |Y| < ∞. By construction, | ˜Y| = |Y|.

From (101) we can claim that for any value ˜x

On the other hand, channel capacity is a convex function of
the channel parameters. By symmetry, PY |X and P ˜Y | ˜X have
the same capacity. Therefore, the convex combination P ˆY | ˆX ,
which is a binary symmetric channel, has a lower capacity.
Thus, binary symmetric channels are the extreme points in
the trade-oﬀ between capacity and total variation. For every
binary channel, there is a binary symmetric channel with
the same capacity but with greater or equal total variation
distance between the conditional output distributions.

Finally, we arrive at Lemma 2 by applying the formula
for channel capacity of a binary symmetric channel. If we
denote by δ the total variation distance between the condi-
tional output distributions, then the cross-over probability
is 1

2 . The channel capacity is then

2 − δ

C = ln 2 − h

(98)
where h(·) is the binary entropy function in nats. Inverting
this equation gives (28).

nats,

2

(cid:18) 1

(cid:19)

− δ
2

(97)

This is justiﬁed by letting X(cid:48) ∼ P ˜X and noting

(cid:13)(cid:13)(cid:13)P ˜Y | ˜X=˜x − P ˜Y

(cid:13)(cid:13)(cid:13)T V

P ˜Y | ˜X=˜x

(0,δ)≈ P ˜Y .

(cid:13)(cid:13)(cid:13)P ˜Y | ˜X=˜x − E(cid:104)
≤ E(cid:104)(cid:13)(cid:13)(cid:13)P ˜Y | ˜X=˜x − P ˜Y | ˜X=X(cid:48)

P ˜Y | ˜X=X(cid:48)

=

≤ δ,

(102)

(cid:105)(cid:13)(cid:13)(cid:13)T V
(cid:13)(cid:13)(cid:13)T V
(cid:105)

(103)

where the ﬁrst inequality is due to Jensen’s inequality and
the convexity of the total variation distance.

Next we decompose mutual information into entropy terms:

I( ˜X; ˜Y ) = H( ˜Y ) − H( ˜Y | ˜X).

(104)

Finally, a continuity property of entropy found in [27] (see
(4) within), derived from optimal coupling and Fano’s in-
equality, bounds the diﬀerence in entropy as a function of
total variation distance and | ˜Y|. Combining this with (102)
and (104) gives

h(δ) + δ ln

ln| ˜Y|,

(cid:16)| ˜Y| − 1
(cid:17)

I( ˜X; ˜Y ) ≤

,

δ ≤ | ˜Y|−1
| ˜Y|
| ˜Y|−1
δ >
| ˜Y|

,

(105)

(106)

The relaxed bound in (27) is established by the fact that
2 is in

the second order Tailor expansion of h(x) about x = 1

≤ h(δ) + δ ln| ˜Y| nats.

53Next we consider the case where maxi |Xi| < ∞. By con-

struction, | ˜X| ≤ maxi |Xi|.

For this case, we take (102) a bit further. In fact,

Let

ˆp =

−

,

µ

(118)

(119)
Then we have that p∗ is the greater part of P and Q, nor-
malized as

ˆq =

µ+.

∗

p

=

=

1

1 + δ

1

1 + δ

1
δ
1
δ

(cid:0)P + µ
−(cid:1)
(cid:0)Q + µ+(cid:1) ,
(cid:19)
(cid:18) δ
(cid:19)
(cid:18) δ

δ + 1

δ + 1

P ˜X, ˜Y

(0,δ)≈ P ˜X P ˜Y .

This is justiﬁed by letting X(cid:48) ∼ P ˜X and noting

(cid:13)(cid:13)P ˜X, ˜Y − P ˜X P ˜Y

(cid:13)(cid:13)T V

= E(cid:104)(cid:13)(cid:13)(cid:13)P ˜Y | ˜X=X(cid:48) − P ˜Y

(cid:13)(cid:13)(cid:13)T V

(107)

(cid:105)

.

(108)

This time we decompose mutual information in the reverse

direction:

I( ˜X; ˜Y ) = H( ˜X) − H( ˜X| ˜Y ).

(109)

To complete the proof we need a continuity argument for
condition entropy. The following lemma is inspired by ideas
from [7] which in turn come from [2].

Lemma 5

(Continuity of conditional entropy). If
P and Q are two distributions on U × V with |U| < ∞, then

|HP (U|V ) − HQ(U|V )| ≤ 2h

P

(0,δ)≈ Q
⇓

(cid:18) δ

δ + 1

(cid:19)

+ 2

δ

δ + 1

log |U|.

(110)

Proof. Since the bound in the lemma is monotonic in δ,

we assume without loss of generality that

(cid:107)P − Q(cid:107)T V = δ.

(111)

We ﬁrst translate closeness in total variation distance to
the existence of a common distribution that is close to both
relative to the boundaries of the set of probability distribu-
tions. To be more precise, there exists a probability distri-
bution p∗ which is a convex combination of P and another
probability distribution, with most of the convex weight on
P , and the same relationship holds between p∗ and Q. That
is:

∗

p

=

=

1

1 + δ

1

1 + δ

P +

Q +

δ

1 + δ

δ

1 + δ

ˆp

ˆq.

(112)

(113)

Once we have established this existence, the exact construc-
tion of p∗, ˆp, and ˆq will have no consequence on the conclu-
sion.
Consider the Hahn decomposition of the signed measure
P − Q into positive and negative parts that are mutually
singular, represented by the non-negative measures µ+ and
µ−, as follows:

P − Q = µ+ − µ

µ+ ≥ 0,
− ≥ 0,
µ
µ+ ⊥ µ
−

−

,

(114)

(115)

(116)

.

(117)
The total measure of each part, µ+ and µ−, is the total
variation between P and Q, which is δ. Thus, to normalize
µ+ and µ− to become probability measures, we must divide
by δ.

which satisﬁes both (112) and (113).

Next, to complete the proof, we show that
|HP (U|V ) − Hp∗ (U|V )| ≤ h

+

δ

δ + 1

(120)

(121)

log |U|,

(122)

|HQ(U|V ) − Hp∗ (U|V )| ≤ h

+

δ

δ + 1

log |U|.

(123)

(cid:16) δ

(cid:17)

By symmetry, an argument for only one of the inequalities
is needed.

The following bound is aided by deﬁning a binary ran-
from which we construct
U,V |B=1 = ˆp. This has no eﬀect on

dom variable B ∼ Bern
U,V |B=0 = P and p∗
p∗
the marginal distribution of p∗
Hp∗ (U|V ) = Hp∗ (U|V, B) + Ip∗ (U ; B|V )

U,V . We have

1+δ

=

1

1 + δ

HP (U|V ) +

δ

1 + δ

H ˆp(U|V ) + Ip∗ (U ; B|V ).
(124)

Subtracting HP (U|V ) from both sides gives

Hp∗ (U|V ) − HP (U|V )

=

δ

1 + δ

(H ˆp(U|V ) − Hp∗ (U|V )) + Ip∗ (U ; B|V ).

(125)

Finally, the argument is completed by bounding the three
non-negative terms. The entropy terms are bounded by
log |U|. For the conditional mutual information, Ip∗ (U ; B|V ) ≤
Hp∗ (B) = h

(cid:16) δ

(cid:17)

.

δ+1

The proof of Lemma 3 is completed by applying Lemma 5
with P ˜X, ˜Y as P and P ˜X P ˜Y as Q, due to (107). Combined
with (109) this gives
I( ˜X; ˜Y ) ≤ 2h

(cid:18) δ

ln| ˜X|

(cid:19)

(126)

+ 2

δ + 1

≤ 2h(δ) + 2δ ln

δ

(cid:16)| ˜X| + 1

δ + 1

(cid:17)

nats.

(127)

54