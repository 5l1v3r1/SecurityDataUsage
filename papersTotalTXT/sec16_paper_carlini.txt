Hidden Voice Commands

Nicholas Carlini and Pratyush Mishra, University of California, Berkeley; Tavish Vaidya, 
Yuankai Zhang, Micah Sherr, and Clay Shields, Georgetown University; David Wagner, 

University of California, Berkeley; Wenchao Zhou, Georgetown University

 https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/carlini

This paper is included in the Proceedings of the 25th USENIX Security SymposiumAugust 10–12, 2016 • Austin, TXISBN 978-1-931971-32-4Open access to the Proceedings of the 25th USENIX Security Symposium is sponsored by USENIX Hidden Voice Commands

Nicholas Carlini∗

Pratyush Mishra

University of California, Berkeley

University of California, Berkeley

Tavish Vaidya

Yuankai Zhang

Micah Sherr

Georgetown University

Georgetown University

Georgetown University

Clay Shields

David Wagner

Wenchao Zhou

Georgetown University

University of California, Berkeley

Georgetown University

Abstract

Voice interfaces are becoming more ubiquitous and are
now the primary input method for many devices. We ex-
plore in this paper how they can be attacked with hidden
voice commands that are unintelligible to human listen-
ers but which are interpreted as commands by devices.

We evaluate these attacks under two different threat
models.
In the black-box model, an attacker uses the
speech recognition system as an opaque oracle. We show
that the adversary can produce difﬁcult to understand
commands that are effective against existing systems in
the black-box model. Under the white-box model, the
attacker has full knowledge of the internals of the speech
recognition system and uses it to create attack commands
that we demonstrate through user testing are not under-
standable by humans.

We then evaluate several defenses, including notify-
ing the user when a voice command is accepted; a verbal
challenge-response protocol; and a machine learning ap-
proach that can detect our attacks with 99.8% accuracy.

1

Introduction

Voice interfaces to computer systems are becoming ubiq-
uitous, driven in part by their ease of use and in part by
decreases in the size of modern mobile and wearable de-
vices that make physical interaction difﬁcult. Many de-
vices have adopted an always-on model in which they
continuously listen for possible voice input. While voice
interfaces allow for increased accessibility and poten-
tially easier human-computer interaction, they are at the
same time susceptible to attacks: Voice is a broadcast
channel open to any attacker that is able to create sound
within the vicinity of a device. This introduces an op-
portunity for attackers to try to issue unauthorized voice
commands to these devices.

An attacker may issue voice commands to any device
that is within speaker range. However, na¨ıve attacks will
be conspicuous: a device owner who overhears such a

∗Authors listed alphabetically, with student authors appearing be-

fore faculty authors.

command may recognize it as an unwanted command
and cancel it, or otherwise take action. This motivates
the question we study in this paper: can an attacker cre-
ate hidden voice commands, i.e., commands that will be
executed by the device but which won’t be understood
(or perhaps even noticed) by the human user?

The severity of a hidden voice command depends upon
what commands the targeted device will accept. De-
pending upon the device, attacks could lead to informa-
tion leakage (e.g., posting the user’s location on Twitter),
cause denial of service (e.g., activating airplane mode),
or serve as a stepping stone for further attacks (e.g.,
opening a web page hosting drive-by malware). Hid-
den voice commands may also be broadcast from a loud-
speaker at an event or embedded in a trending YouTube
video, compounding the reach of a single attack.

Vaidya et al. [41] showed that hidden voice commands
are possible—attackers can generate commands that are
recognized by mobile devices but are considered as noise
by humans. Building on their work, we show more pow-
erful attacks and then introduce and analyze a number of
candidate defenses.

The contributions of this paper include the following:
• We show that hidden voice commands can be con-
structed even with very little knowledge about the
speech recognition system. We provide a general
attack procedure for generating commands that are
likely to work with any modern voice recognition
system. We show that our attacks work against
Google Now’s speech recognition system and that
they improve signiﬁcantly on previous work [41].

• We show that adversaries with signiﬁcant knowl-
edge of the speech recognition system can construct
hidden voice commands that humans cannot under-
stand at all.

• Finally, we propose, analyze, and evaluate a suite
of detection and mitigation strategies that limit the
effects of the above attacks.

Audio ﬁles for the hidden voice commands and
a video demonstration of the attack are available at
http://hiddenvoicecommands.com.

USENIX Association  

25th USENIX Security Symposium  513

1

(cid:94)(cid:393)(cid:286)(cid:286)(cid:272)(cid:346)(cid:3)(cid:396)(cid:286)(cid:272)(cid:381)(cid:336)(cid:374)(cid:349)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:400)(cid:455)(cid:400)(cid:410)(cid:286)(cid:373)

(cid:47)(cid:374)(cid:393)(cid:437)(cid:410)(cid:3)
(cid:258)(cid:437)(cid:282)(cid:349)(cid:381)(cid:3)
(cid:400)(cid:349)(cid:336)(cid:374)(cid:258)(cid:367)

(cid:100)(cid:286)(cid:454)(cid:410)(cid:3)
(cid:381)(cid:437)(cid:410)(cid:393)(cid:437)(cid:410)

(cid:87)(cid:396)(cid:286)(cid:882)(cid:393)(cid:396)(cid:381)(cid:272)(cid:286)(cid:400)(cid:400)(cid:349)(cid:374)(cid:336)

(cid:87)(cid:381)(cid:400)(cid:410)(cid:882)(cid:393)(cid:396)(cid:381)(cid:272)(cid:286)(cid:400)(cid:400)(cid:349)(cid:374)(cid:336)

(cid:38)(cid:349)(cid:367)(cid:410)(cid:286)(cid:396)(cid:286)(cid:282)(cid:3)

(cid:258)(cid:437)(cid:282)(cid:349)(cid:381)(cid:3)(cid:400)(cid:349)(cid:336)(cid:374)(cid:258)(cid:367)

(cid:100)(cid:286)(cid:454)(cid:410)(cid:3)

(cid:393)(cid:396)(cid:286)(cid:282)(cid:349)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:38)(cid:286)(cid:258)(cid:410)(cid:437)(cid:396)(cid:286)(cid:3)
(cid:28)(cid:454)(cid:410)(cid:396)(cid:258)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)

(cid:4)(cid:437)(cid:282)(cid:349)(cid:381)(cid:3)
(cid:296)(cid:286)(cid:258)(cid:410)(cid:437)(cid:396)(cid:286)(cid:400)

(cid:68)(cid:381)(cid:282)(cid:286)(cid:367)(cid:882)(cid:271)(cid:258)(cid:400)(cid:286)(cid:282)(cid:3)
(cid:87)(cid:396)(cid:286)(cid:282)(cid:349)(cid:272)(cid:410)(cid:349)(cid:381)(cid:374)

Figure 1: Overview of a typical speech recognition system.

2 Background and Related Work

To set the stage for the attacks that we present in §3 and
§4, we brieﬂy review how speech recognition works.

Figure 1 presents a high-level overview of a typi-
cal speech recognition procedure, which consists of the
following four steps: pre-processing, feature extrac-
tion, model-based prediction, and post-processing. Pre-
processing performs initial speech/non-speech identiﬁ-
cation by ﬁltering out frequencies that are beyond the
range of a human voice and eliminating time periods
where the signal energy falls below a particular thresh-
old. This step only does rudimentary ﬁltering, but still al-
lows non-speech signals to pass through the ﬁlter if they
pass the energy-level and frequency checks.

The second step, feature extraction, splits the ﬁltered
audio signal into short (usually around 20 ms) frames and
extracts features from each frame. The feature extraction
algorithm used in speech recognition is almost always
the Mel-frequency cepstral (MFC) transform [20, 42].
We describe the MFC transform in detail in Appendix A,
but at a high level it can be thought of as a transformation
that extracts the dominant frequencies from the input.

The model-based prediction step takes as input the ex-
tracted features, and matches them against an existing
model built ofﬂine to generate text predictions. The tech-
nique used in this step can vary widely: some systems
use Hidden Markov Models, while many recent systems
have begun to use recurrent neural networks (RNNs).

Finally, a post-processing step ranks the text predic-
tions by employing additional sources of information,
such as grammar rules or locality of words.

Unauthorized voice commands have
Related work.
been studied by Diao et al. [12] and Jang et al. [21] who
demonstrate that malicious apps can inject synthetic au-
dio or play commands to control smartphones. Unlike in
this paper, these attacks use non-hidden channels that are
understandable by a human listener.

Similar to our work, Kasmi and Lopes Esteves [23]
consider the problem of covert audio commands. There,
the authors inject voice commands by transmitting FM
signals that are received by a headset. In our work, we
do not require the device to have an FM antenna (which
is not often present) and we obfuscate the voice com-

mand so that it is not human-recognizable. Schlegel et al.
[36] show that malicious apps can eavesdrop and record
phone calls to extract sensitive information. Our work
differs in that it exploits targeted devices’ existing func-
tionality (i.e., speech recognition) and does not require
the installation of malicious apps.

Earlier work by Vaidya et al. [41] introduces obfus-
cated voice commands that are accepted by voice inter-
faces. Our work signiﬁcantly extends their black-box
approach by (i) evaluating the effectiveness of their
attacks under realistic scenarios, (ii) introducing more
effective “white-box” attacks that leverage knowledge
of the speech recognition system to produce machine-
understandable speech that is almost never recognized by
humans, (iii) formalizing the method of creating hidden
voice commands, and (iv) proposing and evaluating de-
fenses.

Image recognition systems have been shown to be vul-
nerable to attacks where slight modiﬁcations to only a
few pixels can change the resulting classiﬁcation dramat-
ically [17, 19, 25, 38]. Our work has two key differences.
First, feature extraction for speech recognition is signif-
icantly more complex than for images; this is one of the
main hurdles for our work. Second, attacks on image
recognition have focused on the case where the adversary
is allowed to directly modify the electronic image.
In
contrast, our attacks work “over the air”; that is, we cre-
ate audio that when played and recorded is recognized as
speech. The analogous attack on image recognition sys-
tems would be to create a physical object which appears
benign, but when photographed, is classiﬁed incorrectly.
As far as we know, no one has demonstrated such an at-
tack on image recognition systems.

More generally, our attacks can be framed as an eva-
sion attack against machine learning classiﬁers: if f is
a classiﬁer and A is a set of acceptable inputs, given a
desired class y, the goal is to ﬁnd an input x ∈ A such
that f (x) =y. In our context,
f is the speech recognition
system, A is a set of audio inputs that a human would not
recognize as speech, and y is the text of the desired com-
mand. Attacks on machine learning have been studied
extensively in other contexts [1, 4, 5, 10, 13, 22, 31, 40];
In particular, Fawzi et al. [14] develop a rigorous frame-
work to analyze the vulnerability of various types of clas-
siﬁers to adversarial perturbation of inputs. They demon-
strate that a minimal set of adversarial changes to input
data is enough to fool most classiﬁers into misclassify-
ing the input. Our work is different in two key respects:
(i) the above caveats for image recognition systems still
apply, and moreover, (ii) their work does not necessarily
aim to create inputs that are misclassiﬁed into a partic-
ular category; but rather that it is just misclassiﬁed. On
the other hand, we aim to craft inputs that are recognized
as potentially sensitive commands.

514  25th USENIX Security Symposium 

USENIX Association

2

Finally, Fredrikson et al. [15] attempt to invert ma-
chine learning models to learn private and potentially
sensitive data in the training corpus. They formulate their
task as an optimization problem, similar to our white-box
approach, but they (i) test their approach primarily on im-
age recognition models, which, as noted above, are easier
to fool, and (ii) do not aim to generate adversarial inputs,
but rather only extract information about individual data
points.

3 Black-box Attacks

We ﬁrst show that under a weak set of assumptions an at-
tacker with no internal knowledge of a voice recognition
system can generate hidden voice commands that are dif-
ﬁcult for human listeners to understand. We refer to these
as obfuscated commands, in contrast to unmodiﬁed and
understandable normal commands.

These attacks were ﬁrst proposed by Vaidya et al. [41].
This section improves upon the efﬁcacy and practicality
of their attacks and analysis by (i) carrying out and test-
ing the performance of the attacks under more practical
settings, (ii) considering the effects of background noise,
and (iii) running the experiments against Google’s im-
proved speech recognition service [34].

3.1 Threat model & attacker assumptions

In this black-box model the adversary does not know
the speciﬁc algorithms used by the speech recognition
system. We assume that the system extracts acoustic
information through some transform function such as
an MFC, perhaps after performing some pre-processing
such as identifying segments containing human speech
or removing noise. MFCs are commonly used in current-
generation speech recognition systems [20, 42], making
our results widely applicable, but not limited to such sys-
tems.

We treat the speech recognition system as an oracle to
which the adversary can pose transcription tasks. The ad-
versary can thus learn how a particular obfuscated audio
signal is interpreted. We do not assume that a particular
transcription is guaranteed to be consistent in the future.
This allows us to consider speech recognition systems
that apply randomized algorithms as well as to account
for transient effects such as background noise and envi-
ronmental interference.

Conceptually, this model allows the adversary to iter-
atively develop obfuscated commands that are increas-
ingly difﬁcult for humans to recognize while ensuring,
with some probability, that they will be correctly inter-
preted by a machine. This trial-and-error process occurs
in advance of any attack and is invisible to the victim.

Normal
command

Obfuscated
command

MFCC

parameters

Feature
Extraction

1

7
yes

Recognized 
by human 
attacker?

8

no

Acoustic
features 

2

Inverse MFCC

Candidate

obfuscated command

3

4

5

no

Recognized 
by machine?

6

yes

Attacker

Speech

recognition system

Figure 2: Adversary’s workﬂow for producing an obfuscated
audio command from a normal command.

3.2 Overview of approach

We rerun the black-box attack proposed by Vaidya et
al. [41] as shown in Figure 2. The attacker’s goal is to
produce an obfuscated command that is accepted by the
victim’s speech recognition system but is indecipherable
by a human listener.

The attacker ﬁrst produces a normal command that
it wants executed on the targeted device. To thwart
individual recognition the attacker may use a text-to-
speech engine, which we found is generally correctly
transcribed. This command is then provided as input
(Figure 2, step ) to an audio mangler, shown as the grey
box in the ﬁgure. The audio mangler performs an MFC
with a starting set of parameters on the input audio, and
then performs an inverse MFC (step ) that additionally
adds noise to the output. By performing the MFC and
then inverting the obtained acoustic features back into an
audio sample, the attacker is in essence attempting to re-
move all audio features that are not used in the speech
recognition system but which a human listener might use
for comprehension.

Since the attacker does not know the MFC features
used by the speech recognition system, experimentation
is required. First, the attacker provides the candidate
obfuscated audio that results from the MFC→inverse-
MFC process (step ) to the speech recognition system
(step ). If the command is not recognized then the at-
tacker must update the MFC parameters to ensure that
the result of the MFC→inverse-MFC transformation will
yield higher ﬁdelity audio (step ).

If the candidate obfuscated audio is interpreted cor-
rectly (step ), then the human attacker tests if it is hu-
man understandable. This step is clearly subjective and,
worse, is subject to priming effects [28] since the at-
tacker already knows the correct transcription. The at-
tacker may solicit outside opinions by crowdsourcing.
If the obfuscated audio is too easily understood by hu-
mans the attacker discards the candidate and generates
new candidates by adjusting the MFC parameters to pro-
duce lower ﬁdelity audio (step ). Otherwise, the can-

USENIX Association  

25th USENIX Security Symposium  515

3

Table 1: MFC parameters tuned to produce obfuscated audio.

3.4 Evaluation

Parameter

Description

wintime
hoptime
numcep
nbands

time for which the signal is considered constant
time step between adjacent windows
number of cepstral coefﬁcients
no. of warped spectral bands for aggregating energy levels

didate obfuscated audio command—which is recognized
by machines but not by humans—is used to conduct the
actual attack (step ).

3.3 Experimental setup

We obtained the audio mangling program used by
Vaidya et al. [41]. Conforming to their approach, we also
manually tune four MFC parameters to mangle and test
audio using the workﬂow described in §3.2 to determine
the ranges for human and machine perception of voice
commands. The list of modiﬁed MFC parameters is pre-
sented in Table 1.

Our voice commands consisted of the phrases “OK
google”, “call 911”, and “turn on airplane mode”. These
commands were chosen to represent a variety of po-
tential attacks against personal digital assistants. Voice
commands were played using Harmon Kardon speakers,
model number HK695–01,13, in a conference room mea-
suring approximately 12 by 6 meters, 2.5 meters tall.
Speakers were on a table approximately three meters
from the phones. The room contained ofﬁce furniture
and projection equipment. We measured a background
noise level (Pnoise

dB ) of approximately 53 dB.

We tested the commands against two smart phones, a
Samsung Galaxy S4 running Android 4.4.2 and Apple
iPhone 6 running iOS 9.1 with Google Now app ver-
sion 9.0.60246. Google’s recently updated [34] default
speech recognition system was used to interpret the com-
mands. In the absence of injected ambient background
noise, our sound level meter positioned next to the smart-
phones measured the median intensity of the voice com-
mands to be approximately 88 dB.

We also projected various background noise samples
collected from SoundBible [9], recorded from a casino,
classroom, shopping mall, and an event during which ap-
plause occurred. We varied the volume of these back-
ground noises—thus artiﬁcially adjusting the signal-to-
noise ratio—and played them through eight overhead
JBL in-ceiling speakers. We placed a Kinobo “Akiro”
table mic next to our test devices and recorded all audio
commands that we played to the devices for use in later
experiments, described below.

We found that the phone’s speech
Attack range.
recognition system failed to identify speech when the
speaker was located more than 3.5 meters away or when
the perceived SNR was less than 5 dB. We conjecture
that the speech recognition system is designed to discard
far away noises, and that sound attenuation further limits
the attacker’s possible range. While the attacker’s local-
ity is clearly a limitation of this approach, there are many
attack vectors that allow the attacker to launch attacks
within a few meters of the targeted device, such as obfus-
cated audio commands embedded in streaming videos,
overhead speakers in ofﬁces, elevators, or other enclosed
spaces, and propagation from other nearby phones.

Table 2 shows a side-by-
Machine understanding.
side comparison of human and machine understanding,
for both normal and obfuscated commands.

The “machine” columns indicate the percentage of
trials in which a command is correctly interpreted by
the phone, averaged over the various background noises.
Here, our sound meter measured the signal’s median au-
dio level at 88 dB and the background noise at 73 dB,
corresponding to a signal-to-noise ratio of 15 dB.

Across all three commands, the phones correctly inter-
preted the normal versions 85% of the time. This accu-
racy decreased to 60% for obfuscated commands.

We also evaluate how the amplitude of background
noise affects machine understanding of the commands.
Figure 3 shows the percentage of voice commands that
are correctly interpreted by the phones (“success rate”)
as a function of the SNR (in dB) using the Mall back-
ground noise. Note that a higher SNR denotes more
favorable conditions for speech recognition. Generally,
Google’s speech recognition engine correctly transcribes
the voice commands and activates the phone. The ac-
curacy is higher for normal commands than obfuscated
commands, with accuracy improving as SNR increases.
In all cases, the speech recognition system is able to per-
fectly understand and activate the phone functionality in
at least some conﬁgurations—that is, all of our obfus-
cated audio commands work at least some of the time.
With little background noise, the obfuscated commands
work extremely well and are often correctly transcribed
at least 80% of the time. Appendix B shows detailed re-
sults for additional background noises.

To test human understand-
Human understanding.
ing of the obfuscated voice commands, we conducted a
study on Amazon Mechanical Turk1, a service that pays

1Note on ethics: Before conducting our Amazon Mechanical Turk
experiments, we submitted an online application to our institution’s
IRB. The IRB responded by stating that we were exempt from IRB.
Irrespective of our IRB, we believe our experiments fall well within the

516  25th USENIX Security Symposium 

USENIX Association

4

Table 2: Black-box attack results. The “machine” columns report the percentage of commands that were correctly interpreted by
the tested smartphones. The percentage of commands that were correctly understood by humans (Amazon Turk workers) is shown
under the “human” columns. For the latter, the authors assessed whether the Turk workers correctly understood the commands.

Ok Google

Human

Normal
Obfuscated

Machine

90% (36/40)
95% (38/40)

Turn on airplane mode
Human

Machine

89% (356/400)
22% (86/376)

75% (30/40)
45% (18/40)

69% (315/456)
24% (109/444)

Call 911

Machine

90% (36/40)
40% (16/40)

Human

87% (283/324)
94% (246/260)

Figure 3: Machine understanding of normal and obfuscated variants of “OK Google”, “Turn on Airplane Mode”, and “Call 911”
voice commands under Mall background noise. Each graph shows the measured average success rate (the fraction of correct
transcripts) on the y-axis as a function of the signal-to-noise ratio.

human workers to complete online tasks called Human
Intelligence Tasks (HITs). Each HIT asks a user to tran-
scribe several audio samples, and presents the following
instructions: “We are conducting an academic study that
explores the limits of how well humans can understand
obfuscated audio of human speech. The audio ﬁles for
this task may have been algorithmically modiﬁed and
may be difﬁcult to understand. Please supply your best
guess to what is being said in the recordings.”

We constructed the online tasks to minimize priming
effects—no worker was presented with both the normal
and obfuscated variants of the same command. Due to
this structuring, the number of completed tasks varies
among the commands as reﬂected in Table 2 under the
“human” columns.

We additionally required that workers be over 18 years
of age, citizens of the United States, and non-employees
of our institution. Mechanical Turk workers were paid
$1.80 for completing a HIT, and awarded an additional
$0.20 for each correct transcription. We could not pre-
vent the workers from replaying the audio samples mul-
tiple times on their computers and the workers were in-
centivized to do so, thus our results could be considered
conservative: if the attacks were mounted in practice, de-
vice owners might only be able to hear an attack once.

basic principles of ethical research. With respect in particular to benef-
icence, the Mechanical Turk workers beneﬁted from their involvement
(by being compensated). The costs/risks were extremely low: workers
were fully informed of their task and no subterfuge occurred. No per-
sonal information—either personally identiﬁable or otherwise—was
collected and the audio samples consisted solely of innocuous speech
that is very unlikely to offend (e.g., commands such as “OK Google”).

To assess how well the Turk workers understood nor-
mal and obfuscated commands, four of the authors com-
pared the workers’ transcriptions to the correct transcrip-
tions (e.g., “OK Google”) and evaluated whether both
had the same meaning. Our goal was not to assess
whether the workers correctly heard the obfuscated com-
mand, but more conservatively, whether their perception
conformed with the command’s meaning. For example,
the transcript “activate airplane functionality” indicates a
failed attack even though the transcription differs signif-
icantly from the baseline of “turn on airplane mode”.

Values shown under the “human” columns in Table 2
indicate the fraction of total transcriptions for which the
survey takers believed that the Turk worker understood
the command. Each pair of authors had an agreement
of over 95% in their responses, the discrepancies being
mainly due to about 5% of responses in which one survey
taker believed they matched but the others did not. The
survey takers were presented only with the actual phrase
and transcribed text, and were blind to whether or not the
phrase was an obfuscated command or not.

Turk workers were fairly adept (although not perfect)
at transcribing normal audio commands: across all com-
mands, we assessed 81% of the Turkers’ transcripts to
convey the same meaning as the actual command.

The workers’ ability to understand obfuscated audio
was considerably less: only about 41% of obfuscated
commands were labeled as having the same meaning
as the actual command. An interesting result is that
the black-box attack performed far better for some com-
mands than others. For the “Ok Google” command, we

USENIX Association  

25th USENIX Security Symposium  517

5

decreased human transcription accuracy fourfold without
any loss in machine understanding.

“Call 911” shows an anomaly: human understand-
ing increases for obfuscated commands. This is due to
a tricky part of the black-box attack workﬂow: the at-
tacker must manage priming effects when choosing an
obfuscated command. In this case, we believed the “call
911” candidate command to be unintelligible; these re-
sults show we were wrong. A better approach would
have been to repeat several rounds of crowdsourcing to
identify a candidate that was not understandable; any at-
tacker could do this. It is also possible that among our
US reviewers, “call 911” is a common phrase and that
they were primed to recognize it outside our study.

The
Objective measures of human understanding:
analysis above is based on the authors’ assessment of
Turk workers’ transcripts.
In Appendix C, we present
a more objective analysis using the Levenshtein edit dis-
tance between the true transcript and the Turkers’ tran-
scripts, with phonemes as the underlying alphabet.

We posit that our (admittedly subjective) assessment is
more conservative, as it directly addresses human under-
standing and considers attacks to fail if a human under-
stands the meaning of a command; in contrast, compar-
ing phonemes measures something slightly different—
whether a human is able to reconstruct the sounds of
an obfuscated command—and does not directly capture
understanding. Regardless, the phoneme-based results
from Appendix C largely agree with those presented
above.

4 White-box Attacks

We next consider an attacker who has knowledge of the
underlying voice recognition system. To demonstrate
this attack, we construct hidden voice commands that
are accepted by the open-source CMU Sphinx speech
recognition system [24]. CMU Sphinx is used for speech
recognition by a number of apps and platforms2, mak-
ing it likely that these whitebox attacks are also practical
against these applications.

The purpose of the MFC transformation is to take a
high-dimensional input space—raw audio samples—and
reduce its dimensionality to something which a machine
learning algorithm can better handle. This is done in two
steps. First, the audio is split into overlapping frames.

Once the audio has been split into frames, we run the
MFC transformation on each frame. The Mel-Frequency
Cepstrum Coefﬁcients (MFCC) are the 13-dimensional
values returned by the MFC transform.

After the MFC is computed, Sphinx performs two fur-
ther steps. First, Sphinx maintains a running average of
each of the 13 coordinates and subtracts off the mean
from the current terms. This normalizes for effects such
as changes in amplitude or shifts in pitch.

Second, Sphinx numerically estimates

the ﬁrst
and second derivatives of
this sequence to create
a 39-dimensional vector containing the original 13-
dimensional vector, the 13-dimensional ﬁrst-derivative
vector, and the 13-dimensional-second derivative vector.
Note on terminology: For ease of exposition and clar-
ity, in the remainder of this section, we call the output
of the MFCC function 13-vectors, and refer to the output
after taking derivatives as 39-vectors.

The Sphinx HMM acts
The Hidden Markov Model.
on the sequence of 39-vectors from the MFCC. States in
the HMM correspond to phonemes, and each 39-vector
is assigned a probability of arising from a given phoneme
by a Gaussian model, described next. The Sphinx HMM
is, in practice, much more intricate: we give the complete
description in Appendix A.

Each HMM state
The Gaussian Mixture Model.
yields some distribution on the 39-vectors that could be
emitted while in that state. Sphinx uses a GMM to repre-
sent this distribution. The GMMs in Sphinx are a mixture
of eight Gaussians, each over R39. Each Gaussian has a
mean and standard deviation over every dimension. The
probability of a 39-vector v is the sum of the probabili-
ties from each of the 8 Gaussians, divided by 8. For most
cases we can approximate the sum with a maximization,
as the Gaussians typically have little overlap.

4.1 Overview of CMU Sphinx

4.2 Threat model

CMU Sphinx uses the Mel-Frequency Cepstrum (MFC)
transformation to reduce the audio input to a smaller di-
mensional space. It then uses a Gaussian Mixture Model
(GMM) to compute the probabilities that any given piece
of audio corresponds to a given phoneme. Finally, using
a Hidden Markov Model (HMM), Sphinx converts the
phoneme probabilities to words.

We assume the attacker has complete knowledge of the
algorithms used in the system and can interact with them
at will while creating an attack. We also assume the at-
tacker knows the parameters used in each algorithm. 3

We use knowledge of the coefﬁcients for each Gaus-
sian in the GMM, including the mean and standard de-
viation for each dimension and the importance of each

2Systems that use CMU Sphinx speech recognition include the
Jasper open-source personal digital assistant and Gnome Desktop voice
commands. The Sphinx Project maintains a list of software that uses
Sphinx at http://cmusphinx.sourceforge.net/wiki/sphinxinaction.

3Papernot et al. [32] demonstrated that it is often possible to trans-
form a white-box attack into a black-box attack by using the black-box
as an oracle and reconstructing the model and using the reconstructed
paramaters.

518  25th USENIX Security Symposium 

USENIX Association

6

Gaussian. We also use knowledge of the dictionary ﬁle
in order to turn words into phonemes. An attacker could
reconstruct this ﬁle without much effort.

4.3 Simple approach

Given this additional information, a ﬁrst possible attack
would be to use the additional information about exactly
what the MFCC coefﬁcients are to re-mount the the pre-
vious black-box attack.

Instead of using the MFCC inversion process de-
scribed in §3.2, this time we implement it using gradient
descent—a generic optimization approach for ﬁnding a
good solution over a given space—an approach which
can be generalized to arbitrary objective functions.

Gradient descent attempts to ﬁnd the minimum (or
maximum) value of an objective function over a multi-
dimensional space by starting from an initial point and
traveling in the direction which reduces the objective
most quickly. Formally, given a smooth function f , gra-
dient descent picks an initial point x0 and then repeat-
edly improves on it by setting xi+1 = xi + ε · ∇ f (x0) (for
some small ε) until we have a solution which is “good
enough”.

We deﬁne the objective function f (x) = (MFCC(x) −
y)2 · z, where x is the input frame, y is the target MFCC
vector, and z is the relative importance of each dimen-
sion. Setting z = (1, 1, . . . ,1) takes the L2 norm as the
objective.

Gradient descent is not guaranteed to ﬁnd the global
optimal value. For many problems it ﬁnds only a local
optimum. Indeed, in our experiments we have found that
gradient descent only ﬁnds local optima, but this turns
out to be sufﬁcient for our purposes.

We perform gradient descent search one frame at a
time, working our way from the ﬁrst frame to the last.
For the ﬁrst frame, we allow gradient descent to pick any
410 samples. For subsequent frames, we ﬁx the ﬁrst 250
samples as the last 250 of the preceding frame, and run
gradient descent to ﬁnd the best 160 samples for the rest
of the frame.

As it turns out, when we implement this attack, our
results are no better than the previous black-box-only at-
tack. Below we describe our improvements to make at-
tacks completely unrecognizable.

4.4

Improved attack

To construct hidden voice commands that are more difﬁ-
cult for humans to understand, we introduce two reﬁne-
ments. First, rather than targeting a speciﬁc sequence of
MFCC vectors, we start with the target phrase we wish
to produce, derive a sequence of phonemes and thus a se-
quence of HMM states, and attempt to ﬁnd an input that
matches that sequence of HMM states. This provides

more freedom by allowing the attack to create an input
that yields the same sequence of phonemes but generates
a different sequence of MFCC vectors.

Second, to make the attacks difﬁcult to understand,
we use as few frames per phoneme as possible. In nor-
mal human speech, each phoneme might last for a dozen
frames or so. We try to generate synthetic speech that
uses only four frames per phoneme (a minimum of three
is possible—one for each HMM state). The intuition is
that the HMM is relatively insensitive to the number of
times each HMM state is repeated, but humans are sen-
sitive to it. If Sphinx does not recognize the phrase at the
end of this process, we use more frames per phoneme.

For each target HMM state, we pick one Gaussian
from that state’s GMM. This gives us a sequence of target
Gaussians, each with a mean and standard deviation.

Recall that the MFC transformation as we deﬁned it
returns a 13-dimensional vector. However, there is a sec-
ond step which takes sequential derivatives of 13-vectors
to produce 39-vectors. The second step of our attack is
to pick these 13-vectors so that after we take the deriva-
tives, we maximize the likelihood score the GMM as-
signs to the resulting 39-vector. Formally, we wish to
ﬁnd a sequence yi of 39-dimensional vectors, and xi of
13-dimensional vectors, satisfying the derivative relation

yi = (xi, xi+2 − xi−2, (xi+3 − xi−1) − (xi+1 − xi−3))

and maximizing the likelihood score

exp(cid:31) 39
∑

j=1

∏

i

α j
i − (y j
i − μ j
σ j
i

i )2

(cid:30)

where μi, σi, and αi are the mean, standard deviation,
and importance vectors respectively.

We can solve this problem exactly by using the least-

squares method. We maximize the log-likelihood,

log∏

i

exp(cid:31)∑

j

i − μ j

i )2

−α j

i + (y j
σ j
i

(cid:30) =∑

i

∑

j

i − μ j

i )2

−α j

i + (y j
σ j
i

The log-likelihood is a sum of squares, so maximizing it
is a least-squares problem: we have a linear relationship
between the x and y values, and the error is a squared
difference.

In practice we cannot solve the full least squares prob-
lem all at once. The Viterbi algorithm only keeps track
of the 100 best paths for each preﬁx of the input, so if the
global optimal path had a preﬁx that was the 101st most
likely path, it would be discarded. Therefore, we work
one frame at a time and use the least squares approach to
ﬁnd the next best frame.

This gives us three beneﬁts: First, it ensures that at
every point in time, the next frame is the best possible
given what we have done so far. Second, it allows us to

USENIX Association  

25th USENIX Security Symposium  519

7

try all eight possible Gaussians in the GMM to pick the
one which provides the highest score. Third, it makes our
approach more resilient to failures of gradient descent.
Sometimes gradient descent cannot hit the 13-vector sug-
gested by this method exactly. When this happens, the
error score for subsequent frames is based on the actual
13-vector obtained by gradient descent.

We ﬁrst deﬁne two sub-
Complete description.
routines to help specify our attack more precisely.
LSTDERIV( f , ¯g, g) accepts a sequence of 13-vectors f
that have already been reached by previous iterations of
search, the desired 39-vector sequence ¯g, and one new
39-vector g; it uses least squares to compute the next
13-vector which should be targeted along with the least-
squares error score. Speciﬁcally:
1. Deﬁne A as the 39k × 13(6 + k) dimensional matrix
which computes the derivative of a sequence of 6 + k
13-vectors and returns the k resulting 39-vectors.

2. Deﬁne b as the 39k dimensional vector corresponding
to the concatenation of the k − 1 39-vectors in ¯g and
the single 39-vector g.

3. Split A in two pieces, with AL being the left 13k

columns, and AR being the right 6 × 13 columns.

4. Deﬁne ¯f as the concatenation of the 13-vectors in f .
5. Deﬁne ¯b = b − AL · ¯f .
6. Using least squares, ﬁnd the best approximate solu-

tion ˆx to the system of equations Ar · ˆx = ¯b.

7. Return (|(Ar · ˆx) − ¯b|, ˆx)

GRADDESC(s,t) accepts the previous frame s ∈ R410
and a target 13-vector t, and returns a frame ˆs ∈ R410
such that ˆs matches s in the 250 entries where they over-
lap and MFCC( ˆs) is as close to t as possible. More pre-
cisely, it looks for a 160-dimensional vector x that min-
imizes f (x) =|| MFCC(s160...410||x) − s||2, where || is
concatenation, and returns s160...410||x. We use the New-
ton Conjugate-Gradient algorithm for gradient descent
and compute the derivative symbolically for efﬁciency.

Our full algorithm works as follows:

1. In the following, f will represent a sequence of cho-
sen 13-vectors (initially empty), ¯g a sequence of tar-
get 39-vectors, s the audio samples to return, and i
the iteration number (initially 0).

2. Given the target phrase, pick HMM states hi
such that each state corresponds to a portion of a
phoneme of a word in the phrase.

3. Let g j

i be the 39-vector corresponding to the mean
of the jth Gaussian of the GMM for this HMM state.
One of these will be the target vector we will try to
invert.

4. For each

j,

squares prob-
solve the least
lem (s j, d j) = LSTDERIV( f , ¯g, g j
ˆj =
arg min j s j and ¯d = d ˆj to obtain a sequence of 13-
vectors ¯d0 to ¯di+6. Let ¯di be the “target 13-vector”

i ) and set

t. Append the 39-vector corresponding to t to ¯g.

5. Use gradient descent to get ˆs = GRADDESC(s,t).

Let s := ˆs. Append MFCC(s) to f .

6. Repeat for the next i from step 3 until all states are

completed.

4.5 Playing over the air

The previous attacks work well when we feed the audio
ﬁle directly into Sphinx. However, Sphinx could not cor-
rectly transcribe recordings made by playing the audio
using speakers. We developed three approaches to solve
this complication:

Make the audio easier to play over speaker. Gradi-
ent descent often generates audio with very large spikes.
It’s physically impossible for the speaker membrane
to move quickly enough to accurately reproduce these
spikes. We modiﬁed gradient descent to penalize wave-
forms that a speaker cannot reproduce. In particular, we
add a penalty for large second derivatives in the signal,
with the hope that gradient descent ﬁnds solutions that
do not include such large spikes.

Even with this
Predict the MFCC of played audio.
penalty, the audio is still not perfectly playable over a
speaker. When we compared the waveform of the played
audio and recorded audio, they had signiﬁcant differ-
ences. To address this, we built a model to predict
the MFCC when a ﬁle is played through a speaker and
recorded. Recall that the MFCC transformation essen-
tially computes the function C log(B �Ax�2).

ˆB,

By playing and recording many audio signals, we
learned new matrices ˆA,
ˆC so that for each played
frame x and recorded frame y, C log(B �Ay�2) is close
to ˆC log( ˆB � ˆAx�2). We computed ˆA, ˆB, ˆC by solving a
least-squares problem. This was still not enough for cor-
rect audio recognition, but it did point us in a promising
direction.

The ideas
Play the audio during gradient descent.
above are not enough for recognition of recorded audio.
To see what is going on here, we compare the MFCC of
the played audio (after recording it) and the initial audio
(before playing it). We found the correlation to be very
high (r = .97 for the important coefﬁcients).

Based on this observation, we augment our algorithm
to include an outer iteration of gradient descent. Given a
target MFCC we ﬁrst run our previous gradient descent
algorithm to ﬁnd a sound sequence which (before playing
over the speaker) reaches the target MFCC. Then, we
play and record it over the speaker. We obtain from this
the actual MFCC. We then adjust the target MFCC by
the difference between what was received and what is
desired.

We implemented this approach. Figure 4 plots the L2

520  25th USENIX Security Symposium 

USENIX Association

8

6

5

4

3

2

1

r
o
r
r

E

0

10

20

30

40

50

60

Iteration Number

Figure 4:
Incorporating actually playing the audio over the
speakers into the gradient descent signiﬁcantly reduces the er-
ror. The plot is of the L2 norm of the error of from the target
feature vector to the actually recorded feature vector, over time.

error (the difference between the target MFCC and what
is actually recorded during each iteration of our algo-
rithm) over time. By repeating this procedure 50 times
and taking the frame with the minimum noise, we obtain
an audio ﬁle that is correctly recognized by Sphinx after
being played over the speaker.

Since we perform 50 iterations of the inner gradient
descent per frame, and each iteration takes 30 seconds,
our approach takes nearly 30 hours to ﬁnd a valid attack
sample. In practice, sometimes this process can take even
longer; since we are recording audio, if the microphone
picks up too much background noise, we must discard
the recorded sample and try again. We have built in an
error-detection system to mitigate these effects.

This might seem like a high cost to generate one at-
tack sample. However, once generated, we can reuse the
obfuscated audio on that speaker forever. Even though
the setup cost is high, it must only be performed once;
thereafter the same audio ﬁle can be used repeatedly.

4.6 Evaluation

For the former, we ap-
Machine comprehension.
ply the above techniques and generate three audio com-
mands: “okay google, take a picture”, “okay google, text
12345”, and “okay google, browse to evil.com”. The
speech recognition system is an instance of CMU Sphinx
version 4-1.0beta6.

We determined the minimum number of frames per
phoneme that is sufﬁcient to allow Sphinx to recognize
the command. Some words are more difﬁcult to create
correctly than others, and thus require more frames per
phoneme. Detailed results can be found in Appendix E.
When we modify the lengths of the phonemes to account
for this data, over 90% of generated phrases are correctly
recognized by Sphinx.

To evaluate our attack playing over a microphone, we
equipped our computer with an external Blue Snowball

Table 3: White-box attack results. Percentages show success-
ful comprehension of Normal and Obfuscated version of voice
commands for humans and machines. Our white-box attack
constructs the obfuscated attack without a starting normal at-
tack; we only evaluate the machine understanding of the obfus-
cated commands.

Human

Machine

Understanding Understanding
74% (230/310)

–

0% (1/377)

82% (82/100)

Normal
Obfuscated

microphone. Obfuscated audio commands are played
from an Altec Lansing ASC45.1 speaker located 0.5 me-
ters from the microphone. All of our evaluated phrases
were commands recorded from the microphone. We
trained our MFCC vector predictor with one minute (16
million samples) of audio played over this speaker in a
large room to minimize echoes. The room was isolated,
and thus we encountered nearly no background noise.
The attack took 32 hours to generate the command for
“okay google, take a picture”, but required no human
intervention, using the same number of frames chosen
from the previous attack for a total of 101 frames.

Once the ﬁle was generated, we played the command
over the speaker 100 times. Sphinx was able to recognize
it in 82 of the instances. This is to be expected—for a
given audio played multiple times, there is a very high
(99%+) correlation between the recorded MFCC values.

As with our black-box at-
Human comprehension.
tack, we evaluate our obfuscated commands with the
same Amazon Mechanical Turk study. Workers were
blind to how the obfuscated audio was generated. As a
baseline, we asked workers to transcribe a phrase spoken
by a human naturally. The results show that our white-
box attack is signiﬁcantly better than the black-box at-
tack. No worker was able to transcribe more than half
of the words in the phrase correctly, and 75% of work-
ers transcriptions shared no words in common with the
actual phrase.

Four of the authors independently evaluated 377 tran-
scriptions of attack commands generated by our white-
box attack and compared them with correct transcrip-
tions to answer whether both transcriptions had the same
meaning. The four authors agreed on over 95% of in-
stances. Table 3 contains the results, showing that nor-
mal commands were correctly transcribed by workers
74% of the time, but obfuscated commands were al-
most never transcribed correctly. A comparison using
phoneme-level edit distance yields similar results; see
Appendix C.2.

While these results indicate that obfuscated commands
generated using our white-box attack are very difﬁcult to
understand, we conducted a second study to determine

USENIX Association  

25th USENIX Security Symposium  521

9

if users actually thought the audio was human speech
or just noise. Speciﬁcally, we created audio samples of
a human speaking a phrase, followed by an obfuscated
(different) phrase, and ﬁnally a human speaking a third
different phrase. In this study we were interested in see-
ing if the worker would try to transcribe the obfuscated
speech at all, or leave it out entirely.

Transcription accuracy was 80% for the ﬁrst and last
commands given by a human speaking. Only 24% of
users attempted to transcribe the obfuscated speech. This
study clearly demonstrates that when given a choice
about what they viewed as speech and not-speech, the
majority of workers believed our audio was not speech.

5 Defenses

We are unaware of any device or system that currently
defends against obfuscated voice commands. In this sec-
tion, we explore potential defenses for hidden voice com-
mands across three dimensions: defenses that notify, de-
fenses that challenge, and defenses that detect and pro-
hibit. The defenses described below are not intended to
be exhaustive; they represent a ﬁrst examination of po-
tential defenses against this new threat.

5.1 Defenses that notify

As a ﬁrst-line of defense we consider defenses that alert
the user when the device interprets voice commands,
though these will only be effective when the device op-
erator is present and notiﬁcation is useful (e.g., when it
is possible to undo any performed action).

The “Beep”,
the “Buzz” and the “Lightshow”.
These defenses are very simple: when the device receives
a voice command, it notiﬁes the user, e.g., by beeping.
The goal is to make the user aware a voice command
was accepted. There are two main potential issues with
“the Beep”: (i) attackers may be able to mask the beep,
or (ii) users may become accustomed to their device’s
beep and begin to ignore it. To mask the beep, the at-
tacker might play a loud noise concurrent with the beep.
This may not be physically possible depending on the
attacker’s speakers and may not be sufﬁciently stealthy
depending on the environment as the noise require can
be startling.

A more subtle attack technique is to attempt to mask
the beep via noise cancellation. If the beep were a single-
frequency sine wave an attacker might be able to cause
the user to hear nothing by playing an identical frequency
sine wave that is out of phase by exactly half a wave-
length. We evaluated the efﬁcacy of this attack by con-
structing a mathematical model that dramatically over-
simpliﬁes the attacker’s job and shows that even this
simpliﬁed “anti-beep” attack is nearly impossible. We

present a more detailed evaluation of beep cancelation in
Appendix D.

Some devices might inform the user when they inter-
pret voice commands by vibrating (“the buzz”) or by
ﬂashing LED indicators (“the lightshow”). These noti-
ﬁcations also assume that the user will understand and
heed such warnings and will not grow accustomed to
them. To differentiate these alerts from other vibration
and LED alerts the device could employ different puls-
ing patterns for each message type. A beneﬁt of such
notiﬁcation techniques is that they have low overhead:
voice commands are relatively rare and hence generating
a momentary tone, vibration, or ﬂashing light consumes
little power and is arguably non-intrusive.

Unfortunately, users notoriously ignore security warn-
ing messages, as is demonstrated by numerous studies
of the (in)effectiveness of warning messages in deployed
systems [35, 37, 44]. There is unfortunately little rea-
son to believe that most users would recognize and not
quickly become acclimated to voice command notiﬁca-
tions. Still, given the low cost of deploying a notiﬁcation
system, it may be worth considering in combination with
some of the other defenses described below.

5.2 Defenses that challenge

There are many ways in which a device may seek conﬁr-
mation from the user before executing a voice command.
Devices with a screen might present a conﬁrmation dia-
logue, though this limits the utility of the voice interface.
We therefore consider defenses in which the user must
vocally conﬁrm interpreted voice commands. Present-
ing an audio challenge has the advantage of requiring the
user’s attention, and thus may prevent all hidden voice
commands from affected the device assuming the user
will not conﬁrm an unintended command. A consistent
verbal conﬁrmation command, however, offers little pro-
tection from hidden voice commands: the attacker also
provide the response in an obfuscated manner. If the at-
tacker can monitor any random challenge provided by
the device, it might also be spoofed. To be effective, the
conﬁrmation must be easily produced by the human op-
erator and be difﬁcult to forge by an adversary.

Such a conﬁrmation system
The Audio CAPTCHA.
already exists in the form of audio CAPTCHAs [26]
which is a challenge-response protocol in which the chal-
lenge consists of speech that is constructed to be difﬁcult
for computers to recognize while being easily understood
by humans. The response portion of the protocol varies
by the type of CAPTCHA, but commonly requires the
human to transcribe the challenge.

Audio CAPTCHAs present an possible defense to hid-
den voice commands: before accepting a voice com-
mand, a device would require the user to correctly re-

522  25th USENIX Security Symposium 

USENIX Association

10

spond to an audio CAPTCHA, something an attacker
using machine speech recognition would ﬁnd difﬁcult.
While it is clear that such a defense potentially has us-
ability issues, it may be worthwhile for commands that
are damaging or difﬁcult to undo.

Audio CAPTCHAs are useful defenses against hidden
voice commands only if they are indeed secure. Previous
generations of audio CAPTCHAs have been shown to
be broken using automated techniques [6, 39]. As audio
CAPTCHAs have improved over time [11, 27], the ques-
tion arises if currently ﬁelded audio CAPTCHAs have
kept pace with improvements in speech recognition tech-
nologies. In short, they have not.

We focus our examination on two popular audio
CAPTCHA systems: Google’s reCaptcha [33] offers au-
dio challenges initially consisting of ﬁve random dig-
its spread over approximately ten seconds; and NLP
Captcha [30] provides audio challenges of about three
seconds each composed of four or ﬁve alphanumeric
characters, with the addition of the word “and” before
the last character in some challenges.

We tested 50 challenges of reCaptcha and NLP
Captcha each by segmenting the audio challenges be-
fore transcribing them using Google’s speech recogni-
tion service. Figure 5 shows the results of transcription.
Here, we show the normalized edit distance, which is
the Levenshtein edit distance using characters as alpha-
bet symbols divided by the length of the challenge. More
than half and more than two-thirds of NLP Captchas and
reCaptchas, respectively, are perfectly transcribed using
automated techniques. Moreover, approximately 80% of
CAPTCHAs produced by either system have a normal-
ized edit distance of 0.3 or less, indicating a high fre-
quency of at least mostly correct interpretations. This
is relevant, since audio CAPTCHAs are unfortunately
not easily understood by humans; to increase usability,
reCaptcha provides some “leeway” and accepts almost-
correct answers.

Given the ease at which they can be solved using au-
tomated techniques, the current generation of deployed
audio CAPTCHA systems seems unsuitable for defend-
ing against hidden voice commands. Our results do not
indicate whether or not audio CAPTCHAs are necessar-
ily insecure. However, we remark that since computers
continue to get better at speech recognition developing
robust audio CAPTCHA puzzles is likely to become in-
creasingly more difﬁcult.

5.3 Defenses that detect and prevent

Speaker recognition (some-
Speaker recognition.
times called voice authentication) has been well-
explored as a biometric for authentication [7], with at
least Google recently including speaker recognition as

Figure 5: Accuracy of breaking audio CAPTCHA using ma-
chine based speech-to-text conversion. A normalized edit dis-
tance of zero signiﬁes exact prediction.

an optional feature in its Android platform [18]. Apple
also introduced similar functionality in iOS [2].

However, it is unclear whether speaker veriﬁcation
necessarily prevents the use of hidden voice commands,
especially in settings in which the adversary may be able
to acquire samples of the user’s voice. Existing work
has demonstrated that voices may be mimicked using
statistical properties4; for example, Aylett and Yamag-
ishi [3] are able to mimic President George W. Bush’s
voice with as little of 10 minutes of his speech. Hence,
it may be possible to construct an obfuscated voice com-
mand based on recordings of the user’s voice that will be
accepted both by the speaker recognition and the voice
recognition systems. This is an interesting technical
question which we defer to future work.

Importantly, speaker recognition presents three well-
understood usability issues. First, a non-negligible false
negative rate might limit authorized use, which provides
an incentive for users to deactivate speaker recognition.
Second, speaker recognition requires training, and likely
necessitates the collection of a large speech corpus in or-
der to ensure the level of accuracy necessary for reliable
authentication. This need is compounded for devices
such as Amazon Echo that are intended to be used by
multiple users. Users may be unwilling to perform such
training. Finally, speaker recognition is unsuited for ap-
plications that have no prior interactions with the user,
for example, in kiosks for the visually impaired. We ar-
gue that such usability issues motivate the need for less
intrusive defenses, such as the ones described next.

This defense decreases the ﬁdelity of
The “Filter”.
the input audio before applying speech recognition. This

4CereVoice offers an online service for “[creating] a computer ver-

sion of your own voice” [8].

USENIX Association  

25th USENIX Security Symposium  523

11

somewhat counterintuitive approach leverages the preci-
sion required by hidden voice commands: by slightly de-
grading the audio quality, normal commands are affected
only slightly while obfuscated inputs, already at the cusp
of being comprehensible by machine, are no longer rec-
ognized. The goal of the ﬁlter is thus to ﬁnd a “sweet
spot” such that a slight decrease in ﬁdelity will not too
adversely affect normal audio, but will eliminate hidden
voice commands.

Our ﬁlter implementation considers audio in the time
domain. For a ﬁlter rate f and an audio ﬁle consisting of
s samples, we preserve f · s samples chosen uniformly at
random and remove without replacement the remaining
(1 − f )s samples.

We evaluate the efﬁcacy of the ﬁlter defense using the
West Point Company English speech data corpus from
the University of Pennsylvania’s Linguistic Data Consor-
tium (LDC) [29]. The LDC corpus consists of Ameri-
can English-language speakers, 30 of them speaking the
same 50 sentences each, and another 109 speakers each
speaking a separate set of 185 sentences. For each sen-
tence in the corpus, we construct multiple obfuscated au-
dio ﬁles using the black-box techniques described in §3.
We do not introduce any background noise to either the
normal or obfuscated samples, and send all samples to
Google’s speech recognition system and evaluate the ac-
curacy of its transcriptions. We tested the ﬁlter against
16,440 normal commands and 27,589 obfuscated com-
mands, sampled uniformly at random without replace-
ment. The audio ﬁles were in WAV format with single
channel sampled at 22050 kHz with 32-bit bit depth in
ﬂoat data type.

We deﬁne phoneme edit distance δ as the Levenshtein
edit distance between phoneme sequences of two tran-
scriptions, and L as the phoneme length of unﬁltered sen-
tence. Finally we deﬁne relative phoneme edit distance
as δ /(δ + L), to reﬂect how close the two transcriptions
might sound to a human listener. Figure 6 plots the cu-
mulative distribution of the relative phoneme edit dis-
tances for our tested audio samples. Out of ﬁve tran-
scripts Google returned, we pick the transcript with the
smallest phoneme edit distance and calculate the rela-
tive phoneme edit distance.With an 80% ﬁlter (i.e., 80%
of the original audio is preserved), the hidden voice com-
mand is signiﬁcantly perturbed—the fraction of correctly
transcribed sentences decreases from 41.74% to 7.49%
when the ﬁlter is applied. However, the ﬁlter also signif-
icantly degrades normal audio, decreasing accuracy by
14.35%, which is likely unacceptable for users who de-
pend on voice functionality. Through manual tuning, we
ﬁnd that a 92% ﬁlter offers an attractive sweet spot, caus-
ing the fraction of correctly transcribed obfuscated ﬁles
to fall from 41.74% to 26.60%, while negligibly affect-
ing normal ﬁles, whose accuracy drops only 1.06%.

Figure 6: The cumulative distribution of relative phoneme edit
distances for normal and obfuscated commands, for ﬁltering
rates of 80% and 92%.

As a ﬁnal defense we examine
The “Classiﬁer”.
whether automated techniques can detect our hidden
voice commands—that is, can a computer classify inter-
preted speech as being generated by a human or a ma-
chine? In practice, a speech recognition system could in-
corporate a machine learning classiﬁer as a ﬁnal process-
ing step, dropping voice commands that are determined
to be machine-generated.

We construct a hidden voice command detector using
logistic regression. Our classiﬁer uses mid-term features
(mean and standard deviation) derived from short term
features shown in Appendix F, extracted using the pyAu-
dioAnalysis library [16] from the original audio signal.

We use the same LDC speech corpus as used in our
ﬁlter experiments, and label each sample from the cor-
pus as normal. Using the black-box procedure described
in §3, we produce obfuscated hidden voice commands,
which we label as obfuscated. We overlay the back-
ground noise samples described in §3.3 to mimic a noisy
environment to both the normal and obfuscated samples.
We apply tenfold cross validation and report averages
across the ten runs. The confusion matrix that results
from classiﬁcation is reported in Table 4. 99.8% of ob-
fuscated audio were detected as such, while only incur-
ring a 0.2% false positive rate, meaning that the classi-
ﬁer would incorrectly discard two out of 1000 valid com-
mands.

To better gauge the classiﬁer’s accuracy when pre-
sented with diverse inputs, we performed additional clas-
siﬁcation tests against 51 commands generated using the
white-box technique from §4 and audio from the Accent
GMU dataset [43]. The Accent GMU dataset is com-
prised of 569 audio samples of English text spoken by
different individuals with different regional accents. Nei-
ther the GMU or white-box samples were used to con-

524  25th USENIX Security Symposium 

USENIX Association

12

Table 4: Confusion matrix of our classiﬁer.

Normal Obfuscated

Normal

Obfuscated

49.9%
0.1%

0.1%
49.9%

Table 5: Confusion matrix of our classiﬁer, when classifying
audio from outside corpora.

Normal Attack

White-box Attack

Accent GMU

30.7%
99.2%

69.3%
0.8%

struct the classiﬁer. That is, our results show the efﬁcacy
of a classiﬁer constructed with only normal and black-
box obfuscated command samples as training data. Im-
portantly, the GMU dataset consists of all normal (non-
obfuscated) samples, while the white-box dataset con-
tains only attack commands. The confusion matrix for
this classiﬁcation task is presented in Table 5. For the
GMU dataset, our classiﬁer performs well, incurring less
than a 1% false positive rate. The performance is worse
for the white-box attack. Still, even against this strong
attack which requires complete knowledge of the back-
end speech recognition system, the classiﬁer is able to
ﬂag nearly 70% of the hidden voice commands as being
malicious.

5.4 Summary of defenses

We present the ﬁrst examination of defenses against hid-
den voice commands. Our analysis of notiﬁcation de-
fenses (§5.1) shows that security alerts are difﬁcult to
mask, but may be ignored by users. Still, given their
ease of deployment and small footprint, such defenses
are worth considering. Active defenses, such as au-
dio CAPTCHAs (§5.2) have the advantage that they re-
quire users to afﬁrm voice commands before they be-
come effected. Unfortunately, active defenses also in-
cur large usability costs, and the current generation of
audio-based reverse Turing tests seem easily defeatable.
Most promising are prevention and detection defenses
(§5.3). Our ﬁndings show that ﬁlters which slightly
degrade audio quality can be tuned to permit normal
audio while effectively eliminating hidden voice com-
mands. Likewise, our initial exploration of machine
learning-based defenses shows that simple classiﬁcation
techniques yield high accuracy in distinguishing between
user- and computer-generated voice commands.

6 Limitations and Discussion

While the results of our defenses are encouraging, a limi-
tation of this paper is that the defenses do not offer proofs
of security. In particular, an adversary may be able to

construct hidden voice commands that are engineered to
withstand ﬁltering and defeat classiﬁers.

The random sampling used by our ﬁlter complicates
the task of designing a “ﬁlter-resistant” hidden voice
command since the adversary has no advanced knowl-
edge of what components of his audio command will be
discarded. The adversary is similarly constrained by the
classiﬁer, since the attacks we describe in §3 and §4 sig-
niﬁcantly affect the features used in classiﬁcation. Of
course, there might be other ways to conceal voice com-
mands that are more resistant to information loss yet re-
tain many characteristics of normal speech, which would
likely defeat our existing detection techniques. Design-
ing such attacks is left as a future research direction.

The attacks and accompanying evaluations in §3 and
§4 demonstrate that hidden voice commands are effec-
tive against modern voice recognition systems. There
is clearly room for another security arms race between
more clever hidden voice commands and more robust de-
fenses. We posit that, unfortunately, the adversary will
likely always maintain an advantage so long as humans
and machines process speech dissimilarly. That is, there
will likely always be some room in this asymmetry for
“speaking directly” to a computational speech recogni-
tion system in a manner that is not human parseable.

CMU Sphinx is a “traditional” ap-
Future work.
proach to speech recognition which uses a hidden
Markov model. More sophisticated techniques have re-
cently begun to use neural networks. One natural ex-
tension of this work is to extend our white-box attack
techniques to apply to RNNs.

Additional work can potentially make the audio even
more difﬁcult for an human to detect. Currently, the
white-box hidden voice commands sound similar to
white noise. An open question is if it might be possi-
ble to construct working attacks that sound like music or
other benign noise.

7 Conclusion

While ubiquitous voice-recognition brings many beneﬁts
its security implications are not well studied. We inves-
tigate hidden voice commands which allow attackers to
issue commands to devices which are otherwise unintel-
ligible to users.

Our attacks demonstrate that these attacks are possi-
ble against currently-deployed systems, and that when
knowledge of the speech recognition model is assumed
more sophisticated attacks are possible which become
much more difﬁcult for humans to understand.
(Au-
dio ﬁles corresponding to our attacks are available at
http://hiddenvoicecommands.com.)

These attacks can be mitigated through a number of
different defenses. Passive defenses that notify the user

USENIX Association  

25th USENIX Security Symposium  525

13

an action has been taken are easy to deploy and hard to
stop but users may miss or ignore them. Active defenses
may challenge the user to verify it is the owner who is-
sued the command but reduce the ease of use of the sys-
tem. Finally, speech recognition may be augmented to
detect the differences between real human speech and
synthesized obfuscated speech.

We believe this is an important new direction for future
research, and hope that others will extend our analysis of
potential defenses to create sound defenses which allow
for devices to securely use voice-commands.

We thank the anonymous re-
Acknowledgments.
This paper
viewers for their insightful comments.
is partially funded from National Science Foundation
grants CNS-1445967, CNS-1514457, CNS-1149832,
CNS-1453392, CNS-1513734, and CNS-1527401. This
research was additionally supported by Intel through the
ISTC for Secure Computing, and by the AFOSR under
MURI award FA9550-12-1-0040. The ﬁndings and opin-
ions expressed in this paper are those of the authors and
do not necessarily reﬂect the views of the funding agen-
cies.

References

[1]

I. Androutsopoulos, J. Koutsias, K. V. Chandrinos, and C. D. Spyropoulos.
An Experimental Comparison of Naive Bayesian and Keyword-based Anti-
spam Filtering with Personal e-Mail Messages. In ACM SIGIR Conference
on Research and Development in Information Retrieval (SIGIR), 2000.

[2] Apple. Use Siri on your iPhone, iPad, or iPod touch. Support article.

Available at https://support.apple.com/en-us/HT204389.

[3] M. P. Aylett and J. Yamagishi. Combining Statistical Parameteric Speech
Synthesis and Unit-Selection for Automatic Voice Cloning. In LangTech,
2008.

[4] M. Barreno, B. Nelson, A. D. Joseph, and J. Tygar. The security of machine

learning. Machine Learning, 81(2):121–148, 2010.

[5] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c, P. Laskov, G. Giac-
into, and F. Roli. Evasion Attacks against Machine Learning at Test Time.
In Machine Learning and Knowledge Discovery in Databases, 2013.

[6] E. Bursztein and S. Bethard. Decaptcha: Breaking 75% of eBay Audio
CAPTCHAs. In USENIX Workshop on Offensive Technologies (WOOT),
2009.

[7] J. Campbell, J.P. Speaker Recognition: A Tutorial. Proceedings of the

IEEE, 85(9):1437–1462, 1997.

[8] CereVoice

Me

Voice

Cloning

Service.

https://www.cereproc.com/en/products/cerevoiceme.

[9] Crowd Sounds — Free Sounds at SoundBible. http://soundbible.com/tags-

crowd.html.

[10] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma. Adversarial
Classiﬁcation. In ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD), 2004.

[11] M. Darnstadt, H. Meutzner, and D. Kolossa. Reducing the Cost of Breaking
Audio CAPTCHAs by Active and Semi-supervised Learning. In Interna-
tional Conference on Machine Learning and Applications (ICMLA), 2014.
[12] W. Diao, X. Liu, Z. Zhou, and K. Zhang. Your Voice Assistant is Mine:
How to Abuse Speakers to Steal Information and Control Your Phone. In
ACM Workshop on Security and Privacy in Smartphones & Mobile Devices
(SPSM), 2014.

[13] H. Drucker, S. Wu, and V. Vapnik. Support vector machines for spam

categorization. IEEE Transactions on Neural Networks, 10(5), Sep 1999.

[14] A. Fawzi, O. Fawzi, and P. Frossard. Analysis of classiﬁers’ robustness to

adversarial perturbations. arXiv preprint arXiv:1502.02590, 2015.

[15] M. Fredrikson, S. Jha, and T. Ristenpart. Model inversion attacks that ex-
ploit conﬁdence information and basic countermeasures.
In Proceedings
of the 22nd ACM Conference on Computer and Communications Security,
2015.

[16] T. Giannakopoulos.

Python Audio Analysis Library:

ture Extraction, Classiﬁcation,
https://github.com/tyiannak/pyAudioAnalysis.

Segmentation

Fea-
and Applications.

[17]

I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing
adversarial examples. arXiv preprint arXiv:1412.6572, 2014.

[18] Google. Turn on “Ok Google” on your Android. Support article. Available

at https://support.google.com/websearch/answer/6031948.

[19] Deep Neural Networks are Easily Fooled: High Conﬁdence Predictions for

Unrecognizable Images, 2015. IEEE.

[20] C. Ittichaichareon, S. Suksri, and T. Yingthawornsuk. Speech recognition
using MFCC. In International Conference on Computer Graphics, Simula-
tion and Modeling (ICGSM), 2012.

[21] Y. Jang, C. Song, S. P. Chung, T. Wang, and W. Lee. A11y Attacks: Exploit-
ing Accessibility in Operating Systems. In ACM Conference on Computer
and Communications Security (CCS), November 2014.

[22] A. Kantchelian, S. Afroz, L. Huang, A. C. Islam, B. Miller, M. C. Tschantz,
R. Greenstadt, A. D. Joseph, and J. D. Tygar. Approaches to Adversarial
Drift. In ACM Workshop on Artiﬁcial Intelligence and Security, 2013.

[23] C. Kasmi and J. Lopes Esteves. Iemi threats for information security: Re-
mote command injection on modern smartphones. IEEE Transactions on
Electromagnetic Compatibility, PP(99):1–4, 2015.

[24] P. Lamere, P. Kwok, W. Walker, E. Gouvea, R. Singh, B. Raj, and P. Wolf.
Design of the CMU Sphinx-4 Decoder. In Eighth European Conference on
Speech Communication and Technology, 2003.

[25] A. Mahendran and A. Vedaldi. Understanding deep image representations
by inverting them. In Conference on Computer Vision and Pattern Recog-
nition (CVPR) 2015, 2015.

[26] M. May. Inaccessibility of CAPTCHA: Alternatives to Visual Turing Tests
on the Web. Technical report, W3C Working Group Note, 2005. Available
at http://www.w3.org/TR/turingtest/.

[27] H. Meutzner, S. Gupta, and D. Kolossa. Constructing Secure Audio
CAPTCHAs by Exploiting Differences Between Humans and Machines. In
Annual ACM Conference on Human Factors in Computing Systems (CHI),
2015.

[28] D. E. Meyer and R. W. Schvaneveldt. Facilitation in Recognizing Pairs of
Words: Evidence of a Dependence between Retrieval Operations. Journal
of Experimental Psychology, 90(2):227, 1971.

[29] J. Morgan, S. LaRocca, S. Bellinger, and C. C. Ruscelli. West
Point Company G3 American English Speech. Linguistic Data Con-
sortium,
item LDC2005S30. University of Pennsylvania. Available at
https://catalog.ldc.upenn.edu/LDC2005S30, 2005.

[30] NLP Captcha. http://nlpcaptcha.in/.
[31] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami. The limitations of deep learning in adversarial settings. arXiv
preprint arXiv:1511.07528, 2015.

[32] N. Papernot, P. McDaniel, and I. Goodfellow. Transferability in machine
learning: from phenomena to black-box attacks using adversarial samples.
arXiv preprint arXiv:1605.07277, 2016.
reCAPTCHA. http://google.com/recaptcha.

[33]
[34] H. Sak, A. Senior, K. Rao, F. Beaufays, and J. Schalkwyk. Google
Voice Search: Faster and More Accurate, 2015. Google Research Blog
post. Available at http://googleresearch.blogspot.com/2015/09/google-
voice-search-faster-and-more.html.

[35] S. Schechter, R. Dhamija, A. Ozment, and I. Fischer. The Emperor’s New
Security Indicators: An Evaluation of Website Authentication and the Ef-
fect of Role Playing on Usability Studies. In IEEE Symposium on Security
and Privacy (Oakland), 2007.

[36] R. Schlegel, K. Zhang, X.-y. Zhou, M. Intwala, A. Kapadia, and X. Wang.
Soundcomber: A Stealthy and Context-Aware Sound Trojan for Smart-
phones. In Network and Distributed System Security Symposium (NDSS),
2011.

[37] J. Sunshine, S. Egelman, H. Almuhimedi, N. Atri, and L. F. Cranor. Cry-
ing Wolf: An Empirical Study of SSL Warning Effectiveness. In USENIX
Security Symposium (USENIX), 2009.

[38] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
and R. Fergus. Intriguing Properties of Neural Networks. arXiv preprint
arXiv:1312.6199, 2013.

[39] J. Tam, J. Simsa, S. Hyde, and L. V. Ahn. Breaking Audio CAPTCHAs. In

Advances in Neural Information Processing Systems (NIPS), 2008.

[40] J. Tygar. Adversarial Machine Learning. IEEE Internet Computing, 15(5):

4–6, 2011.

[41] T. Vaidya, Y. Zhang, M. Sherr, and C. Shields. Cocaine Noodles: Exploit-
ing the Gap between Human and Machine Speech Recognition. In USENIX
Workshop on Offensive Technologies (WOOT), August 2015.

[42] O. Viikki and K. Laurila. Cepstral Domain Segmental Feature Vector Nor-
malization for Noise Robust Speech Recognition. Speech Communication,
25(13):133–147, 1998.

[43] S. H. Weinberger. Speech Accent Archive. George Mason University, 2015.

Available at http://accent.gmu.edu.

[44] M. Wu, R. C. Miller, and S. L. Garﬁnkel. Do Security Toolbars Actually
Prevent Phishing Attacks? In SIGCHI Conference on Human Factors in
Computing Systems (CHI), 2006.

526  25th USENIX Security Symposium 

USENIX Association

14

A Additional Background on Sphinx

As mentioned in §4, the ﬁrst transform taken by Sphinx
is to split the audio in to overlapping frames, as shown
in Figure 7. In Sphinx, frames are 26ms (410 samples)
long, and a new frame begins every 10ms (160 samples).

Original audio stream
Frame 0

Frame 1

Frame 2

Frame oﬀset

Frame size

Figure 7: The audio ﬁle is split into overlapping frames.

MFC transform. Once Sphinx creates frames, it runs
the MFC algorithm. Sphinx’s MFC implementation in-
volves ﬁve steps:

1. Pre-emphasizer: Applies a high-pass ﬁlter that re-

duces the amplitude of low-frequencies.

2. Cosine windower: Weights the samples of the
frame so the earlier and later samples have lower
amplitude.

3. FFT: Computes the ﬁrst 257 terms of the (complex-
valued) Fast Fourier Transform of the signal and re-
turns the squared norm of each.

4. Mel ﬁlter: Reduces the dimensionality further by
splitting the 257 FFT terms into 40 buckets, sum-
ming the values in each bucket, then returning the
log of each sum.

5. DCT: Computes the ﬁrst 13 terms of the Discrete
Cosine Transform (DCT) of the 40 bucketed val-
ues.5

Despite the many steps involved in the MFC pipeline,
the entire process (except the running average and deriva-
tives steps) can be simpliﬁed into a single equation:

MFCC(x) = C log(B �Ax�2)

where the norm, squaring and log are done component-
wise to each element of the vector. A is a 410 × 257
matrix which contains the computation performed by the
pre-emphasizer, cosine windower, and FFT. B is a 257 ×
40 matrix which computes the Mel ﬁlter, and C is a 40 ×
13 matrix which computes the DCT.

Sphinx is conﬁgured with a dictionary ﬁle, which lists
all valid words and maps each word to its phonemes,

5While it may seem strange to take the DCT of the frequency-
domain data, this second FFT is able to extract higher-level features
about which frequencies are common, and is more tolerant to a change
in pitch.

T-1

T-2

T-3

T-1
U-1

T-2
U-2

T-3
U-3

Figure 8: The HMM used by Sphinx encoding the word “two”.
Each phoneme is split into three HMM states (which may re-
peat). These HMM states must occur in sequence to complete a
phoneme. The innermost boxes are the phoneme HMM states;
the two dashed boxes represent the phoneme, and the outer
dashed box the word “two”.

and a grammar ﬁle, which speciﬁes a BNF-style formal
grammar of what constitutes a valid sequence of words.
In our experiments we omit the grammar ﬁle and assume
any word can follow any other with equal probability.
(This makes our job as an attacker more difﬁcult.)

The HMM states can be thought of as phonemes, with
an edge between two phonemes that can occur consecu-
tively in some word. Sphinx’s model imposes additional
restrictions: its HMM is constructed so that all paths in
the HMM correspond to a valid sequence of words in the
dictionary. Because of this, any valid path through the
HMM corresponds to a valid sequence of words. For ex-
ample, since the phoneme “g” never follows itself, the
HMM only allows one “g” to follow another if they are
the start and end of words, respectively.

The above description is slightly incomplete. In real-
ity, each phoneme is split into three HMM states, which
must occur in a speciﬁc order, as shown in Figure 8. Each
state corresponds to the beginning, middle, or end of a
phoneme. A beginning-state has an edge to the middle-
state, and the middle-state has an edge to the end-state.
The end-phoneme HMM state connects to beginning-
phoneme HMM states of other phonemes. Each state
also has a self-loop that allows the state to be repeated.

Given a sequence of 39-vectors, Sphinx uses the
Viterbi algorithm to try to ﬁnd the 100 most likely paths
through the HMM model (or an approximation thereto).

B Detailed Machine Comprehension of

Black-box Attack

The detailed results of machine comprehension of black-
box attacks are presented in Figure 9.

We note that Figure 9 contains an oddity:

in a few
instances, the transcription success rate decreases as the
SNR increases. We suspect that this is due to our use
of median SNR, since the background samples contain
non-uniform noise and transient spikes in ambient noise
levels may adversely affect recognition. Overall, how-
ever, we observe a clear (and expected) trend in which
transcription accuracy improves as SNR increases.

USENIX Association  

25th USENIX Security Symposium  527

15

Figure 9: Machine understanding of normal and obfuscated variants of “OK Google”, “Turn on Airplane Mode”, and “Call 911”
voice commands (column-wise) under different background noises (row-wise). Each graph shows the measured average success
rate (the fraction of correct transcripts) on the y-axis as a function of the signal-to-noise ratio.

C Analysis

of

using
Phoneme-Based Edit Distance Met-
rics

Transcriptions

C.1 Black-box attack

To verify the results of the white-box survey and to bet-
ter understand the results of Amazon Mechanical Turk
Study, we ﬁrst performed a simple binary classiﬁcation
of transcription responses provided by Turk workers.

We deﬁne phoneme edit distance δ as the Levenshtein
edit distance between phonemes of two transcriptions.
We deﬁne φ as δ /L, where L is the phoneme length of
normal command sentence. The use of φ reﬂects how
close the transcriptions might sound to a human listener.
φ < 0.5 indicates that the human listener successfully
comprehended at least 50% of the underlying voice com-
mand. We consider this as successful comprehension by
human, implying attack failure; otherwise, we consider
it a success for the attacker. Table 6 shows the results of
our binary classiﬁcation. The difference in success rates
of normal and obfuscated commands is similar to that of
human listeners in Table 2, validating the survey results.

We used relative phoneme edit distance to show the
gap between transcriptions of normal and obfuscated
commands submitted by turk workers. The relative
phoneme edit distance is calculated as δ /(δ + L), L
is again the phoneme length of normal command sen-
tence. The relative phoneme edit distance has a range
of [0, 1), where 0 indicates exact match and larger rel-
ative phoneme edit distances mean the evaluator’s tran-
scription further deviates from the ground truth. By this
deﬁnition, a value of 0.5 is achievable by transcribing si-
lence. Values above 0.5 indicate no relationship between
the transcription and correct audio.

Figure 10 shows the CDF of the relative phoneme edit
distance for the (left) “OK Google”, (center) “Turn on
Airplane Mode” and (right) “Call 911” voice commands.
These graphs show similar results as reported in Table 2:
Turk workers were adept at correctly transcribing normal
commands even in presence of background noise; over
90% of workers made perfect transcriptions with an edit
distance of 0. However, the workers were far less able to
correctly comprehend obfuscated commands: less than
30% were able to achieve a relative edit distance less than
0.2 for “OK Google” and “Turn on Airplane Mode”.

528  25th USENIX Security Symposium 

USENIX Association

16

Table 6: Black-box attack. Percentages show the fraction of human listeners who were able to comprehend at least 50% of voice
commands.

Normal
Obfuscated

OK Google
97% (97/100)
24% (23/94)

Turn On Airplane Mode

Call 911

89% (102/114)
47% (52/111)

92% (75/81)
95% (62/65)

Figure 10: Cumulative distribution of relative phoneme edit distances of Amazon Mechanical Turk workers’ transcriptions for
(left) “OK Google”, (center) “Turn on Airplane Mode” and (right) “Call 911” voice commands, with casino and shopping mall
background noises. The attack is successful for the ﬁrst two commands, but fails for the third.

Table 7: White-box attack. Percentages show the fraction of
human listeners who were able to comprehend at least 50% of
phonemes in a command.

Command

Normal
Obfuscated

97% (297/310)
10% (37/377)

C.2 White-box attack

To verify the results of our authors review of the Turk
study, we computed the edit distance of transcribed com-
mands with actual commands. Table 7 says a command
is a match if at least 50% of phonemes were transcribed
correctly, to eliminate potential author bias. This metric
is less strict both for normal commands and obfuscated
commands, but the drop in quality is nearly as strong.

D Canceling out the Beep

Even when constrained to simplistic and conservative
mathematical models, it is difﬁcult to cancel out a beep
played by a mobile device.

D.1 Two ears difﬁculties

The victim has two ears located at points E
Setup:
and F, and a device at point P. The attacker has complete
control over a speaker at point A.

The attacker has complete knowledge
Threat model:
of the setup, including what the beep sounds like, when
the beep will begin playing, and the location of all four
points E, F, P and A. We assume for simplicity that sound
amplitude does not decrease with distance.

The attacker loses the game if the victim hears a sound
in either ear. Our question, then, is: can the attacker can-
cel out the sound of the beep in both ears simultaneously?
Since sound amplitude does not attenuate with distance,
the attacker can focus solely on phase matching: to can-
cel out a sound, the attacker has to play a signal that is
exactly π radians out of phase with the beep. This means
the attacker has to know the phase of the signal to a good
degree of accuracy.

In our model, canceling out sound at one ear (say E)
is easy for the attacker. The attacker knows the dis-
tance dPE , and so knows tPE , the time it will take for
the sound to propagate from P to E. Similarly, the at-
tacker knows tAE . This is enough to determine the delay
that he needs to introduce: he should start playing his
signal (dPE −dAE ) (mod λ )
(where λ is the wavelength) sec-
onds after the start of the beep (where c is the speed of
sound), and the signal he should play from his speaker is
the inverse of the beep (an “anti-beep”).

c

However, people have two ears, and so there will still
be some remnant of the beep at the other ear F: the beep
will arrive at that ear dPF
seconds after being played,
c
while the anti-beep will arrive dAF
c
anti-beep starts, i.e., dPE −dAE +dAF
seconds after the beep
starts. This means that the anti-beep will be delayed by
dPE −dAE +dAF −dPF

seconds after the

c

c

seconds compared to the beep.

Therefore,

the attacker must be sure that they are
placed exactly correctly so that the cancellation occurs
at just the right time for both ears. This is the set of
points where (dPE − dAE + dAF − dPF ) =0. That is, the
attacker can be standing anywhere along half of a hyper-
bola around the user.

USENIX Association  

25th USENIX Security Symposium  529

17

)

B
d
(
 
d
e
d
r
o
c
e
R
e
m
u
o
V

 

l

5
7

0
7

5
6

0
6

50

60

70

80

90

100

Distance from speaker 1 (cm)

Figure 11: Plot of the amplitude of attempted noise cancellation
of a tone at 440Hz

Finally, there is one more issue: any device which can
perform voice recognition must have a microphone, and
so can therefore listen actively for an attack. This then
requires not only that the attacker be able to produce ex-
actly the inverse signal at both ears, but also zero total
volume at the device’s location. This then ﬁxes the at-
tacker’s location to only one potential point in space.

D.2 Real-world difﬁculties

In the above setup we assumed a highly idealized model
of the real world. For instance, we assumed that the at-
tacker knows all distances involved very precisely. This
is of course difﬁcult to achieve in practice (especially if
the victim moves his head). Our calculations show that
canceling over 90% of the beep requires an error of at
most 3% in the phase. Putting this into perspective, for a
1Khz beep, to eliminate 90% of the noise, the adversary
needs to be accurate to within 3 inches.

In practice, the attack is even more difﬁcult than de-
scribed above. The adversary may have to contend
with multiple observers, and has to consider background
noise, amplitude attenuation with distance, and so on.

Even so, to investigate the ability of an attacker to can-
cel sound in near-ideal conditions, we conducted an ex-
periment to show how sound amplitude varies as a func-
tion of the phase difference in ideal conditions. The setup
is as follows: two speakers are placed facing each other,
separated by a distance d. Both speakers play the same
pure tone at the same amplitude. We placed a micro-
phone in between, and measured the sound amplitude at
various points on the line segment joining the two. For
our experiment, d = 1.5m and the frequency of the tone
is f = 440Hz. The results are plotted in Figure 11.

As can be seen, the total cancellation does follow a

sine wave as would be expected, however there is noise
due to real-world difﬁculties. This only makes the at-
tacker’s job more difﬁcult.

E Machine Interpretation of Obfuscated

Command

Table 8: For each of the three phrases generated in our white-
box attack, the phrase that Sphinx recognized. This data is used
to alter the lengths of each phoneme to reach words more ac-
curately. Some words such as “for” and “four” are pronounced
exactly the same: Sphinx has no language model and so makes
errors here.

Phrases as recognized by CMU Sphinx

Count

Phrase

3
1
1
1
6
2
1
1
1

5
2
1
1
6

okay google browse evil dot com
okay google browse evil that come
okay google browse evil them com
okay google browse for evil dot com
okay google browse two evil dot com
okay google browse two evil that com
okay google browse who evil not com
okay google browse who evil that com
okay up browse evil dot com

okay google picture
okay google take a picture
okay google take of
okay google take of picture
okay google take picture

10
1
2
3

okay google text one three for ﬁve
okay google text one two three for ﬁve
okay google text one who three for ﬁve
okay google text want three for ﬁve

F Short-Term Features used by Classiﬁer

Defense

Table 9: Short term features used for extracting mid-term fea-
tures.

Feature

Description

Zero Crossing Rate

Energy

Entropy of Energy
Spectral Centroid
Spectral Spread
Spectral Entropy

Spectral Flux

Spectral Rolloff

MFCCs

Chroma Vector

Chroma Deviation

The rate of sign-changes of the signal during the du-
ration of a particular frame.
The sum of squares of the signal values, normalized
by the respective frame length.
The entropy of sub-frames’ normalized energies.
The center of gravity of the spectrum.
The second central moment of the spectrum.
Entropy of the normalized spectral energies for a set
of sub-frames.
The squared difference between the normalized
magnitudes of the spectra of the two successive
frames.
The frequency below which 90% of the magnitude
distribution of the spectrum is concentrated.
Mel Frequency Cepstral Coefﬁcients
A 12-element representation of the spectral energy
The standard deviation of the 12 chroma coefﬁ-
cients.

530  25th USENIX Security Symposium 

USENIX Association

18

