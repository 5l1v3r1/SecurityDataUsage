Single Round Access Privacy on Outsourced Storage

Peter Williams and Radu Sion

Network Security and Applied Cryptography Lab
Stony Brook University, Stony Brook, NY, USA

{petertw,sion}@cs.stonybrook.edu

ABSTRACT
We present SR-ORAM,1 the ﬁrst single-round-trip polylog-
arithmic time Oblivious RAM that requires only logarithmic
client storage. Taking only a single round trip to perform a
query, SR-ORAM has an online communication/computation
cost of O(log n log log n), and an oﬄine, overall amortized
per-query communication cost of O(log2 n log log n), requir-
ing under 2 round trips. The client folds an entire inter-
active sequence of Oblivious RAM requests into a single
query object that the server can unlock incrementally, to
satisfy a query without learning its result. This results in
an Oblivious RAM secure against an actively malicious ad-
versary, with unprecedented speeds in accessing large data
sets over high-latency links. We show this to be the most ef-
ﬁcient storage-free-client Oblivious RAM to date for today’s
Internet-scale network latencies.

Categories and Subject Descriptors
D.0 [Software]: General; E.3 [Data Encryption]

Keywords
Access Privacy, Cloud Computing, Oblivious RAM

1.

INTRODUCTION

Oblivious RAM (ORAM) allows a client to read and write
data hosted by an untrusted party, while hiding both the
data and the access pattern from this untrusted host. Ac-
cess pattern privacy is a critical component of data privacy.
Without access pattern privacy, the act of reading and writ-
ing remote data leaks potentially essential information about
the data itself, making it impossible to achieve full data
conﬁdentiality. Since the introduction of the ﬁrst Oblivious
RAM in [6], approaches to increase query throughput have
been relentlessly sought. Nevertheless, and despite the wide

1A preliminary version of this paper appeared in the ACNS
Industrial Track [18].

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$15.00.

range of potential applications, practical Oblivious RAM
constructions have remained elusive until very recently.

One of the most signiﬁcant challenges to providing practi-
cal ORAM is that these interactive protocols require a large
number of client-server round trips, resulting in large, often
impractical, online query latencies. For example, the con-
struction presented by Goodrich et al.
[8] requires log2 n
round trips, translating to an online cost alone of over 1200-
1500ms per query on a 1 terabyte database (e.g., for 10KB
blocks), assuming a network link with a latency of just 50ms.
This paper provides a simple and direct solution to the
challenge: SR-ORAM, a single-round-trip ORAM. Querying
requires a single message to be sent from the client to the
server and thus incurs a single round-trip (for a total online
cost of 50ms in the example above). Moreover, SR-ORAM
does not greatly aﬀect the oﬄine, amortized cost.

The basic idea behind SR-ORAM is to fold the interactive
queries into a single non-interactive request without sacriﬁc-
ing privacy. The client constructs a set of values (a “query
object”) that allows the server to selectively decrypt pieces,
depending on new values obtained during its traversal of
the database. Each component of the query object unlocks
only a speciﬁc single new component—which allows server
database traversal progress while preventing it from learning
anything about the overall success of the query.

Our construction is based on the Bloom ﬁlter ORAM
of [19], since it lends itself conveniently to use of a non-
interactive query object and provides defenses against ac-
tively malicious adversaries (not only curious). We also
make use of the randomized shell sort deﬁned in [9], since
it allows the more stringent client storage requirements of
SR-ORAM (when compared to [19]).

Other ORAMs with constant numbers of round trips ex-
ist; Section 3 reviews recent solutions. However, SR-ORAM
is the ﬁrst to provide a constant-round-trip polylogarithmic
time construction assuming only logarithmic client storage.

2. MODEL

A capacity-constrained client desires to outsource storage
to an untrusted party (the server). The client has enough
local non-volatile storage to manage keys and certiﬁcates,
plus enough volatile RAM to run the ORAM client software
(logarithmic in the size of the outsourced data). Moreover,
since the client reads and writes sensitive data, it needs to
hide both the data content and access pattern. Thus, the
client needs low-latency, private access to this remote disk.
Data is accessed in “blocks”, a term used to denote a ﬁxed-
size record. “Block” is used instead of “word” to convey tar-

293get applications broader than memory access (ﬁle system
and database outsourcing, in particular, are lucrative tar-
gets). Block IDs are arbitrary bit sequences sized O(log n).
Participants
Communication between the user and the ORAM Client
is secured, e.g., with access controls on inter-process com-
munication if they are on the same machine, or with SSL
otherwise. Communication between the ORAM Client and
ORAM Server is also secured, e.g., with a transport-layer
protocol such as SSL.
ORAM Client: the trusted party providing the follow-
ing (self-explanatory) interface to the user: read(id): val;
write(id, val). The Client-Server protocol details are imple-
mentation speciﬁc (and typically optimized to the instance
to minimize network traﬃc and the number of round trips).
The client keeps track of two values between queries: its
secret key and the current access count. From this, the
current level keys, and the reshuﬄe count of each level, can
be derived.
ORAM Server: the untrusted party providing the storage
backend, ﬁlling requests from the instance.
Security Deﬁnitions
We will defend against curious, potentially malicious (not
constrained to follow the protocol) polynomially bounded
adversaries. We operate in the random oracle model2. The
actively malicious defense is inherited from the underlying
ORAM, described by Williams et al. [19].

For simplicity, timing attacks are not discussed here. De-
fenses include the introduction of client-side delays to uni-
formize query times—which can be done without aﬀecting
overall protocol complexity. Additionally, SR-ORAM as-
sumes semantically secure symmetric encryption primitives
and secure hash functions.
Notation
Throughout the paper, n refers to the database size,
in
blocks. The client secret key is sk. The number of times
a given level has been shuﬄed (i.e. reconstructed) is called
the “generation,” and is abbreviated as gen. Oblivious RAM
is ORAM; Bloom Filter is BF. Key size and hash function
output size are both assumed to be c0; c1 is the Bloom ﬁlter
security parameter.

To represent computational and communication costs in a
comparable manner, complexities are represented in words,
not bits. It is assumed that each word can hold an entire
identiﬁer, e.g., O(log n) bits.

3. BACKGROUND

We start with an review of ORAM, and in particular,
Bloom-ﬁlter-based ORAMs. We next review the highly in-
teractive Bloom-ﬁlter-based ORAM [19], which provides a
convenient construction to build SR-ORAM from. Finally,
we look at recent approaches to reduce the round trip cost.
3.1 ORAM Overview

Oblivious RAM[6] provides access pattern privacy to a
single client (or software process) accessing a remote database
2For an ORAM that does not assume access to a random
oracle, we refer the reader to the construction and analysis
by Damg˚ard et al. [5]

(or RAM). The amortized communication and computa-
tional complexities of the construction by Goldreich and
Ostrovsky [6] are O(log3 n) for a database sized n. This
construction requires only logarithmic storage at the client.
We now begin a review of their logarithmic construction,
on which our own is modeled. The server-hosted database is
a set of n semantically secure encrypted blocks (with a secret
key held by the client). Supported operations are read(id),
and write(id, newvalue). The data is organized into log2(n)
levels, as a pyramid. Level i consists of up to 2i blocks; each
block is assigned to one of the 2i buckets at this level as
determined by a hash function.3 Due to hash collisions each
bucket may contain from 0 to O(log n) blocks. 4
ORAM Reads. To obtain the value of block id, a client
must perform a read query in a manner that maintains two
invariants: (i) it never reveals which level the desired block
is at, and (ii) it never looks twice in the same spot for the
same block. To maintain (i), the client always scans a single
bucket in every level, starting at the top (Level 0, 1 bucket)
and working down. The hash function informs the client
of the candidate bucket at each level, which the client then
scans. Once the client has found the desired block, the client
still proceeds to each lower level, scanning random buckets
instead of those indicated by their hash function. For (ii),
once all levels have been queried, the client re-encrypts the
query result with the secret key and a diﬀerent nonce (so it
looks diﬀerent to the server) and places it in the top level.
This ensures that when it repeats a search for this block, it
will locate the block immediately (in a diﬀerent location),
and the rest of the search pattern is randomized. The top
level quickly ﬁlls up; how to dump the top level into the one
below is described later.
ORAM Writes. Writes are performed identically to reads
in terms of the data traversal pattern, with the exception
that the new value is inserted into the top level at the end.
Inserts are performed identically to writes, since no old value
will be discovered in the query phase. Note that semantic
security properties of the re-encryption function ensure the
server is unable to distinguish between reads, writes, and
inserts, since the access patterns are indistinguishable.
Level Overﬂow. Once a level is full, it is emptied into
the level below. This second level is then re-encrypted and
re-ordered, according to a new hash function. Thus, ac-
cesses to this new generation of the second level will hence-
forth be completely independent of any previous accesses.
Each level overﬂows once the level above it has been emp-
tied twice. The resulting re-ordering must be performed
obliviously: once complete, the adversary must be unable
to make any correlation between the old block locations and
the new locations. A sorting network (e.g., [1] or [9]) is used
to re-order the blocks thusly.

To enforce invariant (i), note also that all buckets must
contain the same number of blocks. For example, if the
bucket scanned at a particular level has no blocks in it, then
the adversary would be able to determine that the desired
block was not at that level. Therefore, each re-order pro-
cess ﬁlls all partially empty buckets to the top with fake

3log4(n) levels sized 4i in the original, but for simplicity we
use a branch factor of 2.
4This was originally speciﬁed as log n blocks, with a non-
negligible probability of bucket overﬂow, in which case a
new hash function is tried. It was later shown [13] that this
results in an information leak.

294blocks. Recall that since every block is encrypted with a se-
mantically secure encryption function, the adversary cannot
distinguish between fake and real blocks.
3.2 Bloom ﬁlters

Bloom ﬁlters [3] oﬀer a compact representation of a set of
data items. They allow for relatively fast set inclusion tests.
Bloom ﬁlters are one-way, in that, the “contained” set items
cannot be enumerated easily (unless they are drawn from a
ﬁnite, small space). Succinctly, a Bloom ﬁlter can be viewed
as a string of b bits, initially all set to 0. To insert a certain
element x, the ﬁlter sets to 1 the bit values at index posi-
tions H1(x), H2(x), . . . , Hk(x), where H1, H2, . . . , Hk are a
set of k crypto-hashes. Testing set inclusion for a value
y is done by checking that the bits for all bit positions
H1(y), H2(y), . . . , Hk(y) are set.

By construction, Bloom ﬁlters feature a controllable rate
of false positives r for set inclusion tests—this rate depends
on the input data set size z, the size of the ﬁlter b and the
number of cryptographic hash functions k deployed in its

(cid:2)

(cid:3)k

.

construction: r =

1 − (1 − 1/b)

kz

As will be seen below, the SR-ORAM Bloom ﬁlters are
constrained by two important considerations. First, we need
to minimize k, since this determines directly the number of
disk reads required per lookup. Second, we need to guar-
antee that with high probability, there will be no false pos-
itives; i.e., r must be negligible to prevent a privacy leak,
since a false positive reveals lookup failure to the server.
Encrypted Bloom Filters. The idea behind remotely-
stored encrypted Bloom ﬁlters is to store their bit repre-
sentation encrypted while still allowing client-driven Bloom
ﬁlter lookups. This can be achieved, e.g., by storing the
Bloom ﬁlters as bit strings XORed with client-side PRNG-
driven key strings, or with individual bits stored and en-
crypted separately with semantic security (at the expense of
additional storage).

As will be shown, in SR-ORAM, instead of storing an
encrypted bit for each position of the Bloom ﬁlter, we store
part of a decryption key. Since the server cannot distinguish
between the keys for bit-values of 1 and the keys for bit-
values of 0, we retain the property that the server does not
learn the success of the Bloom ﬁlter lookup.
3.3 Bloom ﬁlter-based ORAMs

The main contribution of [19] is the separation of level
membership testing from item storage.
Instead of check-
ing for an item at a given level by reading the entire rele-
vant bucket of O(log n)-blocks, an encrypted Bloom ﬁlter is
queried ﬁrst. This indicates to the client which of two po-
tential items (the real, if there, or a speciﬁc fake, otherwise)
to retrieve. This saves a factor of O(log n) server storage, as
there is now only a single fake item per real item, instead
of O(log n). These reduced storage requirements simultane-
ously speed up level construction and querying.

More speciﬁcally, item location is encoded via a Bloom ﬁl-
ter; any given item has membership in one of log2 n Bloom
ﬁlters, corresponding to one for each level. The level corre-
sponding to the Bloom ﬁlter that contains this item is the
level where the item must be retrieved from. Bloom ﬁlters
are queried from the top down; maintaining access privacy
requires that any given lookup be performed only once on
any given Bloom ﬁlter. Once an item is found to be at a par-
ticular level, it is retrieved by its label, which is a one-way

keyed hash of the item id, level number, and the number of
times this level has been rebuilt. It is then copied up to the
top, so the request will be satisﬁed at a higher level next
time. Fake lookups are performed on those levels where the
item does not exist. These retrieve a previously stored fake
item identiﬁed by a one-way keyed hash of the level number,
the number of times this level has been rebuilt, and the cur-
rent total access count. Random Bloom ﬁlter lookups are
performed below where the item is found.

This is an interactive process requiring log2 n round trips:
the client needs to know the success of a lookup at a given
level before it can start the query at the next level. Figure
1 illustrates this process of querying.

It has been shown [14] that the security analysis of [19]
is incomplete, suggesting larger Bloom ﬁlters are needed to
obtain negligible false positive rates. They also recommend
a diﬀerent selection in the tradeoﬀ between Bloom ﬁlter size
(aﬀecting server storage and shuﬄe cost), and the number
of hash functions chosen (aﬀecting online cost). This adds a
factor of log log n to the Bloom ﬁlter construction cost. We
apply these insights in the choices of Bloom ﬁlter parameters
(number of hash functions k, and size in bits) and in the
performance analysis (Section 7) of SR-ORAM.

We also note that [19] assumes a signiﬁcant amount of
temporary client storage necessary in the reshuﬄe step. This
assumption is not suitable for our model. Instead, following
the example of others[14, 7], SR-ORAM uses an oblivious
randomized shell sort [9] to support the level reshuﬄe and
construct Bloom ﬁlters obliviously without client storage.
This reduction in client storage requirements comes with
performance penalties, as will be discussed later.
3.4 Other constant-round-trip ORAMs

Other recent approaches provide ways around the penalty
of highly interactive protocols, at the cost of additional hard-
ware or overwhelming requirements of client storage. The
main issue in constructing a single round trip ORAM is
that a request for an item depends on how recently an item
was accessed. Maintaining this information at the client re-
quires storage at least linear in the size of the outsourced
database. Moreover, retrieving this information privately
from the server is almost as diﬃcult as providing ORAM. 5
Secure Hardware. Secure hardware such as the IBM 4764
[11] can be placed server-side, using remote attestation to re-
tain security guarantees for clients [2]. Secure hardware is
typically an order of magnitude more expensive than stan-
dard processors. Due to heat dissipation diﬃculties it is typ-
ically also an order of magnitude slower. Moreover, the ne-
cessity of physical security to provide any guarantees makes
such solutions vulnerable to a diﬀerent class of attacks. Sev-
eral authors have examined the use of secure hardware in the
access privacy scenario [15, 12].
Constant-round-trip protocols using client storage.
[16] maintains item location information at the client. Al-
though at the outset, n log2 n bits of client storage seems
like a big assumption, the authors argue this is reasonable
in some situations, since the block size is typically larger
than log n. They show that in practice, the local required
client storage in practice is only a small fraction of the total
database size. The recursive construction, using a second

5With the diﬀerence that this recursive ORAM only requires
storing O(log log n) bits per item, which is enough location
information about the item to build the query.

295Figure 1: Interactive Bloom ﬁlter querying. Both the lower level Bloom ﬁlter lookups and item lookups are
dependent on the Bloom ﬁlter results of the levels above.

ORAM to store this level membership information, how-
ever, is interactive. SR-ORAM requires only O(log2 n) bits
of client storage (Section 5).

More recently, Goodrich et al. [10] introduced a constant-
round-trip protocol assuming n1/c client storage. The num-
ber of round trips depends on the amount of client storage.
The non-interactive cache-based ORAM presented in 2006
[17] relies on s client storage to provide an
by Wang et al.
amortized overhead of O(n/s). The idea is to add previously
unseen items to a cache, which gets shuﬄed back into the
remote database when it ﬁlls. The high client storage re-
quirements (and poor storage/performance tradeoﬀ) make
it unsuitable for our model. This idea is revisited under
diﬀerent assumptions by Boneh et al. [4], with security for-
malization, but still requiring client storage.

A large number of interactive ORAM solutions have been
proposed. A great review is provided by Kushilevitz et al.
[13]. A full review should also include recent interactive
de-amortized ORAMs such as the construction by Goodrich
et al. [8]. These resolve another drawback of many ORAMs
(SR-ORAM included), the disparity between average-case
and worst-case query cost.

4. A FIRST PASS

This strawman construction modiﬁes a Bloom-ﬁlter-based
ORAM presented by Williams et al. [19]. It has the struc-
ture, but not yet the performance, of the SR-ORAM con-
struction. As detailed in Section 3.3, that Bloom ﬁlter
ORAM uses encrypted Bloom ﬁlters to store level member-
ship of items. To seek an item, the querying client must
request a known fake item from each level, except from the
level containing this item: the item is requested here instead.
Which level the item is at depends only on how recently this
item was last accessed. Since the client does not have stor-
age to keep track of that, it checks the Bloom ﬁlters one at
a time to learn if the item is at each level.

As the main principle of level-based ORAMs requires each
item be sought only once per level instance, it is unsafe to
query the Bloom ﬁlters past the level where this item is
present. This explains why the checks must be interactive:
once the item is found at level i, further accesses at the levels
below (i + 1 through log2 n) entail only random Bloom ﬁlter
queries corresponding to fake item requests. Moving the

found item to the top of the pyramid guarantees that later,
it will be sought and found elsewhere, since it moves back
down to other levels only by riding a wave of level reshuﬄes.
We now turn this, safely, into a non-interactive process.
Observe that in an interactive ORAM, if the client is re-
questing a recently accessed item j that happens to be in
level 2, the access sequence will proceed as follows. This
example is also illustrated in Figure 1. We use j to denote
the item identiﬁer, and sk for the secret key, and gen to
represent the current generation of that level (a function of
the total, global number of accesses, accesscount).

1. The client checks the level 1 Bloom ﬁlter for the item:

reading the positions generated by:
Hash(sk | level=1 | gen | j )

2. Upon seeing Encrypt(0) at one or more of those posi-
tions in the Bloom ﬁlter, the client learns the item is
not at level 1. So it asks for a fake item instead, that
is labeled as:
Hash(sk | level=1 | gen | “fake” | accesscount )

3. The client now checks the level 2 Bloom ﬁlter for the

item: reading the positions indicated by:
Hash(sk | level=2 | gen | j )

4. Seeing Encrypt(1) at every position, the client learns
the item is at this level. This makes it safe to request
the item here; the client requests the block labeled
Hash(sk | level=2 | gen | “item” | j )

5. Having found the item, to maintain appearances and
not reveal this fact to the server, the client continues
to issue random Bloom ﬁlter lookups at each level i
below. At each level it requests the fake blocks labeled
Hash(sk | level=i | gen | “fake” | accesscount )

Note that there are only log2 n possible such access se-
quences, based on which level the item is found at (Figure
2). Each path starts with a real query. Active queries con-
tinue until an item is found, at which point only fake queries
are issued from there on down. This limited number of pos-
sible sequences makes non-interactive querying possible.

Since we have a ﬁnite number of these paths, our goal is
to follow one of these paths non-interactively, not knowing

296Figure 2: Left: potential query paths in unmodiﬁed BF-based ORAM. The client does not know at the time
of querying which of the log2 n possible paths will be taken; it depends on where the data is ultimately found.
As in the example used in Figure 1, the level 1 BF lookup returns false, indicating a fake should be retrieved
from level 1. The level 2 BF lookup returns true, indicating the item should be retrieved from level 2. A fake
BF query is run for level 3 since the item has already been found. Right: the query object in SR-ORAM.
The server learns the edges corresponding to exactly one path. The server will be able to decrypt one such
edge at each level, revealing the data ID, to retrieve and include in the response to the client, and decrypting
a node of the query object in the level below.

ahead of time which level the item is at (and thus which of
the log2 n paths will be followed).

To achieve this, we propose to have the Bloom ﬁlter re-
sults themselves be used in unlocking one of the two possible
edges leading to the next query. A successful lookup will un-
lock the edge leading to a “ﬁnished” set, under which only
fake queries will follow. Conversely, failure must unlock the
edge continuing down the “active” search set. Once in the
“ﬁnished” set, it is impossible to return back to the “active”
set. Most importantly, the server must not gain any ability
at identifying which path it is currently on.

One strawman idea, exponential in the number of Bloom
ﬁlter hashes k, is to make each bit in the Bloom ﬁlter a piece
of a decryption key unlocking an edge to the next node. For
each level, the client prepares 2k results, corresponding to
each possible state of the Bloom ﬁlter. The Bloom ﬁlter
keys are generated deterministically by the client using a
cryptographic hash, so that the client can eﬃciently keep
track of them with only logarithmic storage. That is, a bit
set to 1 at position pos in the Bloom ﬁlter is represented by
Tpos = Hash(sk | pos | level | gen | 1), and a bit set to 0
by Fpos = Hash(sk | pos | level | gen | 0). The server learns
only one of the two (never both).

A Bloom ﬁlter lookup involves k bit positions (k is the
number of underlying Bloom ﬁlter hash functions). For
each new level it traverses, the server needs to know the
k associated Bloom ﬁlter bit positions to retrieve, consti-
tuting this level’s query. For the ﬁrst level, these are pro-
vided by the client. For each successive level, the server
will get this information by incrementally decrypting por-
tions of a client-provided “query object” data structure.

Illustrated in Figure 2 (right), the “query object” is com-
posed of log2 n levels and is traversed by the server top-down
synchronized with the traditional ORAM traversal. The
query object allows the server to progress in its database
traversal without learning anything.

Each level in the query object (with the exception of the
root), contains two nodes: a “ﬁnished” node and an “ac-
tive” node. Each node contains the k positions deﬁning the
current level Bloom ﬁlter query. The nodes also contain a
“keying” set of 2k elements.6

After performing the Bloom ﬁlter lookup, the server will
be able to decrypt one of these elements (only). Once de-
crypted, this element contains a key to decrypt one of the
query object’s next level two nodes; it also contains the iden-
tiﬁer for a current level item to return to the client. To pre-
vent leaks, the server will be asked to return one item for
each level, since we do not want to reveal when and where
we found the sought-after real item.

In eﬀect this tells the server where to look next in the
query object—i.e., which of the query object’s next level two
nodes (“ﬁnished” or “active”) to proceed with. This guides
the server obliviously through either the “ﬁnished” or the
“active” set, as follows:

• If the current level contains the sought-after item, the
server’s work is in fact done. However, the server can-
not be made aware of this. Hence, it is made to con-
tinue its traversal down the ORAM database, via a
sequence of fake queries. The “ﬁnished” node of the
next query object level allows the server to do just
6After encryption, these elements are sent in a random order
to prevent the server from learning any information.

297that, by providing the traversal information down the
“active” set.
• If, however, the current level does not contain the sought-
after item, the server must be enabled to further query
“real” data in its traversal down the ORAM database—
it will thus receive access to “active” node of the next
query object level.

To prevent the server from decrypting more than one ele-
ment from a node’s “keying” set, a special encryption setup
is deployed. Each of the 2k elements of the “keying” set is
encrypted with a special query object element key (QOEK),
only one of which the server will be able to reconstruct cor-
rectly after its Bloom ﬁlter query.

More speciﬁcally, for a Bloom ﬁlter lookup resulting in k
bit representations (i.e., biti is the representation of the bit
7), the QOEK is deﬁned as
at position i – either Ti or Fi
QOEK = Hash(bit1 | bit2 | bit3 | ... | bitk).

The encryption setup of the “keying” set ensures that this
key decrypts exactly one of its elements. The element cor-
responding to a Bloom ﬁlter “hit” (the sought-after element
was found at this level, i.e., all the underlying Bloom ﬁlter
bits are set to 1) leads down the “ﬁnished” set, i.e., the el-
ement that QOEK decrypts now, leads down the “ﬁnished”
set in the query object’s next level.

5. EFFICIENT CONSTRUCTION

We now present an eﬃcient construction, using O(log n)
client storage, O(log n log log n) per-query online message
size, and O(log2 n log log n) amortized communication, but
still only O(1) round trips. We reduce the size of the query
object of Section 4 from 2k log n to just k log n.

The main insight is to allow compression of the 2k de-
cryption key possibilities into only k + 1 possibilities. This
is achieved by representing the Bloom ﬁlter bits and their
combination in a commutative format. By allowing the de-
cryption key pieces stored in the Bloom ﬁlter (described in
the previous section) to be added together, rather than con-
catenated, the client only has to account for k + 1 diﬀerent
outcomes at each level.
To this end, we start by ﬁrst establishing a secret level-
instance-speciﬁc token v = Hash(sk | level | gen), not known
to the server. In the Bloom ﬁlter, a bit set to 1 at position
pos is represented as Tpos = Hash(sk | level | gen | pos); a bit
set to 0 is represented as Fpos = Tpos + v mod 2c0 (Figure
3), where c0 is a security parameter.
In the following we
operate in Z2c0 , and assume that Hash(·) outputs in Z2c0 .
Now, the query object encryption key (QOEK) is gen-
erated by combining (adding) values using modular arith-
metic, instead of concatenation as in the strawman solution.
This allows the client to only account for k + 1 possibilities,
each key corresponding to the number of times v might show
up among the selected Bloom ﬁlter positions.

The server sums together the values found in the Bloom
ﬁlter, and performs a hash, yielding, e.g., Hash(bit1+bit2 . . .+
bitk mod 2c0 ) as the QOEK. The commutativity of modu-
lar addition means each permutation of bits set to 0 and
bits set to 1 in a given Bloom ﬁlter yields the same key. A
successful Bloom ﬁlter lookup occurs in the case that the
7Recall that biti does not reveal to the server anything about
the actual underlying Bloom ﬁlter bit since the server does
not know the hash key sk.

QOEK is Hash(Tpos0 + Tpos1 + ... + Tposk mod 2c0 ), which
unlocks the edge from the “active” to the “ﬁnished” set.

Further, each of the k values from Hash(Tpos0 +Tpos1 +...+
Tposk +v mod 2c0 ) through Hash(Tpos0 +Tpos1 +...+Tposk +
kv mod 2c0 )—the result of a failed Bloom ﬁlter lookup; the
server does not know they are failures—unlocks an edge in
the query object that continues through the current (“ﬁn-
ished” or “active”) set (Figure 2). Figure 4 provides a sum-
mary of the query object format.

Let us now consider an example from the perspective of
the server. Say the client sends a query object for an item
x. This query object contains a single Level 1 node in clear-
text, and two nodes for every level below. The Level 1 ac-
tive node tells it to query positions L1pos1 . . .L 1posk. The
server retrieves the values stored in the Bloom ﬁlter at these
locations, adds them modulus 2c0, and applies the one-way
hash function. This yields a decryption key. The server now
tries this key on all k encrypted values included in the Level
1 node. It ﬁnds one that successfully decrypts, revealing a
data ID to retrieve from Level 1, as well as the decryption
key for one of the two Level 2 nodes. The server appends
the retrieved Level 1 data item to its result, then decrypts
the Level 2 node that it has the key for.

The server now repeats for the Level 2 node. It ﬁnds a
list of Bloom ﬁlter positions, which it again retrieves, adds
modulus 2c0, and hashes, yielding a decryption key which it
tries on the encrypted values included in the Level 2 node.
Again, only one will decrypt. The server never learns which
are the active or ﬁnished nodes; it simply sends back log2 n
data values to the client, of which one will be a real item,
and the others fake items.

5.1 Obliviously building the Bloom ﬁlter and

levels

This section describes how to construct the Bloom ﬁlter
without revealing to the server which Bloom ﬁlter positions
correspond to which items. Further, it shows how to obliv-
iously scramble the items as a level is constructed. Both
processes are mostly non-interactive (only a constant small
number of round trips).

In the Bloom ﬁlter construction, the key privacy require-
ment is that the server is unable to learn ahead of time any
correlation between Bloom ﬁlter positions and data items.
Constructing a new level in a BF-based ORAM requires ﬁrst
randomly and obliviously permuting all the items, renaming
them according to the new level hash function, and intro-
ducing a fake item for each (again, obliviously, and using
the appropriate hash-function deﬁned name). Constructing
the Bloom ﬁlter requires ﬁrst scanning the new set of items,
building an encrypted list of positions that will be need to
set, and then obliviously rearranging this encrypted list into
the appropriately-sized segments of the resulting encrypted
Bloom ﬁlter. The result is a pristine new level, generated
using a deterministic read and write pattern (independent
of the contents or history).

Further, note that we diﬀer from previous work in that we
store decryption keys in the Bloom ﬁlter, instead of single
encrypted bits. Recall that these components are computed
on the client by a secure keyed hash of the position, and
added to the value v if this position is intended to represent
a bit value of 0.

The other main diﬀerence from existing work is that [19]
assumes a signiﬁcant amount of temporary client storage,

298F0 = T0 + v 
F2 = T2 + v 
F3 = T3 + v 
F4 = T4 + v 

T1 = H(sk | level | gen | 1) 

T5 = H(sk | level | gen | 5) 

key0: all True     = H(Tx + Ty + Tz) 

key1: one False = H(Fx + Ty + Tz) 
     = H(Tx + Fy + Tz) 
 
     = H(Tx + Ty + Fz) 
 
     = H(Tx + Ty + Tz + v) 
 
key2: two False positions 

     = H(Tx + Ty + Tz + 2v) 

 

Bloom filter, with client secret key sk, 
for generation gen of level (stored on server) 

Query object decryption keys 
for Bloom filter lookup (x, y, z)

Figure 3: Left: Bloom ﬁlter format in full construction. A bit set to 1 in the Bloom ﬁlter is represented by
a hash of the position and current level key; a bit set to 0 is represented by the same value, plus the secret
value v. Right: decryption keys used with the query object. The server obtains the decryption key for a
given query stage by hashing together the speciﬁed Bloom ﬁlter results. Since there are k hash functions
used in a Bloom ﬁlter check, the client includes in the query object an edge corresponding to each of the
k + 1 possible keys.

which is not suitable in our model. To avoid this requirement
we propose to use two passes of an O(n log2 n) oblivious
randomized shell sort from [9].

This shell sort will be applied to a list produced by the
client as follows. The client starts by producing a list of
(encrypted) positions that need to be set in the Bloom ﬁl-
ter. The client will then also add a number of “segment
delimiters” to this list. These delimiters will aid later. One
delimiter is issued per segment (e.g., 32 adjacent positions)
in the Bloom ﬁlter. These delimiters will later provide an
excuse to output something for positions that are not to be
set, to prevent the server from learning which bits are set.

The client then performs a ﬁrst sorting pass, outputting
the list sorted by positions, with the delimiters interspersed
(delimiters include information that allows their sorting).
This sorted list is then scanned, and, for each of its non-
delimiter elements, a fake 32 bit value is issued. For each
encountered segment delimiter however, a 32 bit (encrypted)
segment of the Bloom ﬁlter is output. This segment’s bits
are set correctly according to the recently seen (since the
last segment’s delimiter encounter) encrypted set positions.
This simple mechanism prevents the server from learning
how many bits are set in each segment. To complete the
process, a second oblivious sort pass then moves the fake
32 bit values to the end of this new list, where they can be
safely removed by the server.

Finally, the encrypted bit-storing Bloom ﬁlter needs to be
converted into a key-storing Bloom ﬁlter in one ﬁnal step:
in a single non-oblivious pass, we read each bit and output
either Tpos for 1s and Fpos for 0s (where pos is the current
bit’s position in the Bloom ﬁlter). Note this multiplies the
size of the remotely stored object by the key size in bits.

We do not need to modify the level construction from [19],
except in replacing their storage-accelerated merge sort with
the storage-free randomized shell sort.
Non-interactivity of sort.
It is worth noting that the
shell sort, like the storage-accelerated merge sort, can be im-
plemented in a small, constant number of round trips. Each
step in both sorting algorithms requires reading two items
from the server, and writing them back, possibly swapped
(which is an interactive process). However, the item request
pattern of both sorting algorithms is known ahead of time
to the server. This allows it to send data to the client ahead

of time, without waiting for the request from the client (al-
ternately, the client can issue requests for future items far
in advance of the time they will be used).

√

This process is trivial in the merge sort: the access pat-
tern consists of simultaneous scans of two (or sometimes
up to
n) arrays; the server streams these to the client.
This process of non-interactive streaming of sort data to the
client is not as trivial in the randomized shell sort. Once the
random seed is chosen, however, both the client and server
know in advance the order the items will be requested in.
The server can run a simulation of this sort, for example, to
know which items the client needs to read next, and avoid
waiting for network round-trips throughout the construction
of the level.

The end result in both scenarios is a sort process whose
cost is almost completely independent of the network la-
tency. For any sort, regardless of the size, the client ﬁrst
sends an initialization message. Then, the server sends a
long stream of all the item contents to the client, as the
client simultaneously writes a long stream of the permuted
item contents back to the server.

6. SECURITY

SR-ORAM directly inherits the privacy properties of the
server-side ORAM database traversal, as well as the in-
tegrity defenses from the base ORAM construction in [19].
We must now establish the privacy of the query object con-
struction, as well as the new Bloom ﬁlter construction.

We establish privacy of the query object construction in
Theorem 1. The server learns only one set of Bloom ﬁlter
positions and one item label to retrieve at each level for a
given query.
In other words, the server sees only what it
would see in an equivalent, interactive instantiation.

Lemma 1. The server gains no non-negligible advantage
at guessing v = Hash (sk | level | gen) from observing (i)
the Bloom ﬁlter contents or (ii) the hashes included in the
query object.

Proof. (sketch) For each position pos in the Bloom ﬁlter,
the server sees a single value X that is either a random
number Tpos, or Tpos + v mod 2c0 . If Tpos and v are both
chosen randomly from Z2c0 , then seeing the entire set of
values gives the server no knowledge of v.

299• Level 1:

– L1 active node, in cleartext:

(integer values)

∗ Level 1 Bloom ﬁlter lookup index positions L1pos1 . . . L1posk
∗ the client computes, but does not send, these k + 1 keys:
· keyL1, success = Hash(TL1pos1 + TL1pos2 + ... + TL1posk )
· keyL1,1 = Hash(TL1pos1 + TL1pos2 + ... + TL1posk + v)
· keyL1,k = Hash(TL1pos1 + TL1pos2 + ... + TL1posk + kv)
· EkeyL1, success (L1 real data ID, and key2F: the key for the
· EkeyL1,1 (L1 fake data ID and key2A: the key for the L2

∗ k + 1 encrypted values, included in a random order:

L2 ﬁnished node)

. . .

active node)
. . .

· EkeyL1,k (L1 fake data ID and key2A: the key for the L2

active node)

• Level 2: Both the L2 active and the L2 ﬁnished nodes, in a random

order:

– L2 active node, encrypted with randomly generated key2A:

. . .

∗ Level 2 Bloom ﬁlter lookup index positions L2pos1 . . . L2posk
∗ the client computes, but does not send, these k + 1 keys:
· keyL2,success = Hash(TL2pos1 + TL2pos2 + ... + TL2posk )
· keyL2,1 = Hash(TL2pos1 + TL2pos2 + ... + TL2posk + v)
· keyL2,k = Hash(TL2pos1 + TL2pos2 + ... + TL2posk + kv)
· EkeyL2,success (L2 real data ID, and key3F)
· EkeyL2,1 (L2 fake data ID and key3A)
· EkeyL2,k (L2 fake data ID and key3A)

∗ k + 1 encrypted values, included in a random order:

. . .

– L2 ﬁnished node, encrypted with randomly generated key2F:

. . .

∗ k random Bloom ﬁlter lookup index positions for Level 2
L2pos1 . . . L2posk
∗ the client computes, but does not send, these k + 1 keys:
· keyL2,0 = Hash(TL2pos1 + TL2pos2 + ... + TL2posk )
· keyL2,1 = Hash(TL2pos1 + TL2pos2 + ... + TL2posk + v)
· keyL2,k = Hash(TL2pos1 + TL2pos2 + ... + TL2posk + kv)
· EkeyL2,0 (L2 fake data ID and key3F)
· EkeyL2,1 (L2 fake data ID and key3F)
· EkeyL2,k (L2 fake data ID and key3F)

∗ k + 1 encrypted values, included in a random order:

. . .

• Both the L3 active node (encrypted with key3A) and the L3 ﬁn-

ished node (encrypted with key3F), in a random order.

• And so forth, for each of the log2 n levels.

Figure 4: Query Object Format. For each level,
the query object is composed of two possible nodes
(with the exception of the top which only has one
node). This set constitutes a total of 2 log n nodes
(containing associated Bloom ﬁlter requests), and
2k log n edges. Of these nodes, the server will even-
tually be able to unlock log n. Each of the unlocked
ones provides k edges, of which the server will be
able to unlock exactly one. These edges contain the
decryption key for a single node at the next level,
as well as the data item ID to retrieve. All addition
is done modulus 2c0 .

The only other values observed by the server that are
linked to v are the outputs of the one-way hash functions,
included in the query objects. Seeing the hash outputs in
the query object provides no advantage at determining the
inputs, beyond using dictionary attacks which are infeasible
for a computationally bounded adversary (in the security
parameter c0), since the inputs all include the random v,
which is c0 bits long.

There is one subtlety here: if the server has some external
knowledge about how recently a given query was last issued,
it can determine that the set of corresponding Bloom ﬁlter
positions at that level are all 1s (and thus, that the values
observed at some positions are in fact Tpos + v mod 2c0 ,
rather than Tpos). However, knowing this still does not give
the server any knowledge of v: all possible values of Tpos
and v are still equally possible and likely.

Lemma 2. For a constant u > 1, a Bloom ﬁlter using k
hashes, with the size-to-contained items ratio of k × u, has
a false positive rate bounded by u
Proof. Suppose a Bloom ﬁlter contains z items and has
a size-to-items ratio of k × u. Thus, the size of the Bloom
ﬁlter in bits is b = z × k × u. The false positive rate is
(Section 3.2)

−k.

(cid:4)

(cid:5)

1 −

r =

1 − 1
b

(cid:7)k

(cid:6)kz

(cid:5)

(cid:6)k

≤

zk
b

−k

= u

Lemma 3. For a security parameter c1, a constant u > 1,
and a number of log n lookups on Bloom ﬁlters with size-to-
contained items ratios of k × u, where k = c1 + logu log n
hashes, the overall false positive rate is negligible in c1.
Proof. From Lemma 2 we know that the false positive
rate for any one lookup is r ≤ u
−k. Taking the union bound
over log n queries, the probability of failure is bounded by
r log n ≤ u

−c1 which is negligible in c1.

Theorem 1. The server can only unlock one path down
the query object, and all paths appear identical to the server.
Proof. (sketch) Each of the 2 log2 n nodes in the query
object, as illustrated in Figure 2, provides simply a list of
Bloom ﬁlter indices, and a set of encrypted values unlocking
edges to one of the two next nodes. The negligible Bloom
ﬁlter false positive rate, established by the base Bloom ﬁlter
ORAM, guarantees that the set of indices will be unique
for every query. Moreover, these indices, chosen by a keyed
secure hash, are indistinguishable from random.

Each edge contains an item label which is again uniquely
requested, only up to once per level. The label is also de-
termined by the keyed secure hash. Since the contents of
any single edge for a given query is indistinguishable from
random, and since these edges are included in the query
object in a random order, unlocking the edge provides no
information to the server about the path.

The contents of the Bloom ﬁlter provide the key and deter-
mine which one edge the server can unlock. Since, as shown
in Lemma 1, the server has no advantage at determining
the blinding value v, the server has no advantage at guess-
ing the alternate value at any position of the Bloom ﬁlter,
and is limited to a dictionary attack against the ciphertexts
of the other edges. Thus, the server can only unlock the
edge corresponding to the Bloom ﬁlter contents.

300Theorem 2. The server learns nothing from the Bloom

ﬁlter construction.

Proof. (sketch) All Bloom ﬁlter construction processes,
for any particular level, appear to the server to be indepen-
dent of which bits are set and which items are represented.
The server sees only the number of bits in the Bloom ﬁlter,
which is known beforehand.

Theorem 3. An honest but curious adversary gains no
non-negligible advantage at guessing the client access pattern
by observing the sequence of requests.

Proof. (sketch) Because of Theorem 1, the adversary
learns nothing it would not learn by observing a standard,
interactive, Bloom ﬁlter-ORAM. We defer to the security
claims of previous ORAMs (e.g.
[19]) and Lemma 3 which
shows the probability of Bloom ﬁlter failure to be negligible
in the considered security parameter.

Theorem 4. An actively malicious adversary has no ad-
vantage over the honest but curious adversary at violating
query privacy.

Proof. (sketch) In the underlying Bloom ﬁlter ORAM
[19], the client detects server protocol deviation, preventing
the server from learning anything new from issuing incorrect
responses. The non-interactive construction creates a slight
diﬀerence in the Bloom ﬁlter authenticity check: the Bloom
ﬁlter correctness check is now implicit. That is, the server
can only unlock the key if the stored value is the one placed
by the client, whereas in previous Bloom ﬁlter ORAMs, the
client had to test the authenticity of the stored Bloom ﬁlter
bits before it was safe to continue the query.

7. ANALYSIS

Following from the construction in Section 5, the query
It consists of 2 log n − 1
object size is O(log n log log n).
nodes, each of which queries k = log log n Bloom ﬁlter po-
sitions. This is transmitted to the server, which performs
k log n decryption attempts (of which log n are successful)
before sending log n blocks back to the client. This yields
the online cost of O(log n log log n).

The amortized oﬄine cost per query considers the time
required to build each level. A level sized z is built twice
every z queries. Shuﬄing these items using a randomized
shell sort costs O(z log z). Since, as shown in Lemma 3, the
Bloom ﬁlter is sized z log log n, and it must also be shuf-
ﬂed, the Bloom ﬁlter construction cost of O(z log z log log n)
dominates asymptotically. Summing over the i levels sized
z = 4i, and amortizing over the queries for each level be-
tween rebuilding, we ﬁnd a total amortized oﬄine cost of
O(log2 n log log n)

A query requires a single online round trip: the client
generates a query object, sends it to the server, and receives
a response containing the answer. The oﬄine shuﬄe process
requires several round trips (as discussed in Section 5.1), but
this cost is amortized over a period corresponding to many
queries, so that the average number of round trips per query
is still well under 2.

We also estimate the cost of querying and shuﬄing for a
sample hardware conﬁguration. The online cost is now lim-
ited by network, disk, and encryption throughput (instead
of, e.g., network round trips in related work). The oﬄine

disk seek cost
total disk throughput
crypto throughput
net throughput
net round trip

0
400 MBytes/sec
200 MBytes/sec
125 MBytes/sec
50 ms

Figure 5: Assumed hardware conﬁg.

Acceptable failure rate
Item block size
Symmetric key size
Bloom ﬁlter hashes

−128
2
10,000 bytes
256 bits
50

Figure 6: Database parameters

cost is limited, as in existing work, by network, disk, and
encryption throughput.

To make the comparison between SR-ORAM and existing
work as general as possible, we consider an ideal storage-
free interactive ORAM, which includes just two core costs
inextricably linked to any pyramidal storage-free ORAM,
and ignores any other costs.

First is the online cost of retrieving an item from an
ORAM interactively, requiring log2 n round trips and the
data transfer cost of log2 n blocks. Second is the oﬄine cost
of obliviously constructing each level, assuming 2i real items
and 2i fakes at each level i, using a randomized shell sort.
Thus, this ideal storage-free interactive ORAM provides a
lower bound for any existing ORAMs that do not assume
super-logarithmic client storage. Existing work, including
[6] and [7], fall into the category of a storage-free interactive
ORAM, but have signiﬁcant other costs, pushing their cost
in fact much above this lower bound.

As discussed in Section 3.2, given an upper bound on ac-
ceptable failure (false positive) rate r, we can calculate the
necessary Bloom ﬁlter size as a function of the number of
hashes used for lookup k.
For example, for a Bloom ﬁlter false positive rate of r =
−64, and constant k = 50 (chosen to optimize a trade-
2
oﬀ between the Bloom ﬁlter construction and online query
costs—see Lemma 3), the resulting optimal Bloom ﬁlter size
is under a third of the item storage size (of 2n blocks), for a
wide variety of database sizes ranging from a megabyte up to
−128, as
a petabyte. For bounding the false positive rate at 2
we assume for the performance analysis below, Bloom ﬁlter
storage of less than the total item storage size still suﬃces.
The target hardware conﬁguration is listed in Figure 5,
and the taret database conﬁguration in Figure 6. Solid state
disks are assumed, resulting in a low disk seek cost. Disk
access time is modeled as a function of the disk throughput;
divergence between this model and reality is examined and
resolved in Section 7.1.

The results are illustrated in Figure 7. The item shuﬄe
cost is unchanged over the ideal ORAM construction, but
the Bloom ﬁlter shuﬄe cost adds a signiﬁcant fraction to
the overall level construction cost. As can be seen, however,
the log2 n round trips imposed by existing work quickly add
up to exceed the SR-ORAM shuﬄe cost, in even moderate
latency networks. In general, the additional oﬄine cost is
small compared to the savings in the online cost.

The SR-ORAM online cost encompasses the cost of trans-
ferring the query object across the network, reading each

301)
c
e
s
(
 
t
s
o
c
 
y
r
e
u
q

 

d
e
z
i
t
r
o
m
A

)
c
e
s
(
 
t
s
o
c
 
y
r
e
u
q

 

d
e
z
i
t
r
o
m
A

Amortized cost per query vs database size, 150ms RTT

Ideal storage-free ORAM amortized cost
SR-ORAM amortized cost
portion of SR-ORAM cost relating to Bloom filter construction

 8

 7

 6

 5

 4

 3

 2

 1

 0
 10000  100000  1e+06  1e+07  1e+08  1e+09  1e+10  1e+11  1e+12  1e+13

Database size (bytes)

Amortized query cost vs latency, 1TB db

Ideal storage-free ORAM amortized cost
SR-ORAM amortized cost

 20

 40

 60

 80

 100

 120

 140

Network round trip (milliseconds)

 7

 6.5

 6

 5.5

 5

 4.5

 4

 3.5

 3

 2.5

 0

Figure 7: Comparison of amortized (online plus
oﬄine) cost of SR-ORAM to the ideal interactive
storage-free ORAM, assuming the hardware conﬁg-
uration of Figure 5. Left: query cost plotted as the
database size grows on a logarithmic X axis. Right:
comparison vs. latency for a ﬁxed database size.

key part from disk, server-side decryption, and transmit-
ting the results back to the client. The oﬄine performance
cost encompasses the disk, encryption, and network costs of
shuﬄing the items using a 4-pass random shell sort, and con-
structing the Bloom ﬁlter. The step shape results from the
additional level shuﬄed and queried every time the database
size doubles. While this no longer adds an additional round
trip, it does increase the query object size, require reading
an extra set of (e.g. 50) Bloom ﬁlter positions (key compo-
nents) from disk, and require additional construction time.
Discussion. Oﬄine shuﬄing in both the ideal storage-free
ORAM and SR-ORAM takes somewhat longer than that
described in [19] (who achieve over 1 query per second on a
1TB database) because we are eliminating their assumption

√

of client storage.
Instead of using a storage-based merge
scramble that requires log log n passes to sort n items, we
use a randomized shell sort, that requires ≈ 24 log n passes.
This is not a result of our technique—storage can be ap-
plied to speed up our result equivalently—but a result of
the diﬀerent model here, under which we do not provide
O(
n log n) client storage.
Online vs. oﬄine cost.
In the preceeding analysis we
consider the “online” cost to be the cost of performing an
individual query, and the “oﬄine” cost to be the cost of con-
structing the associated levels. However, the top level needs
to be rebuilt in between each query. That could perhaps
more appropriately be considered part of the online cost
rather than the oﬄine cost, resulting in a total of 1.5 round
trips. This is because the client must write back the new
version of the top level after each query. In the single-client
usage model considered in this paper, however, this write-
back can be included with the next item request, returning
our online cost to only a single round trip per query.

The rest of the level constructions can also be performed in
a small number of round trips, constant in the level size. De-
amortization techniques are compatible with this ORAM,
and eliminate the waiting during level shuﬄe. We recom-
mend an appropriate de-amortization technique be applied
to any implementation, but discussion is out of scope for
this paper.

7.1 Dealing with Reality

Although it is convenient to model disk data transfer costs
based only on the average disk throughput, this is not a
complete model, not even for solid state disks. In the solid
state disks we consider above, the disk sector size must be
considered: it speciﬁes the minimum size of a read or write
we can perform. That is, reads or writes of smaller amounts
cost the same as reading/writing the entire sector. This is
problematic for one piece of the SR-ORAM cost analysis:
the random shell sort of segments during the Bloom ﬁlter
construction. The Bloom ﬁlter segments (e.g., 32 bytes)
are potentially much smaller than a disk sector (e.g., 4096
bytes), and are accessed in a random pattern during the
random shell sort.

We now describe implementation details that avoid the
expense resulting from short disk reads. First, leading to a
simple but expensive potential solution, recall that the disk
costs are all server-side costs. This makes it plausible to
solve this cost discrepancy with additional hardware. For
example, a large amount of server RAM (e.g., 48 GB for
a 1TB database) would make it possible to cache the en-
tire Bloom ﬁlter during construction, requiring only a single
contiguous write at the end. A key point is that the Bloom
ﬁlter is smaller during these sort procedures than the ﬁnal
resulting Bloom ﬁlter will eventually be. For example, it is
blown up by about a factor of 4 when converting the bits
(with their, e.g., 64-bit positions) into (e.g., 256-bit) keys
at the end. This conversion process requires only a single,
sequential scan and write. This amount of RAM would re-
sult in a sort faster than the estimates used above (pushing
the bottleneck to client crypto speeds instead of disk I/O
speeds), which assume that disk transfer is required for each
of the ≈ 24 log2 n passes across the data in the random shell
sort. This is not necessarily an unreasonable scenario, as
the RAM is only used for the duration of the sort, making
it feasible to “pool” resources across many clients.

302A second option is to assume enough client memory to
build the Bloom ﬁlter locally. For a 1 TB database consist-
ing of 10KB blocks, and taking the acceptable failure rate to
−128, and 50 hash functions, the total number of Bloom
be 2
ﬁlter positions to set can be under 16 billion bits. This ﬁts in
2GB of client memory. Moreover, as this construction is now
done in private client memory, the oblivious Bloom ﬁlter sort
can be avoided, speeding up the Bloom ﬁlter construction
signiﬁcantly. This process now requires only the network
transfer, and sequential disk write of the key-storing Bloom
ﬁlter (.5TB). However, the client memory requirement for
that (cost-optimal) Bloom ﬁlter construction process is lin-
ear in the total database size, a trait we desire avoiding.

Fortunately, such a workaround is not necessary. We now
illustrate how the randomized shell sort can be performed
without incurring the overhead resulting from the minimum
sector size.
In exchange, we require extra server-side se-
quential scans of the data. Observe that a randomized shell
sort consists of dividing the array to be sorted (sized s) into
equal sized regions, and swapping items between the two re-
gions (called the “compareRegions” operation). The region
sizes start at s/2 and decrease all the way down to 1. In
any compareRegions operation, one region is read sequen-
tially, while the other region is read randomly. Likewise,
the regions are written back in the order they are read.
Two properties are key to eliminating disk seek and par-
tial sector read costs. First, observe that for regions that
ﬁt entirely in the server available cache sized M , the disk
seek / minimum sector cost can be avoided altogether with
an aggressive read-ahead page cache. This is because any
one region can be read and cached in its entirety, and small
regions are accessed contiguously.

Second, when the regions are too big to ﬁt in the server
page cache, the access pattern is still predictable by the
server. This means it can sort data according to the future
√
access pattern. Moreover, this sort can be performed in only
logM n passes. This is 2 passes whenever the server has
n
blocks of RAM, which we believe to be a more than rea-
sonable assumption. The idea is, in one pass, sort data in
groups sized M . In the second pass, merge these n/M re-
gions together, reading contiguous chunks from each region
into the page cache (the default behavior of a read-ahead
page cache). This way, during the shell sort, the items be-
ing read can be accessed contiguously.

For simplicity, the data is sorted back into its start loca-
tion after being written. The result is a sort that performs
eﬃciently, even when the sort item size is much smaller than
the disk sector size. The penalty is that now data is scanned
from the disk up to four times in each sort pass (but this is
more than made up for by the savings of not having to read
an entire disk sector for every access of a 32-byte element).

This cost is reﬂected in the graphs above.
A similar argument comes into play when modeling the
encryption/decryption throughput; our model considers the
sustained throughput over large blocks of data, while when
sorting the Bloom ﬁlter positions, many of the decryptions
and encryptions are on short (e.g.
64-bit) blocks. For-
tunately, the encryption block size (e.g., 256-bit) is much
closer to the encryption size, resulting in the actual crypto
throughput staying within a factor of 1/4 of the modeled
crypto throughput. This analysis is also reﬂected in our
graphs, which measure the entire cost of decrypting a ci-

pher block even when the amount of data to be decrypted
in a single operation is smaller than the block.

8. CONCLUSION

We introduced a new single-round-trip ORAM. We ana-
lyzed its security guarantees and demonstrated its utility.
While non-interactivity is of signiﬁcant theoretic interest in
itself, we also showed this to be the most eﬃcient Oblivious
RAM to date for a storage-free client over today’s Internet-
scale network latencies.

9. ACKNOWLEDGEMENTS

This work was supported in part by the NSF under awards
1161541, 0937833, 0845192, 0803197, 0708025. The authors
would also like to thank the anonymous reviewers for their
excellent feedback.

10. REFERENCES
[1] M. Ajtai, J. Komlos, and E. Szemeredi. An O(n log n)

sorting network. In Proceedings of the 25th ACM
Symposium on Theory of Computing, pages 1–9, 1983.
[2] Dimitri Asonov. Querying Databases Privately: A New

Approach to Private Information Retrieval. Springer
Verlag, 2004.

[3] B. H. Bloom. Space/time trade-oﬀs in hash coding

with allowable errors. Commun. ACM, 13(7):422–426,
1970.

[4] Dan Boneh, David Mazi´eres, and Raluca Ada Popa.

Remote oblivious storage: Making Oblivious RAM
practical. Technical report, MIT, 2011.
MIT-CSAIL-TR-2011-018 March 30, 2011.

[5] Ivan Damg˚ard, Sigurd Meldgaard, and Jesper Nielsen.

Perfectly secure Oblivious RAM without random
oracles. In Theory of Cryptography, volume 6597 of
Lecture Notes in Computer Science, pages 144–163.
2011.

[6] Oded Goldreich and Rafail Ostrovsky. Software
protection and simulation on Oblivious RAMs.
Journal of the ACM, 45:431–473, May 1996.

[7] Michael Goodrich and Michael Mitzenmacher.

Mapreduce parallel cuckoo hashing and Oblivious
RAM simulations. In 38th International Colloquium
on Automata, Languages and Programming (ICALP),
2011.

[8] Michael Goodrich, Michael Mitzenmacher, Olga

Ohrimenko, and Roberto Tamassia. Oblivious RAM
simulation with eﬃcient worst-case access overhead. In
ACM Cloud Computing Security Workshop at CCS
(CCSW), 2011.

[9] Michael T. Goodrich. Randomized shellsort: A simple

oblivious sorting algorithm. In Proceedings 21st
ACM-SIAM Symposium on Discrete Algorithms
(SODA), 2010.

[10] Michael T. Goodrich, Michael Mitzenmacher, Olga

Ohrimenko, and Roberto Tamassia. Practical oblivious
storage. In Proceedings of the second ACM conference
on Data and Application Security and Privacy,
CODASPY ’12, pages 13–24, New York, NY, USA,
2012. ACM.

[11] IBM. IBM 4764 PCI-X Cryptographic Coprocessor

(PCIXCC). Online at http://www-03.ibm.com/

303security/cryptocards/pcixcc/overview.shtml,
2006.

[12] A. Iliev and S.W. Smith. Private information storage

with logarithmic-space secure hardware. In
Proceedings of i-NetSec 04: 3rd Working Conference
on Privacy and Anonymity in Networked and
Distributed Systems, pages 201–216, 2004.

[13] Eyal Kushilevitz, Steve Lu, and Rafail Ostrovsky. On

the (in)security of hash-based oblivious RAM and a
new balancing scheme. In Proceedings of the
Twenty-Third Annual ACM-SIAM Symposium on
Discrete Algorithms, SODA ’12, pages 143–156. SIAM,
2012.

[14] Benny Pinkas and Tzachy Reinman. Oblivious RAM

revisited. In CRYPTO, pages 502–519, 2010.

[15] Sean W. Smith and David Saﬀord. Practical server

privacy with secure coprocessors. IBM Systems
Journal, 40(3):683–695, 2001.

[16] Emil Stefanov, Elaine Shi, and Dawn Song. Towards

Practical Oblivious RAM. In Proceedings of the
Network and Distributed System Security Symposium
(NDSS), 2012.

[17] Shuhong Wang, Xuhua Ding, Robert H. Deng, and

Feng Bao. Private information retrieval using trusted
hardware. In Proceedings of the European Symposium
on Research in Computer Security ESORICS, pages
49–64, 2006.

[18] Peter Williams and Radu Sion. SR-ORAM: Single
round-trip Oblivious RAM. In Industrial Track of
ACNS, 2012.

[19] Peter Williams, Radu Sion, and Bogdan Carbunar.

Building castles out of mud: practical access pattern
privacy and correctness on untrusted storage. In ACM
Conference on Computer and Communications
Security (CCS), pages 139–148, 2008.

304