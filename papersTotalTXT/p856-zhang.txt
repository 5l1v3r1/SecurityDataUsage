ALITHEIA: Towards Practical Veriﬁable Graph Processing

Yupeng Zhang

ECE Dept. & UMIACS
University of Maryland
zhangyp@umd.edu

Charalampos Papamanthou

ECE Dept. & UMIACS
University of Maryland
cpap@umd.edu

Jonathan Katz

Computer Science Dept. & UMIACS

University of Maryland
jkatz@cs.umd.edu

ABSTRACT
We consider a scenario in which a data owner outsources storage
of a large graph to an untrusted server; the server performs compu-
tations on this graph in response to queries from a client (whether
the data owner or others), and the goal is to ensure veriﬁability of
the returned results. Existing work on veriﬁable computation (VC)
would compile each graph computation to a circuit or a RAM pro-
gram and then use generic techniques to produce a cryptographic
proof of correctness for the result. Such an approach will incur
large overhead, especially in the proof-computation time.

In this work we address the above by designing, building, and
evaluating ALITHEIA, a nearly practical VC system tailored for
graph queries such as computing shortest paths, longest paths, and
maximum ﬂow. The underlying principle of ALITHEIA is to min-
imize the use of generic VC systems by leveraging various algo-
rithmic techniques speciﬁc for graphs. This leads to both theoret-
ical and practical improvements. Asymptotically, it improves the
complexity of proof computation by at least a logarithmic factor.
On the practical side, we show that ALITHEIA achieves signiﬁ-
cant performance improvements over current state-of-the-art (up to
a 108× improvement in proof-computation time, and a 99.9% re-
duction in server storage), while scaling to 200,000-node graphs.

Categories and Subject Descriptors
K.6.5 [Management of Computing and Information Systems]:
Security and Protection

Keywords
Veriﬁable Computation; Graph Processing; Cloud Computing

1.

INTRODUCTION

Graph algorithms are everywhere. For instance, navigation sys-
tems run the Dijkstra or Floyd-Warshall algorithms to compute
the shortest route between two locations, and various problems in
transportation networks can be modeled as maximum-ﬂow com-
putations. In the era of cloud computing, however, the owner of

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright 2014 ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660354 .

the (data underlying the) graph may not be the same entity run-
ning computations over this graph. Speciﬁcally, a (trusted) data
owner with small local memory might outsource storage of a large
graph to a server, who will then answer queries about the graph
made by various clients. The goal is to ensure veriﬁability of the
returned results, thus protecting clients against bugs in the server’s
code, malicious behavior by the server, or server compromise. (The
naive solution of having the data owner authenticate the graph, and
then having the client download/verify the graph and compute the
result on its own, imposes unacceptable bandwidth, storage, and
computational costs on the client.)

Precisely this setting is addressed by early work on authenticated
data structures [34, 28, 5], as well as more recent work on the
broader problem of veriﬁable computation (VC) [18, 14, 9, 10, 19,
11, 33, 32, 30, 7, 12, 36]. Such schemes enable the server to pro-
vide a cryptographic proof of correctness of the returned result,
which can be veriﬁed by the client posing the query.

There are several parameters of interest when it comes to de-
signing VC protocols. Perhaps the main concerns are that the size
of the proof should be small (ideally, proportional to the size of the
result itself), and the veriﬁcation time for the client should be low.
(We stress, however, that in our setting we are mainly interested
in outsourcing storage rather than computation.) These is particu-
larly important when proof veriﬁcation might be done by resource-
constrained clients (e.g., smartphones). Other measures of interest
include the time for the server to compute the proof (especially im-
portant for latency), the storage required by both the server and the
clients, and the preprocessing time required by the data owner.

A VC protocol for graph algorithms can be derived in theory via
black-box use of existing general-purpose VC schemes [18, 14, 24,
9, 10, 19, 11], or one of the systems that have been built to apply
these techniques [33, 32, 30, 7, 12, 36]. Applying these general-
purpose protocols to graph algorithms, however, does not necessar-
ily yield practical protocols. (See Table 1.) This is partly just a
consequence of their generality, but is more speciﬁcally due to the
fact that most of these systems require the computation being per-
formed to be represented as a (boolean or arithmetic) circuit. For
graph computations, a circuit-based representation will not be op-
timal, and a RAM-based representation is preferable. Recent work
on RAM-based VC schemes by Braun et al. [12] and Ben-Sasson
et al. [7], however, are not quite practical yet—see Table 1.
Our contributions. To address the above problems we design,
build, and evaluate ALITHEIA, a system for nearly practical veriﬁ-
able graph processing. Currently, ALITHEIA handles shortest-path,
longest-path, and maximum-ﬂow queries over weighted directed
and undirected graphs (as applicable). The speciﬁc contributions
of ALITHEIA are as follows:

856Table 1: Summary of our results, and comparison with existing approaches, for verifying shortest-path queries. For asymptotic results, we
consider a planar graph with n nodes and m edges, and let |p| denote the length of the shortest path. Experimental results refer to the (planar)
road network of the city of Rome (n = 3,353 nodes, m = 8,870 edges, |p| = 13), taken from the 9th DIMACS implementation challenge for
shortest paths [3]. Nodes in this graph correspond to intersections between roads, and edges correspond to road segments. Results marked
with ∗ are estimates (see Section 6), since we could only run [30, 12] on small graphs and the full implementation of [7] is not yet available.

PINOCCHIO [30]
(Circuit-Based VC)

Preprocessing Time O(nm)
Prover Time
Proof Size
Veriﬁcation Time
Preprocessing Time
Prover Time
Proof Size
Veriﬁcation Time

O(nm log m)
O(1)
O(|p|)
19,000 hours∗
7,600 hours∗
288 bytes
0.008 seconds∗

PANTRY [12]
(RAM-Based VC)
O(m log m)
O(m log2 m)
O(1)
O(|p|)
270 hours∗
550 hours∗
288 bytes
0.008 seconds∗

SNARKS FOR C [7] ALITHEIA
(RAM-Based VC)
O(m log m)
O(m log2 m)
O(1)
O(|p|)
52 hours∗
30 hours∗
288 bytes
0.049 seconds∗

(general graphs)
O(m)
O(m log m)
O(|p|)
O(|p|)
69 minutes
3.3 minutes
704 bytes
0.19 seconds

ALITHEIA
(planar graphs)
√
√
O(m
m log m)
m log2 m)
O(
O(log m + |p|)
O(log m + |p|)
4.1 minutes
13 seconds
3,872 bytes
0.592 seconds

1. On the theoretical side, ALITHEIA asymptotically reduces
the preprocessing and proof-computation time relative to the
best previous approaches. E.g., for shortest-path queries in a
graph with m edges, it saves a factor of O(log m) (see Sec-
tion 3). For planar graphs (ones that can be drawn in the
plane without edge crossings), ALITHEIA reduces the num-
ber of cryptographic operations needed for proof computa-
tion even more, by a factor of O(

√
m) (Section 4).

2. On the practical side, ALITHEIA achieves signiﬁcant per-
formance improvements, bringing veriﬁable computation on
graphs closer to practice (Section 5). Speciﬁcally, compared
to existing state-of-the-art it improves the running time of
the prover by over a factor of 108 (for graphs with 100,000
nodes), and the server storage by 99.9%. We validate such
performance savings on real-world graphs as well (e.g., the
numbers in Table 1 are from running ALITHEIA on the road
network of the city of Rome [3]). Finally, ALITHEIA is the
ﬁrst VC system that can scale up to 200,000-node graphs.

3. ALITHEIA supports dynamic updates (where edges can be
added to or removed from the original graph) in logarithmic
time (Section 3.4). Some existing RAM-based VC schemes
support such updates in theory (e.g., see [12]), but not in
practice for graphs of reasonable size.

Table 1 provides a detailed theoretical and practical compar-
ison of ALITHEIA with current state-of-the-art systems [30, 12,
7] for answering shortest-path queries on general graphs. Espe-
cially for planar graphs, ALITHEIA offers the best practical per-
formance among all presented schemes. Preprocessing time and
proof-computation time are much improved, at the expense of a
reasonable increase in the proof size and veriﬁcation time. We note
also that although the overall asymptotic complexity for prepro-
cessing planar graphs using ALITHEIA is larger than for general
graphs, the number of cryptographic operations needed in the pla-
nar case is also O(m), but with a much lower constant. Asymptot-
ically, for general graphs, although the proof size in ALITHEIA in-
creases from O(1) to O(|p|) (where p is the shortest path), the pro-
tocol’s asymptotic bandwidth remains the same (i.e., O(|p|)) since
the actual path p must be communicated to the client anyway.
Our techniques. We ﬁrst brieﬂy describe the approach used by
ALITHEIA for handling shortest-path queries in a general, non-
planar (un)directed graph G = (V, E); see Section 3 for further
details. Consider a request for the shortest path from some node

s to another node t. At a high level, what we want to do is to en-
code correct computation of the result as an NP statement whose
validity can then be veriﬁed using existing systems (e.g., [30, 12]).
The naive way to do this would be to certify correct execution of,
say, Dijkstra’s shortest-path algorithm on the given inputs; this ap-
proach, however, would be prohibitively slow. Instead, we rely on
a certifying algorithm for shortest paths [31]. This allows us to en-
code the correct result as a simple set of constraints (see Relation 1)
on the shortest paths from s to all nodes in the graph, which can be
computed by the server with no cryptographic work. The only cryp-
tographic work required is for the veriﬁcation of these constraints
using existing systems. We use similar techniques to design veriﬁ-
able protocols for longest-path and maximum-ﬂow queries.

Although the above technique signiﬁcantly reduces the practi-
cal overhead of existing solutions, it requires the use of a general-
purpose system (in our case, PINOCCHIO) on a relation of size
O(m), where m is the number of edges of the graph. Unfortu-
nately, as we show in our experiments, such an approach does not
scale for graphs that exceed 10,000 nodes. We address this problem
in Section 4 (and scale up to 200,000 nodes) by taking advantage
of the special structure of planar graphs, i.e., those that can be em-
bedded in the plane without any crossings. Planar graphs are inter-
esting in our context since they generally provide a good model for
vehicular and road networks used by navigation applications. We
derive a more efﬁcient protocol for shortest-path queries in planar
graphs by leveraging a data structure based on the celebrated planar
√
separator theorem [22]. This data structure answers shortest-path
n log n) time, and its main operation relies on per-
queries in O(
√
forming a MIN computation over the sum of two vectors of size
n). To verify the data structure’s operation, we cannot use
O(
√
common authenticated data-structure techniques (e.g., [34, 28, 5])
n) size, where n is the num-
since these would yield proofs of Ω(
ber of nodes in the graph. Instead, we achieve logarithmic-sized
proofs by using a general-purpose system only on the MIN rela-
tion. This approach, combined with an additively-homomorphic
vector commitment scheme [27], yields an improvement of 29× in
the prover time for graphs with 10,000 nodes (compared to the ap-
proach described before), and allows us to produce shortest-path
proofs on graphs with up to 200,000 nodes.
Other related work. We have already discussed generic VC pro-
tocols above, so here we only brieﬂy mention the few prior VC
protocols we are aware of that are speciﬁcally tailored to graph
computations. Yiu et al. [37] presented a veriﬁable protocol for
shortest-path queries. However, although their proof-computation

857time is shorter than ours, their protocols have worst-case proof size
linear in the number of the edges of the graph. Goodrich et al. [20]
presented authenticated data structures for various graph queries
such as graph connectivity/biconnectivity but their work does not
cover advanced graph computations such as shortest-path queries.
Using certifying algorithms for fast cryptographic veriﬁcation
has been proposed for NP-complete problems on graphs [7], where
it is clear that, assuming P(cid:54)=NP, verifying is cheaper than comput-
ing. This is known to be the case for only a few problems in P.
Efﬁcient and simple certifying algorithms have been used for ver-
ifying set queries [29] and data-structure queries [35], but to the
best of our knowledge they have not been used for graph queries.

2. PRELIMINARIES

We now present deﬁnitions for the cryptographic primitives that
we use: (1) veriﬁable computation for graphs, (2) vector commit-
ments [13], and (3) succinct non-interactive argument of knowledge
(SNARKs) [19].
Veriﬁable computation for graphs. In our setting, there are three
parties: A trusted data owner, an untrusted server, and a client
(who may also correspond to the data owner). The data owner out-
sources storage of a graph G to the untrusted server, who answers
queries posed by the client.

DEFINITION 1

(VC FOR GRAPHS). A VC scheme V consists
of 3 PPT algorithms: (1) {ekG, vkG} ← genkey(1k, G): Given a
graph G, it outputs evaluation key ekG and veriﬁcation key vkG;
(2) {πq, α} ← compute(q, ekG): On input a graph query q, it out-
puts a proof πq and an answer α; (3) {0, 1} ← verify(πq, q, α, vkG):
On input πq, q, and α, it outputs 0 or 1.

The above algorithms are used as follows. First, the data owner
executes genkey and sends the evaluation key ekG and the graph
G to the server. The veriﬁcation key vkG can be published (in the
setting of public veriﬁability) or held privately by the data owner
(in the setting of private veriﬁability). The client, who is assumed
to know vkG, can then send a query q to the server (e.g., q might
ask for the shortest path from s to t). The server computes the
answer α (e.g., the shortest path p) and proof πq using compute.
The client then veriﬁes the validity of the response α by executing
verify. Deﬁnitions of correctness and security for VC for graphs
can be found in the Appendix (see Deﬁnition 3).
Vector commitments. A vector commitment scheme (VCS) en-
ables a prover to commit to a vector S via a small digest d(S) (usu-
ally of constant size). The prover can later open speciﬁc entries
of S. A concrete example is given by a Merkle hash tree [23], if the
entries of the vector are placed at the leaves of the tree. Deﬁnitions
of a VCS and it security requirements are given in the Appendix
(see Deﬁnition 4); we stress that for our applications we care about
binding but not privacy.

In our protocol for planar graphs (see Section 4), we use a VCS
that is additively-homomorphic, i.e., having the property that for
any two vectors S1 and S2, it holds that d(S1) + d(S1) = d(S1 +
S2). We implement an additively-homomorphic VCS (see Fig-
ure 10 in the Appendix) using the streaming authenticated data
structure of Papamanthou et al. [27], which is based on a cryp-
tographic assumption related to lattices.
SNARKs. A SNARK enables an untrusted prover to prove that
some statement x is indeed in some NP language L. Speciﬁcally,
what is proved is that there exists a valid NP witness w for x. The
proof for such a statement is succinct (i.e., constant size), even if
the witness w is large. In the following deﬁnition, we let a two-

input circuit C deﬁne the language L where x ∈ L iff there exists
a w such that C(x, w) = 1.

DEFINITION 2

(SNARK [30]). A SNARK scheme G consists
of three PPT algorithms: (1) {ekL, vkL} ← genkey(1k, C): On
input the security parameter and a circuit C, output evaluation key
ekL and veriﬁcation key vkL; (2) πx ← compute(x, ekL): On
input x ∈ L, output proof πx; (3) {0, 1} ← verify(πx, x, vkL):
On input πx and x, output either 0 or 1.

Deﬁnitions of correctness and security for SNARKs can be found
in the Appendix (see Deﬁnition 5). As in the case of VC, a SNARK
can be publicly or privately veriﬁable. As with VC, a SNARK is
secure if no polynomially-bounded adversary can compute a proof
π and a statement x /∈ L such that 1 ← verify(π, x, vkL), except
with negligible probability. One important property of SNARKs,
not shared with VC, is that they support extractability; namely,
there is an extractor that can use a valid proof πx for x to extract
the corresponding NP witness w for x.

As we discuss in Section 5, we use the recent SNARK imple-
mentation by Parno et al. [30] in our schemes. In their SNARK,
the complexity of genkey is O(|C|), the complexity of compute
is O(|C| log |C|), and the complexity of verify is O(|x|) (i.e., the
length of the ﬁrst input to C). Also, the size of the proof πx is
O(1) (288 bytes), the size of the evaluation key is O(|C|), and the
size of the veriﬁcation key is O(|x|). We note here that the extra
multiplicative logarithmic overhead of a SNARK is removed by the
bootstrapping approach of [10], implemented in [8]. However, this
does not change the bottom line of our work since both the generic
and tailored solution beneﬁt.
3. VC FOR GENERAL GRAPHS
Let G = (V, E) be an (un)directed graph with positive weights
cuv on its edges. Set |V | = n and |E| = m. Let p = v1v2 . . . vk
i=1 cvivi+1. In this section
we show how to construct VC schemes and protocols for general
(un)directed graphs. Sections 3.1, 3.2, 3.3, 3.4 refer to shortest
paths while Section 3.5 refers to longest paths and maximum ﬂow.
3.1 Strawman Solution

denote a path in G of length |p| = (cid:80)k−1

We can construct a VC scheme for shortest paths in a graph G as
follows. In the genkey algorithm of the VC scheme, we compute
and then cryptographically sign the shortest paths (u, v, puv) for
all (u, v) ∈ V × V . Then we output all the signatures as the evalu-
ation key and the public key of the signature as the veriﬁcation key.
Depending on the user query, the compute algorithm just returns
the respective signature along with the path itself. The verify al-
gorithm veriﬁes the signature and decides whether the correct path
was returned. This solution however is very expensive: It requires
O(n3) cost for setup and produces an evaluation key of O(n2) size
(all the signatures).
3.2 Using General-Purpose Systems

To reduce the asymptotic complexity of the strawman approach
above, one can use a general-purpose system for VC/SNARKs [30]
to veriﬁably execute BFS—which we did in Section 5 (assume unit-
weight graphs for the purpose of this section). However, most ex-
isting systems implementing VC require the computation (in our
case, the BFS algorithm) to be expressed as a circuit. This leads
to a blow-up in complexity since the size of a circuit implement-
ing BFS is O(mn) (see Figure 3), which is not linear. As we
show in the evaluation section, this signiﬁcantly affects practical
performance—apart from the quadratic complexity itself, large con-
stants are also involved.

858To avoid expressing the computation as a circuit, one can use
recently proposed methods [12, 7] for verifying RAM programs.
Roughly speaking, for a RAM program executing in time T (n)
using memory of size n, such an approach requires using a VC
scheme on a circuit of size O(T (n) log n). Also, there is the extra
multiplicative logarithmic overhead of a SNARK, thus the prover
runs in O(T (n) log T (n) log n) time. E.g., to veriﬁably execute
BFS, the prover cost is O(m log2 m). Unfortunately, as we show
in the evaluation section, such approaches do not scale in practice.
3.3 Our Method: Using Certifying Algorithms
Let s be the source and t be the destination of our shortest path
query, and let S[v] denote the distance to node v from node s for
v ∈ V (we view S as a vector of n entries).

Our approach is based on the observation that in order to verify
that path p is the shortest path from s to t in G, there is no need to
verify every step of Dijksrta’s algorithm that computes p. Instead
one can verify a few constraints that need to hold on the claimed
distance vector S and on the path p. (The distance vector S can be
computed independently with any algorithm we wish.) Speciﬁcally,
consider the following NP language

 (s, t, p) : ∃ S such that :

(1) S[s] = 0 ∧ S[t] = |p| ;
(2) ∀(u, v) ∈ E : S[v] ≤ S[u] + cuv .

 .

(1)

FG =

One can easily see that if (s, t, p) ∈ FG then |p| ≤ dst, where dst
is the actual shortest path length from s to t in G (a straightforward
proof for that claim can be found in [31]). Therefore we can write

p is a shortest path from s to t in G ⇔
(s, t, p) ∈ FG ∧ p is an s-to-t path in G ,

(2)
since, if p is an s-to-t path in G, there is no way it can be shorter
than the shortest path from s to t and therefore it has to be the case
that |p| = dst. As we see in the following, our VC scheme is based
on exactly verifying the above relation.
VC construction. We build the VC scheme for shortest paths as
follows. First, in the genkey algorithm, we instantiate a SNARK
for FG and also sign all the edges (u, v, cuv) of the graph G.

Given query (s, t), the compute algorithm just returns the signa-
tures on the edges (and weights) comprising the shortest path p and
a proof π that (s, t, p) ∈ FG. Computing the SNARK proof takes
time O(m log m) [30]. Moreover, it scales in practice because FG
has a very efﬁcient circuit representation (see below).

The verify algorithm outputs 1 only if it all the signatures on
the edges of p are valid and the SNARK proof π is correct. The
security of the scheme follows directly from the security of the used
SNARK for FG and the security of the signature scheme.
An efﬁcient circuit for FG. The constraints in Relation 1 can be
represented with an efﬁcient circuit that takes as input (s, t, p) and
some distance vector S and outputs 1 if and only of the constraints
are satisﬁed. The size of this circuit is O(m) since only two ran-
dom accesses are required: One for accessing S[s] and one for ac-
cessing S[t] (this is because s and t can change depending on user
input). These two accesses can be “unrolled” into an O(n)-sized
circuit and therefore the circuit’s asymptotic size is not affected.
The second constraint is easily “hardcoded” into the circuit since
the graph G is ﬁxed. The resulting circuit is dramatically simpler
than the circuit representing BFS or Dijkstra’s algorithm.

3.4 Support for Dynamic Graphs

As we showed before, the circuit for FG hardcodes the con-
straints on the edges of the graph. Therefore whenever G changes,
one must re-execute the genkey(1k,FG) algorithm (see Deﬁni-
tion 2) to output the new evaluation and veriﬁcation keys of the
SNARK. This results in an O(m) cost for updates. We now de-
scribe a VC scheme without this problem, with efﬁciently updat-
able veriﬁcation and evaluation keys. For clarity of presentation,
we consider the case where the number of the edges of the graph
remains the same (equal to m) and the updates we are supporting
are replacement of an edge e with another edge e(cid:48).

First, instead of signing every edge (u, v) of G (as we did in
the previous construction), we represent G with a matrix E of n2
entries such that (i, j) ∈ E if and only if E[i, j] = 1—otherwise
E[i, j] = 0 (sometimes we abuse notation and write (i, j) ∈ E).
Then we use a vector commitment scheme VE to produce a graph
digest dE = VE.digest(E) (we view the matrix as a vector in the
obvious way). Digest dE comprises part of the veriﬁcation key
which can be efﬁciently updated using algorithm update from our
additively-homomorphic VCS in Figure 10 in the Appendix.
Now, instead of hardcoding the edge constraints in FG’s circuit,

we can write

FG =



(dE, s, t, p) : ∃ E and S such that :

(1) VE.digest(E) = dE ;
(2) S[s] = 0 ∧ S[t] = |p| ;
(3) ∀(u, v) ∈ E : S[v] ≤ S[u] + 1 .

 .

(3)

Although the above representation allows for efﬁcient edge updates
(one can just update dE), it has Ω(n2) size since it must iterate
through all the entries of E in constraints (1) and (3).

To avoid this limitation we can rewrite this language by extend-
ing its inputs such that: (i) the edges (i, j) such that E[i, j] = 1 are
given as explicit input; (ii) the distances di and dj corresponding to
the endpoints of these edges (i, j) are also given as input. All these
are captured by the version of FG in Figure 9 in the Appendix. It is
easy to see that the size of the circuit implementing FG in Figure 9
is proportional to O(m · poly(log n)), where the polylogarithmic
factor depends on the type of VCS that we use.
3.5 Longest Paths, Maximum Flows

Finding longest paths in directed acyclic graphs (DAGs) from a
source s to a sink t can be used in scheduling (e.g., ﬁnding the crit-
ical path) or in graph drawing (e.g., computing a layered drawing
of a graph). It is straightforward to verify longest paths on DAGs
by slightly changing Relation 1. Speciﬁcally, for longest paths, one
has to check that ∀(u, v) ∈ E it is S[v] ≥ S[u] + cuv instead of
S[v] ≤ S[u] + cuv. Note that the edge set E is now directed.

Additionally, ALITHEIA can handle maximum-ﬂow queries. We
use the maxﬂow-mincut theorem [15] stating that given a directed
graph G with source s, sink t, and capacities cuv on the edges
(u, v) ∈ E, a maximum ﬂow F always equals the minimum cut. To
verify a maximum ﬂow based on this theorem we build a SNARK
that takes as input the source s and destination t, the ﬂow assigne-
ment on every edge f, a disjoint partition of the node set (S, T ),
and the maximum ﬂow F. Besides, the capacity of every edge is
hardcoded in the SNARK. The following relation is checked:

THEOREM 1. Let G be a graph with n nodes and m edges.
Our VC scheme for shortest paths in G has (i) O(m) preprocessing
time; (ii) O(m log m) prover time (iii) O(|p|) proof size and (iv)
O(|p|) veriﬁcation time, where p is the output shortest path.

MG =



(s, t, F): ∃ f and disjoint S and T such that :
(1) s ∈ S ∧ t ∈ T ;
e∈out(s) fe = F ;
(3) ∀e ∈ E : fe ≤ ce ;

(2)(cid:80)
(4) ∀u /∈ {s, t} : (cid:80)
(5)(cid:80)

e∈in(u) fe =(cid:80)

e∈S×T ce = F .

e∈out(u) fe ;

 .

859We note here that verifying the above maximum ﬂow relation
takes O(m) time, while there is no existing algorithm with linear
asymptotic complexity to solve maximum ﬂow problem.

4. VC FOR PLANAR GRAPHS

A planar graph is a graph that can be drawn in the plane without
any crossings [6]. Planar graphs have various applications, e.g.,
they can model vehicular and road networks. Due to their special
structure, more efﬁcient algorithms are known for planar graphs
and ALITHEIA takes advantage of such structure.

Speciﬁcally, in this section we construct a VC scheme for verify-
ing shortest path queries in (un)directed planar graphs. Contrary to
√
the case of general graphs, we show that for planar graphs we can
√
n log n) (which is equiva-
construct a prover that runs in time O(
m log m) since in planar connected graphs it always is
lent to O(
m = Θ(n)). As we will see in the experimental section, this trans-
lates into signiﬁcant practical savings as well, enabling us to scale
veriﬁable computation on 200,000-node graphs for the ﬁrst time.
4.1 The Planar Separator Data Structure

3 , |G2| ≤ 2n

√
3 and |G0| = O(

Our approach is based on a novel data structure that makes use
of the planar separator theorem [22]. The planar separator theo-
rem states the following: For every planar graph G = (V, E) of
n nodes, one can always partition the vertices V of G in three sets
G1, G0, G2 such that |G1| ≤ 2n
n)
and such that all the paths from G1 to G2 go through G0. Many
data structures have appeared in the literature (e.g., see [16]) that
use various versions of the above theorem to answer shortest path
queries in sublinear time—instead of quasilinear time that Dijk-
stra’s algorithm would require. We describe one simple such tech-
nique (and the one we are using in our approach) in the following.
Data structure setup. Let G = (V, E) be a planar undirected
graph of n nodes that has positive weights cuv, where (u, v) ∈ E.
We ﬁrst decompose G into the partition (G1, G0, G2) using the
planar separator theorem. Then we recursively apply the planar
√
separator theorem on G1 and G2 until we are left with partitions of
n) nodes. After the recursion terminates, the initial graph G
O(
will be represented with a binary tree T (called separator tree) of
√
n) nodes and O(log n) depth such that an internal tree node
O(
t contains the nodes of the separator of the graph which is induced
√
by the nodes contained in t’s subtree. See Figure 1.
n) graph nodes.
Let now u be a node of the original graph G. We deﬁne path(u)
to be a tree path from tree node u that contains u to the root r of T .
The separator tree data structure will contain, for all graph nodes
u ∈ G, the following precomputed distances (shortest paths):

Every internal separator tree node contains O(

{Sup : p ∈ path(u)} ,

√

√
n

where Sup is a vector of size |p| storing all the shortest paths suv
√
from u to all v ∈ p. Since tree T has O(
n) nodes and each
node requires O(
n log n) space, the total space of the data
structure is O(n3/2 log n). Note this is a signiﬁcant improvement
over the naive data structure that precomputes the shortest paths
and requires O(n2) space.
Data structure querying. Suppose now we want to query the
shortest path from u to v. First locate separator tree nodes u and v
that contain u and v respectively. We now distinguish the cases:

1. If tree node u is an ancestor of the tree node v or vice-versa,
then simply return suv (which was precomputed in setup and
is an element of the vector Suv);

Figure 1: A planar graph (left) along with its planar separator tree
(right). Nodes 1, 2, 3 comprise the main separator of the graph and
nodes 4, 5 and 6, 7 comprise the separators at the second level.

2. Otherwise, ﬁnd tree nodes t1, t2, . . . , tk that are common in

path(u) and path(v). Then return

suv = min

i=1,...,k

{Suti + Svti} ,

(4)

where + above denotes vector addition.

Therefore, by using the separator tree data structure, we have re-
duced the problem of computing shortest paths on planar graphs to
the problem of performing one minimum computation.

The above approach works because all the paths from one node
of the planar separator tree to another node of the planar separa-
tor tree go through nodes that are common ancestors in the tree.
√
Since the separator tree has O(log n) levels and each node of it
√
n) nodes, it follows that the output of Relation 4 can be
has O(
computed in O(
n log n) time.
4.2 VC Construction

We now present the VC construction for verifying shortest paths

in planar graphs using the above data structure.
Setup. At setup, we initialize an additively-homomorphic vec-
tor commitment scheme V (as in Section 2). We also initialize a
SNARK scheme G for the following language

 .

(5)



L =

(dig, min) : ∃ S and ind such that :

(1) V.digest(S) = dig ;
(2) S[ind] ≤ S[i] ∀ i = 0, . . . , M − 1 ;
(3) S[ind] = min .

Namely language L contains pairs consisting of a vector digest
dig and an element min such that element min is the minimum
among the elements contained in the vector represented with dig.
The SNARK is used for verifying a relation similar to Relation 4.
Then, working on graph G, we build the planar separator tree
T and compute the shortest path vectors Sup for all u ∈ G and
p ∈ path(u). Thereafter, we commit to the shortest path vectors
by computing the digests (using the vector commitment scheme V)

dup ← V.digest(Sup,V.pk) for all u ∈ G and p ∈ path(u).

For clarity of presentation, we are going to assume that the veriﬁca-
tion key of the ﬁnal VC scheme contains (i) The digests dup for all
u ∈ G and p ∈ path(u); (ii) the structure path(u) for all u ∈ G;
(iii) the graph G itself, along with the weights cuv on the edges
(u, v). Although storing all this information requires at least linear
space, it is easy to outsource it by computing a digital signature of
each object above. Actually, this is how our implementation works
(with the difference that an HMAC instead of a signature is used).

5123467dfgebca1354dfgebca276860Proof computation and veriﬁcation.
In the proof computation
phase, a proof must be constructed showing that suv is the shortest
path from u to v. Let now v be the separator tree node containing
graph node v and let u be the separator tree node containing graph
node u. Then we need to distinguish two cases, depending on the
location of the nodes u and v on the separator tree. We have the
following cases:
Case 1. If v belongs on the separator tree path from u to the sep-
arator tree root r (or vice versa), then the shortest path suv has
been precomputed and is an element of the vector Suv. Therefore it
sufﬁces for the prover to return a proof for Suv’s element that cor-
responds to the shortest path suv. Then the veriﬁer can verify this
proof using the digest duv.
Case 2. Otherwise, the prover takes the following steps:

1. It computes the common ancestors t1, t2, . . . , tk of u and v in
the separator tree T . Recall, that, due to the planar separator
structure, all the shortest paths from u to v must pass through
one of these nodes.

2. Let now mini (for i = 1, . . . , k) be the minimum element
of the vector Suti + Svti, occurring at node wi ∈ ti, i.e.,
mini = suwi + svwi. For i = 1, . . . , k, the prover outputs
a SNARK proof πi for yi = (duti + dvti , mini) ∈ L by
calling G.compute(yi, ekL). This proof is used to prove that
mini is the minimum of vector Suti + Svti (and therefore a
potential length for the shortest path from u to v). After all
SNARK proofs for mini (i = 1, . . . , k) are veriﬁed by the
veriﬁer, then he can verify the length of the shortest path as
min{min1, min2, . . . , mink}.

The detailed description of our VC scheme is shown in Figure 2.
Asymptotic complexity and security. Let n = Θ(m) be the num-
ber of nodes in a planar graph. First, the most costly operation
of genkey is the computation of the planar separator data struc-
ture. By using standard results from the literature [16], this cost is
O(m3/2 log m). Note that this is an one-time cost.

As far as algorithm compute is concerned, the cost is dominated
by computing one proof using the vector commitment scheme, which
√
takes O(log m) time—see [27]) and O(log m) SNARK proofs.
m log m) time (since
Since computing a SNARK proof takes O(
m)),
the description of the language we are encoding has size O(
the total worst-case cost of computing the proof is O(
m log2 m).
Finally, the size of the proof is O(log m + |p|) since O(log m)
SNARK proofs must be returned as well as signatures on the edges
of the path, and the veriﬁcation time is O(log m+|p|). The security
of the ﬁnal VC scheme follows directly from the security of VCS
scheme and the security of the SNARK scheme. We summarize the
above in the following theorem:

√

√

√
THEOREM 2. Let G be a planar graph with m = Θ(n) edges.
√
Our VC scheme for shortest paths in G has (i) O(m
m log m)
m log2 m) prover time (iii) O(log m+
preprocessing time; (ii) O(
|p|) proof size and (iv) O(log m +|p|) veriﬁcation time, where p is
the output shortest path.

5.

IMPLEMENTATION

In this section we present the implementation of ALITHEIA. Re-
call that the main building blocks used by ALITHEIA are an ad-
ditively homomorphic vector commitment scheme (VCS) and a
SNARK. We give details about those in what follows.
Implementing VCS. A vector commitment scheme (VCS) can be
implemented with a Merkle hash tree using a collision-resistant

Algorithm {ekG, vkG} ← genkey(1k, G)

• V.pk ← V.genkey(1k, M ).
• {G.ekL,G.vkL} ← G.genkey(1k, L), where L is from (5).
• Compute planar separator tree T .
• Compute vectors Sup ∀u ∈ G and ∀p ∈ path(u).
• Set dup ← V.digest(Sup,V.pk) ∀u ∈ G and ∀p ∈ path(u).
Evaluation key ekG contains keys V.pk, G.ekL, the separator tree
T and the vectors Sup for all u ∈ G and p ∈ path(u). Veriﬁcation
key vkG contains V.pk, G.vkL, the digests dup for all u ∈ G and
p ∈ path(u) and the information path(u) for all u ∈ G.
Algorithm {π, p} ← compute((u, v), ekG)

• If path(u) ⊆ path(v) (or vice-versa), output

(suv, πuv) ← V.query(v, Suv,V.pk), where v ∈ v.

• Otherwise, let {t1, t2, . . . , tk} = path(u)(cid:84) path(v).

For i = 1, . . . , k, let mini be the minimum element of the
vector Suti + Svti occurring at graph node wi ∈ ti.
For i = 1, . . . , k, output πi ← G.compute(yi, ekL), where
yi = (duti + dvti , mini) ∈ L.

Output πuv or πi (i = 1, . . . , k) as π and p as the shortest path.
Algorithm {1, 0} ← verify(π, (u, v), p, vkG)

• If path(u) ⊆ path(v) (or vice-versa), check that
1 ← V.verify(v, suv, πuv, duv, pk) .

• Otherwise, let {t1, t2, . . . , tk} = path(u)(cid:84) path(v).

For i = 1, . . . , k check that

1 ← G.verify(πi, (duti + dvti , mini), vkL).

• Check that path p in G has length min{min1, . . . , mink}.
• If all checks succeed output 1, else output 0.

Figure 2: A VC scheme for shortest paths in a planar graph G.

hash function such as SHA-2, e.g., see [23]. However, such a sim-
ple scheme is not additively-homomorphic and our planar separator
scheme requires to use such a scheme. Luckily enough, additive
homomorphism is satisﬁed by various existing schemes, e.g., see
the vector commitment scheme of Catalano and Fiore [13] or the
veriﬁable data streaming scheme of Papamanthou et al. [27].

For practical efﬁciency reasons (e.g., lack of modular exponen-
tiations), our planar separator implementation uses the vector com-
mitment scheme of Papamanthou et al. [27], the security of which
is based on the difﬁculty of the small integer solution (SIS) problem
in lattices (see Assumption 1 in [27]). Also its query and veriﬁca-
tion complexity is O(log2 M ), where M is the size of the input
vector. The exact algorithms we implemented are in Figure 10 in
the Appendix.

Roughly speaking, this scheme is a Merkle hash tree [23] that,
instead of SHA-2, uses the lattice-based hash function h(x, y) =
Lx + Ry mod q, where L, R ∈ Zk×m
and x, y ∈ [N ]m [4]. Due
to additive properties of the lattice-based hash function, the digest

q

861(cid:88)

i

d(S) of the produced Merkle hash tree can be expressed as

d(S) =

S[i]f (i) mod q ,

(6)

where f (i) ∈ {0, 1}m is a function of the index i (called a “partial
label” in [27]) and S[i] is the vector element at position i which is a
scalar in Zq. The above digest is clearly additively-homomorphic.
The parameters k, q, m, N used above depend on the size M of
the vector and on the maximum vector entry max and are computed
by algorithm genkey() in Figure 10 in the Appendix. Speciﬁcally,
algorithm genkey() calls parameters() which in turn imposes two
constraints on the parameters k, q, m, N. The ﬁrst constraint guar-
antees that the security reduction from a hard lattices problem to
breaking the security of the vector commitment scheme will be suc-
cessful [25]. The second constraint guarantees a level of security
that is at least 100 bits [26]. An example set of parameters that sat-
isfy the above constraints and that we have used in our experiments
are N = 2 × 106, k = 850, q ≈ 108.862 and m = 25500. These
parameters are good for any vector of size M and maximum entry
max such that M × max = N = 2 × 106.

Another reason (besides the additive homomorphism) that we
use the lattice-based scheme [27] for implementing VCS is its efﬁ-
cient algebraic nature (i.e., lack of exponentiations), which allows
for very efﬁcient implementations in PINOCCHIO [30]. For exam-
ple, we veriﬁed that while PINOCCHIO takes roughly 300 seconds
to compute the SHA-1 Merkle hash of a vector of length 100, it
only requires 3.6 seconds using the lattice-based hash. This was
among the most crucial factors for the efﬁciency of planar separa-
tor implementation.
Existing SNARKs implementations. Parts of our implementa-
tion use PINOCCHIO [30], a SNARK implementation which is open-
source and implements an optimized version of the SNARK con-
struction presented in [19]. It can support any NP language L. To
use it, one needs to write a C program that takes as input the NP
statement and the witness and veriﬁes the validity of the witness.
Then this program is compiled into a boolean/arithmetic circuit
which is used to produce the evaluation and veriﬁcation keys. Our
implementation uses a recently developed version of PINOCCHIO
by Kosba et al. [21] that uses open-source libraries for the bilinear
pairing function.
We emphasize here that PINOCCHIO is operating in Zp, where p
is a 254-bit prime number, while some computation of our schemes
takes place modulo q. For example, to verify the digest in PINOC-
CHIO computed in Relation 6, the following check is performed:

(cid:88)

i

S[i]f (i) ?= dinput mod q ,

where dinput is the trusted digest input from the client, S is the un-
trusted vector input from the server and the whole equation should
be evaluated in Zq, as required by the lattice-based hash function.
Since PINOCCHIO does not support modulo-q operations, our im-
plementation lets the server also input the quotient and uses PINOC-
CHIO to check the following equivalent condition

S[i]f (i) ?= dinput + quotinput × q .

(cid:88)

i

However, the server can potentially ﬁnd a fake pair S(cid:48) and quot(cid:48)
which still satisﬁes the condition in Zp.1
1E.g., for any vector S(cid:48) the server can set

input

quotinput = q

−1

S(cid:48)

[i]f (i) − dinput

mod p .

(cid:33)

(cid:32)(cid:88)

i

if x = s

head = ﬁrst position in Q;
tail = ﬁrst position in Q;

tail++;
for each node x ∈ V

PINOCCHIO_BFS(G,s)
1
2
3 Q[tail] = s;
4
5
6
7
8
9
10
11
12
13
14
15
16
17

x.parent = −1;
for round = 1 to |V |
for each node x ∈ V
if x = Q[head]

head++;

for each node v ∈ Adj[x]

if v.visited = 0

v.visited = 1;
v.parent = Q[head];
Q[tail] = v;
tail++;

Figure 3: The BFS pseudocode we implemented in PINOCCHIO.
Array Q has |V | positions and simulates the queue in BFS.

To solve this problem, we instead let the client input quotinput.
Speciﬁcally, quotinput can be computed, signed and outsourced
together with the digest. Meanwhile, extra conditions are added
to bound every element in S such that it is impossible to overﬂow
p—i.e., we set 0 ≤ S[i] ≤ max. Note that such conditions are
“for free,” since they are required to guarantee collision resistance
anyways, as mentioned in the beginning of Section 5.
5.1 Implementation of our VC protocols

We now present the implementations of four different VC schemes
for shortest paths, one VC scheme for dynamic graphs and one VC
scheme for maximum ﬂow, that were described in Section 3 and
Section 4. The parts that did not require PINOCCHIO (e.g., build-
ing the planar seperator tree) were implemented in C++.

Our implementation replaces expensive digital signatures (e.g.,
for signing the graph edges) with HMACs from openssl [1]. There-
fore the VC schemes we are implementing are in the secret-key set-
ting. To make our VC schemes publicly-veriﬁable without affect-
ing performance, we could use standard techniques that combine
Merkle hash trees with one signature of the roothash. To do a fair
comparison, in our experiments we “turn off” public veriﬁability
and zero knowledge which are both offered by PINOCCHIO.
Strawman scheme. As our baseline, we implemented the straw-
man algorithm as described in Section 3.1. Since we will be run-
ning experiments on unit-weight graphs (see next section), we use
n rounds of BFS instead of the Floyd-Warshall algorithm to pre-
compute and all shortest paths, which has reduced O(nm) com-
plexity (instead of O(n3)). Subsequently we compute an HMAC
of each shortest path.
PINOCCHIO BFS scheme. Our second attempt was to execute the
BFS code directly in PINOCCHIO. However, due to various lim-
itations analyzed in Section 3.2, we were not able to code up the
linear-time algorithm from [15]. Figure 3 shows the BFS pseu-
docode that we eventually wrote. One of the most important lim-
itations inherent to the circuit representation is that non-constant
array index operations must be implemented by iterating through
all elements in the array, which is why the complexity increases
from O(m + n) to O(mn)—see the portions of the pseudocode in
Figure 3 (Lines 5-7 and 9-11).

862Figure 4: Preprocessing time. We were only able to execute BFS for
graphs of up to 50 nodes. All other points (shaded bars) are estimated.

Figure 5: Proof-computation time. We were only able to execute the
certifying algorithm for up to 10,000 nodes.

Also, we simulated the BFS queue with an array Q of ﬁxed size
since we cannot implement dynamic data structures in a circuit. In
Figure 3, the index pointer head records the starting point of the
queue and the index pointer tail records the end of the queue. The
size of Q is equal to the number of the nodes of the graph, since
every node is enqueued and dequeued exactly once.

However, we note that the same technique cannot be easily gen-
eralized to Dijkstra’s algorithm on a weighted graph, since instead
of a plain queue, a more complicated priority queue is required.

Due to the above awkward implementation, the BFS performance
was not good and scaled up to only 50 nodes (see more details in
the evaluation section).
Certifying algorithm scheme. We implemented the certifying al-
gorithm with an efﬁcient circuit of O(m) size, as explained in Sec-
tion 3.3. We compute the shortest path vector S that stores the dis-
tances from the source s to all the other nodes of the graph (which
the server provides as input to Relation 1) using the BFS imple-
mentation in the LEDA library (version 6.4) [2]. We note here that,
contrary to BFS, the certifying approach can be naturally applied
for weighted graphs—see Relation 1.
During the implementation of Relation 1 in PINOCCHIO, we ob-
served that comparison operations (≤,≥) are much more expensive
than addition and multiplication. Therefore, in the case of a unit-
weight graph, we replaced the second constraint of Relation 1 by
an equivalent equality constraint with an additional input auv, i.e.,

S[v] − S[u] − avu = 0, where avu ∈ {−1, 0, 1} .

To check the domain of avu, we write avu = a1 + a2 − 1, where
a1, a2 ∈ {0, 1}. To check that ai ∈ {0, 1} we leverage the equiva-
lence ai × ai = ai ⇔ ai ∈ {0, 1} which can be implemented with
one multiplication gate. Similarly, the equality constraint above can
be checked by connecting the evaluation of bvu = S[v]−S[u]−avu
to the two inputs and one output of an addition gate. In this way,
bvu must be 0 to pass the circuit (bvu + bvu = bvu ⇔ bvu = 0).
This optimized version of the certifying algorithm improved the
prover performance by 55×. However, this method cannot be ap-
plied to graphs with general weights since the domain of the addi-
tional input is much larger and checking their domain might even
slow down the performance.

We also implemented the Dynamic Graph Scheme from Sec-
tion 3.4 in the same way as the certifying algorithm scheme. Maxi-
mum Flow Scheme is implemented following the maxﬂow-mincut
theorem described in Section 3.5.
Planar-separator scheme. We implemented the VC scheme for
planar graphs described in Figure 2. To build the planar separator
tree, we ﬁrst triangulate the input planar graph using the LEDA

library [2] and the triangulated graph is input recursively into the
recent planar separator implementation by Fox-Epstein et al. [17].
The digests of the shortest path vectors are computed using our
implementation of the additively-homomorphic lattice-based VCS
described before. These digests, along with precomputed distances
and paths, are HMACed using openssl and then outsourced. For
every vector of different length storing precomputed distances, a
SNARK for Relation 5 using PINOCCHIO is constructed.

6. EVALUATION

We now evaluate the shortest path SNARKs for (i) the straw-
man scheme; (ii) the PINOCCHIO BFS scheme; (iii) the certifying
algorithm scheme; and (iv) the planar separator scheme.

We also present experiments for the dynamic graph scheme as
well as for the maximum ﬂow scheme. We do not present results
for longest paths since we use the same certifying algorithm as the
shortest-path one (with a change in the direction of the inequality).
Experiment setup. We executed our experiments on an Amazon
EC2 machine with 15GB of RAM running a Linux kernel.

Plots for preprocessing time, proof-computation time, veriﬁca-
tion time and server storage are presented (in log scale). All schemes
were run on the same randomly-generated planar undirected graph
(we use the LEDA function random_planar_graph for that)
with unit weights. We collected 10 runs for each data point, and we
report the average in Figures 4, 5, 6, and 7.

Also, in all these ﬁgures, estimated data points (due to increased
memory/time requirements) are marked as lightly shaded bars. We
experiment on planar graphs with n = 10, 102, . . . , 105, where the
number of edges is at most 3(n − 2) (due to planarity). Our planar
scheme was the only one to execute on a graph of 200,000 nodes.
Preprocessing time. Figure 4 shows the comparison of preprocess-
ing time among the schemes. The preprocessing time of both BFS
and certifying algorithm schemes is deﬁned as the time to compile
the corresponding PINOCCHIO codes into circuits plus the time to
generate the keys. The results show that the certifying algorithm
outperforms BFS by orders of magnitude. Speciﬁcally, the opti-
mized certifying algorithm runs approximately 10,000 times faster
than BFS on a graph with 10,000 nodes.

In our experiments, the PINOCCHIO BFS implementation is so
inefﬁcient that it takes too long to get a result even on a graph with
only 100 nodes. Thus the statistics on graphs with more than 100
nodes for BFS are estimated based on data points on small graphs.
All estimated data points are marked as lighted bars in the ﬁgures.
We use minimum mean square error for the estimation.

101102103104105Nodes10−21001021041061081010Time(seconds)StrawmanPINOCCHIOBFSCertifyingAlgorithmPlanarSeparator101102103104105Nodes10−21001021041061081010Time(seconds)StrawmanPINOCCHIOBFSCertifyingAlgorithmPlanarSeparator863Figure 6: Veriﬁcation time.

Figure 7: Server storage.

On the contrary, the certifying algorithm schemes can run on
larger graphs. The only limitation in this case is that the memory
consumption is proportional to the input size (note the input in this
case is O(m) instead of O(1) as in the BFS) and our machine runs
out of memory when compiling the certifying algorithm code on a
graph with more than 10,000 nodes.

Surprisingly, although the complexity of preprocessing time in
the planar separator scheme is O(n3/2 log n), it executes faster
than the certifying algorithm (that has linear preprocessing time),
because most of the work is non-cryptographic and can be imple-
mented efﬁciently in regular C++ code. In particular, the planar
separator scheme outperforms the certifying algorithm scheme in
the preprocessing time by 29× on a graph with 10,000 nodes and
it can scale to a graph with up to 200,000 nodes. Also, as shown
in Table 3, for small graphs, compiling PINOCCHIO codes to cir-
cuits and generating keys contribute to the biggest part of the pre-
processing, while in large graphs, the portion of the separator tree
construction as well as computing and HMACing the precomputed
distances and paths dominates most of the time.

Finally, since the strawman scheme does no cryptographic work
other than HMACs, it runs extremely fast on small graphs. How-
ever, its execution time becomes equal to that of the planar sepa-
rator scheme on a graph with 10,000 nodes, and is 12× worse on
a graph with 100,000 nodes. We had to estimate the preprocessing
time of the strawman approach on a graph of 100,000 nodes since
storing an all pair shortest path matrix of size 100,000X100,000 re-
quires too much memory (we ran BFS 100,000 times and compute
100,0002 HMACs to estimate the cost).
Proof-computation time. Figure 5 shows a comparison among
the implemented schemes in terms of proof-computation time. The
results clearly indicate that the certifying algorithm approach out-
performs the BFS approach and that the planar separator approach
outperforms both BFS and the certifying algorithm dramatically—
which is expected since the planar separator proof-computation time
grows sublinearly with the number of nodes. In particular, proof-
computation time of the planar separator scheme has a speedup of
more than 1.4X105× on a 1,000-node graph, and a speedup of more
than 2.3X108× on a 100,000-node graph (compared to BFS).

We note here, that in the case of the planar separator scheme, we
report worst case results in Figure 5. Worst case proof-computation
time is derived when the source s and the destination t of our query
are sibling leaves in the planar separator tree, in which case we need
to perform the maximum number of MIN computations (approxi-
mately O(log n)). This is because the proof computation algorithm
always examines all common parents of the two planar separator
tree nodes containing s and t. On the contrary, if there is only one
common parent, namely the root, of the two tree nodes, the proof

computation only does one MIN computation and is deﬁned as the
best case. Table 4 shows the comparison between the worst and
the best case. It can be observed that the time of the worst case
is roughly log n times the time of the best case. Veriﬁcation times
also have similar relationships.

The planar separator scheme reduces the proof-computation time
dramatically and this is one of the main contributions of this work
since this metric (proof-computation time) is the most expensive in
existing work [30, 12]. For example, our work shows that graph
processing can scale up to 200,000 nodes since it takes only tens of
seconds as shown in Table 4 (and in the best case, only 7 seconds!)
to produce a proof for such large graphs. The exact numbers of our
proof-computation time can be found in Table 7 in the Appendix.
Veriﬁcation time. Figure 6 shows statistics about veriﬁcation time.
In accordance with the asymptotics (see Table 1), the veriﬁcation
for BFS and the certifying algorithm is faster than the planar sepa-
rator scheme. Still, as shown in Figure 6, the veriﬁcation time of the
planar separator scheme does not grow that much. It requires less
than 2 seconds on a graph with 100,000 nodes. Therefore, consid-
ering the signiﬁcant improvements on proof-computation time, the
small increase in the veriﬁcation time is a good trade-off. Finally,
similarly to proof-computation time, the veriﬁcation of the straw-
man approach requires a small amount of time since it only requires
verifying HMACs of the shortest path edges, which is negligible.
Server storage. Here we compare the total amount of storage re-
quired on the server side. In the strawman scheme, the server stores
the all-pairs-shortest-path matrix and the corresponding HMACs.
In the BFS scheme, the server stores the PINOCCHIO circuit and
the PINOCCHIO evaluation key. Note that in both these schemes, it
is not necessary to store the graph G itself (in the strawman scheme
the shortest paths are precomputed; in the BFS scheme the graph is
embedded in the circuit) and therefore we do not count the graph
size. On the contrary, the certifying algorithm scheme requires
the server to store the graph (to compute the shortest paths), the
PINOCCHIO circuit, the respective evaluation key and the HMACs
of edges—all these are included in the server storage of this scheme.
Finally, in the planar separator scheme, the separator tree is part of
the server storage in addition to that of the certifying algorithm.

Figure 7 shows the comparison of the server storage. Since the
PINOCCHIO evaluation key size is proportional to the size of the
circuit and the certifying algorithm has a more efﬁcient circuit im-
plementation than the one of BFS in Figure 3, the server storage for
the certifying algorithm is much smaller. In particular, the server
storage is reduced by 32340× on a graph with 1,000 nodes and the
gap grows on larger graphs.

101102103104105Nodes00.20.40.60.81.01.21.41.6Time(seconds)StrawmanPINOCCHIOBFSCertifyingAlgorithmPlanarSeparator101102103104105Nodes10−21001021041061081010ServerStorage(MB)StrawmanPINOCCHIOBFSCertifyingAlgorithmPlanarSeparator864The planar separator scheme has the least storage requirements.
As shown in Figure 7, although the server storage is 4.7× larger on
a small graph with 10 nodes, it is reduced by 7× for larger graphs
and in particular by 1.9X106×compared to BFS on a graph with
100,000 nodes. Finally, the server storage is a major drawback of
the strawman approach, as the server needs to store n2 HMACs
(each being 256 bits) for the shortest paths. As such, our planar
separator tree scheme outperforms the strawman scheme by 3× on
an 1000-node graph and by 669× on an 100,000-node graph.
Proof size. One drawback of our approaches compared to the BFS
approach is the proof size. The proof size of the PINOCCHIO BFS
scheme is always a constant (288 bytes)—see [30]. However, in our
approaches (certifying algorithm and planar separator), the proof
contains a 256-bit (32 bytes) HMAC for each edge contained in
the shortest path, therefore being proportional to the size |p| of the
shortest path p. Speciﬁcally, for the case of the certifying algorithm
scheme the proof size is |p| × 32 + 288 bytes while for the planar
separator scheme the proof size is |p| × 32 + 288 × num bytes,
where num is the number of PINOCCHIO circuits involved in the
proof computation of the planar separator scheme (num is bounded
above by the levels of the planar separator tree). As a reference, the
proof size of the strawman scheme is (|p|+1)×32 since the length
of the path as well as every path edge needs to be HMACed.
Table 2 compares the proof size of all four schemes for |p| = 10.
Note that although the proof size of our approach slightly increases,
the bandwidth of all approaches is the same and proportional to |p|,
since the answer p is always required to be returned to the client.

Table 2: Proof size in KB for |p| = 10.

n

Strawman

100
1,000
10,000

0.344
0.344
0.344

PINOCCHIO

BFS
0.281
0.281
0.281

Certifying
Algorithm Separator

Planar

2.781
2.781
2.781

3.906
5.312
6.719

Evaluation of the dynamic graph scheme. We compared the
proof-computation time for the dynamic graph scheme (Section 3.4)
with the certifying algorithm and the planar separator scheme. As
shown in Figure 8, the proof-computation time of the dynamic
graph scheme does not scale that well, when compared to the other
schemes (which is expected, given its O(n2) complexity). How-
ever, it is still feasible to execute it on small graphs.

For better performance, we also tried to implement the RAM-
based, improved version of the dynamic graph scheme described
in Figure 9. However, only verifying one digest in step (3) takes
more than 10 seconds, on a small graph with 16 edges, and there
are 2m such veriﬁcations to be performed, thus it is not practical
at this time. Therefore, although the complexity of the algorithm
is O(m log2 n), the constants involved are really large due to the
veriﬁcation scheme used. We hope to develop more efﬁcient algo-
rithms for verifying RAM-based tasks in the future.

Table 3: Breakdown for planar separator preprocesssing cost.

n

100
1,000
10,000
100,000

total

time (s)
59.273
126.88
838.924
6844.024

construction

tree

Planar Separator Scheme
digest and
HMAC
0.47%
4.16%
26.44%
37.50%

0.20%
2.18%
8.70%
44.47%

PINOCCHIO

work
99.32%
93.19%
57.52%
18.03%

Evaluation of the maximum ﬂow scheme. Table 5 shows the
proof time comparison between implementing the maximum ﬂow

Table 4: Worst and best case for planar proof-computation time.

Planar Separator Scheme

proof computation

veriﬁcation

worst case (s)

best case (s) worst case (s)

best case (s)

11.425
22.223
66.374
93.853

1.572
4.099
6.147
7.472

0.547
0.944
1.350
1.729

0.063
0.085
0.127
0.194

n

1,000
10,000
100,000
200,000

n
10
100
1,000

Table 5: Maximum ﬂow scheme proof-computation time.

PINOCCHIO Implementation (s) Maximum Flow Scheme (s)

>32.072
>20,782

>2,413,013

0.701
31.1509
1,003

Figure 8: Comparison of the dynamic graph scheme, the certify-
ing algorithm scheme and the planar separator scheme in terms of
proof-computation time.

algorithm directly on PINOCCHIO and using the certifying algo-
rithm for maximum ﬂow. Actually, maximum ﬂow algorithms are
much more complicated than BFS and we could not implement
them directly on PINOCCHIO easily. However, we observed that
the Edmonds-Karp algorithm [15] computes maximum ﬂows by
calling BFS as a subroutine. Thus we use the proof time of BFS
on the same graph as a lower bound. Even so, Table 5 shows that
the certifying algorithm for maximum ﬂow outperforms the lower
bound by orders of magnitude. In particular, the certifying algo-
rithm speeds up by 2405× on a graph with 1,000 nodes.

Table 6: PANTRY [12] benchmarks (in seconds).

n Veriﬁable PUT Veriﬁable GET estimated PANTRY BFS

100
1,000

3.3
30.92

2.52
31.22

6,200
650,000

6.1 Using RAM-based VC

One of the main sources of inefﬁciency in our PINOCCHIO im-
plementations is the circuit representation. To overcome this limi-
tation, Braun et al. [12] presented PANTRY, a system for verifying
RAM computations. PANTRY solves the non-constant index prob-
lem by providing a digest of the memory as an input to the SNARK,
along with a Merkle hash tree proof of the accessed index.

We estimated the time that BFS would require to execute on
PANTRY by multiplying the time overhead of each RAM PUT/GET
operation with the number of such operations the BFS algorithm
takes—3m + 2n + 1 PUT operations and 3m + n + 2 GET op-
erations. Table 6 shows the estimated times for running BFS on
PANTRY for graphs with 100 and 1,000 nodes.

We also estimated the performance of the system of Ben-Sasson
et al. [7]. First, we wrote BFS in TinyRAM. Then we computed

101102103104105Nodes10−21001021041061081010Time(seconds)2×105DynamicGraphCertifyingAlgorithmPlanarSeparator865the three parameters that affect the performance of the system: the
number of instructions (L = 127), the number of cycles (T =
100m + 27) and the size of the input (N = m + 2). Based on
Figure 9 of [8] (a follow-up work of [7]), we used the time for a
TinyRAM program with parameters (L, T , N) that are closest to
ours. Numbers in column 3 of Table 1 are derived in this way.

Acknowledgments
This research was sponsored in part by the U.S. Army Research
Laboratory and the U.K. Ministry of Defence and was accomplished
under Agreement Number W911NF-06-3-0001. The views and
conclusions contained in this document are those of the author(s)
and should not be interpreted as representing the ofﬁcial policies,
either expressed or implied, of the U.S. Army Research Labora-
tory, the U.S. Government, the U.K. Ministry of Defence, or the
U.K. Government. The U.S. and U.K. Governments are autho-
rized to reproduce and distribute reprints for Government purposes
notwithstanding any copyright notation hereon.

We thank Roberto Tamassia for useful discussions.

7. REFERENCES
[1] https://www.openssl.org/docs/crypto/hmac.html.
[2] http://www.algorithmic-solutions.com/leda/index.htm.
[3] http://www.dis.uniroma1.it/challenge9/.
[4] M. Ajtai. Generating Hard Instances of Lattice Problems

(extended abstract). In STOC, pp. 99–108, 1996.

[5] A. Anagnostopoulos, M. T. Goodrich, and R. Tamassia.

Persistent Authenticated Dictionaries and Their
Applications. In ISC, pp. 379–393. 2001.

[6] G. D. Battista, P. Eades, R. Tamassia, and I. G. Tollis. Graph

Drawing: Algorithms for the Visualization of Graphs.
Prentice Hall PTR, Upper Saddle River, NJ, USA, 1998
[7] E. Ben-Sasson, A. Chiesa, D. Genkin, E. Tromer, and M.

Virza. SNARKs for C: Verifying Program Executions Succin
-ctly and in Zero Knowledge. In CRYPTO, pp. 90–108, 2013.
[8] E. Ben-Sasson, A. Chiesa, E. Tromer, and M. Virza. Succinct

Non-Interactive Zero Knowledge for a von Neumann
Architecture. In USENIX Security, 2014.

[9] N. Bitansky, R. Canetti, A. Chiesa, and E. Tromer. From

Extractable Collision Resistance to Succinct Non-interactive
Arguments of Knowledge, and Back Again. In ITCS, pp.
326–349, 2012.

[10] N. Bitansky, R. Canetti, A. Chiesa, and E. Tromer. Recursive

Composition and Bootstrapping for SNARKS and
Proof-carrying Data. In STOC, pp. 111–120, 2013.
[11] N. Bitansky, A. Chiesa, Y. Ishai, R. Ostrovsky, and

O. Paneth. Succinct Non-interactive Arguments via Linear
Interactive Proofs. In TCC, pp. 315–333, 2013.

[12] B. Braun, A. J. Feldman, Z. Ren, S. T. V. Setty, A. J.

Blumberg, and M. Walﬁsh. Verifying Computations with
State. In SOSP, pp. 341–357, 2013.

[13] D. Catalano and D. Fiore. Vector Commitments and Their

Applications. In PKC, pp. 55–72, 2013.

[14] K.-M. Chung, Y. T. Kalai, and S. P. Vadhan. Improved
Delegation of Computation Using Fully Homomorphic
Encryption. In CRYPTO, pp. 483–501, 2010.

[15] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.

Introduction to Algorithms (Vol. 3.). MIT Press, 2009.

[16] J. Fakcharoenphol. Planar Graphs, Negative Weight Edges,

Shortest Paths, and Near Linear Time. In Journal of
Computer and System Sciences, 72(5):868-889, 2006.

[17] E. Fox-Epstein, S. Mozes, P. M. Phothilimthana, and

C. Sommer. Short and Simple Cycle Separators in Planar
Graphs. In ALENEX, pp. 26–40, 2013.

[18] R. Gennaro, C. Gentry, and B. Parno. Non-interactive

Veriﬁable Computing: Outsourcing Computation to
Untrusted Workers. In CRYPTO, pp. 465–482, 2010.

[19] R. Gennaro, C. Gentry, B. Parno, and M. Raykova. Quadratic

Span Programs and Succinct NIZKs without PCPs. In
EUROCRYPT, pp. 626–645, 2013.

[20] M. T. Goodrich, R. Tamassia, and N. Triandopoulos.

Efﬁcient Authenticated Data Structures for Graph
Connectivity and Geometric Search Problems. Algorithmica,
60(3):505–552, 2011.

[21] A. E. Kosba, D. Papadopoulos, C. Papamanthou, M. F.

Sayed, E. Shi, and N. Triandopoulos. TrueSet: Faster
Veriﬁable Set Computations. In USENIX Security, 2014.
[22] R. J. Lipton and R. E. Tarjan. A Separator Theorem for
Planar Graphs. SIAM Journal on Applied Mathematics,
36(2):177–189, 1979.

[23] R. C. Merkle. A Certiﬁed Digital Signature. In CRYPTO, pp.

218–238, 1990.

[24] S. Micali. Computationally Sound Proofs. SIAM Journal on

Computing, 30(4):1253–1298, 2000.

[25] D. Micciancio and C. Peikert. Hardness of SIS and LWE

with small parameters. In CRYPTO, pp. 21–39, 2013.

[26] D. Micciancio and O. Regev. Lattice-based Cryptography. In

Post-quantum cryptography, pp. 147–191, 2009.

[27] C. Papamanthou, E. Shi, R. Tamassia, and K. Yi. Streaming

Authenticated Data Structures. In EUROCRYPT, pp.
353–370, 2013.

[28] C. Papamanthou and R. Tamassia. Time and Space Efﬁcient
Algorithms for Two-party Authenticated Data Structures. In
ICICS, pp. 1–15, 2007.

[29] C. Papamanthou, R. Tamassia, and N. Triandopoulos.

Optimal Veriﬁcation of Operations on Dynamic Sets. In
CRYPTO, pp. 91–110, 2011.

[30] B. Parno, J. Howell, C. Gentry, and M. Raykova. Pinocchio:

Nearly Practical Veriﬁable Computation. In SSP, pp.
238–252, 2013.

[31] R. M. McConnell, K. Mehlhorn, S. Näher, and P. Schweitzer.

Certifying Algorithms. Computer Science Review,
5(2):119–161, 2011.

[32] S. T. V. Setty, B. Braun, V. Vu, A. J. Blumberg, B. Parno, and

M. Walﬁsh. Resolving the Conﬂict Between Generality and
Plausibility in Veriﬁed Computation. In EUROSYS, pp.
71–84, 2013.

[33] S. T. V. Setty, R. McPherson, A. J. Blumberg, and

M. Walﬁsh. Making Argument Systems for Outsourced
Computation Practical (sometimes). In NDSS, 2012.

[34] R. Tamassia. Authenticated Data Structures. In ESA, pp. 2–5,

2003.

[35] R. Tamassia and N. Triandopoulos. Certiﬁcation and

Authentication of Data Structures. In AMW, 2010.

[36] V. Vu, S. T. V. Setty, A. J. Blumberg, and M. Walﬁsh. A

Hybrid Architecture for Interactive Veriﬁable Computation.
In SSP, pp. 223–237, 2013.

[37] M. L. Yiu, Y. Lin, and K. Mouratidis. Efﬁcient Veriﬁcation
of Shortest Path Search via Authenticated Hints. In ICDE,
pp. 237–248, 2010.

866(dE , s, t, p) :

FG =

∃{[ei = (ui, vi), πei ], [dui , πui ], [dvi , πvi ]}m
(1) 1 ← VE .verify(uin + vi, 1, πei , dE , pk) for i = 1, . . . , m ;
(2) V.digest(S, pk) = dig ;
(3) 1 ← V.verify(ui, dui , πui , dig, pk) and 1 ← V.verify(vi, dvi , πvi , dig, pk) for i = 1, . . . , m ;
(4) S[s] = 0 ∧ S[t] = |p| ;
(5) S[vi] ≤ S[ui] + 1 for i = 1, . . . , m .

i=1 and S and dig such that :



Figure 9: The language for the dynamic updates. To support dynamic updates, we need to build a SNARK for this language instead of 1.

Appendix

.

q

tree T set λ(v) = (cid:80)

Algorithm pk ← genkey(1k, M )
Set N = M · max, where max is the maximum entry that can appear
in vector S. Call {q, m} ← parameters(1k, N ). Set pk = {L, R, q},
where L, R are matrices picked uniformly at random from Zk×m
Algorithm d(S) ← digest(S, pk)
Let T be a binary tree built on top of vector S. For each node v of the
i∈range(v) S[i]Lv(i), where Lv(i) is the partial
label of node v with respect to i and range(v) is the range of node v,
both deﬁned in [27]. Finally set d(S) = λ(), where  is the root of T .
Algorithm d(S) ← update(d(S), i, α, pk)
Set d(S) = d(S) +L(i)(α− S[i]), where L(i) is the partial label of
the root  with respect to i (deﬁned in [27]).
Algorithm {S[i], Πi} ← query(i, S, pk)
Let v(cid:96), . . . , v1 be the path in T from node i to the child v1
of the root  of T . Let also w(cid:96), . . . , w1 be the sibling nodes
of v(cid:96), . . . , v1.
Proof Πi contains the ordered sequence of pairs
{(λ(v(cid:96)), λ(w(cid:96))), (λ(v(cid:96)−1), λ(w(cid:96)−1)), . . . , (λ(v1), λ(w1))}.
Algorithm{1, 0} ← verify(i, S[i], Πi, d(S), pk)
Parse proof Πi as {(λ(v(cid:96)), λ(w(cid:96))), . . . , (λ(v1), λ(w1))}. If λ(v(cid:96)) (cid:54)=
S[i]1 or λ(v(cid:96)), λ(w(cid:96)) (cid:54)= [N ]m, output 0. Otherwise compute the val-
ues y(cid:96)−1, y(cid:96)−2, . . . , y0 as yi = L·λ(vi+1)+R·λ(wi+1) (if vi+1 is vi’s
left child) or yi = R·λ(vi+1)+L·λ(wi+1) (if vi+1 is vi’s right child).
For i = (cid:96) − 1, . . . , 1, if f (λ(vi)) (cid:54)= yi or λ(vi), λ(wi) /∈ [N ]m out-
put 0. If f (d(S)) (cid:54)= y0, output 0 (function f (.) takes as input a radix-2
representation and returns the respective number, see [27]). Output 1.
{q, m} ← parameters(1k, N ).
Let q be a prime and k ∈ N.
min{q, 2

such that q/(cid:112)(cid:100)log q(cid:101) ≥ √

√
0.0086·k log q}. Set m = k(cid:100)log q(cid:101).

Find smallest q and k
2k log q <

2 · N · k0.50001 and

√

Figure 10: The additively-homomorphic veriﬁable vector scheme,
adjusted from [27]. Note that the procedure parameters is called
by algorithm genkey.

DEFINITION 3. We say that a VC scheme for graphs V is cor-
rect if, for all graphs G, for all k ∈ N, for all ekG, vkG output by
algorithm genkey, for all queries q on G and for all πq, α output
by algorithm compute(q, ekG), it is 1 ← verify(πq, q, α, vkG).
We say that a VC scheme V is secure if, for all graphs G, for all
k ∈ N, for all ekG, vkG output by algorithm genkey and for any
PPT adversary Adv it is

 {q, πq, α} ← Adv(ekG, vkG);

1 ← verify(πq, q, α, vkG);
α is incorrect.

Pr

 ≤ neg(k) .

DEFINITION 4

(VECTOR COMMITMENT). A vector commit-
ment scheme V has ﬁve PPT algorithms: (1) pk ← genkey(1k, M ):

On input the security parameter k and the vector size M, it outputs
the public key pk; (2) d(S) ← digest(S, pk): On input vector S and
pk, it outputs the digest d(S); (3) d(S) ← update(d(S), i, α, pk):
On input vector S, (i, α) and pk, it sets S[i] = α and outputs the
new digest d(S); (4) (S[i], Πi) ← query(i, S, pk): On input an in-
dex i and pk, it returns the value S[i], along with a proof Πi (run
by prover); (5) {1, 0} ← verify(i, S[i], Πi, d(S), pk): On input an
index i, a value S[i], a proof Πi, a digest d(S) and pk, it outputs
either 1 or 0 (run by veriﬁer);
We say that a VCS scheme V is correct if, for all k, M ∈ N, for
all pk output by genkey, for all S and for all d(S) output by algo-
rithm digest(S, pk) (or update(d(S), i, α, pk)) and for all S[i], Πi
output by query(i, S, pk), it is 1 ← verify(i, S[i], Πi, d(S), pk).
We say that a VCS scheme is secure if for all k, M ∈ N, for all
pk output by algorithm genkey, for all S and for all d(S) output by
algorithm digest(S, pk) (or algorithm update(d(S), i, α, pk)) and
for any PPT adversary Adv it is

 {i, Π, α} ← Adv(1k, pk);

1 ← verify(i, α, Π, d(S), pk);
S[i] (cid:54)= α.

Pr

 ≤ neg(k) .

DEFINITION 5. We say that a SNARK G is correct if, for all
k ∈ N, for all NP languages L, for all ekL and vkL output by
genkey, for all x ∈ L, it is 1 ← verify(compute(x, ekL), x, vkL).
We say that a SNARK G is secure if, for all k ∈ N, for all NP
languages L, for all ekL and vkL output by algorithm genkey and
for any PPT adversary Adv it is for all x ∈ L, it is

w /∈ RL(x)

Table 7: Proof-computation time (seconds).

Strawman

10
100
1,000
10,000
100,000
200,000

0.01
0.01
0.01
0.01
0.01
0.01

PINOCCHIO

BFS
32.072
21,000∗
2,400,000∗
240,000,000∗
25,000,000,000∗
98,000,000,000∗

Certifying
Algorithm Separator

Planar

0.044
0.477
5.502
56.02
560∗
1120∗

0.853
4.616
11.425
22.223
66.374
93.853

A SNARK should also have an extractor: I.e., for any polynomial-
sized prover Prv, there exists an extractor Ext such that for any
statement x, auxiliary information µ, the following holds:

1 ← verify(πx, x, vkL);
x /∈ L.

 {πx, x} ← Adv(1k, ekL, vkL);


πx ← Prv(ekL, x, µ)
1 ← verify(πx, x, vkL)
w ← Ext(ekL, vkL, x, πx)

{ekL, vkL} ← genkey(1k, L)

∧

 ≤ neg(k) .
 ≤ negl(k) .

Pr

Pr

867