Password Portfolios and the Finite-Effort User: 

Sustainably Managing Large Numbers of Accounts

Dinei Florêncio and Cormac Herley, Microsoft Research;  

Paul C. van Oorschot, Carleton University

https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/florencio

This paper is included in the Proceedings of the 23rd USENIX Security Symposium.August 20–22, 2014 • San Diego, CAISBN 978-1-931971-15-7Open access to the Proceedings of  the 23rd USENIX Security Symposium is sponsored by USENIXPassword Portfolios and the Finite-Effort User:

Sustainably Managing Large Numbers of Accounts∗

Dinei Florˆencio and Cormac Herley
Microsoft Research, Redmond, USA

Paul C. van Oorschot

Carleton University, Ottawa, Canada

Abstract. We explore how to manage a portfolio of pass-
words. We review why mandating exclusively strong
passwords with no re-use gives users an impossible task
as portfolio size grows. We ﬁnd that approaches justiﬁed
by loss-minimization alone, and those that ignore impor-
tant attack vectors (e.g., vectors exploiting re-use), are
amenable to analysis but unrealistic. In contrast, we pro-
pose, model and analyze portfolio management under a
realistic attack suite, with an objective function costing
both loss and user effort. Our ﬁndings directly challenge
accepted wisdom and conventional advice. We ﬁnd, for
example, that a portfolio strategy ruling out weak pass-
words or password re-use is sub-optimal. We give an op-
timal solution for how to group accounts for re-use, and
model-based principles for portfolio management.

1

Introduction

Due to the growth in online services, many users now
manage dozens of password-protected accounts. Many
service providers, awareness campaigns (US DHS [1]),
and government entities (US-CERT [2]) stress two foun-
dations for password security:

A1: Passwords should be random and strong; and
A2: Passwords should not be re-used across accounts.

Despite this, users have long been observed to choose
weak passwords. Leaked datasets, such as the 32 mil-
lion plaintext passwords from Rockyou, reveal that most
users fall far short of following “traditional” advice on
password strength. Evidence also indicates widespread
password re-use [21]. While admonitions against this
are almost universal, ignoring that advice seems equally
universal. Clearly, users ﬁnd managing a large password
portfolio burdensome. Both password re-use, and choos-
ing weak passwords, remain popular coping strategies.

∗USENIX Security 2014, August 20-22.

Numerous efforts have been made to address the ne-
glect of password strength by users. Many sites stress the
importance of, and offer tips on how strong passwords
can be made easier to construct and remember; e.g.,
US-CERT [2] and others commonly suggest passphrase-
based and other mnemonic approaches. But while signif-
icant attention has been devoted to motivating and help-
ing users choose strong individual passwords, there is lit-
tle guidance on how to choose and manage large numbers
of them. We aim to give, and justify, such guidance.

We explore how a large portfolio of passwords can be
maintained without ignoring that users have limited abil-
ities. Can password re-use be part of sensible portfolio
management, or is it never justiﬁable? Is a unique strong
password for every account, including blog sites and
throw-away accounts, truly the best use of limited hu-
man memory resources? In practice, many users gather
accounts into groups that re-use a password, but little
guidance exists on choosing appropriate groups. Given
that re-use does and will happen, we explore how to do
so in a principled way, and answer these questions.

Our ﬁndings directly challenge some conventional
wisdom. For example, we ﬁnd: strategies that rule out
password re-use or the use of weak passwords are sub-
optimal. Both are valuable tools in balancing the alloca-
tion of effort between higher and lower value accounts.
We ﬁrst review password-related demands on users,
and consider users’ options under the reasonable but
too-rare assumption of ﬁnite user effort. This realism
yields an inherent trade-off between two desired out-
comes: greater password strength and avoiding re-use.
Acknowledging ﬁxed user effort budgets, more of one
means less of the other.

We explore the implications of password re-use, and
outline an optimal password-sharing strategy: for a ﬁxed
number of passwords and a given set of accounts, how to
partition accounts to minimize total expected loss. Loss
analysis is greatly complicated by cross-contamination
issues due to password re-use. We address this by a novel

USENIX Association  

23rd USENIX Security Symposium  575

1

partitioning of attacks into three broad classes covering
the major threat vectors, itself of independent interest.

2 Related Work

In 2000, Dhamija and Perrig [18] interviewed 30 par-
ticipants reporting 1–7 unique passwords for 10–50 web
sites. Circa 2001, Sasse and Brostoff [45] surveyed 144
employees reporting on average 16 passwords including
non-online activity. A 2004 survey of 218 college stu-
dents by Brown et al.
[11] indicated on average 8.18
password accounts serviced by 4.45 unique passwords.
In 2006 Gaw and Felten [24] surveyed 58 (mainly stu-
dent) participants by online questionnaire with in-lab
follow-up of 49, exploring how users manage online
passwords, the extent of reuse and how users justify it,
and the use of related passwords; they reported on aver-
age 13 passwords and found reuse increased over time—
new accounts accumulated faster than new passwords.
Riley’s 2006 survey [44] of 315 college students (8.5 ac-
counts on average) reported: 74.9% have a set of prede-
termined passwords they frequently re-use; 54.6% very
frequently or always use a same password for multiple
accounts; 33% use some variation of a same password
for multiple accounts; and 60% do not vary the complex-
ity of their passwords with the nature of a site. In a 2007
study of password use/re-use across three months by over
a half million users, Florˆencio and Herley [21] reported
on average 25 accounts serviced by 6.5 unique pass-
words, re-used passwords used on average at 5.7 sites,
and strong passwords re-used less.

their

Notoatmodo’s 2007 thesis [42] explored password
re-use and users’ perspectives of
real-world
passwords—and especially relevant to our work, how
users mentally group both accounts and passwords into
categories, relationships between account and password
groups, and details of users’ reasons both for, and for not,
reusing passwords. The 26 participants surveyed had on
average 12.9 accounts and 8.1 passwords; most reused
passwords (132 of 336 accounts had unique passwords).
Reuse was found again (see above) to increase with num-
ber of accounts. A hypothesis progressed was that users
manage their accounts and passwords by mentally sep-
arating both into categories based on perceived account
similarities and password similarities,1 Regarding group-
ing accounts, and which accounts they felt were “high
importance”, most participants had only one high impor-
tance account group (1.54 such groups on average), and
high importance groups were found to be smaller (fewer

1Examples of similarities for grouping passwords: “school stuff”,
email accounts, online banking, and semantic properties related to se-
curity (e.g., overall length, number of letters). Examples for account
grouping: type of service related to the account (e.g., ﬁnancial, educa-
tion, communication), similar levels of risk or importance.

accounts per group: mean 1.84 vs. 2.78 for low impor-
tance groups). 45% reported reusing at least one pass-
word from a high importance group vs. 96% reusing at
least one password from a low importance group; 70%
had passwords exclusively used for an account in high
importance groups (2.9 such passwords on average). In
line with our views, Notoatmodo suggests “reusing pass-
words on unimportant accounts which contain no sensi-
tive information should not be discouraged ... Expecting
users to create unique, strong passwords for all their ac-
counts is ... unreasonable ... Instead, users should be
educated to identify which accounts [not to] reuse pass-
words on.” While granting that password re-use is dan-
gerous, Karp [36] also argues for re-use (“human nature
being what it is, not reusing passwords is equally danger-
ous”) but in a different direction: by a password manager
tool re-using a user password as a master password com-
bined with details of a target site (e.g., site name) for
site-speciﬁc passwords.

The “domino effect” of password re-use is well-
documented (e.g., Ives et al. [32]; Gouda et al. [25]).
The need for re-use is exacerbated by large numbers of
passwords consuming user’s memory capacity [3].
In
scarce empirical work on implications of password re-
use, Bonneau and Preibusch [9] analyze password imple-
mentations across 150 free websites, explaining techni-
cal means by which password re-use allows low-security
sites—often unmotivated to spend effort or user experi-
ence securing passwords—to compromise high-security
sites. The same authors [43] explore this question as
a negative externality of password policies, ﬁnding a
tragedy of the commons whereby sites with the low-
est security needs can endanger those with the highest.
Florˆencio and Herley [22] ﬁnd that the imposition of
stringent password policies is better correlated with insu-
lation from the consequences of poor usability than the
need for greater security.

While the degree of password re-use naturally varies
with the users studied, their circumstances and envi-
ronment at a give time, evidence clearly shows it is
widespread. The accuracy of self-reported re-use statis-
tics is debatable, but a lower bound on re-use in real life
is possible from leaked password databases from two
different sites: from each database, recover a (userid,
password) list, ﬁnd userids common to both (e.g., re-
used email addresses), then count password re-use in-
stances. Das et al. [17] estimate that 43-51% of users
re-use passwords across sites, and give algorithms that
improve an attacker’s ability to exploit this fact;
this
exceeds the 12-20% rate of some earlier studies noted
above [24, 21]. Lemos [38] reports that intersection of
the breached database pair (Yahoo Voices, Sony online)
and (Sony online, Gawker) found usernames had re-used
passwords across two sites 59% and two-thirds of the

576  23rd USENIX Security Symposium 

USENIX Association

2

time in the two pairs. RockYou’s leaked dataset [30]
was explored by Bonneau [7, p.83] and Weir et al. [48].
Zhang et al. [49] easily predict new passwords from old
when password aging policies force updates.

Over a 2011 two-week diary study of password use by
Hayashi et al. [28], 20 participants reported 8.6 accounts
on average, and to not use any memory aids for 60% of
accounts; 19 of 20 said they reused passwords for mul-
tiple accounts. This may indicate under-estimating pass-
word re-use risk vs. writing passwords down. From a
2010 one-week diary-based study wherein 32 staff from
two organizations produced just over 6 passwords each,
Inglesant et al. [31] suggest that password polices be de-
signed not to maximize password strength but rather to
aid users in setting strengths appropriate to speciﬁc use
contexts. Grawemeyer et al. [26] explore re-use among
coping strategies in managing collections of passwords,
in a detailed 2011 diary study of 22 participants over 7
days. In 2014, Stobert et al. [47] also explore user cop-
ing strategies for managing passwords, with guided inter-
views and questionnaires on 27 participants—noting as a
user concern “rationing effort to best protect important
accounts”, and that “many participants [reported] having
a speciﬁc password that they reused widely on accounts
of low interest, low importance, or infrequent use”.

The idea of grouping passwords, e.g., by level of im-
portance, has seen little academic study, but Cheswick
et al. [15, pp.140-141] suggested four categories: worth-
less, slightly important, quite secure, and top security.
Cheswick more recently [13] suggests three classes:
those that (a) have no importance; (b) are inconvenient
if stolen; or (c) result in a major problem if abused. Five
categories each are given by Grosse et al. [27] (based on
account value) and Florˆencio et al. [23] (based on con-
sequence of compromise). Cheswick [14] also reviews
common password guidance, and the ongoing suitabil-
ity of circa-1985 U.S. government password guidelines.
Florˆencio and Herley suggest defender goals are well
modelled by minimizing loss plus effort [20].

Nithyanand et al. [41] explore issues related to pass-
word re-use, under an attack model focused on server-
side breakin (excluding phishing, client-side malware);
seek solutions to maximize “remaining value” (i.e., min-
imize loss, vs.
loss plus effort herein); show their
password allocation problem is NP-complete; and ﬁnd
heuristic solutions to special cases (for accounts of equal
value, with identical compromise probabilities, etc.).

3 The Difﬁculty of Managing a Portfolio

Issues related to complexities of human memory, encod-
ing and recalling information (see [10]) currently pre-
clude a satisfactory cognitive model or measure of the
load passwords place on users. Nonetheless we begin

with a naive model to highlight impossible-to-meet as-
sumptions, and to position and motivate later discussion.
We stress that our later modeling (Sections 4 onward)
abandons these assumptions and this naive model, tack-
ling a more realistic setting. We acknowledge that our
equations in this Section, e.g., for the difﬁculty of as-
sociating passwords with accounts, give at best crude
estimates. We emphasize also that this paper considers
ordinary text passwords, not text or graphical variations
using cues; we make no claims regarding such schemes.
An active web-user may have a hundred or more
password-protected accounts. Ideally a user with N ac-
counts chooses N strong passwords. If passwords were
random collections of equi-probable characters, the dif-
ﬁculty of remembering them would be related to their
length. Assume each such password is lgS bits. The
effort required to manage the portfolio might naively ap-
pear to be N lgS. But beyond remembering N passwords,
users must remember which matches which account. We
now explicitly consider this often overlooked sub-task.

3.1 Matching Passwords to Accounts
There are N · (N − 1)···1 = N! possible mappings of N
unique passwords to accounts; no encoding of this infor-
mation uses less than lg (N!) bits, unless passwords con-
tain clues as to which site they serve, violating A1 above.
Thus the number of bits to be remembered to manage a
portfolio of N passwords, each of lgS bits, is at least:

E(N) =N · lgS + lg (N!).

(1)

(As noted above, this approximation fails to address the
complexities of human cognition, but sufﬁces for the ar-
gument below.) Clearly this grows rapidly with N, the
second term super-linearly by Stirling’s approximation
(lnN! ≈ N lnN −N). Consider a conscientious user, with
N = 100 accounts. Choosing unique random passwords
of 40 bits for each account rewards him with the obliga-
tion to remember 100×40+lg(100!) = 4525 bits (equiv-
alent to 1362 random digits or 170 random 8-digit PINs).
This burden far exceeds what users can manage by mem-
orization (i.e., without other aids); for most it is insup-
portable. How can users reduce it? An obvious shortcut,
with signiﬁcant side effect, is to choose weaker (less ran-
dom) passwords; the linear dependence on lgS suggests
reducing strength as much as possible.

While using weaker passwords clearly reduces the ﬁrst
term of (1), no matter how weak N distinct passwords
are, the second term is unaffected. Considering that term
alone, N = 100 yields lg(N!) =525. This is double the
lg (52!) =226 bits required to memorize the order of a
shufﬂed card deck, and equivalent to remembering 158
random digits—random since as noted, no encoding of
an N × N assignment takes fewer than lg(N!) bits. Thus,

USENIX Association  

23rd USENIX Security Symposium  577

3

the assignment burden alone, including the problem of
password interference [16], is evidently beyond a rea-
sonable expectation of users.

So the two staples A1, A2 of password advice ap-
pear impossible to meet individually, let alone simulta-
neously. How do users proceed? They “cheat” on A1
by choosing passwords far weaker than advised. But this
isn’t enough—no matter how weak the passwords, a user
must still remember lg (N!) random bits for password as-
signment. A further coping strategy is needed.

3.2 Password Re-use as a Coping Strategy
Consider next a user with N accounts using G ≤ N pass-
words to cover them. Assume for now each password is
used at n = N/G accounts, that the password-to-group
assignment is random and (for simplicity) that G di-
vides N. The burden of remembering passwords drops
to G · lgS bits. What of the further burden of remem-
bering which password goes where? The G groups of
accounts each have n = N/G elements. There are CN
n
possible combinations for the ﬁrst group, CN−n
for the
second, etc., so the number of possible assignments of N
accounts to G equal-sized groups is:

n

(cid:31) N
n (cid:30)·(cid:31) N − n

n (cid:30)···(cid:31) n

n (cid:30) =

N!
(n!)G .

Thus the user effort (memory burden in bits) drops to:

EG(N) =G lgS + lg (N!)− G· lg (n!)

≈ GlgS + N lgG

(2)

the last line following by Stirling’s approximation again.
Now compare the burden of managing a portfolio
with and without password re-use. For example, even
if lgS is as low as 20, from (2), the burden of manag-
ing 100 accounts with 10 passwords is E10(100) = 506
bits, while from (1) the burden of doing so with 100 is
E(100) = 2525 bits. Thus, in this instance, password re-
use reduces the memorization burden by a factor of ﬁve.

3.3 Tradeoff: Re-use & Password Strength
What other solutions use the same effort? A portfolio of
N passwords can be managed in many ways. If EG(N) is
ﬁxed then lgS ≈ (EG(N)−N lgG)/G. So, lgS falls faster
than 1/G: doubling the number of passwords more than
halves the number of bits per password. So, if G = N (no
password re-use), then lgS must be small.

Fig.1 shows the locus of solutions in the G-lgS plane
when N = 100 and the budget is EG(100) =400, 550 and
700 bits. This reveals the essential tradeoff: less re-use
(i.e., increasing G) implies weaker passwords. For exam-
ple, at ﬁxed effort EG(N) =400, two possible operating

points are (G = 4,lgS = 52.5) and (G = 5,lgS = 36.2).
At ﬁxed effort, the question is not whether achieving
password strength and avoiding re-use are good, but how
these relative goods are best traded off. Deciding, e.g.,
between these two operating points depends on whether
reducing password re-use (by increasing G from 4 to 5)
reduces the risk of harm by more or less than reducing
password strength (from 52.5 to 36.2 bits). The rapid
decline of lgS in Fig.1 as G increases suggests that, far
from being unallowable, password re-use is a necessary
and sensible tool in managing a portfolio. Re-use ap-
pears unavoidable if lgS must remain above some min-
imum (and effort below some maximum). Fig.1 further
suggests that G should be small: high values of G seem
to imply very low values of lgS. This enormous saving
in user effort that password re-use provides may explain
its ongoing prevalence in practice [24, 19, 21, 45].

Note on entropy: caution is needed to avoid historical
pitfalls such as assuming particular ranges for lgS based
on metrics appropriate only for random passwords, or
misleading rules-of-thumb on what is necessary to with-
stand attack. We intend lgS to represent user effort to re-
member a password, not attacker guessing difﬁculty; the
two may be correlated but should not be used in place
of each other. For example, if a user has 7 unique pass-
words, and each names a major Hawaiian island, then
lgS = lg7 ≈ 2.8 bits. But this offers little guide on how
It is also well un-
hard these passwords are to guess.
derstood [48, 8, 37, 39] that t · lgC and NIST’s crude
password entropy estimate [12], signiﬁcantly overesti-
mate the difﬁculty of guessing user-chosen length-t pass-
words from C-character alphabets. Equally, such metrics
must not be mis-used to estimate users’ capabilities or ef-
fort, lest we drastically over-estimate what a reasonable
cognitive burden is.

4 Objective Function: Loss + Effort

Suppose a user has N password-protected accounts. Ad-
vice such as A1, A2 implicitly assume the goal is to min-
imize loss. Let Pi be the probability of compromise in a
given period (e.g., per year, under the current password
management strategy), and Li the loss endured upon such
compromise. We intend that Pi capture the probabil-
ity that an attacker gains the means to access account
i, whether or not that means is used or results in loss.
We intend Li to capture the expected value of the con-
sequences of account compromise (regardless of attack
vector), including direct losses and any indirect costs in-
volved in remediation. The total expected loss is

L =

4

N

∑

i=1

Pi · Li.

(3)

578  23rd USENIX Security Symposium 

USENIX Association

factor in the effort they must spend to reduce that loss.
They may be willing to spend effort to reduce loss, but at
some point there are diminishing returns; and it is waste-
ful to continue after the cost of further effort exceeds
expected reduction in loss. Thus, rather than an uncon-
strained optimization [29] ignoring reluctance to increase
effort, we should solve a constrained problem explicitly
including cost of user effort.

One way to incorporate a constraint is to minimize loss
subject to a bound—e.g., say ∑Ei < Emax to model users
with an upper limit on the effort they are willing to ex-
ert. This is reminiscent of the compliance budget [5, 4];
it can be achieved with Lagrangian multipliers. A more
general approach, which we follow, is to minimize not
the expected loss, but the sum of effort plus loss, L + E.
As a precedent for this approach, economics Nobel lau-
reate Becker notes [6] that attempts to minimize crime
lead to perverse results, and it is preferable to minimize
the costs of crime plus the costs of detecting, prosecut-
ing and punishing it. For example, it makes little sense
to spend $1 more on policing effort if that reduces the
effects of crime by less than $1.

To illustrate the importance of the objective function
we revisit the question of ﬁnding optimum allocation of
effort2 when dPi/dE j = 0,i (cid:30)= j (i.e., no cross-account
attacks). The optimum occurs when the derivative (with
respect to E) of objective function L + E is 0: dL/dE +
1 = 0. Using (3) to substitute for L gives the system

dPi
dEi

Li ·

= −1

for i = 1,2,3,···, N.

(5)

Thus, at optimum effort allocation,3 the marginal return
on effort to reduce Pj is a factor Li/L j higher than that to
reduce Pi :

.

(6)

dPj
dE j

=

Li
L j ·

dPi
dEi

If the loss for the most important account is, say, 104
times that for the least important (L1 = 104LN) then the
marginal return on effort should differ by that factor.
Thus, effort should not be spent equally on all accounts.
While we are unlikely to ﬁnd an exact form for how
the probability of harm varies with effort, using a para-
metric form can help illustrate the relations. Basing an
example on Shamir’s quote “to halve your vulnerability
you have to double your expenditure” [46], we exam-
ine what happens when there is a reciprocal relation be-
tween them, i.e., Pi(Ei) ∝ 1/Ei. This gives dP(Ei)/dEi ∝
−1/E2
i . Substituting into (6) indicates how the relative
effort for two accounts should depend on the relative
losses:

E j =(cid:31) L j

Li · Ei.

2This includes max cumulative effort and where/how to allocate it.
3This is also true at other points, though not proven here.

Figure 1: Locus of achievable solutions trading off re-use and
password strength for ﬁxed effort EG(N) =400, 550,700 in (2)
at N = 100. Note that when effort is kept constant, lower levels
of re-use are only achieved by having weaker passwords.

A major complication we will ﬁnd—and defer to Section
5—is that some attacks affect more than one account;
e.g., malware and system attacks affect all accounts, and
if passwords are re-used then an attack against one ac-
count can affect many others. But for now, suppose that
attacks are only against individual accounts. Then the Pi
depend on effort Ei devoted to account i but not to E j.
The probability of compromise Pi = Pi(Ei) is presum-
ably monotonically non-increasing with effort. Investing
more effort generally reduces Pi; a stronger password re-
duces the risk that it falls to password-guessing attacks.
If aiming to minimize expected loss L, the solution
occurs when L has derivative zero with respect to E =
∑i Ei, which from (3) gives the system of equations

dPi
dEi

= 0 for

i = 1,2,3,···, N.

(4)

The solution is trivial:
the optimum is achieved when
further effort can’t reduce the probability of loss for any
of the N accounts. Thus to minimize L, we should in-
crease each Ei until no further reduction is possible (fur-
ther effort does not affect Pi). If Pi(Ei) is monotonically
decreasing—so further effort always reduces Pi—then
expected loss is minimized at inﬁnite effort. Thus the
lack of a constraint on effort leads to an unrealistic solu-
tion. Note also that users gravitate toward a solution very
different from this effort-maximizing one. From this we
infer: the objective function users minimize is not merely
expected loss—if it were they’d always invest effort that
could reduce loss and always follow A1, A2.

This optimization has an obvious ﬂaw: minimizing L
implicitly values user effort at zero. Users presumably
care about loss due to account compromise, but they also

USENIX Association  

23rd USENIX Security Symposium  579

5

So if two accounts differ in value by factor 104, ideally
the effort expended would differ by a factor 100. We reit-
erate: this analysis, as it depends on the parametric form
for Pi(Ei), is for illustrative purposes only.

Now contrast this solution minimizing L +E, with that
found by minimizing L alone (system (4) above). First, if
minimizing L, all passwords should be as strong as pos-
sible, meaning that (at the optimum) no additional effort
can reduce the risk for any account. When minimizing
L + E this isn’t the case: (5) says that (at the optimum)
additional effort may still reduce risk for every account,
but it is sub-optimal to spend it. Second, when minimiz-
ing L, the optimum protection given to an account is in-
dependent of Li. When minimizing L + E some accounts
should be (possibly far) less protected than others: (5)
shows that the rate of return on effort should be inversely
related to the account value.

Thus, using objective function L + E (not L) makes an
enormous difference in solutions. We posit that much of
the advice directed at users aims to minimize L only, and
is ignored as users implicitly care about E also and have
found operating points attempting to minimize their ob-
jective function; these points may or may not be optimal,
but have been arrived at by ad hoc methods. We note that
in minimizing L + E we neglect the non-linear response
to probabilities predicted by Prospect Theory [35]. We
believe that the rational model which offers (Kahneman
[34]) “great precision in some situations and good ap-
proximation in many others” is the most realistic one that
we can currently make progress on, and signiﬁcantly ad-
vances a model that neglects E. Finally, use of the term
portfolio is not accidental. Since 1952 [40] it has been
recognized that managing a portfolio of equities raises
issues drastically different from managing individual se-
curities. In an analogous situation for passwords, due to
cross-account attacks, the security of accounts cannot be
considered in isolation, yet the literature has given little
attention to the portfolio problem.

5 Modeling Loss, Effort, Attack Classes

While (5) offers to guide effort allocation when minimiz-
ing L + E, it assumed dPi/dE j = 0 for i (cid:31)= j; we post-
poned issues of cross-account attacks. This might be
reasonable if guessing were the only attack and account
passwords were unique; the probability Pi of compromise
of account i would then depend only on how passwords
withstood attack. But that over-simpliﬁes. With pass-
word re-use, compromise of one account can leak to oth-
ers, and client-side malware affects all accounts. Such
attack vectors are too important to ignore. Pi depends on
effort not just devoted to account i but also, e.g., to ad-
dress client malware or avoid phishing, and the security
of other sites the password is re-used on.

If we can’t assume partial derivatives of zero, then on

minimizing L + E, instead of (5) we get the system

N

∑

i=1

∂ Pi
∂ E j · Li = −1

for j = 1,2,3,···, N.

(7)

This is not simply a linear system. The N unknowns
E j, speciﬁed implicitly by the constraint on N2 partial
derivatives, relate non-linearly to the Li. The intuition of
(5) is now lost. A simple interpretation (e.g., marginal
return on effort should be inversely related to loss) is
no longer discernible, as instead of appearing singly, the
partial derivatives are now constrained by a sum.

Note that if we minimize L instead of L + E we get
a system similar to (7), but with zero on the right side.
Since losses must be non-negative and the partial deriva-
tives are non-positive, the solution is achieved by set-
ting ∂ Pi/∂ E j = 0 for all i, j. This would again indi-
cate optimality occurs when no further effort can reduce
any of the loss probabilities. Thus, the fully general
system is tractable if we use the wrong objective func-
tion. Alternatively, a simpliﬁed system (i.e., assuming
∂ Pi/∂ E j = 0 for i (cid:31)= j) is tractable using a realistic ob-
jective function. However, the general system using the
realistic objective function is challenging. Our way for-
ward is to re-structure the problem to isolate types of at-
tack affected by different types of effort. By including
the major attack vectors, the model is necessarily more
complicated than that yielding (5), but will allow insight
on how to manage a portfolio when minimizing L + E.

5.1 Attack Classes and Attack Vectors
We partition attacks into three classes:

• Class I attacks (FULL): these compromise all
password-protected accounts of a user. They in-
volve general attack vectors targeting the client ma-
chine. Upon success, the attacker acquires actual
passwords. Example: client-side malware (e.g.,
persistent keyloggers), which we assume provides
attacker access to all of a user’s passwords.

• Class II attacks (GROUP): these compromise all
of a user’s accounts protected by the same shared
(“group”) password, with the attacker obtaining
that password; this includes singleton groups. Ex-
amples: phishing, brute-force and other guessing,
shoulder-surﬁng, server break-ins to obtain pass-
word ﬁles, network channel compromise. We as-
sume the attacker will try appropriate credentials
with this password on all relevant sites (a ﬁnite
number), determining associated account userids
from public information or otherwise, and gain ac-
cess to all accounts that use this password. Com-

580  23rd USENIX Security Symposium 

USENIX Association

6

Class I

(Full, direct)

Class II

(Group, direct)

Phishing, password guessing,

Attack
Vectors

Client-side malware
(keyloggers, etc.)

Effort elements
addressing attack

Run AV, disable unused apps/interfaces,
run up-to-date software (apply patches),

avoid suspicious links,

don’t click on email attachments

shoulder-surﬁng,

system-side database compromise,

network channel compromise

Choose strong passwords,
don’t re-use passwords,
change passwords often,

don’t write down,
avoid phishing sites

Class III

(Single, indirect)

Session hijacking,
cross-site scripting,

password reset mechanisms

little advice

Table 1: Attack classes for password-protected accounts, attack vectors, and relevant user effort elements (defensive actions).

promising one account thus may imply losses in all
same-password accounts of the user.

• Class III attacks (SINGLE): these compromise
only a target account, without obtaining the actual
password.4 Example attack vectors: cookie steal-
ing, single-session hijacking (e.g., by cross-site re-
quest forgery), exploiting password reset vectors
(but not those that mail-back original passwords).
The attacker may gain account access, but cannot
leverage this to access other accounts, even same-
password accounts.

While this classiﬁcation is still a simpliﬁcation—e.g.,
some passwords are easily derived from related pass-
words [49]—it allows us to model cross-contamination.
To handle the case where passwords are modiﬁed-and-
shared rather than simply shared between groups, as ob-
served by Das etal [17], would require an adjustment to
this model (e.g. by modifying Class II). Table 1 synop-
sizes the attack classes, their principal vectors and user
effort that addresses them. Note that the user effort re-
lated to passwords (e.g., strength, avoiding re-use, avoid-
ing phishing sites) is concentrated in Class II. Class I
deals with system-wide attacks. Class III deals with at-
tacks affecting only a single account, not others sharing
the same password.

The probability of individual account compromise can

now be split as:

Pi ≈ PI + PII

i + PIII
i

(8)

where superscripts denote attack class. Here, and
throughout the paper, the compromise probabilities are
assumed small enough that the well-known approxima-
tion (1−∏i (1− Pi)) ≈ ∑i Pi can be used. For Class I we
omit the subscript from attack probability PI, since it has
the same value for all accounts. Now, if a user has G ≤ N
unique passwords, sharing password w j across a set AJ

4In the case of password resets mentioned next, the attack may re-
cover a new temporary reset password, but not the original password
possibly shared across other accounts.

of accounts, the expected loss becomes:

L = PI

N

∑

i=1

Li +

G

∑

J=1

= PI

N

∑

i=1

Li +

J=1

( ∑
i∈AJ
G
∑

i )( ∑
PII
i∈AJ
N
∑

PJ · LJ +

i=1

Li) +

N

∑

i=1

PIII
i Li

PIII
i Li.

(9)

To distinguish, e.g., account i from password-sharing
group J, we abuse notation with upper-case indices; and
similarly subscripts to denote sums over groups, so

(10)

Li

and PJ = PII

LJ = ∑
i∈AJ
J ’s superscript as this is for Class II only.

J = ∑
i∈AJ

PII
i ,

dropping PII

The three terms on the right side of (9) match the three
attack classes. The ﬁrst term is the probability of a Class
I attack, weighted by the entire portfolio value. The
second term is the sum across the G password-sharing
groups, each weighted by the value of the accounts in
that group. This highlights the drawback of password
re-use: a compromise is not isolated to one account, but
spreads to others. The third term is the sum of proba-
bility of individual account compromise weighted by the
account value.

5.2 Modeling Effort Allocation and Effec-

tiveness

To minimize an objective function that includes both loss
and effort, both must be mapped to the same dimen-
sion. For simplicity, we assign a monetary value E for
the time and effort—a mapping that is naturally user-
dependent. The cost of this management has different
components; preventing different attacks often requires
different mechanisms. Thus again, this is split based on
the class of attack the effort addresses:

E = EI + EII + EIII

(11)

= EI +

G

∑

J=1

EII
J +

N

∑

i=1

EIII
i

.

Under the assumption that effort is applied independently
∂ E/∂ EI =
across classes, from (11) we also have:

7

USENIX Association  

23rd USENIX Security Symposium  581

∂ E/∂ EII = ∂ E/∂ EIII = 1. EI is the cost of defensive
effort related to Class I attacks—including, e.g., the total
cost and time/effort associated with purchasing/running
anti-virus software, and all effort related to keeping a
computer malware-free. EII
J is the cost of effort involved
in combating Class II attacks on a group that share the
same password (brute force, social engineering, etc.).
Clearly, EG(N) given in (2), the cost of managing the
password portfolio, is a portion of EII. However, EII
also includes effort devoted to other Class II attacks, such
as phishing [33]. EIII relates to account-speciﬁc efforts,
which may include, e.g., managing one-time passwords
or second-factor authentication devices.

Assuming the three types of efforts can be controlled

independently, objective L + E is minimized when

∂ (L + E)

∂ EI =
which simpliﬁes to:

∂ (L + E)

∂ EII =

∂ (L + E)
∂ EIII = 0

∂ L
∂ EI =

∂ L
∂ EII =

∂ L
∂ EIII = −1.

(12)

(13)

Substituting our expression for loss (9) into each of these
three equalities, the parade of equations concludes with:

(cid:31) N
∑

i=1

Li(cid:30) ∂ PI
∂ EI = −1
∂ PJ
LJ ·
∂ EJ
∂ PIII
i
Li ·
∂ EIII

i

= −1,J = 1···G
= −1,i = 1···N .

(14)

(15)

(16)

Note that we have used the fact that the effort devoted to
group J does not affect either the probability of loss for
group K (i.e., ∂ PII
K /∂ EII
J = 0 when K (cid:29)= J) or the effort
devoted there (i.e., ∂ EII
K /∂ EII

J = 0 for K (cid:29)= J).

Implications of the Model

5.3
Equations (14)-(16) help formalize the concept of opti-
mization of defensive investment (i.e., effort) related to
expected loss. We brieﬂy discuss each further.

Class I equation. Eqn (14) relates to Class I attacks,
e.g., client-end malware like keyloggers. It isolates the
cost of avoiding such attacks from efforts directly related
to password management. The sum over all Li reﬂects
the deﬁnition: Class I attacks compromise all of a user’s
passwords—thus the loss may be quite large, especially
if the sum strongly dominates individual Li values. The
absence of individual Pi in (14) reﬂects that defensive ef-
fort (cost) related to reducing likelihood of Class I losses
is unrelated to costs associated with managing individual
passwords. This is notable as current password advice

to end-users is predominantly related to managing indi-
vidual passwords (e.g., choosing stronger, more complex
passwords, not re-using across accounts), none of which
is related to (14).

Common advice related to (14) includes (see Table 1):
keeping software up-to-date with patches; using AV
(anti-virus) protection; disabling unused applications and
interfaces; “hardening” the platform OS.

Regarding overall investment in client-end protection,
(14) informs us that effort expended defending Class I
attacks should be driven by: (i) the total value of all
accounts the user accesses from the client device—the
larger this value, the more worthwhile even small de-
fensive efforts which reduce the probability of losses;
and (ii) the degree to which incremental defensive ef-
fort reduces the probability of Class I attacks. Note that,
counter-intuitively, the effort optimally expended is not
driven by the absolute probability of Class I attacks—
since effort spent doesn’t necessarily reduce the proba-
bility of successful attack, even if Pi is large.

Class II equation. Note that (15) is a set of equations,
one for each password-sharing group J. LJ accumulates
losses over the accounts sharing a password, based on
the assumption that once a password is compromised, all
accounts sharing it may suffer. PJ sum probabilities over
all accounts in the group, for a similar reason.

Regarding overall investment in defenses against Class
II attacks, (15) informs us that the allocation of such ef-
fort should be driven by the following, considered now
for each group: (i) the total value of all accounts in the
shared-password group—the larger this value, the more
worthwhile defensive efforts which reduce the PJ; and
(ii) the cumulative sum, across all groups accounts, of
the degree to which incremental defensive effort reduces
the probability of Class II attacks. As above for Class I,
the optimal effort expended is not driven by the absolute
probability of Class II attacks; the same is true for (16)
and Class III.

The similarity between (15) and (5) should be obvi-
ous: we again have a constraint involving a single partial
derivative. A few conclusions can be drawn that mirror
those drawn about the simpler model in Section 4. First,
all passwords should not be equally strong (that would
be wasteful, allocating excessive effort to low-value ac-
count groups at the expense of high-value ones). Sec-
ond, the rate of change of PJ with respect to effort should
be inversely proportional to LJ. This means that (unless
a user has excess capacity of effort they wish to spend,
and no higher-value groups to spend it on) groups with
LJ ≈ 0 should be very exposed and should have weak
passwords, since as 1/LJ → ∞, they should be at the
point where ∂ PJ/∂ EJ is extremely high; thus even tiny
invested effort would reduce PJ signiﬁcantly, but spend-
ing effort there would be wasteful as we care not about

582  23rd USENIX Security Symposium 

USENIX Association

8

PJ but PJ · LJ. Effort is better spent on an account group
with high LJ (even if ∂ PJ/∂ EJ is very low). It makes no
sense to invest at all on accounts where LJ = 0, so long
as any other account has LJ > 0.

Toy example. To illustrate (15), suppose two bank ac-
counts sharing a common password have loss values 10
and 12. Assume that the ﬁrst account is phished, and
thereafter an attacker tries the same password with ap-
propriate obtained userid on all banks. Assume further
that additional effort δ E = 3 units (e.g., a stronger group
password) reduces individual account compromise prob-
abilities from 0.1 to 0.09 (ﬁrst account) and from 0.05
to 0.03 (second). Then the initial expected loss (see (9))
of (10 + 12)(0.1 + 0.05) = 3.3 is reduced, by extra ef-
fort, to (10 + 12)(0.09 + 0.03) =2.64. Thus extra effort
of 3 units reduced loss by only 0.66. This can also be
observed by looking at the differences (or derivatives, as
in (15)); the change is (10 + 12)(−0.01/3− 0.02/3) =
−0.22. And, in this example, as −0.22 is less negative
than −1, we have higher investment than optimal—the
cost of effort invested exceeds the reduction in loss it
provides. The equations thus conﬁrm our expectations,
despite the “units of measure” carrying little meaning.

Class III equation. Finally, (16) reminds us that, re-
gardless of password policies, we must keep in mind and
beware reset mechanisms and alternative access paths.
Class III attacks involve only a single account and are un-
related to group sharing of passwords, being unrelated to
the actual choice of passwords. As noted in Table 1, users
get little advice related to Class III attacks (and hence
∂ PIII
i ≈ 0). In the sequel, (16) is discussed little,
as risks associated with these attacks are largely imper-
vious to user effort, our present focus. Regarding overall
effort defending Class III attacks, (16) tells us that, con-
sidering now each account individually, the allocation of
such effort should depend on: (i) the account value; and
(ii) the degree to which new effort reduces the probability
of Class III attacks on it.

i /∂ EIII

6 Account Grouping for Password Re-use

We saw in Section 3 that, without additional coping
mechanisms, re-use is unavoidable for large N. We now
show that it can help, even for smaller portfolios. Since
we seek to minimize L+E there are two components to
consider: changes in effort, and in expected loss. For
loss, we need consider only Class II attacks, as Class I
and III attacks are unaffected by re-use.

Consider the case of three accounts, two relatively
low-value (L1,L2), one high-value (L3) so L3/(L1 +
1 ≈ PII
L2) =m >> 1. For simplicity assume further PII
2
(we will drop superscripts II, as only Class II attacks
are relevant). Now compare Case A (using three unique
passwords) vs. Case B (re-use one password across low-

value accounts, with unique password for high-value).
For Case A, expected Class II losses are: P1L1 + P2L2 +
P3L3. For Case B, re-use increases the expected loss
over the ﬁrst two accounts by ∆L = (P1 + P2)(L1 + L2)−
(P1L1 + P2L2); as P1 = P2 now, this is P1L2 + P2L1 =
P1(L1 + L2) > 0, but the user manages one fewer pass-
word. Assume the saved effort ∆E is used to strengthen
the high-value password5 reducing the expected loss re-
lated to the third account from P3L3 to (P3(1 − e))L3
where 0 < e < 1. So Case B is preferable (has lower
expected loss) provided the increase ∆L in expected loss
over the ﬁrst two accounts is less than the expected de-
crease on the third, i.e., provided: P1(L1 + L2) < eP3L3,
or equivalently,

m > P1/(eP3)

(17)
We expect (17) often holds—e.g., if m = 50 (a ﬁnancial
account with value 100 times that of a free or low-value
subscription site) and P1 ≈ P3, then (17) is true for e >
1/50 = .02, i.e., a 2% or greater reduction in probability
of loss due to a strengthened password. The right side
of (17) becomes even smaller if P3 > P1, and if P3 < P1
then (17) still holds for a correspondingly larger e. Thus
certainly, re-use can be beneﬁcial.

Of course, guessing is but one possible Class II at-
tack; some others also increase the consequences of re-
use. The risks of some, like phishing, can be reduced by
the user, while that of others, like server-side attacks, are
largely impervious to user effort (see 7.3).

6.1 Share among Accounts of Similar P/L
We now explore how to re-use passwords “properly”.
Based on the loss model, we give an optimal password
re-use strategy in the following sense: for a ﬁxed num-
ber of passwords, and a given set of accounts (thus effort
is ﬁxed), ﬁnd how to group accounts to minimize total
expected loss.

As before, assume a user splits N accounts into G
groups each sharing a unique password. Per the second
term on the right of (9), the total Class II loss is:

LII =

G

∑

J=1

( ∑
i∈AJ

i )( ∑
PII
i∈AJ

Li)

(18)

Is there an optimal way to partition this set of accounts
into shared-password groups AJ?

We ﬁrst address the case of adding a new account to
an existing portfolio; i.e., we have G groups and must
decide to which group a new account is best added. From
(18), adding a new account with (Pi,Li) to group J, the
incremental loss is (with LJ,PJ as in 5.1):

∆L = PiLJ + LiPJ + PiLi

(19)

5If users do not do this, the case for re-use is lost; this is critical.

USENIX Association  

23rd USENIX Security Symposium  583

9

From (2), the incremental effort is ∆E ≈ lgG. Since nei-
ther ∆E, nor the third term of ∆L depend on the group J,
the objective function, L + E, depends on the group as-
signment only through the ﬁrst two terms of (19). Thus
the new account should be added to the group AJ mini-
mizing PiLJ + LiPJ. This brings an interesting insight: if
any group J exists such that PJ < PK and LJ < LK for all
G (i.e., the group has both a smaller total probability and
a smaller total loss than all other groups), then all new
accounts should be added to that group J, until one of
the two inequalities fails.

Thus without loss of generality, the remaining case is
in deciding between two groups AJ, AK when PJ < PK
and LJ > LK. Here, new account i should be assigned to
AJ (vs. AK) if and only if:

PiLJ + PJLi ≤ PiLK + PKLi

This can be rewritten as
Li
Pi ≥

LJ − LK
PK − PJ

(20)

(21)

Fig.2 illustrates this constraint graphically. Recall that
a line of slope m in the PL plane is given by L = m ·
P + c. Thus, (21) says that account i should be placed in
group AJ (vs. AK) if and only if point (Pi,Li) lies above
a line with slope (LJ − LK)/(PK − PJ) going through the
origin. Fig.2 shows the construction of a (solid red) line
with slope (LJ − LK)/(PK − PJ); it passes through points
(PK,LJ), (PJ,LK). The dashed red line is one of the same
slope, but through the origin.

In summary, the decision boundary between adjacent

groups AJ and AK is given by the line:

L =(cid:31) LJ − LK

PK − PJ(cid:30)· P.

(22)

A necessary condition for optimality is the absence of
proﬁtable single moves in the following sense: if a par-
titioning of accounts is optimal, the total loss cannot be
decreased by moving any account i from group AJ to any
other group AK. This can be expressed as

PiLJ∗ + PJ∗Li ≤ PiLK + PKLi

for all K.

(23)

Here (PK,PJ∗) are the total
loss probabilities, and
(LK,LJ∗) the total losses, resp., for groups K and J∗
where J∗ denotes group J after removing account i. Sim-
ilar to (21), we can rewrite (23) as

Li
Pi ≥

LJ∗ − LK
PK − PJ∗

for all K.

(24)

Consider the case when the number of accounts N be-
comes large, in which case Pi and Li are typically small
relative to P and L. We can then assume the total loss

LJ 

LK 
Li 

Pi 

PJ 

PK 

Figure 2: Optimal assignment of a new account (Pi,Li) be-
tween two groups J and K. If the new account falls above the
dashed red line, total loss will be smaller when the account is
assigned to group J.

and probability of each group does not change much by
adding or removing a single account. Thus PJ∗ ≈ PJ (i.e.,
PJ ≈ (PJ + Pi)).
We ﬁrst show that given an optimal grouping, for any
groups J and K the decision boundary is bounded by:

LK
PK ≤

LJ − LK
PK − PJ ≤

LJ
PJ

.

(25)

The decision boundary slope (Fig.2, dashed red line) thus
must be between that of the green and blue lines.

To show this, note that group K must contain at least
one account i with Li/Pi ≥ LK/PK (since all of the Li
and Pi are ≥ 0). Thus (21) holds, implying account i
belongs in group AJ rather than AK unless the righthand
inequality of (25) holds. The reverse argument applies to
show the lefthand inequality in (25).

Now (22) tells us that the decision boundaries are lines
through the origin; so each group has at most two neigh-
bors. Further, (25) when applied to every pair of “adja-
cent” groups in the PL plane, implies the same ordering
applies to not only the ratio of L and P differences as in
(22), but also the ratio of their values:
LG
PG

L2
P2 ≥ ··· ≥

L1
P1 ≥

(26)

where, without loss of generality, the groups have been
ordered clockwise, according to their order in the PL
plane. In general for groups AJ, AK, recall that PJ < PK
implies LJ > LK. From this it follows that, given an or-
dering for the ratio, the same ordering must apply to the
expected loss and the reverse ordering for probability,
i.e.,

P1 ≤ ··· ≤P G and L1 ≥ ··· ≥ LG.

(27)
Thus ordering the account groups by decreasing total
loss, they have increasing total probability; due to the
possibility of equality, none of the orderings is strict.

584  23rd USENIX Security Symposium 

USENIX Association

10

6.2 Groups Similarly Weighted by PL
Consider next how large and how disparate different
groups will be. We show that under certain conditions,
the groups formed have similar individual products PL.
With focus again on the outcome as G increases, from
Section 6.1 the groups obey an ordering in terms of P,
L, and L/P, and the decision line slope (dashed red line
in Fig.2) must be between the slopes of the two adja-
cent groups. Thus, assuming accounts exist around ev-
ery point in the PL plane, as G increases the adjacent
groups have increasingly similar slopes, with bounds
on the decision boundary slope per (25). Since, from
(27), the Li are non-increasing, and from (27), the Pi
are non-decreasing, we have LJ ≥ (LJ + LK)/2 ≥ LK and
PJ ≤ (PJ + PK)/2 ≤ PK. It follows from (25) that, as G
increases:
(28)

.

(LJ + LK)/2
(PJ + PK)/2

LJ − LK
PK − PJ ≈

Re-arranging yields:

(LJ − LK)(PJ + PK) ≈ −(PJ − PK)(LJ + LK)

(29)

Expanding products and eliminating common terms,

PJLJ ≈ PKLK

(30)

Thus the product of probability and loss for adjacent
groups is about equal, increasingly so as the numbers of
groups G and accounts per group increase.

6.3 Pedagogical Illustration through Two

Generated Datasets

To illustrate, we generated two datasets, assigning ac-
counts to groups with an optimization program obeying
the “no proﬁtable moves” rule. The simulation models
100 accounts, with randomly assigned Pi and Li, to be
divided in ﬁve groups (shown by different colors in the
ﬁgures). The program assigns accounts to a group one at
a time. After each assignment, it tests all possible sin-
gle moves and swaps, performing any proﬁtable moves
before moving on to assign the next account. In the ﬁrst
dataset, corresponding to Fig.3 and Table 2, Pi and Li are
independently drawn from uniform distributions.

In practice, the combination of (23), (25), and (30)
means that whenever passwords are to be re-used across
accounts, the optimum strategy is to do so across ac-
counts with similar P/L ratio, and add enough accounts
per group to achieve similar total PL products for each
group. The resulting account assignments split the PL
plane into slices (see Fig.3). This implies that most high-
value accounts end up in the same group (particularly if
they have low compromise probability), and most low-
value accounts end up in another group (particularly if

x 104

10

s
s
o
L

 

d
e

t
c
e
p
x
E

9

8

7

6

5

4

3

2

1

0

0

0.1

0.2

0.3

0.4

0.5

Probability of Compromise

0.6

0.7

0.8

0.9

1

Figure 3: Password grouping, under the “no proﬁtable moves”
strategy. For this example, 100 accounts are uniformly placed
at random in the PL plane, and optimally assigned to one of 5
groups. Note the linear decision boundaries, corresponding to
P/L ranges (slices).

they have high compromise probability)—apparently in
line with what many users currently do. Table 2 reports
selected characteristics of the 5 password groups; note
the similar values of PL across groups, strictly decreas-
ing L, and strictly increasing P and P/L.

While the dataset used to produce Fig.3 allows visual-
ization of the linear decision boundaries, such a dataset
with independent distribution over P and L is not what
we would expect in practice. We thus generated a second
dataset (see Fig.4 and Table 3) where Li follows a power
law distribution and the expected value of Pi is inversely
proportional to (the square of) Li. While all observations
on the previous dataset still hold, further insights are ev-
ident. As on this dataset high-value accounts are less
likely to have high Pi, the high-value accounts end up
grouped together. Indeed, group 1 includes 53 accounts,
more than half of the set, while group 5 has only 4 ac-
counts (see Table 3).

The total resulting loss across all ﬁve groups is 7.94×
104. To see how this optimal assignment compares to a
random assignment, we computed total loss on the same
dataset on randomly assigning accounts to the 5 groups
(in 100,000 Monte Carlo trials), ﬁnding an average PL of
1.16× 108 (std deviation 0.24× 108). Thus the optimal
loss was 1500 times smaller than by random assignment,
and 5 standard deviations below the mean.

We emphasize that both datasets are modelled exam-
ples to illustrate principles. For other datasets, the gen-
eral ﬁndings will hold, but actual construction of groups
may signiﬁcantly differ depending on the data.

USENIX Association  

23rd USENIX Security Symposium  585

11

Group #

P

1
2
3
4
5

1.88e+01
1.14e+01
9.35e+00
7.94e+00
4.91e+00

Table 2:

Group #

P

1
2
3
4
5

2.29e+01
6.70e-01
6.42e-02
7.04e-03
1.26e-03

Table 3:

L

PL

P/L

max P/L min P/L Group Size
28
6.09e-04
3.96e+05
16
1.77e-05
8.08e+05
13
1.08e-05
9.71e+05
8.72e-06
16
1.14e+06
1.82e+06
27
4.73e-06
Characteristics of each group in the grouping corresponding to Figure 3.

7.44e+06
9.17e+06
9.08e+06
9.06e+06
8.93e+06

4.75e-05
1.40e-05
9.63e-06
6.96e-06
2.70e-06

1.80e-05
1.09e-05
8.93e-06
4.79e-06
3.22e-07

L

PL

P/L

max P/L min P/L Group Size
53
9.46e+02
3.24e+02
24
1.24e-03
1.76e+04
11
1.66e-06
2.40e+05
1.66e-08
8
2.45e+06
2.19e+07
4
2.80e-10
Characteristics of each group in the grouping corresponding to Figure 4.

7.41e+03
1.18e+04
1.54e+04
1.72e+04
2.77e+04

7.06e-02
3.80e-05
2.68e-07
2.88e-09
5.76e-11

1.44e-03
4.45e-06
3.01e-08
4.09e-10
9.29e-12

106

104

102

100

s
s
o
L

 

d
e

t
c
e
p
x
E

10−2

0

0.1
Probability of Compromise

0.2

0.4

0.6

0.8

1

Figure 4: Password grouping, second dataset (Pi’s drawn from
a distribution with mean inversely proportional to L2
i ). Due to
the non-linear axis, the decision boundaries are no longer lin-
ear. The number of accounts in each group differs from Fig.3.

7 Special Cases

Next, special cases illustrate how the model addresses
additional assumptions and circumstances.

7.1 Case 1:

Unknown Pi (Modeled as Equal)

The model highlights two variables with large effect on
the problem:
loss and compromise probability. Most
users could give some estimate of loss that would re-
sult from compromise of a speciﬁed account—perhaps
not entirely accurate, but representative of expected loss,
even if only in relative terms. In contrast, user estimates
of probabilities would likely be far worse, perhaps with-

out sense of even relative Pi’s. We thus consider here
what results from the optimization model on assuming
equal probabilities p = Pi for all i. Slice-based partition-
ing still applies, as does the ordering—the latter now eas-
ier with all accounts on a vertical line in the PiLi plane.
The main question is how many accounts will each group
have, and how does that relate to the Li of accounts in
each group.

If group J has NJ accounts, write PJ = pNJ. Then (30)

yields (pNJ)(LJ) ≈ (pNK)(LK), or, equivalently:

NJ

NK ≈(cid:31) LK/NK

LJ/NJ

.

(31)

Thus groups with high-value accounts will have fewer
accounts; optimally, the number of accounts NJ in a
group J varies inversely with the square root of the av-
erage loss LJ/NJ in that group.

To illustrate, we re-run the optimization process on the
second dataset (see Section 6.3), but now assuming igno-
rance of individual probabilities, modeling equal Pi. The
principles discussed earlier now result in the accounts be-
ing split by strict ordering of losses. The number of ac-
counts in the 5 groups is now (82, 11 4, 2, 1), vs. (53, 24,
11, 8, 4) in Table 3. As might be expected, total losses
increase to 1.24 × 106, vs. 7.94 × 104 for optimization
using known probabilities. This is 16× higher than the
optimum, but still 93× smaller than the average loss from
random assignment (see Section 6.3).
7.2 Case 2:

Group Passwords of Unequal Strength
We showed in Section 5.3 that passwords should not have
the same strength. We now show how the assumption
made in Section 6 (that Pi did not change much when we
moved account i from password group J to group K) can
be relaxed, so that there is no incompatibility. Here we

586  23rd USENIX Security Symposium 

USENIX Association

12

brieﬂy analyze the impact, on optimization results, when
Pi is password-dependent and groups have passwords of
different strength. Denote the (now group-dependent)
compromise probabilities Pi∈J, Pi∈K. Then by the argu-
ment used in (20), account i should be assigned to AJ if
and only if

Pi∈JLJ + PJLi ≤ Pi∈KLK + PKLi.

(32)

We again seek a bounding condition on Li/Pi, but now
using what group-neutral Pi value? We use the geometric
average Pi = √Pi∈JPi∈K and deﬁne the squareroot ratio
r =(cid:31)Pi∈J/Pi∈K. Then (32) yields
rLJ − (1/r)LK
PK − PJ

Li
Pi ≥

(33)

.

Note r>1 if group J has password weaker than K. Thus
with respect to group assignment, a weaker group J pass-
word has an effect equivalent to scaling up group losses
LJ, making it harder to satisfy the condition for assign-
ment to group J. Other results regarding the slicing, P
and L ordering, and so on remain as before.

7.3 Case 3:

Unequal Server Break-in Probabilities
Finally, consider the effects of different levels of secu-
rity at the server. The probability of server break-in is
largely outside users’ control, but the consequences are
not: a user may decide to share a password across ac-
counts, only to have one of the servers leak her password,
compromising all accounts sharing it. While the previous
analysis already takes into consideration server break-in
(as a Class II attack), we now analyze how two sites with
different server break-in probabilities will affect the op-
timum allocation.

Consider two accounts i and j, with same values
Li = L j but different probabilities, Pi = Pj + δi, where
δi is the added break-in probability due to a site i server
poorly managed compared to j. Upon assigning account
i (poorly managed) to a group, the added probability
δi will imply a higher ratio Pi/Li, so the account will
(likely) be grouped with accounts with higher P/L, typi-
cally lower-value accounts. Furthermore, as discussed in
Section 5.3, these groups may have a weaker password.
Thus, for a server with higher break-in probability, opti-
mum password grouping seems to push towards group-
ing the related account with lower-value accounts.

Related to this, our criteria for optimality depend on
how loss probabilities change with respect to effort, but
not on the magnitudes of the probabilities themselves.
Consider the possible case of a threat unaddressable by
user effort, swamping all others. Let Pi = Pi,u + Pi,u,
where dPi,u/dE = 0. If Pi,u > 103Pi,u, it may be fruitless

to spend substantial user effort if such expenditure af-
fects only the third decimal place in Pi. Nonetheless, this
is what our criterion for optimality suggests. System-
side or back-end (server) risks may swamp risks under
user control; we simply do not know.

7.4 Case 4: Coping Alternatives including

Password Managers

Despite violating long-standing password guidance,
writing passwords down is, if properly done, increas-
ingly accepted as a coping mechanism. Other strategies
to cope with the human impossibility of using strong
passwords everywhere without re-use include single-
sign-on, use of email-based password reset mechanisms,
and password managers. Such “password concentra-
tors”, a form of password re-use, allow access to many
accounts from one master access point, with account
passwords stored either locally or in the cloud. While
not explored in detail here, each can be analyzed in our
framework; we illustrate for password managers.

The main threats (recall Table 1) when re-use is em-
ployed are client-side malware (all accounts fall), and
various Class II attacks such as guessing, phishing, sniff-
ing wireless links and server breaches (all accounts in
the same sharing group fall). We must modify this pic-
ture slightly if a password manager is used. For Case
A (password store on a user’s local machine), the main
risk is still Class I attacks like client-side malware. There
is a decreased risk of phishing presumably, as users re-
member fewer individual passwords; similarly for guess-
ing attacks, as arbitrarily strong passwords now require
no user effort, and the master password that unlocks the
store resides on the client. A server-side breach com-
promises only a single account. Thus, a password man-
ager with client-side store approximates our model with
G = N. The cost, of course, is that portability across dif-
ferent client devices is lost as the passwords (if they are
unique and random) are effectively anchored to the client
on which they are stored.

Consider next (Case B) a cloud-based store, protected
by a single password. Phishing and guessing attacks
against any system-assigned secrets at the end-servers
remain unchanged. Now however, additional guessing,
phishing and server breach attacks exist against the sin-
gle master password which can result in the compromise
of all accounts. Class I attacks (e.g. due to malware on
the client) are unchanged. A password manager with a
password-protected cloud-based store approximates our
system with G = 1. It trades one set of risks for another:
the use of random and unique passwords in such a sys-
tem reduces both the risks related to any single manager-
chosen password being stolen and those related to re-use
in the face of server compromise. However, it introduces

USENIX Association  

23rd USENIX Security Symposium  587

13

severe new risks: if the master password is guessed or
used on any malware-infected client, or the cloud store is
compromised, then all credentials are lost.

8 Discussion and Implications

Recapping, recall ﬁrst the task of end-users: to choose
passwords random and strong (entropy lgS bits) without
re-use. The effort to manage N such passwords without
re-use is modelled as N lgS + lg (N!); as portfolio size
increases, this overwhelms user capability.

M1: Remembering random and unique passwords is in-

feasible for other than very small portfolios.

Users coping strategies include weak passwords and re-
use. There is a large disconnect: what standard advice
mandates as essential turns out to be impossible. We sug-
gest this is due to a failure to explicitly include user effort
in the objective function. Seeking to minimize loss alone
leads to unrealistic effort-maximizing solutions. While
some recent work [5, 4, 29] criticizes the practice of ig-
noring the burden password advice places on users, it has
not to our knowledge been included directly in the objec-
tive function. We make a related observation:

M2: While advice typically minimizes L over a single or
small set of sites, user best interest is to minimize
L + E over an entire portfolio.

The diversity of attacks complicates our search for an
optimum effort allocation. Short-cuts are tempting; we
can minimize L +E while ignoring cross-account attacks
(as in Section 4), or consider all attack types and min-
imize L alone. The ﬁrst scopes the problem too nar-
rowly, the second leads to the unrealistic demand to in-
vest unbounded effort. While both yield “solutions” that
are simpler than the model in Section 5, our work sug-
gests that realistic analysis must address a realistic attack
model and a realistic objective function.

M3: Realistic analysis of password effort allocation re-
quires incorporating attack vectors affecting 1) all
accounts; 2) accounts sharing a password; and 3)
single accounts.

Our segmentation of the space into Class I, II and III
attacks yields interesting insights. Minimizing L + E
over a portfolio implies user effort be spent unequally
across accounts. As can be seen from (15), all passwords
should not be equally strong; equal spending overspends
on low-value, and underspends on high-value accounts
(or account groups). Recall that, from (27), there is an
ordering of the group values LJ; the largest may be many
times greater than the smallest (L1 (cid:31) LG). Any group
for which LJ ≈ 0 should have ∂ PJ/∂ EJ high (meaning

a weak password). If we again invoke the reciprocal re-
lation between PJ and EJ suggested in Section 4, we’d

again ﬁnd E1 =(cid:31)L1/LG · EG. Thus a 104× value differ-
ence between the most and least valuable groups would
imply a 100× difference in invested effort. In this sense,
not only are weak passwords understandable and allow-
able, but their absence would be sub-optimal:

M4: A password portfolio strategy that rules out weak

passwords is sub-optimal.

Next, while sharing a password across a group of ac-
counts can amplify consequences if it is compromised,
we ﬁnd it is sub-optimal not to re-use. First, (1) indicates
re-use becomes unavoidable when N is large. Second,
(2) and Fig.1 demonstrate the tradeoff involved even if
N is small enough that re-use is theoretically avoidable;
i.e., re-use increases the probability of loss from certain
attacks, but also reduces effort. The question then is not
whether re-use is good or bad, but whether the effort re-
quired to avoid re-use can be better spent on other attack
types. Section 6 gives an example.

M5: A password portfolio strategy that rules out pass-

word re-use is sub-optimal.

The optimal strategy places accounts with similar P/L
ratio in groups sharing a password. Enough accounts
are added to each group to achieve similar PL products
per group. Most high-value accounts (particularly if they
have low Pi) end up in the same group(s), and most low-
value accounts (particularly if they have high Pi) in an-
other group(s).

M6: Optimal password grouping tends to (i) group to-
gether accounts with high value and low probability
of compromise; and (ii) group together accounts of
low value and high compromise probability.

The above observation lines up well with anecdotal ac-
counts of what many users actually do. Our ﬁndings
also agree with the informal claim [29], that users’ actual
effort allocation represents an efﬁcient operating point.
Thus, actual user password-related behavior is closer to
optimal than current expert advice.

Password managers (cf. Section 7.4) may improve us-
ability and reduce some risks, but remain vulnerable to
Class I attacks (e.g., client-side malware). Managers that
store passwords only on the client improve resistance to
Class II attacks, since they can choose better passwords
and eliminate re-use. However, in storing only on the
client this gives up one of the major advantages of pass-
words, i.e. portability. Managers that store passwords
in the cloud remove this restriction, but introduce a new
system-wide attack: as before if the client is infected
with malware all accounts are compromised, but now this

588  23rd USENIX Security Symposium 

USENIX Association

14

happens also if the cloud store is breached or the master
password is stolen or guessed. Thus, cloud-storage man-
agers trade one type of vulnerability for another.

M7: Password managers using client-only storage allow
a portfolio with random passwords and no re-use,
but lose cross-client portability. However, if cloud
storage is used it resembles a portfolio with only
one group, since a new attack on either the master
password or the store itself threatens all accounts.

Another disconnect stems from many password-
related threats being unrelated to the standard advice on
maintaining a portfolio: Class I attacks, server breaches
and Class III attacks are not reduced by password ad-
vice staples such as A1 and A2. Since successful Class
I attacks sum the losses across all accounts, the advice
to protect against them is disappointingly vague, while
advice to protect against the less consequential Class II
attacks is far more detailed and effort-consuming. It ap-
pears that users are given the advice that is most eas-
ily given, rather than the advice that would have greatest
impact. Comparing (14) and (15) shows that at optimal-
ity the marginal return on effort spent on Class I attacks
should be lower than that for any Class II group (e.g.,
effort should not be wasted strenghtening passwords for
a group with low LJ if any effective Class I measure re-
mains undone). Greater focus is needed to explore which
advice, for example from Table 1, provides protection
against which attack vectors:

M8: We lack metrics for the cost to end-users, of follow-
ing standard advice, and the effectiveness of follow-
ing it on reducing overall expected loss.

An important outcome of our review is that, when
minimizing L + E, optimality depends on the losses Li,
and on how the probability of loss varies with respect to
effort ∂ Pi/∂ Ei. In contrast if one minimizes L, the so-
lution depends on neither. Without better knowledge of
real-world values for L, and especially ∂ Pi/∂ Ei, we are
unlikely to achieve optimal resource allocation in prac-
tice. Conventional user behavior appears to be based al-
most exclusively on L, which users may be able to es-
timate; ∂ Pi/∂ Ei values are almost entirely overlooked.
This points to an important research direction: while re-
cent work has greatly improved understanding of pass-
word guessing resistance [8], we are almost entirely ig-
norant on how this evolves with effort.

M9: Without better estimates of how loss probability
changes with effort, we should not expect to be able
to allocate effort (even close to) optimally.

Finally, can concrete advice for users be distilled from
our ﬁndings? For example, absent knowing how Pi

change as a function of various types of effort, we lack
a prescriptive way to determine the optimal number of
groups G. Nonetheless, the knee of the curves in Fig.1,
and what we know of user behavior [24, 21, 14] points to
the number of groups being below 10 if no other aids
are used. The values of loss probabilities Pi are en-
tirely unknown; expected loss values Li, or at least rel-
ative importance, are more easily estimated or ordered.
Thus the variables needed to ﬁnd an optimal grouping
are (and are likely to remain) unavailable to most users.
We might however simplify, e.g., assuming all Pi equal,
or that Pi values differ by an order of magnitude between
heuristically-deﬁned categories (e.g., banks, merchants,
throwaway accounts, etc.).

While the optimal strategy involves selective re-use
and weaker passwords, beneﬁts accrue only if the effort
saved is re-deployed elsewhere for better returns. Users
must not arbitrarily weaken and re-use passwords. Thus
empirical studies are needed to determine if our guide-
lines can be followed by users.

We hesitate to give deﬁnitive advice. First, this re-
quires more insight than our current understanding of LJ
and ∂ PJ/∂ EJ values allows. Second, we are reminded
how far bad assumptions (e.g., minimizing L vs. L + E)
can lead us astray. Consider, however, a strategy that
chooses G in the range 5 to 10, and assigns accounts
to groups by value so that the number of accounts in a
group is as in Section 7.1. Given the uncertainty about
unknown parameters, a strategy like this may be the best
we have—and may even be optimal.

9 Concluding Remarks
We have explored the task of managing a portfolio of
passwords. A starting point for our analysis was the
critical observation that to be realistic, efﬁcient password
management should consider a realistic suite of attacks
and minimize the sum of expected loss and user effort.
Our model yields detailed results; it indicates that any
strategy that rules out weak passwords or re-use will be
sub-optimal. We have shown that optimality requires
forming groups whose accounts in sum have similar
PL values (P = ∑Pi,L = ∑Li). This suggests simple
guidelines, such as: if Pi is similar across accounts, then
optimal grouping will put high-value accounts in smaller
(or singleton) groups, and low-value accounts in larger
groups. Our ﬁndings are consistent with certain user
behaviors (e.g., [47]) that contradict accepted advice,
offering to justify the behavior and giving evidence for
the model’s utility. We ﬁnd that optimally, marginal
return on effort is inversely proportional to account
values. We note that while password re-use must be
part of an optimal portfolio strategy, it is no panacea.
Far from optimal outcomes will result if accounts are

USENIX Association  

23rd USENIX Security Symposium  589

15

grouped arbitrarily.

Acknowledgements. We thank Robert Biddle, Joseph
Bonneau, and anonymous referees for their comments
which helped improve this paper. The third author ac-
knowledges an NSERC Discovery Grant and Canada Re-
search Chair in Authentication and Computer Security.

References
[1] Stop.Think.Connect. http://www.stopthinkconnect.org/.
[2] US-Cyber Emergency Response Readiness Team: CyberSecurity

Tips. http://www.us-cert.gov/cas/tips/.

[3] A. Adams and M. A. Sasse. Users are not the enemy. C.ACM,

pages 40–46, December 1999.

[4] A. Beautement and A. Sasse. The economics of user effort in
information security. Computer Fraud & Security, pages 8–12,
October 2009.

[5] A. Beautement, M. Sasse, and M. Wonham. The Compliance
In

Budget: Managing Security Behaviour in Organisations.
NSPW, 2008.

[6] G. S. Becker. Crime and punishment: An economic approach. In
Essays in the Economics of Crime and Punishment, pages 1–54.
UMI, 1974.

[7] J. Bonneau. Guessing human-chosen secrets. University of Cam-

bridge. Ph.D. thesis, May 2012.

[8] J. Bonneau. The science of guessing: analyzing an anonymized
corpus of 70 million passwords. In Proc. IEEE Symp. on Security
and Privacy, pages 538–552, 2012.

[9] J. Bonneau and S. Preibusch. The password thicket: Technical
and market failures in human authentication on the web. In WEIS,
2010.

[10] J. Bonneau and S. Schechter. Towards reliable storage of 56-bit

secrets in human memory. In Proc. USENIX Security, 2014.

[11] A. Brown, E. Bracken, S. Zoccoli, and K. Douglas. Generat-
ing and remembering passwords. Applied Cognitive Psychology,
18(6):641–651, 2004.

[12] W. Burr, D. F. Dodson, and W. Polk. Electronic Authentication

Guideline. In NIST Special Pub 800-63, 2006.

[13] W. Cheswick.

Rethinking passwords.

USENIX LISA,
http://www.usenix.org/event/lisa10/tech/

2010.
slides/cheswick.pdf.

[14] W. Cheswick. Rethinking passwords. ACM Queue, 10(12):50–

56, 2012.

[15] W.

Cheswick,

S.

Bellovin,

and

A.

Rubin.

Firewalls and Internet Security, 2/e. Addison-Wesley, 2003.

[16] S. Chiasson, A. Forget, E. Stobert, P. C. van Oorschot, and R. Bid-
dle. Multiple password interference in text passwords and click-
based graphical passwords. In Proc. ACM CCS, 2009.

[17] A. Das, J. Bonneau, M. Caesar, N. Borisov, and X. Wang. The

tangled web of password reuse. NDSS, 2014.

[18] R. Dhamija and A. Perrig. Deja vu: a user study using images for

authentication. In USENIX Security, 2000.

[19] S. Egelman, A. Sotirakopoulos, I. Muslukhov, K. Beznosov, and
C. Herley. Does my password go up to eleven? the impact of
password meters on password selection. In Proc. CHI, 2013.

[20] D. Florˆencio and C. Herley. Where Do All the Attacks Go? Proc.

WEIS, 2011, Fairfax, VA.

[21] D. Florˆencio and C. Herley. A Large-Scale Study of Web Pass-

word Habits. Proc. WWW, 2007.

[22] D. Florˆencio and C. Herley. Where Do Security Policies Come

From? Proc. SOUPS, 2010.

[23] D. Florˆencio, C. Herley, and P. van Oorschot. An Administrator’s
Guide to Internet Password Research. In Proc. USENIX LISA,
2014.

[24] S. Gaw and E. Felten. Password Management Strategies for On-

line Accounts. In ACM SOUPS, 2006.

[25] M. Gouda, A. Liu, L. Leung, and M. Alam. Single password,

multiple accounts. In ACNS (Industry Track), 2005.

[26] B. Grawemeyer and H. Johnson. Using and managing multi-
ple passwords: A week to a view. Interacting with Computers,
23(3):256–267, 2011.

[27] E. Grosse and M. Upadhyay. Authentication at scale.

IEEE

Security & Privacy, 11(1):15–22, 2013.

[28] E. Hayashi and J. Hong. A diary study of password usage in daily

life. In CHI (note), pages 2627–2630, 2011.

[30] Imperva.

[29] C. Herley. So Long, And No Thanks for the Externalities: Ratio-
nal Rejection of Security Advice by Users. Proc. NSPW, 2009.
2010.

Consumer Password Worst Practices.
http://www.imperva.com/docs/WP_Consumer_
Password_Worst_Practices.pdf.

[31] P. Inglesant and M. A. Sasse. The true cost of unusable password

policies: Password use in the wild. In CHI, 2010.

[32] B. Ives, K. Walsh, and H. Schneider. The Domino Effect of Pass-

word Re-use. C. ACM, 47(4):75–78, 2004.

[33] M. Jakobsson and S. Myers. Phishing and Countermeasures:
Understanding the Increasing Problem of Electronic Identity
Theft. Wiley, 2006.

[34] D. Kahneman. Thinking, fast and slow. Macmillan, 2011.
[35] D. Kahneman and A. Tversky. Prospect theory: An analysis of
decision under risk. Econometrica: Journal of the Econometric
Society, pages 263–291, 1979.

[36] A. Karp. Forum (comment). C. ACM, 47(6):11–12, 2004.
[37] P. G. Kelley, S. Komanduri, M. L. Mazurek, R. Shay, T. Vidas,
L. Bauer, N. Christin, L. F. Cranor, and J. Lopez. Guess again
(and again and again): Measuring password strength by simu-
lating password-cracking algorithms.
In Proc. IEEE Symp. on
Security and Privacy, 2012.

[38] R. Lemos. Yahoo breach highlights password reuse threat.

eWeek. July 7, 2012.

[39] J. Ma, W. Yang, M. Luo, and N. Li. A study of probabilistic
password models. Proc. IEEE Symp. on Security and Privacy,
2014.

[40] H. Markowitz.

Portfolio selection.

The Journal of Finance,

7(1):77–91, 1952.

[41] R. Nithyanand and R. Johnson. The password allocation problem.

In WPES, 2013. Nov. 4, 6 pages.

[42] G. Notoatmodjo. Exploring the ‘weakest link’: A study of
personal password security. C.S. Dept., University of Auckland,
2007. M.Sc. thesis.

[43] S. Preibusch and J. Bonneau. The password game: negative ex-
ternalities from weak password practices. In Decision and Game
Theory for Security, pages 192–207. Springer Berlin Heidelberg,
2010.

[44] S. Riley. Password security: what users know and what they

actually do. Usability News, 8(1), 2006.

[45] M. Sasse, S. Brostoff, and D. Weirich. Transforming the “weak-
est link”: a human-computer interaction approach to usable and
effective security. BT Tech. J., 19(3):122–131, 2001.

[46] A. Shamir. 2002 Turing Award Lecture. http://amturing.

acm.org/vp/shamir_2327856.cfm.

[47] E. Stobert and R. Biddle. The password life cycle: user behaviour

in managing passwords. In Proc. SOUPS, 2014.

[48] M. Weir, S. Aggarwal, M. Collins, and H. Stern. Testing metrics
for password creation policies by attacking large sets of revealed
passwords. In Proc. ACM CCS, 2010.

[49] Y. Zhang, F. Monrose, and M. K. Reiter. The security of modern
password expiration: An algorithmic framework and empirical
analysis. In Proc. ACM CCS, 2010.

590  23rd USENIX Security Symposium 

USENIX Association

16

