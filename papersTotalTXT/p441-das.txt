Do You Hear What I Hear? Fingerprinting Smart Devices

Through Embedded Acoustic Components

Anupam Das

University of Illinois at
Urbana-Champaign
das17@illinois.edu

Nikita Borisov

University of Illinois at
Urbana-Champaign
nikita@illinois.edu

Matthew Caesar
University of Illinois at
Urbana-Champaign

caesar@illinois.edu

ABSTRACT
The widespread use of smart devices gives rise to privacy concerns.
Fingerprinting smart devices can jeopardize privacy by allowing re-
mote identiﬁcation without user awareness. We study the feasibil-
ity of using microphones and speakers embedded in smartphones
to uniquely ﬁngerprint individual devices. During fabrication, sub-
tle imperfections arise in device microphones and speakers, which
induce anomalies in produced and received sounds. We exploit
this observation to ﬁngerprint smartphones through playback and
recording of audio samples. We explore different acoustic features
and analyze their ability to successfully ﬁngerprint smartphones.
Our experiments show that not only is it possible to ﬁngerprint de-
vices manufactured by different vendors but also devices that have
the same maker and model; on average we were able to accurately
attribute 98% of all recorded audio clips from 50 different Android
smartphones. Our study also identiﬁes the prominent acoustic fea-
tures capable of ﬁngerprinting smart devices with a high success
rate, and examines the effect of background noise and other vari-
ables on ﬁngerprinting accuracy.

Categories and Subject Descriptors
K.6.m [Management of Computing and Information Systems]:
Miscellaneous — Security; H.5.1 [Multimedia Information Sys-
tems]: Audio input/output

Keywords
Fingerprinting; Privacy; Acoustic feature; Microphone; Speaker

1.

INTRODUCTION

Mobile devices, including smartphones, PDAs, and tablets, are
quickly becoming widespread in modern society. In 2012 a total
of 1.94 billion mobile devices were shipped, of which 75% were
smart and highly-featured phones [5, 8, 14]. Canalys predicted that
the mobile device market will reach 2.6 billion units by 2016, with
smartphones and tablets continuing to dominate shipments [14].
The rapid uptake of intelligent mobile devices is not surprising, due

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright 2014 ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660325.

to the numerous advantages they provide consumers, from enter-
tainment and social applications to business and advanced comput-
ing capabilities. However, smartphones, with all their interactive,
location-centric, and connectivity-based features impose threaten-
ing concerns on user privacy [9, 34, 39, 51].

In this paper we thoroughly analyze a technique for ﬁngerprint-
ing the hardware of smartphones. The observation is that even if
the software on mobile devices is strengthened [35,62,70], hardware-
level idiosyncrasies in microphones and speakers can be used to
ﬁngerprint physical devices. During manufacturing, imperfections
are introduced in the analog circuitry of these components, and as
such, two microphones and speakers are never alike. Through an
observational study, we ﬁnd that these imperfections are substantial
enough, and prevalent enough, that we can reliably distinguish be-
tween devices by passively recording audio streams, and conduct-
ing simple spectral analyses on the recorded audio streams. Our
approach can substantially simplify the ability for an adversary to
track and identify people in public locations which can threaten
the privacy of mobile device users. Our approach requires small
amounts of data — for example, we show that with our technique,
an adversary could even use the short ringtones produces by mo-
bile device speakers to reliably track users in public environments.
Alternatively, a stealthy app (e.g., an online game) can access the
microphone to uniquely distinguish all users running the app.

Our approach centers around the development of a ﬁngerprint-
ing mechanism, which aims to “pull out” imperfections in device
circuitry. Our mechanism has two parts: a method to extract au-
ditory ﬁngerprints and a method to efﬁciently search for matching
ﬁngerprints from a database. To generate ﬁngerprints of speakers
we record audio clips played from smartphones onto an external
device (i.e., laptop/PC) and vice versa for generating ﬁngerprints
of microphones. We also generate the combined ﬁngerprint of
speaker and microphone by playing and recording audio clips si-
multaneously on smartphones. We use two different classiﬁers to
evaluate our ﬁngerprinting approach. Moreover, we test our ﬁn-
gerprinting approach for different genre of audio clips at various
frequencies. We also study various audio features that can be used
to accurately ﬁngerprint smartphones. Our studies reveals that mel-
frequency cepstral coefﬁcients (MFCCs) are the dominant features
for ﬁngerprinting smartphones. Lastly, we analyze the sensitivity of
our ﬁngerprinting approach against different factors like sampling
frequency, distance between speaker and recorder, training set size
and ambient background noise.
Contributions. Our contributions are summarized below:

• We analyze the feasibility of ﬁngerprinting smart devices by
leveraging the manufacturing idiosyncrasies of microphones
and speakers embedded in smart devices.

441• We study a large spectrum of existing audio features and their
ability to accurately ﬁngerprint smartphones. We ﬁnd that
mel-frequency cepstral coefﬁcients (MFCCs) perform partic-
ularly well in ﬁngerprinting smartphones.
• We investigate two different classiﬁers to evaluate our ﬁnger-
printing approach. We conclude that Gaussian mixture mod-
els (GMMs) are more effective compared to k-NN classiﬁers
in classifying recorded audio ﬁngerprints.
• We perform experiments across several different genres of
audio excerpts. We also analyze how different factors like
sampling frequency, distance between speaker and recorder,
training set size and ambient background noise impact our
ﬁngerprinting accuracy.

Roadmap. The remainder of this paper is organized as follows.
We give an overview of our ﬁngerprinting approach in Section 2.
We discussed related work in Section 3. In Section 4, we discuss
why microphones and speakers integrated in smartphones can be
used to generate unique ﬁngerprints. In Section 5, we describe the
different audio features considered in our experiments, along with
the classiﬁcation algorithms used in our evaluation. We present our
experimental results in Section 6. We also discuss some limitations
of our approach in Section 7. Finally, we conclude in Section 8.

2. OVERVIEW

In this section we give an overview of our approach and present
several viable attack scenarios. We also identify the key challenges
that we address in this paper.

The key observation behind our work is that imperfections in
smart device hardware induce unique signatures on the received
and transmitted audio streams, and these unique signatures, if iden-
tiﬁed, can be used by an adversary to ﬁngerprint the device. We
consider three ﬁngerprinting scenarios: speaker, microphone, and
joint speaker-microphone ﬁngerprinting.
In the ﬁrst case, an at-
tacker in a public environment, such as a cafe or shopping mall,
records audio generated by a smartphone speaker, such as a ring-
tone. The attacker can then use the recorded audio samples to
track and identify users. Alternately, the attacker may obtain audio
recorded by a smartphone microphone and use that to identify the
user who made the recording; this can have forensic applications.
A third way to track users is to convince them to install a malicious
application (e.g., a free online game), which can play and record au-
dio clips using the device’s speaker and microphone. The app can
then stealthily upload the recorded audio clips to the attacker (e.g.,
piggybacking it on log-in information or game state), who can then
use the audio samples to uniquely distinguish each user. To do this,
the application would require access to both the speaker and mi-
crophone, as well as network access, but such permissions are not
unusual for applications and are unlikely to raise alarm, especially
given that a signiﬁcant portion of the users cannot comprehend the
full consequences of smartphone permissions [36, 45].

Our approach consists of two main tasks. The ﬁrst task is acquir-
ing a set of audio samples for analysis in the ﬁrst place. To do this,
we have a listener module, responsible for receiving and recording
device audio. The listener module could be deployed as an applica-
tion on the smart device (many mobile OSes allow direct access to
microphone inputs), or as a stand-alone service (e.g., the adversary
has a microphone in a public setting to pick up device ringtones).
The next task is to effectively identify device signatures from the
received audio stream. To do this, we have an analyzer module,
which leverages signal processing techniques to localize spectral
anomalies, and constructs a ‘ﬁngerprint’ of the auditory character-
istics of the device. A critical part of this task involves determining

what sort of acoustic features and audio analysis techniques are
most effective in identifying unique signatures of device-hardware.
There are a large number of audio properties which could be used
(spectral entropy, zero crossings, etc.) as well as a broad spec-
trum of analysis algorithms that can be used to summarize these
properties (principle component analysis, linear discriminant anal-
ysis, feature selection, etc.). We will study alternative properties to
characterize hardware-induced auditory anomalies in Section 5.1
as well as algorithms for effectively clustering them in Section 5.2.

3. RELATED WORK

Fingerprints have long been used as one of the most common
bio-metrics in identifying human beings [27, 61]. The same con-
cept was extended to identifying and tracking unique mobile trans-
mitters by the US government during 1960s [47]. Later on with
the emergence of cellular networks people were able to uniquely
identify transmitters by analyzing the externally observable char-
acteristics of the emitted radio signals [60].

Physical devices are usually different at either the software or
hardware level even if they are produced by the same vendor. In
terms of software based ﬁngerprinting, researchers have looked at
ﬁngerprinting techniques that differentiate between unique devices
over a Wireless Local Area Network (WLAN) simply through a
timing analysis of 802.11 probe request frames [30]. Others have
looked at exploiting the difference in ﬁrmware and device driver
running on IEEE 802.11 compliant devices [37]. 802.11 MAC
headers have also been used to track unique devices [40]. Pang et
al. [57] were able to exploit trafﬁc patterns to carry out device ﬁn-
gerprinting. Open source toolkits like Nmap [50] and Xprobe [68]
can remotely ﬁngerprint an operating system by identifying unique
responses from the TCP/IP networking stack.

Another angle to software based ﬁngerprinting is to exploit ap-
plications like browsers to carry out device ﬁngerprinting [33]. Yen
et al. [69] were successful at tracking users with high precision by
analyzing month-long logs of Bing and Hotmail. Researchers have
also been able to exploit JavaScript and popular third-party plug-
ins like Flash player to obtain the list of fonts installed in a de-
vice which then enabled them to uniquely track users [18]. Other
researchers have proposed the use of performance benchmarks for
differentiating between JavaScript engines [54]. Furthermore, brows-
ing history can be exploited to ﬁngerprint and track web users [56].
The downside of software based ﬁngerprints is that such ﬁnger-
prints are generated from the current conﬁguration of the system
which is not static, rather it is likely to change over time.

Hardware based ﬁngerprinting approaches rely on some static
source of idiosyncrasies. It has been shown that network devices
tends to have constant clock skews [53] and researchers have been
able to exploit these clock skews to distinguish devices through
TCP and ICMP timestamps [46]. However, clock skew rate is
highly dependent on the experimental environment [67]. Researchers
have also extensively looked at ﬁngerprinting the unique transient
characteristics of radio transmitters (also known as RF ﬁngerprint-
ing). RF ﬁngerprinting has been shown as a means of enhancing
wireless authentication [49, 55]. It has also been used for location
detection [58]. Manufacturing imperfections in network interface
cards (NICs) have also been studied by analyzing analog signals
transmitted from them [21,38]. More recently Dey et al. have stud-
ied manufacturing idiosyncrasies inside smartphone accelerometer
to distinguish devices [31]. However, their approach requires some
form of external stimulation/vibration to successfully capture the
manufacturing imperfection of the on-board accelerometer. More-
over, there are different contexts in which audio prints can be more
useful, e.g., software that is not allowed to access the accelerome-

442ter, as well as an external adversary who ﬁngerprints nearby phones
with a microphone.

Our work is inspired by the above work in hardware-based ﬁn-
gerprinting, but we focus on ﬁngerprinting on-board acoustic com-
ponents like speakers and microphones. In this setting, Clarkson’s
work [26] is perhaps the most closely related to ours. He showed
that it is possible to distinguish loudspeakers by analyzing recorded
audio samples emitting from them. However, his experiments used
special audio clips that contained 65 different frequencies, whereas
we are using common audio excerpts like ringtones. Moreover,
his experiments ignored the subtlety introduced by microphones.
In fact in one experiment, though statistically not meaningful as it
tested only two similar microphones, he found no variation across
microphones. We, on the other hand found that microphones can
vary across different units. Finally, his study did not thoroughly an-
alyze the different acoustic features that can be used to successfully
carry out device ﬁngerprinting. As a result, he was able to achieve
only 81% accuracy in distinguishing heterogeneous loudspeakers.
Audio ﬁngerprinting has a rich history of notable research [23].
There are studies that have looked at classifying audio excerpts
based on their content [41, 65]. Others have looked at distinguish-
ing human speakers from audio segments [20, 22]. There has also
been work on exploring various acoustic features for audio classi-
ﬁcation [52]. One of the more popular applications of audio ﬁnger-
printing has been music genre and artist recognition [43, 48].

Our work takes advantage of the large set of acoustic features
that have been explored by existing work in audio ﬁngerprinting.
However, instead of classifying the content of audio segments, we
utilize acoustics features to capture the manufacturing imperfec-
tions of microphones and speakers embedded in smart devices.

4. SOURCE OF FINGERPRINTS

In this section we will take a closer look at the microphones and
speakers embedded on today’s smartphones. This will provide an
understanding of how microphones and speakers can act as a po-
tential source for unique ﬁngerprints.
4.1 Closer Look at Microphones

Microphones in modern smartphones are based on Micro Electro
Mechanical Systems (MEMS) [10,12,17]. To enhance active noise
and echo canceling capabilities, most smartphones today have more
than one MEMS microphone. For example, the iPhone 5 has a total
of three embedded MEMS microphones [10]. According to the
IHS-iSuppli report, Apple and Samsung were the top consumers of
MEMS microphones in 2012, accounting for a combined 54% of
all shipped MEMS microphones [17]. .

A MEMS microphone, sometimes called a microphone chip or
silicon microphone, consists of a coil-less pressure-sensitive di-
aphragm directly etched into a silicon chip. It is comprised of a
MEMS die and a complementary metal-oxide-semiconductor (CM-
OS) die combined in an acoustic housing [7,11]. The CMOS often
includes both a preampliﬁer as well as an analog-to-digital (AD)
converter. Modern fabrication techniques enable highly compact
deigns, making them well suited for integration in digital mobile
devices. The internal architecture of a MEMS microphone is shown
on Figure 1. From the ﬁgure we can see that the MEMS micro-
phone’s physical design is based on a variable capacitor consist-
ing of a highly ﬂexible diaphragm in close proximity to a perfo-
rated, rigid back-plate. The perforations permit the air between
the diaphragm and back-plate to escape. When an acoustic signal
reaches the diaphragm through the acoustic holes, the diaphragm
is set in motion. This mechanical deformation causes capacitive
change which in turn causes voltage change.
In this way sound

pressure is converted into an electrical signal for further processing.
The back-chamber acts as a acoustic resonator and the ventilation
hole allows the air compressed inside the back chamber to ﬂow out,
allowing the diaphragm to move back into its original place.

The sensitivity of the microphone depends on how well the di-
aphragm deﬂects to acoustic pressure; it also depends on the gap
between the static back-plate and the ﬂexible diaphragm. Unfor-
tunately, even though the manufacturing process of these micro-
phones has been streamlined, no two chips roll off the assembly
line functioning in exactly the same way. Imperfections can arise
for the following reasons: slight variations in the chemical compo-
sition of components from one batch to the next, wear in the manu-
facturing machines or changes in temperature and humidity. While
subtle imperfections in the microphone chips may go unnoticed by
human ears, computationally such discrepancies may be sufﬁcient
to discriminate them, as we later show.
4.2 Closer Look at Microspeakers

Micro-speakers are a scaled down version of a basic acoustic
speaker. So lets ﬁrst look at how speakers work before we dis-
cuss how microspeakers can be used to generate unique ﬁnger-
prints. Figure 2(a) shows the basic components of a speaker. The
diaphragm is usually made of paper, plastic or metal and its edges
are connected to the suspension. The suspension is a rim of ﬂexible
material that allows the diaphragm to move. The narrow end of the
diaphragm’s cone is connected to the voice coil. The voice coil is
attached to the basket by a spider (damper), which holds the coil in
position, but allows it to move freely back and forth. A permanent
magnet is positioned directly below the voice coil.

Sound waves are produced whenever electrical current ﬂows thr-
ough the voice coil, which acts as an electromagnet. Running vary-
ing electrical current through the voice coil induces a varying mag-
netic ﬁeld around the coil, altering the magnetization of the metal
it is wrapped around. When the electromagnet’s polar orientation
switches, so does the direction of repulsion and attraction. In this
way, the magnetic force between the voice coil and the permanent
magnet causes the voice coil to vibrate, which in turn vibrates the
speaker diaphragm to generate sound waves.

Figure 2(b) shows a typical MEMS microspeaker chip and Fig-
ure 2(c) shows the components inside the microspeaker [24]. The
components are similar to that of a basic speaker; the only differ-
ence is the size and fabrication process [25, 44, 63]. The amplitude
and frequency of the sound wave produced by the speaker’s di-
aphragm is dictated respectively by the distance and rate at which
the voice coil moves. Each speaker component can introduce varia-
tions into the generated sound. For example, variations in the elec-
tromagnetic properties of the driver can cause differences in the rate
and smoothness at which the diaphragm moves. Therefore, due to
the inevitable variations and imperfections of the manufacturing
process, no two speakers are going to be alike, resulting in subtle
differences in the produced sound. In our work, we develop tech-
niques to computationally localize and evaluate these differences.

5. FEATURES AND ALGORITHMS USED
In this section we brieﬂy describe the acoustic features that we
used in generating device ﬁngerprints. We also discuss the classi-
ﬁcation algorithms used in identifying the devices from which the
ﬁngerprints originated.
5.1 Audio Features

Given our knowledge that imperfections exist in device audio
hardware, we now need some way to detect them. To do this,
our approach identiﬁes acoustic features from an audio stream, and

443⇒

Figure 1: The internal architecture of MEMS microphone chip used in smartphones.

Figure 2: (a) The basic components of a speaker, (b) A typical MEMS microspeaker, (c) The internal architecture of a microspeaker chip.

(c)

(a)

(b)

uses the features to construct a ﬁngerprint of the device. Computing
acoustic features from an audio stream has been a subject of much
research [19, 23, 52, 65]. To gain an understanding of how a broad
range of acoustic features are affected by device imperfections we
investigate a total of 15 acoustics features (listed in Table 1), all
of which have been well-documented by researchers. Due to space
limitation we exclude detailed description of each acoustic feature,
however, an elaborate description of the audio features is available
in our technical report [28].

5.2 Classiﬁcation Algorithms

Next, we need some way to leverage the set of features to per-
form device identiﬁcation. To achieve this, we leverage a clas-
siﬁcation algorithm, which takes observations (features) from the
observed device as input, and attempts to classify the device into
one of several previously-observed sets.

To do this, our approach works as follows. First, we perform
a training step, by collecting a number of observations from a set
of devices. Each observation (data point) corresponds to a set of
features observed from that device, represented as a tuple with one
dimension per feature. As such, data points can be thought of as
existing in a hyper-dimensional space, with each axis correspond-
ing to the observed value of a corresponding feature. Our approach
then applies a classiﬁcation algorithm to build a representation of
these data points, which can later be used to associate new observa-
tions with device types. When a new observation is collected, the
classiﬁcation algorithm returns the most likely device that caused
the observation.

To do this effectively, we need an efﬁcient classiﬁcation algo-
rithm. In our work, we compare the performance of two alternate
approaches described below: k-nearest neighbors (associates an
incoming data point with the device corresponding to the nearest

“learned” data points), and Gaussian mixture models (computes a
probability distribution for each device, and determines the maxi-
mal likely association).
k-NN: The k-nearest neighbors algorithm (k-NN) is a nonpara-
metric lazy learning algorithm. The term “non-parametric” means
that the k-NN algorithm does not make any assumptions about
the underlying data distribution, which is useful in analyzing real-
world data with complex underlying distribution. The term “lazy
learning” means that the k-NN algorithm does not use the training
data to make any generalization, rather all the training data are used
in the testing phase making it computationally expensive (however,
optimizations are possible). The k-NN algorithm works by ﬁrst
computing the distance from the input data point to all training data
points and then classiﬁes the input data point by taking a majority
vote of the k closest training records in the feature space [32]. The
best choice of k depends upon the data; generally, larger values of
k reduce the effect of noise on the classiﬁcation, but make bound-
aries between classes less distinct. We will discuss more about the
choice of k in Section 6.
GMM: A Gaussian mixture model is a probabilistic model that
assumes all the data points are generated from a mixture of a ﬁnite
number of Gaussian distributions with unknown parameters. The
unknown patterns and mixture weights are estimated from training
samples using an expectation–maximization (EM) algorithm [29].
During the matching phase the ﬁngerprint for an unknown record-
ing is ﬁrst compared with a database of pre-computed GMMs and
then the class label of the GMM that gives the highest likelihood is
returned as the expected class for the unknown ﬁngerprint. GMMs
are often used in biometric systems, most notably in human speaker
recognition systems, due to their capability of representing a large
class of sample distributions [59, 65].

Sound WaveDistanceVariable CapacitanceFlexible DiaphragmPerforated rigid back−plateAcoustic holesElectrodeVentilation holeBack−chamberMovable diaphragmCompressed AirPermanent MagnetDiaphragmSpiderBasketBack−plateSuspensionVoice coil444#
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

Feature
RMS
ZCR

Low-Energy-Rate
Spectral Centroid
Spectral Entropy

Spectral Irregularity

Spectral Spread
Spectral Skewness
Spectral Kurtosis
Spectral Rolloff

Spectral Brightness
Spectral Flatness

MFCCs

Chromagram
Tonal Centroid

Dimension

Description

Table 1: Explored acoustic features

1
1
1
1
1
1
1
1
1
1
1
1
13
12
6

Square root of the arithmetic mean of the squares of the signal strength at various frequencies

The rate at which the signal changes sign from positive to negative or back

The percentage of frames with RMS power less than the average RMS power for the whole audio signal

Represents the center of mass of a spectral power distribution

Captures the peaks of a spectrum and their locations

Measures the degree of variation of the successive peaks of a spectrum

Deﬁnes the dispersion of the spectrum around its centroid

Represents the coefﬁcient of skewness of a spectrum

Measure of the ﬂatness or spikiness of a distribution relative to a normal distribution
Deﬁnes the frequency below which 85% of the distribution magnitude is concentrated

Computes the amount of spectral energy corresponding to frequencies higher than a given cut-off threshold

Measures how energy is spread across the spectrum

Compactly represents spectrum amplitudes residing inside the mel-frequency range

Representation of the distribution of energy along the 12 distinct semitones or pitch classes

Maps a chromagram onto a six-dimensional Hypertorus structure

6. EVALUATION

In this section we perform a series of experiments to evaluate
how well we can ﬁngerprint smartphones by exploiting the manu-
facturing idiosyncrasies of microphones and speakers embedded in
them. We start by describing how we performed our experiments
(Section 6.1). Next, we brieﬂy discuss the setup for ﬁngerprinting
devices through speaker, microphone and a combination of both
(Section 6.2). We then discuss a framework for determining the
dominant (most-relevant) set of audio features that can be used in
ﬁngerprinting smartphones (Section 6.3). Then, we look at ﬁnger-
printing devices, ﬁrst, of different maker-and-model (Section 6.4),
followed by devices of same maker-and-model (Section 6.5) and
ﬁnally, a combination of both with multiple units of different mod-
els (Section 6.6). The performance of our approach is affected by
certain aspects of the operating environment, and we the study sen-
sitivity to such factors in Section 6.7.
6.1 Methodology

To perform our experiments, we constructed a small testbed en-
vironment with real smartphone device hardware. In particular, our
default environment consisted of a 266 square foot (14’x19’) ofﬁce
room, with nine-foot dropped ceilings with polystyrene tile, com-
prising a graduate student ofﬁce in a University-owned building.
The room was ﬁlled with desks and chairs, and opens out on a pub-
lic hall with foot trafﬁc. The room also receives a minimal amount
of ambient noise from air conditioning, desktop computers, and ﬂo-
rescent lighting. We placed smartphones in various locations in the
room. To emulate an attacker, we placed an ACER Aspire 5745
laptop in the room. To investigate performance with inexpensive
hardware, we used the laptop’s built-in microphone to collect audio
samples. We investigate how varying this setup affects performance
of the attack in Section 6.7.
Devices and tools: We tested our device ﬁngerprinting on devices
from ﬁve different manufacturers. Table 2 highlights the model and
quantities of the different phones used in our experiments.

Table 2: Types of phones used
Maker
Apple
Google

Quantity

Model
iPhone 5
Nexus One
Nexus S
Galaxy S3
Galaxy S4
Droid A855

Samsung
Motorola

Sony Ericsson

W518

Total

1
14
8
3
10
15
1
52

We also investigate different genres of audio excerpts. Table 3
describes the different types of audio excerpts used in our experi-
ments. Duration of the audio clips varies from 3 to 10 seconds. The
default sampling frequency of all audio excerpts is 44.1kHz unless
explicitly stated otherwise. All audio clips are stored in WAV for-
mat using 16-bit pulse-code-modulation (PCM) technique.

Table 3: Types of audio excerpts

Type

Description

Variations

Instrumental Musical instruments playing together, e.g., ringtone
Human speech

Small segments of human speech

Song

Combination of human voice & instrumental sound

4
4
3

For analysis we leverage the following audio tools and analytic
modules: MIRtollbox [13], Netlab [15], Audacity [3] and the An-
droid app Hertz [6]. Both MIRtoolbox and Netlab are MATLAB
modules providing a rich set of functions for analyzing and ex-
tracting audio features. Audacity and Hertz are mainly used for
recording audio clips on computers and smartphones respectively.
For analyzing and matching ﬁngerprints we use a desktop ma-
chine with the following conﬁguration: Intel i7-2600 3.4GHz pro-
cessor with 12GiB RAM. We found that the average time required
to match a new ﬁngerprint was around 5–10 ms for k-NN classiﬁer
and around 0.5–1 ms for GMM classiﬁer.
Evaluation metrics: We use standard multi-class classiﬁcation
metrics—precision, recall, and F1-score [64]—in our evaluation.
Assuming there are ﬁngerprints from n classes, we ﬁrst compute
the true positive (T P ) rate for each class, i.e., the number of traces
from the class that are classiﬁed correctly. Similarly, we compute
the false positive (F P ) and false negative (F N), as the number
of wrongly accepted and wrongly rejected traces, respectively, for
each class i (1 ≤ i ≤ n). We then compute precision, recall, and
the F1-score for each class using the following equations:

Precision, P ri = T Pi/(T Pi + F Pi)
Recall, Rei = T Pi/(T Pi + F Ni)

F1-Score, F1 i = (2 × P ri × Rei)/(P ri + Rei)

(1)
(2)
(3)

The F1-score is the harmonic mean of precision and recall; it pro-
vides a good measure of overall classiﬁcation performance, since
precision and recall represent a trade-off: a more conservative clas-
siﬁer that rejects more instances will have higher precision but
lower recall, and vice-versa. To obtain the overall performance of

445the system we compute average values in the following way:

Avg. Precision, AvgPr =

Avg. Recall, AvgRe =

Avg. F1-Score, AvgF1 =

i=1 Rei

n

2 × AvgP r × AvgRe

AvgP r + AvgRe

(cid:80)n
(cid:80)n

i=1 P ri

n

(4)

(5)

(6)

Each audio excerpt is recorded/played 10 times, 50% of which
is used for training and the remaining 50% is used for testing. We
report the maximum evaluation obtained by varying the number
of neighbors (k) from 1 to 5 for the k-NN classiﬁer and consider-
ing 1 to 5 Gaussian distributions per class. Since GMM parame-
ters are produced by the randomized EM algorithm, we perform 10
parameter-generation runs for each instance and report the average
classiﬁcation performance. We also compute the 95% conﬁdence
interval, but we found it to be less than 0.01 and therefore, do not
report it in the rest of the paper.
6.2 Fingerprinting Acoustic Components

6.2.1 Process of Fingerprinting Speaker
An attacker can leverage our technique to passively observe au-
dio signals (e.g., ringtones) emitting from device speakers in pub-
lic environments. To investigate this, we ﬁrst look at ﬁngerprinting
speakers integrated inside smartphones. For ﬁngerprinting speakers
we record audio clips played from smartphones onto a laptop and
we then extract acoustic features from the recorded audio excerpts
to generate ﬁngerprints as shown in Figure 3. We look at devices
manufactured by both the same vendor and different vendors.

Figure 3: Steps of ﬁngerprinting speakers.

6.2.2 Process of Fingerprinting Microphone
Attackers may also attempt to ﬁngerprint devices by observing
imperfections in device microphones, for example by convincing
the user to install an application on their phone, which can observe
inputs from the device’s microphone. To investigate the feasibility
of this attack, we next look at ﬁngerprinting microphones embed-
ded in smartphones. To do this, we record audio clips played from
a laptop onto smartphones as shown in Figure 4. Again we consider
devices made by both the same vendor and different vendors.

Figure 4: Steps of ﬁngerprinting microphones.

6.2.3 Process of Fingerprinting both Speaker and Mic
An attacker may attempt to ﬁngerprint devices by observing im-
perfections in both device microphone and speaker, for example by

convincing the user to install a game on their phone which requires
access to device speaker and microphone to interact with the game
(something like Talking Tom Cat). The attacker could potentially
play a theme song at the start of the game and at the same time
make a recording of the audio clip. To investigate the feasibility of
this attack, we build an android app that plays and records audio
clips simultaneously and uploads the data to a remote server. The
recorded audio clips would then enable the attacker to characterize
the imperfections of microphones and speakers embedded inside
smartphones. Figure 5 summarizes the whole process.

Figure 5: Steps of ﬁngerprinting both microphones and speakers.

6.3 Feature Exploration

At ﬁrst glance, it seems that we should use all features at our
disposal to identify device types. However, including too many
features can worsen performance in practice, due to their varying
accuracies and potentially-conﬂicting signatures. Hence, in this
section, we provide a framework to explore all the 15 audio fea-
tures described in Section 5.1 and identify the dominating subset
of all the features, i.e., which combination of features should be
used. For this purpose we adopt a well known machine learning
strategy known as feature selection [42, 66]. Feature selection is
the process of reducing dimensionality of data by selecting only a
subset of the relevant features for use in model construction. The
main assumption in using feature selection technique is that the
data may contain redundant features. Redundant features are those
which provide no additional beneﬁt than the currently selected fea-
tures. Feature selection techniques are a subset of the more general
ﬁeld of feature extraction, however, in practice they are quite dif-
ferent from each other. Feature extraction creates new features as
functions of the original features, whereas feature selection returns
a subset of the features. Feature selection is preferable to feature
extraction when the original units and meaning of features are im-
portant and the modeling goal is to identify an inﬂuential subset.
When the features themselves have different dimensionality, and
numerical transformations are inappropriate, feature selection be-
comes the primary means of dimension reduction.

Feature selection involves the maximization of an objective func-
tion as it searches through the possible candidate subsets. Since ex-
haustive evaluation of all possible subsets are often infeasible (2N
for a total of N features) different heuristics are employed. We
use a greedy search strategy known as sequential forward selection
(SFS) where we start off with an empty set and sequentially add the
features that maximize our objective function. The pseudo code of
our feature selection algorithm is described in Algorithm 1.

The algorithm works as follows. First, we compute the F1-score
that can be achieved by each feature individually. Next, we sort
the feature set based on the achieved F1-score in descending order.
Then, we iteratively add features starting from the most dominant
one and compute the F1-score of the combined feature subset. If
adding a feature increases the F1-score seen so far we move on to
the next feature, else we remove the feature under inspection. Hav-
ing traversed through the entire set of features, we return the subset
of features that maximizes our device classiﬁcation task. Note that
this is a greedy approach, therefore, the generated subset might not

446Algorithm 1 Sequential Feature Selection

Input: Input feature set F
Output: Dominant feature subset D
F 1_score ← []
for f ∈ F do

F 1_score[f ] ← Classif y(f )

end for
F (cid:48) ← sort(F, F 1_score) #In descending order
max_score ← 0
D ← ∅
for f ∈ F (cid:48) do
D ← D ∪ f
temp ← Classif y(D)
if temp > max_score then
max_score ← temp
D ← D − {f}

else

end if
end for
return D

always provide optimal F1-score. However, for our purpose, we
found this approach to perform well, as we demonstrate in latter
sections. We test our feature selection algorithm for all three types
of audio excerpts listed in Table 3. We evaluate the F1-score using
both k-NN and GMM classiﬁers.

Note that the audio excerpts used for feature exploration and the
ones used for evaluating our ﬁngerprinting approach in the follow-
ing sections are not identical. We use different audio excerpts be-
longing to the three categories listed in Table 3, so as to not bias
our evaluations.
6.4 Different Maker-and-Model Devices

In this section we look at ﬁngerprinting smartphones manufac-
tured by ﬁve different vendors. We take one representative smart-
phone from each row of Table 2 giving us a total of 7 different
smartphones. We look at ﬁngerprinting these devices ﬁrst by using
the microphone and speaker individually and next, by combining
both microphone and speaker.
6.4.1 Feature Exploration
First, we look at exploring different acoustic features with the
goal of obtaining the dominant subset of features. Table 4 high-
lights the maximum F1-score achieved by each acoustic feature for
the three different types of audio excerpt. The maximum F1-score
is obtained by varying k from 1 to 5 (for k-NN classiﬁer) and also
considering 1 to 5 gaussian distributions per class (for GMM classi-
ﬁer). Each type of audio is recorded 10 times giving us a total of 70
samples from the 7 representative handsets; 50% of which (i.e., 5
samples per handset) is used for training and the remaining 50% is
used for testing. All the training samples are labeled with their cor-
responding handset identiﬁer. Both classiﬁers return the class label
for each audio clip in the test set and from that we compute F1-
score. The table also highlights the subset of features selected by
our sequential feature selection algorithm and their corresponding
F1-score. We ﬁnd that most of the time MFCCs are the dominant
features for all categories of audio excerpt.

To get a better understanding of why MFCCs are the dominant
acoustic features we plot the MFCCs of a given audio excerpt from
three different handsets on Figure 6. All the coefﬁcients are ranked
in the same order for the three handsets. We can see that the magni-
tude of the coefﬁcients vary across the handsets. For example, coef-

ﬁcient 3 and 5 vary signiﬁcantly across the three handsets. Hence,
MFCCs are highly suitable features for ﬁngerprinting smartphones.
6.4.2 Fingerprinting using Speaker
We test our ﬁngerprinting approach using three different types
of audio excerpt. Each audio sample is recorded 10 times, 50%
of which is used for training and the remaining 50% is used for
testing. We repeat this procedure for the three different types of au-
dio excerpt. Table 5 summarizes our ﬁndings (values are reported
as percentages). We simply use the acoustic features obtained from
our sequential feature selection algorithm as listed in Table 4. From
Table 5 we see that we can successfully (with a maximum F1-score
of 100%) identify which audio clip came from which smartphone.
Thus, ﬁngerprinting smartphones manufactured by different ven-
dors seems very much feasible using only few acoustic features.

Table 5: Fingerprinting different smartphones using speaker output

Audio
Type

Instrumental
Human speech

Song

k-NN

Features∗ AvgP r AvgRe AvgF 1
97.4
[1,7]
94.8
[13]
[15]
97.4

97.6
95.2
97.6

97.1
94.3
97.1

GMM

Features∗ AvgP r AvgRe AvgF 1
100
100
100

[13]
[13]
[13]

100
100
100

100
100
100

∗ Features taken from Table 4

6.4.3 Fingerprinting using Microphone
Similar to speakers, we ﬁnd microphone properties differ quite
substantially across vendors. We exploit this phenomenon to ﬁn-
gerprint smartphones through microphones. To test our hypothesis
we test our ﬁngerprinting approach using three different types of
audio excerpt. Each audio sample is again recorded 10 times; we
use 50% for training and the other 50% for testing. Table 6 summa-
rizes our ﬁndings (values are reported as percentages). We use the
same set of features obtained from our sequential feature selection
algorithm as listed in Table 4. From Table 6 we see that we can
achieve an F1-score of over 97%. These results suggest that smart-
phones can be successfully ﬁngerprinted through microphones too.

Table 6: Fingerprinting different smartphones using mic
Audio
Type

GMM

k-NN

Instrumental
Human speech

Features∗ AvgP r AvgRe AvgF 1
94.8
[13,1]
94.8
[15,9,1]
[13,1,12]
97.4
∗ Features taken from Table 4

95.2
95.2
97.6

94.3
94.3
97.1

Song

Features∗ AvgP r AvgRe AvgF 1
100
[13,1,7]
97.4
[13,15,11]
[13,1,9]
100

100
97.1
100

100
97.6
100

6.4.4 Fingerprinting using Microphone and Speaker
We now look at ﬁngerprinting smartphones through both mi-
crophones and speakers. For this experiment we build an android
app to collect data from different smartphones. Our app plays and
records different audio clips simultaneously and uploads the data
to a remote server. As we are using an android app for our data
collection, we had to exclude iPhone5 and Sony Ericsson W518
handset from this experiment (reducing our pool of handsets to 5
devices). Again each audio sample is recorded 10 times, half of
which is used for training and the other half for testing. We use the
features obtained from Table 4. Table 7 summarizes our ﬁndings
(values are reported as percentages). We see that we can achieve
an F1-score of 100%. Thus, a malicious app having access to only
speaker and microphone can successfully ﬁngerprint smartphones.

Table 7: Fingerprinting different smartphones using mic & speaker

Audio
Type

Instrumental
Human speech

Song

k-NN

Features∗ AvgP r AvgRe AvgF 1
96.3
96.3
96.3

96.7
96.7
96.7

[10]
[12]
[10]

96
96
96

GMM

Features∗ AvgP r AvgRe AvgF 1
100
100
100

[13]
[13]
[13]

100
100
100

100
100
100

∗ Features taken from Table 4

447Table 4: Feature exploration using sequential forward selection technique for different maker-and-model smartphones

Fingerprinting Speakers
Maximum F1-Score (%)

Fingerprinting Microphones

Maximum F1-Score (%)

#

Feature

RMS
ZCR

Spectral Irregularity

Low-Energy-Rate
Spectral Centroid
Spectral Entropy

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Sequential Feature Selection

Spectral Spread
Spectral Skewness
Spectral Kurtosis
Spectral Rolloff

Spectral Brightness
Spectral Flatness

Chromagram
Tonal Centroid

MFCCs

Max F1-Score

Song

Human Speech

Instrumental
k-NN GMM k-NN GMM k-NN GMM k-NN
97.4
87.2
76.4
57.6
47
69.2
73.7
91.5
52.1
84
31.3
49.3
90.4
90.2
71.4
68.3
63.9
76.7
37.8
86.6
66.4
87.5
92.5
84.5
97.4
94.8
88.4
84.3
91.9
86.4
[13,1]
[1,7]
100
97.4

98
52.1
51.8
88.4
80.5
51.4
89.2
82.1
79.5
86.4
85.2
84
100
79.4
88.4
[13]
100

80.8
63.7
52.8
59.4
59.4
37.5
56.7
69
68.1
85.8
70.9
61.6
100
81.1
80.3
[13]
100

78.3
48.6
38.2
55.8
46.5
46.8
56.7
62
60.2
66.7
87.7
61.3
100
100
98.2
[13]
100

88.8
77
59.6
88.1
91.5
43.2
91
82.9
88.6
74.8
85.5
95.1
94.8
97.4
100
[15]
100

86.2
77
58.8
87.7
91
50.3
85.7
79.9
86.9
76.1
77.1
97.4
100
100
100
[13]
100

Instrumental
GMM
84.4
74.3
42.2
69.7
56.5
55
86.2
63.4
61.2
42.7
64.8
92.5
94.8
77.7
92.7

Human Speech
GMM
70.3
76.8
19.5
77.9
59.8
49.7
54.8
63.6
65.1
83.8
60.2
68.9
92.1
86
86.4

k-NN
62.7
75.7
26
70
68
52.6
50.5
65.6
65.2
82.9
60.3
66.7
79.9
88.7
92.1

Song

k-NN
80.2
72.4
38.8
73.7
68.8
53
81.6
61.8
85.4
67.7
63.7
74.8
90.4
78.4
89.6

GMM
82.8
71.3
36.6
76
58.8
48.7
76.5
49.7
58
73
67.4
74.4
95.4
87.7
92.5

Fingerprinting Speakers and Microphones

Maximum F1-Score (%)

Song

Human Speech

Instrumental
k-NN GMM k-NN GMM k-NN GMM
96.3
93.1
96.3
83.3
96.3
93.1
76.1
82.3
96.3
78.3
74.6
81.1
93.1
86.5
96.3
92.3
85.3
89.5
100
96
90.2
80.4
96.3
85.1
100
93.1
100
96.3
100
96.3
[13]
[10]
100
100

93.1
93.1
96.3
74.2
96.3
90.1
93.1
96.3
81.6
100
87.4
100
92.7
86.5
100
[10]
100

92.7
83.3
90
85.1
72.4
72.7
86.5
89.7
86.3
96.3
79.4
85.1
100
93.1
100
[13]
100

89
84.6
81.7
40.5
81
55.4
92.7
84.6
60.7
92.7
81.6
96.3
96.3
88.6
96.3
[12]
96.3

80.8
78.8
77.7
42.7
67.9
53.6
92.4
86.2
59.5
92.7
67.6
92.7
96.3
96.3
96.3
[13]
96.3

[13,1,7]

100

[15,9,1]

92.1

[13,15,11]

93

[13,1,12]

92.5

[13,1,9]

97.4

Figure 6: MFCCs of the same audio sample taken from three different handsets manufactured by the same vendor. We can see that some of
the coefﬁcients vary signiﬁcantly, thus enabling us to exploit this feature to ﬁngerprint smartphones.

6.5 Same Maker-and-Model Devices

In this section, we look at ﬁngerprinting smartphones manufac-
tured by the same vendor and are of the same model. From Table 2
we see that we have 15 Motorola Droid A855 handsets which is the
largest number among all the other different types of smartphones
in our collection. We therefore use these 15 devices for all the ex-
periments in this section. We found that ﬁngerprinting smartphones
of the same maker-and-model was relatively a tougher problem.
We again look at ﬁngerprinting these devices, ﬁrst, through the mi-
crophone and speaker individually and then by combining both the
microphone and speaker.

6.5.1 Feature Exploration
First, we determine the dominating subset of acoustic features
that can be used for ﬁngerprinting smartphones of the same model.
To obtain the ﬁngerprinting data we record audio clips played from
15 Motorola Droid A855 handsets. Each type of audio is recorded
10 times giving us a total of 150 samples from the 15 handsets; 50%
of which is used for training and the remaining 50% is used for test-
ing. Table 8 shows the maximum F1-score achieved by each acous-
tic feature for the three different types of audio excerpt. The table
also highlights the dominating subset of features selected by our
sequential feature selection algorithm. We again ﬁnd that MFCCs
are the dominant features for all categories of audio excerpt.

6.5.2 Fingerprinting using Speaker
We now look at ﬁngerprinting the 15 Motorola Droid A855 hand-
sets. Table 9 highlights our ﬁndings. We test our ﬁngerprinting
approach against three different forms of audio excerpt. We use
the acoustic features obtained from our sequential feature selection
algorithm as listed in Table 8. From Table 9, we see that we can

achieve an F1-score of over 94% in identifying which audio clip
originated from which handset. Thus ﬁngerprinting smartphones
through speaker seems to be a viable option.

Table 9: Fingerprinting similar smartphones using speaker output

k-NN

Audio
Type

Features∗ AvgP r AvgRe AvgF 1
96.3
[13,14]
98.8
92.6

96
98.7
92

Instrumental
Human speech

96.7
98.9
93.2
∗ Feature numbers taken from Table 8

[13]
[13,7]

Song

GMM

Features∗ AvgP r AvgRe AvgF 1
98.3
[13,14]
98.8
[13,14]
[13,14]
94.5

98.4
98.9
95.6

98.1
98.7
93.3

6.5.3 Fingerprinting using Microphone
We now investigate ﬁngerprinting similar smartphones manufac-
tured by the same vendor through microphone-sourced input. We
again use 15 Motorola Droid A855 handsets for these experiments.
We use the features obtained through Algorithm 1 which are listed
in Table 8. Table 10 shows our ﬁndings. We see similar results
compared to ﬁngerprinting speakers. We were able to achieve an
F1-score of 95% in identifying the handset from which the audio
excerpt originated. Thus ﬁngerprinting smartphones through mi-
crophones also appears to be a feasible option.

Table 10: Fingerprinting similar smartphones using microphone

k-NN

Audio
Type

Features∗ AvgP r AvgRe AvgF 1
95.3
[13,8,12]
98.8
96.2

94.7
98.7
96

[13]

Instrumental
Human speech

95.9
98.9
96.4
∗ Feature numbers taken from Table 8

[13,14,10]

Song

GMM

Features∗ AvgP r AvgRe AvgF 1
95.3
[13,8,12]
100
[13,14]
[13,14]
96.1

94.7
100
95.7

96
100
96.5

6.5.4 Fingerprinting using Microphone and Speaker
We now look at the effect of combining microphone and speaker
in ﬁngerprinting similar smartphones. We use our android app to

12345678910111213−2.5−2−1.5−1−0.500.51Set 1Mel−Frequency Cepstral CoefficientsMagnitude12345678910111213−2.5−2−1.5−1−0.500.51Set 2Mel−Frequency Cepstral CoefficientsMagnitude12345678910111213−2.5−2−1.5−1−0.500.511.5Set 3Mel−Frequency Cepstral CoefficientsMagnitude448Table 8: Feature exploration using sequential forward selection technique for same model smartphones

Fingerprinting Speakers
Maximum F1-Score (%)

Fingerprinting Microphones

Maximum F1-Score (%)

Fingerprinting Speakers and Microphones

Maximum F1-Score (%)

Instrumental

Human Speech

Song

Instrumental

Human Speech
k-NN
GMM
20.1
19.6
22.6
26.2
7.4
5.2
12.9
16.6
15.1
15.2
17.9
13.2
16.4
14.9
13.7
20.8
14
20.3
11.6
15.1
16
12.6
12.2
17.2
96.2
98.8
75
88.7
70
70.8
[13]
98.8

97.5

[13,14,2]

Song

k-NN
23.5
44.5
10.7
33.7
40.3
15.8
36.2
38
45.8
46.1
33.1
39.2
94.1
87.3
83.1

[13,14,10]

96.3

Instrumental

Human Speech

Song

GMM k-NN GMM k-NN GMM k-NN GMM
89.2
28.6
75.2
41.9
64.5
13.4
32.7
35.7
81.8
36
67.1
18.6
34.8
87.4
68.9
43.1
76.2
39.2
71.8
44
79.5
27.4
87.2
35.5
100
97.5
85.3
96.5
98.8
79.4
[13]
[13,14]
97.9
100

89.5
67.6
69.7
35.1
81.8
62.5
87
70.9
82.2
77.2
87.5
86.4
100
100
100
[13]
100

87.2
59.8
67.4
30.1
78.5
63.9
84.6
85.7
80.3
79.4
86
79.8
100
98.8
98.8
[13]
100

83
60.5
70.9
27.5
70.3
54.6
81.3
88.3
80.6
73.4
88
79
100
95.8
94.8
[13]
100

92.1
56.8
30.1
25.7
52.9
32.5
67.6
58.7
51.9
46.9
75.2
45.5
98.7
97.6
95.2
[13]
98.7

93
58.4
36.7
30.1
54.8
33.6
62.8
54.4
49.9
51.5
69.2
45.4
100
100
92.7
[13]
100

#

Feature

RMS
ZCR

Spectral Irregularity

Low-Energy-Rate
Spectral Centroid
Spectral Entropy

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Sequential Feature Selection

Spectral Spread
Spectral Skewness
Spectral Kurtosis
Spectral Rolloff

Spectral Brightness
Spectral Flatness

Chromagram
Tonal Centroid

MFCCs

Max F1-Score

k-NN
34.9
29.7
12.5
28
20.9
14.5
36.4
33.9
30.5
40.4
32.1
34.9
90.4
79.1
77

[13,14]
97.5

GMM k-NN
16.6
33.8
12.2
26.5
15
14.8
12.2
30.5
14.2
19.8
7.4
11.7
43.7
11.3
13.3
29.1
11.6
29.1
14.9
39
18.9
31.6
19.8
31
91.3
96.5
70.6
72.9
65.4
60
[13]
93.7

[13,14]
97.7

GMM k-NN
12.3
14.4
5.7
19
16.6
14.7
14.3
15.5
16
14.3
21.8
13.3
97.5
66
53.4
[13,14]
98.2

20
13
21.8
39.9
33.9
11.8
35.2
31.5
31.1
38.7
18.5
32.4
90
80.6
63.6
[13,7]
91.5

GMM
25.7
7.1
18.7
40.3
26.3
17.5
38.4
40.3
36.8
41.1
17.9
30
91.4
80
53.8
[13,14]
92.9

k-NN
40.2
22.7
22.6
17.3
29.1
12.6
17.2
31.8
28.5
30
22.5
24.6
89
71.5
67.8

GMM
36.9
29.6
24.8
24.8
22.2
16.3
22.6
28.1
26.1
32.8
20.3
23.8
93.5
55.3
51.3

[13,8,12]

93

[13,8,12]

96.7

collect 10 samples per audio clip, half of which is used for training
and the remaining half for testing. Table 11 highlights our ﬁndings.
We see that we were able to ﬁngerprint all test samples accurately.
Thus combining the idiosyncrasies of both the speaker and micro-
phone seems to be the best option to distinguish smartphones of
same maker and model. So, if a malicious app can get access to
the speaker (which does not require explicit permission) and mi-
crophone (which may require explicit permission, but many games
nowadays require access to microphone anyway) it can success-
fully track individual devices.

Table 11: Fingerprinting similar smartphones using mic & speaker

Audio
Type

Instrumental
Human speech

Song

k-NN

Features∗ AvgP r AvgRe AvgF 1
100
100
100

[13]
[13]
[13]

100
100
100

100
100
100

GMM

Features∗ AvgP r AvgRe AvgF 1
100
100
100

[13]
[13]
[13]

100
100
100

100
100
100

∗ Features taken from Table 8

6.6 All Combination of Devices

In this section we look at ﬁngerprinting all the devices in our
collection (i.e., 50 android smartphones after excluding iphone5
and Sony Ericsson W518). We combine microphone and speaker
to generate the auditory ﬁngerprint of smartphones. We do so be-
cause in the previous sections we found that combining speaker and
microphone yielded the highest accuracy. First, we perform acous-
tic feature exploration to determine the dominant features. Table 12
highlights our ﬁndings. We see that again MFCCs are the dominant
features for all categories of audio excerpt. This is expected as we
saw similar outcomes in Table 4 and Table 8.

Next, we evaluate how effectively we can ﬁngerprint the 50 an-
droid smartphones. The setting is similar to all the previous exper-
iments where each audio clip is recorded 10 times, 50% of which
is used for training and the remaining 50% for testing. We use our
android app to collect all the audio samples. Table 13 shows our
obtained ﬁngerprinting results. We see that we can obtain an F1-
score of over 98% in ﬁngerprinting all the 50 smartphones. This
result suggests that a malicious app having access to microphone
and speaker can easily ﬁngerprint smartphones.
6.7 Sensitivity Analysis

In this section we investigate how different factors such as audio
sampling rate, training set size, the distance from audio source to
recorder, and background noise impact our ﬁngerprinting perfor-
mance. Such investigations will help us determine the conditions
under which our ﬁngerprinting approach will be feasible, specially
if the attacker is tracking devices in public locations. For the fol-

Table 12: Feature exploration using sequential forward selection
technique for all smartphones

#

Feature

RMS
ZCR

Spectral Irregularity

Low-Energy-Rate
Spectral Centroid
Spectral Entropy

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Sequential Feature Selection

Spectral Spread
Spectral Skewness
Spectral Kurtosis
Spectral Rolloff

Spectral Brightness
Spectral Flatness

Chromagram
Tonal Centroid

MFCCs

Max F1-Score

Maximum F1-Score (%)

Song

Human Speech

Instrumental
k-NN GMM k-NN GMM k-NN GMM
76.8
82.7
51.3
45.9
33.8
45.2
30.1
35.6
67.7
56.2
35.3
46.1
74.1
57.4
50.3
59.9
54.2
45
62.8
49.5
61.7
52.1
68.3
61
99.6
100
96.2
98.2
98.5
96
[13]
[13]
100
99.6

80
48.2
40.6
34.7
60.8
47
57
53.9
47.7
53.5
54.5
60.1
100
93.4
89
[13]
100

87.3
50.3
19.4
23.7
46.3
25.9
54.2
34.5
37.1
48.4
38.1
61.6
100
98.9
95.5
[13]
100

84
45.9
15.4
25.8
48.1
23.6
49.7
32.5
38.6
45.9
35.3
63.4
99.6
95.8
91.8
[13]
99.6

78.7
48.5
31.9
25.7
67.7
26.9
70.9
52.7
51.5
59.1
59.2
67.3
100
99.6
98.5
[13]
100

Table 13: Fingerprinting all smartphones using mic & speaker

Audio
Type

Instrumental
Human speech

Song

Features∗ AvgP r AvgRe AvgF 1

k-NN

[13]
[13]
[13]

99.3
99.7
99.7

98.8
99.6
99.6

99
99.6
99.6

GMM

Features∗ AvgP r AvgRe AvgF 1
98.3
99.3
100

98.6
99.4
100

98.1
99.2
100

[13]
[13]
[13]

∗ Features taken from Table 12

lowing set of experiments we only focus on ﬁngerprinting similar-
model smartphones from the same vendor (as this has been shown
to be a tougher problem in the previous section) and consider only
ﬁngerprinting speakers as this is applicable to the scenario where
the attacker is tracking devices in public locations. We also con-
sider recording only ringtones (i.e., audio clips belonging to our
deﬁned ‘Instrumental’ category in Table 3) for the following ex-
periments. Since we are recording ringtones we use the features
highlighted in Table 8 under the ‘Instrumental’ category.
6.7.1
First, we investigate how the sampling rate of audio signals im-
pacts our ﬁngerprinting precision. To do this, we record a ringtone
at the following three frequencies: 8kHz, 22.05kHz and 44.1kHz.
Each sample is recorded 10 times with half of them being used for
training and the other half for testing. Figure 7 shows the average
precision and recall obtained under different sampling rates. As we
can see from the ﬁgure, as sampling frequency decreases, the pre-
cision/recall also goes down. This is understandable, because the
higher the sampling frequency the more ﬁne-tuned information we
have about the audio sample. However, the default sampling fre-

Impact of Sampling Rate

449quency on most hand-held devices today is 44.1kHz [4], with some
of the latest models adopting even higher sampling rates [1]. We,
therefore, believe sampling rate will not impose an obstacle to our
ﬁngerprinting approach, and in future we will be able to capture
more ﬁne grained variations with the use of higher sampling rates.

Figure 7: Impact of sampling frequency on precision/recall.

6.7.2 Varying Training Size
Next, we consider performance of the classiﬁers in the presence
of limited training data. For this experiment we vary the training
set size from 10% to 50% (i.e., from 1 to 5 samples per class) of all
available samples. Table 14 shows the evolution of the F1-score as
training set size is increased (values are reported as percentages).
We see that as the training set size increases the F1-score also rises
which is expected. However, we see that with only three samples
per class we can achieve an F1-score of over 90%. This suggests
that we do not need too many training samples to construct a good
predictive model.

Table 14: Impact of varying training size
GMM

k-NN

Training
samples
per class AvgP r AvgRe AvgF 1 AvgP r AvgRe AvgF 1

Features [13,14]∗

Features [13,14]∗

1
2
3
4
5

42
79.2
91.3
95.3
96.7

49.3
80
89.3
94.7
96

45.3
79.6
90.2
95
96.3

50
80.4
91.7
95.6
98.4

53.3
80
89.3
94.7
98.1

51.6
80.2
90.5
95.1
98.3

∗ Feature numbers taken from Table 8

animal sounds from a far distance) could help in increasing the ﬁn-
gerprinting precision at even longer distances.

Table 15: Impact of varying distance
GMM

k-NN

Features [13,14]∗

Features [13,14]∗

AvgP r AvgRe AvgF 1 AvgP r AvgRe AvgF 1

Distance
(in meters)

0.1
1
2
3
4
5

96.7
92.7
88.2
76.7
70.2
64.5

96
91.5
87.6
76
64
62.7

96.3
92
87.9
76.3
67
63.6

98.4
95.2
94.5
78.9
76.8
77

98.1
94.7
92
84
76
73.3

98.3
94.9
93.2
81.4
76.4
75.1

∗ Feature numbers taken from Table 8

Impact of Ambient Background Noise

6.7.4
In this section we investigate how ambient background noise im-
pacts the performance of our ﬁngerprinting technique. For this ex-
periment we consider scenarios where there is a crowd of people
using their smart devices and we are trying to ﬁngerprint those de-
vices by capturing audio signals (in this case ringtones) from the
surrounding environment. Table 16 highlights the four different
scenarios that we are considering. To emulate such environment,
external speakers (2 pieces) are placed between the smartphone and
microphone while recording is taking place. The external speak-
ers are constantly replaying the respective ambient noise in the
background. We consider a distance of two meters from the audio
source to recorder. The ambient background sounds were obtained
from PacDV [2] and SoundJay [16]. We also compute the signal-
to-noise (SNR) ratio between the original ringtone and the different
ambient background noises. The RMS (root-mean-square) value
of the different background noises varied from approximately 13%
(17.77 dB) to 18% (14.92 dB) of the RMS value of the ringtone un-
der consideration. Table 16 shows our ﬁndings (values are reported
as percentages). We can see that even in the presence of various
background noise we can achieve an F1-score of over 91%.

Table 16: Impact of ambient background noise
GMM

k-NN

Features [13,14]∗

Features [13,14]∗

AvgP r AvgRe AvgF 1 AvgP r AvgRe AvgF 1

88.8
90.5
91.7
91.3

85.3
89.7
90
89.5

87
90.1
90.8
90.4

95.1
92.5
95.2
94.5

93.3
90.7
94.1
93.3

94.2
91.6
94.6
93.9

Environments

Shopping Mall
Restaurant/Cafe

City Park

Airport Gate

SNR
(dB)
15.85
17.77
15.43
14.92

∗ Feature numbers taken from Table 8

6.7.3 Varying Distance between Speaker and Recorder
Next, we inspect the impact of distance between the audio source
(i.e., smartphone) and recorder (i.e., laptop/PC) on ﬁngerprinting
precision. For this experiment we use a separate external micro-
phone as the signal capturing capacity of the microphone embed-
ded inside a laptop degrades drastically as distance increases. We
use the relatively inexpensive ($44.79) Audio-Technica ATR-6550
shotgun microphone for this experiment and vary the distance be-
tween the external microphone and smartphone from 0.1 meter to 5
meters. Table 15 summarizes the F1-scores obtained as the distance
between the smartphone and microphone varies. We see that as dis-
tance increases, F1-score decreases. This is expected, because the
longer the distance between the smartphone and microphone, the
harder it becomes to capture the minuscule deviations between au-
dio samples. However, we see that even up to two meters distance
we can achieve an F1-score of 93%. This suggests that our device
ﬁngerprinting approach works only up to a certain distance using
any commercial microphones. However, using specialized micro-
phones, such as parabolic microphones (usually used in capturing

7. DISCUSSION AND LIMITATIONS

Our approach has a few limitations. First, we experimented with
52 devices manufactured by different vendors; it is possible that a
larger target device pool would lower accuracy. That said, distinc-
tions across different device types are more clear; additionally, au-
dio ﬁngerprints may be used in tandem with other techniques, like
accelerometer ﬁngerprinting [31], to better discriminate between
devices. Secondly, most of the experiments took place in a lab set-
ting. However, we studied the impact of ambient background noise
and still found our approach to be applicable. Lastly, all the phones
used in our experiments were not in mint condition and some of the
idiosyncrasies of individual microphones and speakers may have
been the result of uneven wear and tear on each device; we believe,
however, that this is likely to occur in the real world as well.

8. CONCLUSION

In this paper we show that it is feasible to ﬁngerprint smart de-
vices through on-board acoustic components like microphones and

 70 75 80 85 90 95 10044.122.058AvgPr/AvgRe (%)Sampling Frequency (kHz)k-NN AvgPrk-NN AvgReGMM AvgPrGMM AvgRe450speakers. As microphones and speakers are one of the most stan-
dard components present in almost all smart devices available to-
day, this creates a key privacy concern for users. To demonstrate the
feasibility of this approach, we collect ﬁngerprints from 52 differ-
ent smartphones covering a total of ﬁve different brands of smart-
phones. Our studies show that it is possible to successfully ﬁn-
gerprint smartphones through microphones and speakers, not only
under controlled environments, but also in the presence of ambi-
ent noise. We believe our ﬁndings are important steps towards un-
derstanding the full consequences of ﬁngerprinting smart devices
through acoustic channels.

Acknowledgment
We would like to thank Thomas S. Benjamin for his valuable in-
put during the initial phase of the project and all the anonymous
reviewers for their valuable feedback. We would specially like to
thank Romit Roy Choudhury and his group at UIUC for providing
us with the bulk of smartphones used in our experiments. On the
same note we would like to extend our gratitude to the Computer
Science department at UIUC for providing us with the Motorola
Droid phones. This paper reports on work that was supported in
part by NSF CNS 0953655.

9. REFERENCES
[1] 5 of the best DACs. http://www.stuff.tv/music/5-best-dacs-

how-make-your-digital-music-sound-amazing/feature.
Accessed 05/15/2014.

[2] Ambient Sound Effects.

http://www.pacdv.com/sounds/ambience_sounds.html.
Accessed 05/15/2014.

[3] Audacity is free, open source, cross-platform software for

recording and editing sounds.
http://audacity.sourceforge.net/. Accessed 05/15/2014.

[4] Audio 4 Smartphones – Wolfson Microelectronics.

http://www.wolfsonmicro.com/documents/uploads/misc/en/
Audio4Smartphones.pdf. Accessed 05/15/2014.

[5] Global mobile statistics 2013.

http://mobithinking.com/mobile-marketing-tools/latest-
mobile-stats/a. Accessed 05/15/2014.

[6] Hertz, the WAV recorder. https://play.google.com/store/apps/

details?id=uk.ac.cam.cl.dtg.android.audionetworking.hertz.
Accessed 05/15/2014.

[7] How MEMS Microphones Fucntion.

http://www.eeherald.com/section/design-guide/mems-
microphone.html. Accessed 05/15/2014.

[8] IPhone and Android Apps Breach Privacy.

http://www.gartner.com/newsroom/id/2335616. Accessed
05/15/2014.

[9] IPhone and Android Apps Breach Privacy.

http://online.wsj.com/article/
SB10001424052748704694004576020083703574602.html.
Accessed 05/15/2014.

[10] MEMS microphone market. http://www.digikey.com/supply-
chain-hq/us/en/articles/semiconductors/mems-microphone-
market-revenues-soar-42-in-2012/1497. Accessed
05/15/2014.

[11] MEMS Microphone Model.

http://www.comsol.com/blogs/mems-microphone-model-
presented-asa-166-san-francisco/. Accessed 05/15/2014.
[12] MEMS microphone shipments to climb 30 percentage in

2013. http://electroiq.com/blog/2013/02/mems-microphone-
shipments-to-climb-30-percent-this-year/. Accessed
05/15/2014.

[13] MIRtoolbox. https://www.jyu.ﬁ/hum/laitokset/musiikki/en/

research/coe/materials/mirtoolbox. Accessed 05/15/2014.
[14] Mobile device market to reach 2.6 billion units by 2016.

http://www.canalys.com/newsroom/mobile-device-market-
reach-26-billion-units-2016. Accessed 05/15/2014.

[15] Netlab: Algorithms for Pattern Recognition. http://www1.

aston.ac.uk/eas/research/groups/ncrg/resources/netlab/book/.
Accessed 05/15/2014.

[16] SOUNDJAY-Ambient Sound Effects.

http://www.soundjay.com/ambient-sounds.html. Accessed
05/15/2014.

[17] Top MEMS Microphone Suppliers.

http://www.isuppli.com/MEMS-and-Sensors/MarketWatch/
pages/Top-MEMS-Microphone-Suppliers-All-CanCount-
on-Apple-for-Clear-and-Resounding-Success.aspx.
Accessed 05/15/2014.

[18] G. Acar, M. Juarez, N. Nikiforakis, C. Diaz, S. Gürses,

F. Piessens, and B. Preneel. FPDetective: dusting the web for
ﬁngerprinters. In Proceedings of the 2013 ACM conference
on Computer and Communications Security, CCS ’13, pages
1129–1140, 2013.

[19] M. A. Bartsch and G. H. Wakeﬁeld. Audio Thumbnailing of
Popular Music Using Chroma-based Representations. IEEE
Transactions on Multimedia, 7(1):96–104, Feb 2005.
[20] F. Bimbot, J.-F. Bonastre, C. Fredouille, G. Gravier,

I. Magrin-Chagnolleau, S. Meignier, T. Merlin,
J. Ortega-Garcia, D. Petrovska-Delacretaz, and D. A.
Reynolds. A Tutorial on Text-Independent Speaker
Veriﬁcation. EURASIP Journal on Advances in Signal
Processing, 4:430–451, 2004.

[21] V. Brik, S. Banerjee, M. Gruteser, and S. Oh. Wireless
Device Identiﬁcation with Radiometric Signatures. In
Proceedings of the 14th ACM International Conference on
Mobile Computing and Networking, MobiCom ’08, pages
116–127, 2008.

[22] J. Campbell, J.P. Speaker recognition: a tutorial. Proceedings

of the IEEE, 85(9):1437–1462, Sep 1997.

[23] P. Cano, E. Batlle, T. Kalker, and J. Haitsma. A Review of

Audio Fingerprinting. J. VLSI Signal Process. Syst.,
41(3):271–284, Nov 2005.

[24] J. Chang and Y. Peng. Speaker, yoke thereof and method for

manufacturing yoke, Jan 2012. US Patent
8,094,867.http://www.google.com/patents/US8094867.

[25] M. Cheng, W. Huang, and S. R. Huang. A silicon

microspeaker for hearing instruments. J. of Micromechanics
and Microengineering, 14(7):859–866, Jul 2004.

[26] W. B. Clarkson. Breaking Assumptions: Distinguishing

Between Seemingly Identical Items Using Cheap Sensors.
PhD thesis, Princeton, NJ, USA, 2012.

[27] S. COLE and S. Cole. Suspect Identities: A History of

Fingerprinting and Criminal Identiﬁcation. Harvard
University Press, 2009.

[28] A. Das, N. Borisov, and M. Caesar. Fingerprinting smart
devices through embedded acoustic components. CoRR,
abs/1403.3366, 2014. http://arxiv.org/abs/1403.3366.

[29] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum

Likelihood from Incomplete Data via the EM Algorithm. J.
of the Royal Statistical Society. Series B, 39(1):1–38, 1977.
[30] L. C. C. Desmond, C. C. Yuan, T. C. Pheng, and R. S. Lee.

Identifying Unique Devices Through Wireless
Fingerprinting. In Proceedings of the 1st ACM Conference on
Wireless Network Security, WiSec ’08, pages 46–55, 2008.
[31] S. Dey, N. Roy, W. Xu, R. R. Choudhury, and S. Nelakuditi.

AccelPrint: Imperfections of Accelerometers Make
Smartphones Trackable. In Proceedings of the 20th Annual
Network and Distributed System Security Symposium,
NDSS’14, Feb 2014.

[32] R. Duda, P. Hart, and D. Stork. Pattern classiﬁcation. Wiley,

2001.

[33] P. Eckersley. How Unique is Your Web Browser? In

Proceedings of the 10th International Conference on Privacy
Enhancing Technologies, PETS’10, pages 1–18, 2010.

[34] M. Egele, C. Kruegel, E. Kirda, and G. Vigna. PiOS:

Detecting Privacy Leaks in iOS Applications. In Proceedings
of the 17th Annual Network and Distributed System Security
Symposium, NDSS ’11, 2011.

451[35] W. Enck, P. Gilbert, B.-G. Chun, L. P. Cox, J. Jung,

P. McDaniel, and A. N. Sheth. TaintDroid: An
Information-ﬂow Tracking System for Realtime Privacy
Monitoring on Smartphones. In Proceedings of the 9th
USENIX Conference on Operating Systems Design and
Implementation, OSDI’10, pages 1–6, 2010.

[36] A. P. Felt, E. Ha, S. Egelman, A. Haney, E. Chin, and

D. Wagner. Android Permissions: User Attention,
Comprehension, and Behavior. In Proceedings of the 8th
Symposium on Usable Privacy and Security, SOUPS ’12,
pages 3:1–3:14, 2012.

[37] J. Franklin, D. McCoy, P. Tabriz, V. Neagoe,

J. Van Randwyk, and D. Sicker. Passive Data Link Layer
802.11 Wireless Device Driver Fingerprinting. In
Proceedings of the 15th USENIX Security Symposium, 2006.

[38] R. M. Gerdes, T. E. Daniels, M. Mina, and S. F. Russell.
Device identiﬁcation via analog signal ﬁngerprinting: A
matched ﬁlter approach. In Proceedings of the 13th Annual
Network and Distributed System Security Symposium, 2006.

[39] C. Gibler, J. Crussell, J. Erickson, and H. Chen.

AndroidLeaks: Automatically Detecting Potential Privacy
Leaks in Android Applications on a Large Scale. In
Proceedings of the 5th International Conference on Trust and
Trustworthy Computing, TRUST’12, pages 291–307, 2012.
[40] F. Guo and T. cker Chiueh. Sequence Number-Based MAC

Address Spoof Detection. In Proceedings of 8th
International Symposium on Recent Advances in Intrusion
Detection, RAID ’05, 2005.

[41] G. Guo and S. Li. Content-based audio classiﬁcation and

retrieval by support vector machines. IEEE Transactions on
Neural Networks, 14(1):209–215, Jan 2003.

[42] I. Guyon and A. Elisseeff. An Introduction to Variable and
Feature Selection. Journal of Machine Learning Research,
3(26):1157–1182, Mar 2003.

[43] J. Haitsma and T. Kalker. A Highly Robust Audio
Fingerprinting System. In Proceedings of the 2002
International Symposium on Music Information Retrieval,
pages 107–115, 2002.

[44] S.-S. Je, F. Rivas, R. Diaz, J. Kwon, J. Kim, B. Bakkaloglu,

S. Kiaei, and J. Chae. A Compact and Low-Cost MEMS
Loudspeaker for Digital Hearing Aids. IEEE Transactions on
Biomedical Circuits and Systems, 3(5):348–358, 2009.

[45] P. Kelley, S. Consolvo, L. Cranor, J. Jung, N. Sadeh, and

D. Wetherall. A Conundrum of Permissions: Installing
Applications on an Android Smartphone. In Financial
Cryptography and Data Security, pages 68–79. 2012.

[46] T. Kohno, A. Broido, and K. C. Claffy. Remote Physical
Device Fingerprinting. IEEE Trans. Dependable Secur.
Comput., 2(2):93–108, Apr 2005.

[47] L. Langley. Speciﬁc emitter identiﬁcation (SEI) and classical

parameter fusion technology. In WESCON ’93, pages
377–381, Sep 1993.

[48] T. Li, M. Ogihara, and Q. Li. A Comparative Study on

Content-based Music Genre Classiﬁcation. In Proceedings of
the 26th Annual International ACM SIGIR Conference on
Research and Development in Informaion Retrieval, SIGIR
’03, pages 282–289, 2003.

[49] Z. Li, W. Xu, R. Miller, and W. Trappe. Securing Wireless
Systems via Lower Layer Enforcements. In Proceedings of
the 5th ACM Workshop on Wireless Security, WiSe ’06,
pages 33–42, 2006.

[50] G. Lyon. Nmap: a free network mapping and security
scanning tool. http://nmap.org/. Accessed 05/15/2014.
[51] K. Mahaffey and J. Hering. App Attack: Surviving the

Explosive Growth of Mobile Apps. 2010.
https://media.blackhat.com/bh-us-
10/presentations/Mahaffey_Hering/Blackhat-USA-2010-
Mahaffey-Hering-Lookout-App-Genome-slides.pdf.

[52] M. Mckinney and J. Breebaart. Features for Audio and

Music Classiﬁcation. In Proceedings of the 2003
International Symposium on Music Information Retrieval,
pages 151–158, 2003.

[53] S. Moon, P. Skelly, and D. Towsley. Estimation and removal

of clock skew from network delay measurements. In
Proceedings of the 18th Annual IEEE International
Conference on Computer Communications, INFOCOM ’99,
pages 227–234, 1999.

[54] K. Mowery, D. Bogenreif, S. Yilek, and H. Shacham.

Fingerprinting Information in JavaScript Implementations. In
Proceedings of W2SP 2011, May 2011.

[55] N. T. Nguyen, G. Zheng, Z. Han, and R. Zheng. Device

ﬁngerprinting to enhance wireless security using
nonparametric Bayesian method. In Proceedings IEEE
INFOCOM, pages 1404–1412, April 2011.

[56] L. Olejnik, C. Castelluccia, and A. Janc. Why Johnny Can’t

Browse in Peace: On the Uniqueness of Web Browsing
History Patterns. In Proceedings of the 5th Workshop on Hot
Topics in Privacy Enhancing Technologies (HotPETs), 2012.

[57] J. Pang, B. Greenstein, R. Gummadi, S. Seshan, and

D. Wetherall. 802.11 User Fingerprinting. In Proceedings of
the 13th Annual International Conference on Mobile
Computing and Networking, pages 99–110, 2007.

[58] N. Patwari and S. K. Kasera. Robust Location Distinction

Using Temporal Link Signatures. In Proceedings of the 13th
Annual ACM International Conference on Mobile Computing
and Networking, MobiCom ’07, pages 111–122, 2007.
[59] D. A. Reynolds, T. F. Quatieri, and R. B. Dunn. Speaker

veriﬁcation using adapted gaussian mixture models. Digital
Signal Processing, 10(1-3):19–41, 2000.

[60] M. Riezenman. Cellular security: better, but foes still lurk.

IEEE Spectrum, 37(6):39–42, Jun 2000.

[61] A. Ross and A. Jain. Information fusion in biometrics.
Pattern Recognition Letters, 24(13):2115 – 2125, 2003.

[62] A. Shabtai, Y. Fledel, U. Kanonov, Y. Elovici, S. Dolev, and

C. Glezer. Google Android: A Comprehensive Security
Assessment. IEEE Security and Privacy, 8(2):35–44, 2010.

[63] I. Shahosseini, E. Lefeuvre, M. Woytasik, J. Moulin,

X. Leroux, S. Edmond, E. Dufour-Gergam, A. Bosseboeuf,
G. Lemarquand, and V. Lemarquand. Towards high ﬁdelity
high efﬁciency MEMS microspeakers. In IEEE Sensors,
pages 2426–2430, 2010.

[64] M. Sokolova and G. Lapalme. A systematic analysis of

performance measures for classiﬁcation tasks. Information
Processing and Management, 45(4):427–437, 2009.

[65] G. Tzanetakis and P. Cook. Musical genre classiﬁcation of

audio signals. IEEE Transactions on Speech and Audio
Processing, 10(5):293–302, 2002.

[66] Y. Yang and J. O. Pedersen. A Comparative Study on Feature

Selection in Text Categorization. In Proceedings of the
Fourteenth International Conference on Machine Learning,
ICML ’97, pages 412–420, 1997.

[67] Z. Yang, L. Cai, Y. Liu, and J. Pan. Environment–aware
clock skew estimation and synchronization for wireless
sensor networks. In Proceedings of the 31st Annual IEEE
International Conference on Computer Communications,
INFOCOM ’12, pages 1017–1025, 2012.

[68] F. Yarochkin, M. Kydyraliev, and O. Arkin. Xprobe project.

http://oﬁrarkin.wordpress.com/xprobe/.

[69] T.-F. Yen, Y. Xie, F. Yu, R. P. Yu, and M. Abadi. Host
Fingerprinting and Tracking on the Web:Privacy and
Security Implications. In Proceddings of the 19th Annual
Network and Distributed System Security Symposium,
NDSS’12, 2012.

[70] Y. Zhou, X. Zhang, X. Jiang, and V. Freeh. Taming
Information-Stealing Smartphone Applications (on
Android). In Proceedings of the 4th International Conference
on Trust and Trustworthy Computing, pages 93–107. 2011.

452