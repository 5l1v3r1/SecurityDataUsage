Secure Deduplication of Encrypted Data without Additional

Independent Servers

Jian Liu

Aalto University
jian.liu@aalto.ﬁ

N. Asokan

Aalto University and
University of Helsinki
asokan@acm.org

Benny Pinkas
Bar Ilan University

benny@pinkas.net

Abstract
Encrypting data on client-side before uploading it to a cloud
storage is essential for protecting users’ privacy. However
client-side encryption is at odds with the standard practice
of deduplication. Reconciling client-side encryption with
cross-user deduplication is an active research topic. We
present the ﬁrst secure cross-user deduplication scheme that
supports client-side encryption without requiring any ad-
ditional independent servers.
Interestingly, the scheme is
based on using a PAKE (password authenticated key ex-
change) protocol. We demonstrate that our scheme provides
better security guarantees than previous eﬀorts. We show
both the eﬀectiveness and the eﬃciency of our scheme, via
simulations using realistic datasets and an implementation.
Categories and Subject Descriptors
E.3 [Data Encryption]
Keywords
Cloud Storage; Deduplication; Semantically Secure Encryp-
tion; PAKE

1.

INTRODUCTION

Cloud storage is a service that enables people to store
their data on a remote server. With a rapid growth in user
base, cloud storage providers tend to save storage costs via
cross-user deduplication: if two clients upload the same ﬁle,
the storage server detects the duplication and stores only a
single copy. Deduplication achieves high storage savings [18]
and is adopted by many storage providers. It is also adopted
widely in backup systems for enterprise workstations.

Clients who care about privacy prefer to have their data
encrypted on the client-side using semantically secure en-
cryption schemes. However, na¨ıve application of encryption
thwarts deduplication since identical ﬁles are uploaded as
completely independent ciphertexts. Reconciling dedupli-
cation and encryption is an active research topic. Current
solutions either use convergent encryption [10], which is sus-

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
c(cid:13) 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813623.

ceptible to oﬄine brute-force attacks, or require the aid of
additional independent servers [3, 21, 23], which is a strong
assumption that is very diﬃcult to meet in commercial con-
texts. Furthermore, some schemes of the latter type are
susceptible to online brute-force attacks.
Our Contributions. We present the ﬁrst single-server
scheme for secure cross-user deduplication with client-side
encrypted data. Our scheme allows a client uploading an
existing ﬁle to securely obtain the encryption key that was
used by the client who has previously uploaded that ﬁle.
The scheme builds upon a well-known cryptographic primi-
tive known as password authenticated key exchange (PAKE)
[6], which allows two parties to agree on a session key iﬀ they
share a short secret (“password”). PAKE is secure even if the
passwords have low entropy. In our deduplication scheme,
a PAKE-based protocol is used to compute identical keys
for diﬀerent copies of the same ﬁle. Speciﬁcally, a client up-
loading a ﬁle ﬁrst sends a short hash of this ﬁle (10-20 bits
long) to the server. The server identiﬁes other clients whose
ﬁles have the same short hash, and lets them run a single
round PAKE protocol (routed through the server) with the
uploader using the (long, but possibly low entropy) hashes
of their ﬁles as the “passwords”. At the end of the protocol,
the uploader gets the key of another client iﬀ their ﬁles are
identical. Otherwise, it gets a random key.

Our scheme uses a per-ﬁle rate limiting strategy to prevent
online brute-force attacks. Namely, clients protect them-
selves by limiting the number of PAKE instances they will
participate in for each ﬁle. Compared with the commonly
used per-client rate limiting (used in DupLESS [3]), which
limits the number of queries allowed for each client (during
a time interval), our scheme is signiﬁcantly more resistant to
online brute-force attacks by an adversary who has compro-
mised multiple clients, or by the storage server. Per-client
rate limiting is not fully eﬀective against such attacks be-
cause the adversary can use diﬀerent identities.

At a ﬁrst glance, it seems that our scheme incurs a high
communication and computation overhead because a client
uploading a ﬁle is required to run PAKE many times due to
the high collision rate of the short hash. In fact, the number
of PAKE runs for an upload request is limited to a certain
level by the rate limiting strategy. For a requested short
hash, the server only checks a subset of existing ﬁles (in de-
scending order of ﬁle popularity) that have the same short
hash. This implies that our scheme may fail to ﬁnd dupli-
cates for some requests, and this will certainly reduce the
deduplication eﬀectiveness. Nonetheless, our simulations in
Section 6 show that this negative eﬀect is very small. The

874reason is that the ﬁle popularity distribution is far from be-
ing uniform, and popular ﬁles account for most of the beneﬁt
from deduplication. Our scheme can almost always ﬁnd du-
plicates for these popular ﬁles.

To summarize, we make the following contributions:
• Presenting the ﬁrst single-server scheme for cross-
user deduplication that enables client-side semanti-
cally secure encryption (except that, as in all dedu-
plication schemes, ciphertexts leak the equality of the
underlying ﬁles), and proving its security in the mali-
cious model (Section 5);

• Showing that our scheme has better security guar-
antees than previous work (Section 5.2). As far as we
know, our proposal is the ﬁrst scheme that can prevent
online brute-force attacks by compromised clients or
server, without introducing an identity server;

• Demonstrating, via simulations with realistic datasets,
that our scheme provides its privacy beneﬁts while re-
taining a utility level (in terms of deduplication eﬀec-
tiveness) on par with standard industry practice.
Our use of per-ﬁle rate limiting implies that an incom-
ing ﬁle will not be checked against all existing ﬁles in
storage. Notably, our scheme still achieves high dedu-
plication eﬀectiveness (Section 6);

• Implementing our scheme to show that it incurs min-
imal overhead (computation/communication) com-
pared to traditional deduplication schemes (Section 7).

2. PRELIMINARIES
2.1 Deduplication

Deduplication strategies can be categorized according to
the basic data units they handle. One strategy is ﬁle-level
deduplication which eliminates redundant ﬁles [10]. The
other is block-level deduplication, in which ﬁles are segmented
into blocks and duplicate blocks are eliminated [22]. In this
paper we use the term “ﬁle” to refer to both ﬁles and blocks.
Deduplication strategies can also be categorized accord-
ing to the host where deduplication happens. In server-side
deduplication, all ﬁles are uploaded to the storage server,
which then deletes the duplicates. Clients are unaware of
deduplication. This strategy saves storage but not band-
width. In client-side deduplication, a client uploading a ﬁle
ﬁrst checks the existence of this ﬁle on the server (by sending
a hash of the ﬁle). Duplicates are not uploaded. This strat-
egy saves both storage and bandwidth, but allows a client
to learn if a ﬁle already exists on the server.

The eﬀectiveness of deduplication is usually expressed by
the deduplication ratio, deﬁned as the “the number of bytes
input to a data deduplication process divided by the num-
ber of bytes output” [12].
In the case of a cloud storage
service, this is the ratio between total size of ﬁles uploaded
by all clients and the total storage space used by the ser-
vice. The dedpulication percentage (sometimes referred to as
“space reduction percentage”) [12, 15] is 1−
deduplication ratio .
A perfect deduplication scheme will detect all duplicates.

1

The level of deduplication achievable depends on a number
of factors. In common business settings, deduplication ratios
in the range of 4:1 (75%) to 500:1 (99.8%) are typical. Wendt
et al. suggest that a ﬁgure in the range 10:1 (90%) to 20:1
(95%) is a realistic expectation [24].

Although deduplication beneﬁts storage providers (and
hence, indirectly, their users), it also constitutes a privacy
threat for users. For example, a cloud storage server that
supports client-side, cross-user deduplication can be exploited
as an oracle that answers “did anyone upload this ﬁle?”.
An adversary can do so by uploading a ﬁle and observing
whether deduplication takes place [15]. For a predictable ﬁle
that has low entropy, the adversary can construct all pos-
sible ﬁles, upload them and observe which ﬁle causes dedu-
plication. Harnik et al. [15] propose a randomized threshold
approach to address such online brute-force attacks. Specif-
ically, for each ﬁle F , the server keeps a random threshold
tF (tF ≥ 2) and a counter cF that indicates the number of
clients that have previously uploaded F . Client-side dedu-
plication happens only if cF ≥ tF . Otherwise, the server
does a server-side deduplication.

In Section 8, we survey the state-of-the-art of deduplica-

tion with encrypted data.
2.2 Hash Collisions

A hash function H: Φ → {0, 1}n is a deterministic func-
tion that maps a binary string in Φ of arbitrary length to
a binary string h of ﬁxed length n. The term cryptographic
hash function is used to denote that a hash function has ran-
dom outputs, being one-way and infeasible to ﬁnd collisions.
We model the hash function H as a random oracle1.

A cryptographic hash function with a long output is colli-
sion resistant, whereas a hash function with a short output
has many collisions. As we will see in Section 4 and 5, we use
a short hash function SH to improve eﬃciency and privacy.
2.3 Additively Homomorphic Encryption

A public key encryption scheme is additively homomor-
phic if given two ciphertexts c1 = Enc(pk, m1; r1) and c2 =
Enc(pk, m2; r2), it is possible to eﬃciently compute Enc(pk, m1+
m2; r) with a new random value r (which depends on r1 and
r2 but cannot be determined by anyone who knows only r1
or r2), even without knowledge of the corresponding private
key. Examples of such schemes are Paillier’s encryption [19],
and lifted ElGamal encryption [13] where plaintexts are en-
coded in the exponent. The Lifted ElGamal Encryption is
described as follows:

• Gen(1λ) returns a generator g of a cyclic group G of
order p, and an integer h = gx where x is a random
value in Zp. The tuple (G, p, g, h) is the public key pk
and (G, p, g, x) is the private key sk;

• Enc(pk, m) chooses a random value y in Zp and re-
turns the ciphertext c = (c1, c2) where c1 = gy and
c2 = gm · hy;

• Dec(sk, c) returns c2

= gm·hy

gxy = gm. Note that, in
general, calculating the discrete logarithm of gm is
hard, but knowing gm is enough in our application.

cx
1

We use E()/D() to denote symmetric encryption/decryption,
and use Enc()/Dec() to denote additively homomorphic en-
cryption/decryption. We abuse the notation and use Enc(pk, m)
1Our deduplication scheme (see Section 5) will be based on
a PAKE protocol (see Section 2.4) that uses the random
oracle model. Therefore we choose this model for all our
analyses.

875Inputs:

• Alice’s input is a password pwa;
• Bob’s input is a password pwb.

Outputs:

• Alice’s output is ka;
• Bob’s output is kb.

where if pwa = pwb then ka = kb, and if pwa (cid:54)= pwb
then Bob (resp. Alice) cannot distinguish ka (resp. kb)
from a random string of the same length.

Figure 1: The ideal functionality Fpake for password
authenticated key exchange.

to denote Enc(pk, m; r) where r is chosen uniformly at ran-
dom. In addition, we use ⊕ and (cid:9) to denote homomorphic
addition and subtraction respectively.
2.4 Password Authenticated Key Exchange

Password-based protocols are commonly used for user au-
thentication. However, such protocols are vulnerable to of-
ﬂine brute-force attacks (also referred to as dictionary at-
tacks) since users tend to choose passwords with relatively
low entropy that are hence guessable. Bellovin and Merritt
[6] were the ﬁrst to propose a password authenticated key
exchange (PAKE) protocol, in which an adversary making
a password guess cannot verify the guess without an online
attempt to authenticate itself with that password.
The ideal functionality of PAKE, Fpake, is shown in Fig-
ure 1. We use it as a building block, and require the following
properties in addition to the ideal functionality:

• Implicit key exchange: At the end of the protocol, nei-
ther party learns if the passwords matched or not. (In
fact, many PAKE protocols were designed to be ex-
plicit so that parties can learn this information.)

• Single round: The protocol must be single-round so
that it can be easily facilitated by the storage server.
• Concurrent executions: The protocol must allow mul-
tiple PAKE instances to run in parallel. There are
two common security notions for such PAKE protocols.
One stronger notion is “UC-secure PAKE” [9], which
guarantees security for composition with arbitrary pro-
tocols. The other notion is “concurrent PAKE”, de-
ﬁned by [5, 7], where each party is able to concurrently
run many invocations of the protocol.
Implementa-
tions of this notion are much more eﬃcient than im-
plementations of UC-secure PAKE. We therefore use
a concurrent PAKE protocol in our work.

Our deduplication scheme (see Section 5) uses the SPAKE2
protocol of Abdalla and Pointcheval [1], which is described
in Figure 2. This protocol is secure in the concurrent setting
and random oracle model. It requires each party to compute
only three exponentiations, and send just a single group ele-
ment to the other party. Theorem 5.1 in [1] states that this
protocol is secure assuming that the computational Diﬃe-
Hellman problem is hard in the group used by the protocol.
Same-Input-PAKE. In our deduplication protocol, one
client (Alice) runs multiple PAKE instances with other clients.

Inputs:

• Alice’s input is a password pwa;
• There are n parties P1, . . . , Pn, with passwords

pw1, . . . , pwn, respectively.

Outputs:

• Alice’s output is ka,1, . . . , ka,n;
• Pi’s output is kb,i.

where ∀i ∈ [1, n] if pwa = pwi then ka,i = kb,i,
and otherwise Pi (resp. Alice) cannot distinguish ka,i
(resp. kb,i) from a random string of the same length.
Figure 3: The ideal functionality Fsame−input−pake.

The protocol must ensure that the client uses the same input
in all these PAKE instances. We deﬁne this requirement in
the functionality of same-input-PAKE described in Figure 3.
We list three possible methods for implementing the same-
input-PAKE functionality: (1) The protocol can be based
on the SPAKE2 protocol, where Alice uses the same ﬁrst
message X∗ in her interactions with all clients, thus using
the same input in all these instances.2 We do not know
how to prove security for this variant of PAKE, and leave
(2) Alice can run independent
it as a heuristic solution.
SPAKE2 instances, with a diﬀerent ﬁrst message in each
instance, and in addition prove in zero-knowledge that her
inputs to all instances are identical. The proof can be based
on standard sigma protocols for Diﬃe-Hellman tuples and
the Fiat-Shamir heuristic (see, e.g. [16], Chap. 7), and re-
quires only one additional exponentiation from Alice and
two exponentiations from each other party.
(3) The pro-
tocol can use generic protocols for non-interactive secure
computation (NISC) [2]. These protocols are single round
and secure against malicious adversaries. A variant called
multi-sender NISC [2], has one party sending the ﬁrst mes-
sage of the protocol (committing to its input) and then has
multiple other parties independently answering this message
with messages encoding their input. The drawback of this
approach in terms of performance is that the protocol re-
quires an oblivious transfer for each input bit of Alice, and
is therefore less eﬃcient than protocols based on SPAKE2
or similar speciﬁc PAKE protocols.

3. PROBLEM STATEMENT

3.1 General Setting
The generic setting for cloud storage systems consists of
a storage server (S) and a set of clients (Cs) who store their
ﬁles on S. Cs never communicate directly, but exchange mes-
sages with S, and S processes the messages and/or forwards
them as needed. Additional independent servers (ISs) can
be introduced to assist deduplication [3, 23, 21]. But they

2This change is similar to the transformation from the (in-
teractive) Diﬃe-Hellman key exchange protocol to the (non-
interactive) ElGamal encryption: The latter can be consid-
ered as being generated from Diﬃe-Hellman key exchange,
where Alice’s public key is her ﬁrst message.

876Public information: A ﬁnite cyclic group G of prime order p generated by an element g. Public elements Mu ∈ G associated
with user u. A hash function H modeled as a random oracle.
Secret information: User u has a password pwu.
The protocol is run between Alice and Bob:

1. Each side performs the following computation:

• Alice chooses x ∈R Zp and computes X = gx. She deﬁnes X∗ = X · (MA)pwA .
• Bob chooses y ∈R Zp and computes Y = gy. He deﬁnes Y ∗ = Y · (MB)pwB .

2. Alice sends X∗ to Bob. Bob sends Y ∗ to Alice.

3. Each side computes the shared key:

• Alice computes KA = (Y ∗/(MB)pwA )x. She then computes her output as SKA = H(A, B, X∗, Y ∗, pwA, KA).
• Bob computes KB = (X∗/(MA)pwB )y. He then computes his output as SKB = H(A, B, X∗, Y ∗, pwB, KB).

Figure 2: The SPAKE2 protocol of Abdalla and Pointcheval [1].

are unrealistic in commercial settings3 and can be bottle-
necks for both security and performance. We do not require
any ISs to take part in our scheme.
We assume that the parties communicate through secure
channels, so that an adversary (A) cannot eavesdrop and/or
tamper with any channel.

We introduce new notations as needed. A summary of

notations appears in Appendix A.
3.2 Security Model
Ideal Functionality. We deﬁne the ideal functionality
Fdedup of deduplicating encrypted data in Figure 4. There
are three types of participants: the storage server S, the
uploader C attempting to upload a ﬁle and existing clients
{Ci} who have already uploaded a ﬁle. A protocol imple-
menting Fdedup is secure if it implements Fdedup. Speciﬁ-
cally, the protocol leaks no information about Cs’ ﬁles and
keys, and only S knows whether deduplication happens or
not. Furthermore, we require the protocol to be secure in
the malicious model [14] where participants can behave ar-
bitrarily (are allowed to refuse to participate in the protocol,
substitute their inputs with other values, and abort the pro-
tocol prematurely).
Threat Model. An adversary A might compromise C, S,
any subset of {Ci}, or any collusion of these parties. The
security of a single upload procedure is captured by requir-
ing that the protocol implements Fdedup according to the
ideal/real model security deﬁnitions. However, additional
attacks are possible when considering the long-term opera-
tion of the system:

• Online brute-force attack by a compromised active up-
loader: as we described in Section 2.1, for a predictable
ﬁle, an uploader can construct all candidate ﬁles, up-
load them and observe which one causes deduplication;
• Oﬄine brute-force attack by a compromised passive S:
if S gets a deterministic representation (e.g., crypto-
graphic hash or convergent encryption) of a predictable
ﬁle, it can construct all candidate ﬁles and verify them
oﬄine;

3It is diﬃcult to ﬁnd business justiﬁcation for an indepen-
dent party to run an IS solely for improving privacy in cloud
storage services.

Inputs:

• The uploader C has input F ;
• Each existing client Ci has inputs Fi and kFi ;
• S’s input is empty.

Outputs:

• C gets an encryption key kF for F . If F is iden-
tical to an existing ﬁle Fi then kF = kFi . Oth-
erwise kF is random;

• Each Ci’s output is empty;
• S gets the ciphertext E(kF , F ).

If there is a
ciphertext E(kFj , Fj) that is equal to E(kF , F )
in its storage, it learns j as well. Otherwise, S
learns that F is not in the storage.

Figure 4: The ideal functionality Fdedup of dedupli-
cating encrypted data.

• Online brute-force attack by a compromised active S:
S can also masquerade as Cs running the protocol for
every “guess” and checking if deduplication occurs.

Other than the brute-force attacks, a compromised S can
easily detect whether a certain ﬁle is in the storage by run-
ning the deduplication protocol once. We claim that no
known deduplication scheme can prevent this attack, as S
always knows that deduplication happens. This attack is
not included in our threat model.
3.3 Design Goals
Security goals. We deﬁne the following security goals for
our scheme:
S1 Realize Fdedup in malicious model;
S2 Prevent online brute-force attacks by compromised ac-

tive uploaders;

S3 Prevent oﬄine brute-force attacks by compromised pas-

sive S;

S4 Prevent online brute-force attacks by compromised ac-

tive S (masquerading as multiple Cs).

877Functional goals.
meet certain functional goals:

In addition, the protocol should also

F1 Maximize deduplication eﬀectiveness (exceed realistic

expectations, as discussed in Section 2.1);

F2 Minimize computational and communication overhead
(i.e., the computation/communication costs should be
comparable to storage systems without deduplication).

4. OVERVIEW OF THE SOLUTION
Overview. We ﬁrst motivate salient design decisions in our
scheme before describing the details in Section 5.
When an uploader C wants to upload a ﬁle F to S, we need
to address two problems: (a) determining if S already has
an encrypted copy of F in its storage and (b) if so, securely
arranging to have the encryption key transferred to C from
some Ci who uploaded the original encrypted copy of F .
In traditional client-side deduplication, when C wants to
upload F to S, it ﬁrst sends a cryptographic hash h of F to S
so that it can check the existence of F in S’s storage. Na¨ıvely
adapting this approach to the case of encrypted storage is
insecure since a compromised S can easily mount an oﬄine
brute-force attack on the hash h if F is predictable. There-
fore, instead of h, we let C send a short hash sh = SH(F ).
Due to the high collision rate of SH(), S cannot use sh to
reliably guess the content of F oﬄine.

Now, suppose that another Ci previously uploaded E(kFi , Fi)

using kFi as the symmetric encryption key for Fi and that
sh = shi. Our protocol needs to determine if this happened
because F = Fi and, in that case, arrange to have kFi se-
curely transferred from Ci to C. We do this by having C and
Ci engage in an oblivious key sharing protocol which allows
C to receive kFi iﬀ F = Fi, and a random key otherwise. We
say that Ci plays the role of a checker in this protocol.
Our solution for an eﬃcient oblivious key sharing is having
C and Ci run a PAKE protocol, using the hash values of their
ﬁles, namely h and hi, as their respective “passwords”. The
protocol results in Ci getting ki and C getting k(cid:48)
i, which are
equal if h = hi and are independent otherwise. The next
step of the protocol uses these keys to deliver a key kF to C,
i. C uses this key to encrypt
which is equal to kFi iﬀ ki = k(cid:48)
its ﬁle, and S can deduplicate that ﬁle if the ciphertext is
equal to the one uploaded by Ci. Several additional issues
need to be solved:

1. How to prevent uploaders from learning about stored
ﬁles? Our protocol supports client-side deduplication,
and as such informs C whether deduplication takes
place. In order to solve the problem, we use the ran-
domized threshold strategy of [15] (see Section 5.1).

2. How to prevent a compromised S from mounting an
online brute-force attack where it initiates many inter-
actions with C/Ci to identify F /Fi. Each interaction
essentially enables a single guess about the content of
the target ﬁle. Clients therefore use a per-ﬁle rate
limiting strategy to prevent such attacks. Speciﬁcally,
they set a bound on the maximum number of PAKE
protocols they would service as a checker or an up-
loader for each ﬁle. (See Section 5.2.) Our simulations
with realistic datasets in Section 6 show that this rate
limiting does not aﬀect the deduplication eﬀectiveness.
3. What if there aren’t enough checkers? If S has a large
number of clients, it is likely to ﬁnd enough online

root

sh1

sh2

...

E(kF1 , F1)E(kF2 , F2)

...

C1

C2

...

Figure 5: S’s record structure.

checkers who have uploaded ﬁles with the required
short hash. If there are not enough checkers, we can
let the uploader run PAKE with the currently avail-
able checkers and with additional dummy checkers to
hide the number of available checkers (See Section 5.3).
Again, our experiments in Section 6 show that this
does not aﬀect the deduplication eﬀectiveness (since
the scheme is likely to ﬁnd checkers for popular ﬁles).
Relaxing Fdedup. The protocol we described implements
the Fdedup functionality of Figure 4 with the following re-
laxations: (1) S learns a short hash of the uploaded ﬁle F
(in our simulations we set the short hash to be 13 bits long).
(2) Fdedup is not applied between the uploader and all ex-
isting clients, but rather between the uploader and clients
which have uploaded ﬁles with the same short hash as F .
Therefore these clients learn that a ﬁle with the same short
hash is being uploaded.

We observe that in a large-scale system a short hash matches

many ﬁles, and uploads of ﬁles with any speciﬁc short hash
happen constantly. Therefore these relaxations leak limited
information about the uploaded ﬁles. For example, since
the short hash is random and short (and therefore matches
many uploaded ﬁles), the rate with which a client who up-
loaded a ﬁle is asked to participate in the protocol is rather
independent of whether the same ﬁle is uploaded again.

5. DEDUPLICATION PROTOCOL
In this section we describe our deduplication protocol in
detail. The data structure maintained by S is shown in
Figure 5. Cs who want to upload a ﬁle also upload the
corresponding short hash sh. Since diﬀerent ﬁles may have
the same short hash, a short hash sh is associated with the
list of diﬀerent encrypted ﬁles whose plaintext maps to sh.
S also keeps track of clients (C1, C2, . . .) who have uploaded
the same encrypted ﬁle.

Figure 6 shows the basic deduplication protocol. Note that
after the PAKE protocol (in Step 3), a na¨ıve solution would
be to just have Ci send E(ki, kFi ) to C. However, this would
enable a subtle attack by C to identify whether F has been
uploaded already.4 Note also in Step 5 C uses r inside the
4The problem with this approach is that C learns keys ki
for multiple other Cs, and should send S information about
each key, S then tells C which key index to use (and chooses
a random index if no match is found). A corrupt C might

8780. For each previously uploaded encrypted ﬁle E(H1(kFi ), Fi),a S also stores the corresponding short hash shi (which is

SH(Fi)).

1. Before uploading a ﬁle F , the uploader C calculates both the cryptographic hash h (which is H2(F )) and the short

hash sh (which is SH(F )), and sends sh to S.

2. S ﬁnds the checkers {Ci} who have uploaded ﬁles {Fi} with the same short hash sh. Then it asks C to run the

same-input-PAKE protocol with {Ci}b. C’s input is h and Ci’s input is hi.

3. After the invocation of the same-input-PAKE protocol, each Ci gets a session key ki and C gets a set of session keys

{k(cid:48)

i} corresponding to the diﬀerent Ci’s.c

4. Each Ci ﬁrst uses a pseudorandom function to extend the length of ki and then splits the result to kiL||kiR

kiL and (kFi + kiR) to S.e

d. It sends

5. For each k(cid:48)

i, C extends and splits it to k(cid:48)

iL||k(cid:48)

iR in the same way as Ci does. Then it sends S its public key pk, {k(cid:48)

iL}

and {Enc(pk, k(cid:48)

iR + r)} where r is a random element chosen by C from the plaintext group.

6. After receiving these messages from {Ci} and C, S checks if there is an index j such that kjL = k(cid:48)

jL.

(a) If so, S uses the homomorphic properties of the encryption to compute e = Enc(pk, kFj +kjR)(cid:9)Enc(pk, k(cid:48)
(b) Otherwise it sends e = Enc(pk, r(cid:48)), where r(cid:48) is a random element chosen by S from the plaintext group.

Enc(pk, kFj − r), and sends e back to C;

jR +r) =

7. C calculates kF = Dec(sk, e) + r, and sends E(H1(kF ), F ) to S.
8. S deletes E(H1(kF ), F ) if it is equal to a stored E(H1(kFj ), Fj), and then allows C to access E(H1(kFj ), Fj). Otherwise,

S stores E(H1(kF ), F ).

a kFi is a full-length element in the plaintext group of the additively homomorphic encryption scheme used by the protocol.
We use a cryptographic hash function H1 to hash down it to the length of the keys used by E().
bAll communication is run via S. There is no direct interaction between C and any Ci. C’s input to the same-input-PAKE
protocol was sent together with sh in Step 1.
cWith overwhelming probability, ki = k(cid:48)
d After extension, the result must be long enough to be divided into kiL||kiR that satisﬁes: (1) kiL is long enough so that
the probability of two random instances of this key having the same value is small (namely, |kiL| >> log N where N is
the number of clients participating in the protocol); (2) kiR is a full-length element in the plaintext group of the additively
homomorphic encryption scheme.
eAddition is done in the plaintext group of the additively homomorphic encryption scheme.

i iﬀ Fi = F .

Figure 6: The deduplication protocol.

encryption (rather than sending it in the clear) to prevent a
malicious S from being able to set the value of kF .

Implementation notes. We use the ﬁrst option of im-
plementing the same-input-PAKE protocol as described in
Section 2.4. We use AES as the pseudorandom function
used in Step 3. Most importantly, in order to improve the
performance of the additively homomorphic encryption, we
implement it using lifted ElGamal encryption as described
in Section 2.3. The encryption is modulo a prime p of length
2048 bits (although encryption can also be done using ECC).
As a result, kF , kFi , kiR and k(cid:48)
iR are 2048 bits elements.
This requires us to apply the following modiﬁcations to the
protocol in our implementation:

• In Step 4, Ci sends g(kFi
+kiR) instead of (kFi + kiR);
• In Step 5, C sends S a lifted ElGamal encryption of

iR + r, i.e. an ElGamal encryption of gk(cid:48)
k(cid:48)

iR+r;

replace some keys with dummy values. If it is then told by
S to use an index of one of these keys then it knows that no
match was found. The protocol must therefore send back
to C a key without specifying the index to which this key
corresponds.

• In Step 6a, Enc(pk, kFj + kjR) is a lifted encryption of
+kjR);
• Similarly, in Step 6b, Enc(pk, r(cid:48)) is a lifted ElGamal

(kFj + kjR), i.e. an ElGamal encryption of g(kFj

encryption of r(cid:48);

• In Step 7, C calculates gkF = Dec(sk, e) · gr, where
. Then, C uses H1(gkF ) as
Dec(sk, e) is gkFj
the encryption key for F , and uses gkF as the input
when it acts as a checker. Note that C knows nothing
about kF (similarly, Ci knows nothing about kFi ).

−r or gr(cid:48)

Theorem 1. The deduplication protocol in Figure 6 im-
plements Fdedup with security against malicious adversaries,
if the same-input-PAKE protocol is secure against malicious
adversaries, the additively homomorphic encryption is se-
mantically secure and the hash function H2 is modeled as a
random oracle.

Proof for Theorem 1 is shown in the full version of this

paper [17].

Based on Theorem 1, we conclude that compromised par-
ties cannot run the computation in a way that is diﬀerent
than is deﬁned by (relaxed) Fdedup. This makes our scheme

879satisfy the requirement S1 and S3. We now discuss several
extensions to the basic protocol to account for the types of
issues we alluded to in Section 4.
5.1 Randomized Threshold

The protocol in Figure 6 is for server-side deduplication.
To save bandwidth, we transform it to support client-side
deduplication. In order to satisfy requirement S2 and pro-
tect against a corrupt uploader, we use the randomized thresh-
old approach from Harnik et al. [15]: for each ﬁle F , S main-
tains a random threshold tF (tF ≥ 2), and a counter cF that
indicates the number of Cs that have previously uploaded F .

In step 6 of the deduplication protocol,
• In the case of a match (6a), if cFi < tFi , S tells C to
upload E(kF , F ) as if no match occurred (but S does
not store this copy). Otherwise, S informs C that the
ﬁle is duplicated and there is no need to upload it;

• In the case of a no match (6b), S asks C to upload

E(kF , F ).

5.2 Rate Limiting
A compromised active S can apply online brute-force at-
tacks against C or Ci. Speciﬁcally, if Fi is predictable, S can
pretend to be an uploader sending PAKE requests to Ci to
guess Fi. S can also pretend to be a checker sending PAKE
responses to C to guess F . Therefore both uploaders and
checkers should limit the number of PAKE runs for each ﬁle
in their respective roles. This per-ﬁle rate limiting strategy
can both improve security (see below) and reduce overhead
(namely the number of PAKE runs) without damaging the
deduplication eﬀectiveness (as shown in Section 6).
We use RLc to denote the rate limit for checkers, i.e., each
Ci can process at most RLc PAKE requests for Fi and will
ignore further requests. Similarly, RLu is the rate limit for
uploaders, i.e., a C will send at most RLu PAKE requests
to upload F . Suppose that n is the length of the short hash
and m is the min-entropy of a predictable ﬁle F , and x is
the number of clients who potentially hold F . Our scheme
can prevent online brute-force attacks from S if

2m > 2n · x · (RLu + RLc)

(1)
because S can run PAKE with all owners of F to conﬁrm its
guesses. So the uncertainty in a guessable ﬁle (2m−n) must
be larger than x times per-ﬁle rate limit (RLu + RLc).

As a comparison, DupLESS [3] uses a per-client rate limit-
ing strategy to prevent such online brute-force attacks from
any single C. The rate limit must still support a client C
that needs to legitimately upload a large number of ﬁles
within a brief time interval (such as backing up a local ﬁle
system). Therefore the authors of DupLESS chose a large
bound (825 000) for the total number of requests a single C
can make during one week. The condition for DupLESS to
resist online brute-force attack by a single C is

2m > y · RL

(2)

where y is the number of compromised clients and RL is the
rate limit (825 000/week) for each client. Recall also, that a
compromised active S can masquerade as any number of Cs
that is needed, and therefore y could be as large as possible.
Consequently, DupLESS cannot fully deal with an online-
brute force attack by a compromised S.

To prevent a compromised active S from masquerading
multiple Cs, the authors of [23] introduce another indepen-
dent party called identity server. When Cs ﬁrst join the
system, the identity server is responsible for verifying their
identity and issuing credentials to them. However, it is hard
to deploy an independent identity server in a real world set-
ting. As far as we know, our scheme is the ﬁrst deduplication
protocol that can prevent online brute-force attacks (satis-
fying requirement S4) without the aid of an identity server.
5.3 Checker Selection

If an uploader is required to run only a few (or none)
PAKE instances, due to no short hash matches, it will learn
that it is less likely that its ﬁle is already in S’s storage.
To avoid this leakage of information, S ﬁxes the number
of PAKE runs (i.e., RLu) for an upload request to be con-
stant. For a requested short hash sh, checkers are selected
according to the following procedure:

1. S selects the most popular ﬁle among the ﬁles whose
short hash is sh and which were not already selected
with respect for the current upload (popularity is mea-
sured in terms of the number of Cs who own the ﬁle).
2. S selects a checker for that ﬁle in ascending order of
engagement among the Cs that are currently online
(in terms of the number of PAKE requests they have
serviced so far for that speciﬁc ﬁle).

3. If the number of selected ﬁles is less than RLu, repeat

Step 1-3.

4. If the total number of selected ﬁles for which there are
online clients is smaller than RLu, S uses additional
dummy ﬁles and clients, until reaching RLu ﬁles.

Then, S lets the uploader run PAKE instances with the
selected RLu clients (S itself runs as dummy clients).

6. SIMULATION

Our use of rate limiting can impact deduplication eﬀec-
tiveness. In this section, we use realistic simulations to study
the eﬀect of various parameter choices in our protocol on
deduplication eﬀectiveness.
Datasets. We want to consider two types of storage envi-
ronments. The ﬁrst consists predominantly of media ﬁles,
such as audio and video ﬁles from many users. We did not
have access to such a dataset.
Instead, we use a dataset
comprising of Android application prevalence data to repre-
sent an environment with media ﬁles. This is based on the
assumption that the popularity of Android applications is
likely to be similar to that of media ﬁles: both are created
and published by a small number of authors (artists/developers),
made available on online stores or other distribution sites,
and are acquired by consumers either for free or for a fee.
We call this the media dataset. We use a publicly available
dataset5. It consists of data collected from 77 782 Android
devices. For each device, the dataset identiﬁes the set of
(anonymized) application identiﬁers found on that device.
We treat each application identiﬁer as a “ﬁle” and consider
the presence of an app on a device as an “upload request”
to add the corresponding “ﬁle” to the storage. This dataset

5https://se-sy.org/projects/malware/

880(and hence setting RLc = 70), maximizes ρ to be (97.58%
and 99.9332%, respectively. These values are extremely close
to the perfect deduplication percentages in both datasets
(97.59% and 99.9339% respectively). A major conclusion is
that rate limiting can improve security and reduce overhead
without negatively impacting deduplication eﬀectiveness.

has 7 396 235 “upload requests” in total, of which 178 396
are for distinct ﬁles.

The second is the type of storage environments that are
found in enterprise backup systems. We use data gathered
by the Debian Popularity Contest6 to approximate such an
environment. The popularity-contest package on a Debian
device regularly reports the list of packages installed on that
device. The resulting data consists of a list of debian pack-
ages along with the number of devices which reported that
package. We took a snapshot of this data on Nov 27, 2014. It
consists of data collected from 175 903 Debian users. From
this data we generated our enterprise dataset:
it has 217
927 332 “upload requests” (debian package installations) of
which 143 949 are for distinict ﬁles (unique packages).

Figure 7 shows the ﬁle popularity distribution (i.e., the
number of upload requests for each ﬁle) in logarithmic scale
for both datasets. We map each dataset to a stream of
upload requests by generating the requests in random order,
where a ﬁle that has x copies generates x upload requests at
random time intervals.

Figure 8: Dedup. percentage VS. rate limits.

Figure 7: File popularity in both datasets.

Parameters. To facilitate comparison with DupLESS [3],
we set the min-entropy to log(825000). We then set the
length of the short hash n = 13, and (RLu + RLc) = 100
(i.e., a C will run PAKE at most 100 times for a certain ﬁle
as both uploader and checker), so that we achieve the bound
in inequality 2 in Section 5.2: A cannot uniquely identify a
ﬁle within the rate limit. We use these parameters in our
simulations.

We measure deduplication eﬀectiveness using the dedupli-
cation percentage (Section 2.1). We assume that all ﬁles are
of equal size so that the deduplication percentage ρ is:

ρ = (1 − N umber of all f iles in storage
T otal number of upload requests

) · 100% (3)
Rate limiting. We ﬁrst assume that all Cs are online during
the simulation, and study the impact of rate limits. Hav-
ing selected RLu+RLc to be 100, we now see how selecting
speciﬁc values for RLu and RLc aﬀects the deduplication
eﬀectiveness. Figure 8 shows ρ resulting from diﬀerent rate
limit choices in both datasets. We see that setting RLu = 30

6http://popcon.debian.org

Figure 9: Dedup. percentage VS. oﬄine rates.

Oﬄine rate. The possibility of some Cs being oﬄine may
adversely impact deduplication eﬀectiveness. To estimate
this impact, we assign an oﬄine rate to each C as its prob-
ability to be oﬄine during one upload request. Using the
chosen rate limits (RLu = 30 and RLc = 70), we measured
ρ by varying the oﬄine rate. The results for both datasets
are shown in Figure 9. It shows that ρ is still reasonably high
if the oﬄine rate is lower than 70%. But drops quickly be-
yond that. We can solve this by introducing deferred check,
which we discuss in the full version of this paper [17].
Evolution of deduplication eﬀectiveness. Figure 10
shows that the ρ achieved by our scheme increases as more
ﬁles are added to the storage, and it meets the realistic ex-
pectation (95%) quickly: after receiving 304 160 (4%) upload

File ID100101102103104105106Number of Upload Requests100101102103104105106File popularity in media datasetFile popularity in enterprise datasetRLu(RLc)Deduplication Percentage %96.59797.59898.59999.510010(90)20(80)30(70)40(60)50(50)60(40)70(30)80(20)90(10)Deduplication percentage in media datasetDeduplication percentage in enterprise datasetPerfect deduplication in media datasetPerfect deduplication in enterprise datasetOffline Rate00.10.20.30.40.50.60.70.80.9Deduplication Percentage %93949596979899100Deduplication percentage in media datasetDeduplication percentage in enterprise dataset881the server S;

run PAKE with C;

{Ci};

2. S forwards requests to 30 checkers Ci and lets them

3. S waits for responses in all instances back from C and

We used the GNU multiple precision arithmetic library9 to
implement the public key operations. The additively homo-
morphic encryption scheme used in the protocol is ElGamal
encryption where the plaintext is encoded in the exponent,
see Section 5.
Test setting and methodology. We ran the server-side
program on a remote server (Intel Xeon with 4 2.66 GHz
cores) and the client-side program on an Intel Core i7 ma-
chine with 4 2.2 GHz cores. We measured the running time
using the Date module in Javascript and measured the band-
width usage using TCPdump..

As the downloading phase in our protocol is simply down-
loading an encrypted ﬁle, we only consider the uploading
phase. We set the length of short hash to be 13, and set
RLu = 30. We considered the case where the uploader C
runs PAKE with 30 checkers Ci. So we simulate the upload-
ing phase in our protocol as:

1. C sends the short hash of the ﬁle it wants to upload to

4. S chooses one instance and sends the result to C;
5. C uses the resulting key to encrypt its ﬁle with AES

and uploads it to S.

We measured both running time and bandwidth usage
during the whole procedure above. Network delay for all
parties was included in the ﬁnal results. We compare the
results to two baselines: (1) uploading without encryption
and (2) uploading with AES encryption. As in [3], we repeat
our experiment using ﬁles of size 22i KB for i ∈ {0, 1, ..., 8},
which provides a ﬁle size range of 1KB to 64 MB. For each
ﬁle, we upload it 100 times and calculate the mean. For ﬁles
that are larger than the computer buﬀer, we do loading,
encryption and uploading at the same time by pipelining
the data stream. As a result, uploading encrypted ﬁles uses
almost the same amount of time as uploading plain ﬁles.
Results. Figure 12 reports the uploading time and band-
width usage in our protocol compared to the two baselines.
For ﬁles that are smaller than 1 MB, the overhead intro-
duced by our deduplication protocol is relatively high. For
example, it takes 15 ms (2 508 bytes) to upload a 1 KB en-
crypted ﬁle, while it takes 319 ms (145 359 bytes) to upload
the same ﬁle in our protocol. However, the overhead intro-
duced by our protocol is independent of the ﬁle size (about
10 ms for each PAKE run), and becomes negligible when the
ﬁle is large enough. For ﬁles that are larger than 64 MB ﬁle,
the time overhead is below 2%, and the bandwidth overhead
is below 0.16%. So our scheme meets F2.

8. RELATED WORK

There are several types of schemes that enable deduplica-
tion with client-side encrypted data. The simplest approach
(which is used by most commercial products) is to encrypt
Cs’ ﬁles using a global key which is encoded in the client-
side software. As a result, diﬀerent copies of F result in the

9https://gmplib.org

Figure 10: Dedup. percentage VS. number of up-
load requests.

requests in the media dataset, and 121 110 (0.05%) upload
requests in the enterprise dataset. Given that the dedupli-
cation eﬀectiveness of our scheme is close to that of perfect
deduplication and exceeds typical expected values, we can
conclude that it satisﬁes functionality goal F1. Using rate
limits implies that ρ increases more slowly in our scheme
than in perfect deduplication. Figure 11 shows that this dif-
ference stablizes as the number of upload requests increases.
Explanation. The fact that our scheme achieves close
to perfect deduplication even in the presence of rate lim-
its may appear counter-intuitive at ﬁrst glance. But this
phenomenon can be explained by Zipf ’s law [26]. As seen
from Figure 7, beyond the initial plateau, the ﬁle popularity
distribution is a straight line and thus follows a power law
distribution (also known as Zipf distribution).
Even though we use rate limits, S always selects ﬁles based
on descending order of popularity (step 1 in Section 5.3).
Since ﬁle popularity follows the Zipf distribution, selecting
ﬁles based on popularity ensures that popular uploaded ﬁles
have a much higher likelihood of being selected and thus
deduplicated. There are other examples of using the Zipf
distribution to design surprisingly eﬃcient systems. Web
proxy caching proxies are such an example [8].

7. PERFORMANCE EVALUATION

Our deduplication scheme incurs some extra computation
and communication due to the number of PAKE runs. In
this section, we demonstrate that the overhead is negligible
for large ﬁles by implementing a proof-of-concept prototype.
Prototype. Our prototype consists of two parts: (1) a
server program which simulates S and (2) a client program
which simulates C (performing ﬁle uploading/downloading,
encryption/decryption, and assisting S in deduplication).
We used Node.js7 for the implementation of both parties,
and Redis8 for the implementation of S’s data structure.
We used SHA-256 as the cryptographic hash function and
AES with 256-bit keys as the symmetric encryption scheme,
both of which are provided by the Crypto module in Node.js.
7http://nodejs.org
8http://redis.io

Number of Upload Requests (x30 in enterprise dataset)#106012345678Deduplication Percentage %0102030405060708090100Evolution of deduplication percentage in media datasetEvolution of deduplication percentage in enterprise dataset882Figure 11: Deviation from perfect deduplication VS. Number of upload requests.

Figure 12: Time (left) and bandwidth usage (right) VS. ﬁle size.

same ciphertext and can therefore be deduplicated. This
approach is, of course, insecure if S is untrusted.

Another approach is convergent encryption [10], which
uses H(F ) as a key to encrypt F , where H() is a publicly
known cryptographic hash function. This approach ensures
that diﬀerent copies of F result in the same ciphertext. How-
ever, a compromised passive S can perform an oﬄine brute-
force attack if F has a small (or medium) entropy. Bellare et
al. proposed message-locked encryption (MLE), which uses
a semantically secure encryption scheme but produces a de-
terministic tag [4]. So it still suﬀers from the same attack.
Other solutions are based on the aid of additional inde-
pendent servers (ISs). For example, Cloudedup is a dedu-
plication system that introduces an IS for encryption and
decryption [21]. Speciﬁcally, C ﬁrst encrypts each block with
convergent encryption and sends the ciphertexts to IS, who
then encrypts them again with a key only known by itself.
During ﬁle retrieval, blocks are ﬁrst decrypted by IS and

sent back to C.
In this scheme, a compromised active S
can easily perform an online brute-force attack by upload-
ing guessing ﬁles and see if deduplication happens.
Stanek et al. propose a scheme that only deduplicates pop-
ular ﬁles [23]. Cs encrypt their ﬁles with two layers of encryp-
tion: the inner layer is obtained through convergent encryp-
tion, and the outer layer is obtained through a semantically
secure threshold encryption scheme with the aid of an IS.
S can decrypt the outer layer of F iﬀ the number of Cs who
have uploaded F reaches the threshold, and thus perform
a deduplication. In addition, they introduce another IS as
an identity server to prevent online brute-force attacks by
multiple compromised Cs.
Both [21] and [23] are vulnerable to oﬄine brute-force at-
tacks by compromised ISs. To prevent this, Bellare et al.
propose DupLESS that enables Cs to generate ﬁle keys by
running an oblivious pseudorandom function (OPRF) with
IS. Speciﬁcally, in the key generation process of convergent

Number of Upload Requests#106012345678Deviation in Deduplication Percentage %00.050.10.150.20.250.30.350.40.45Deviation from perfect deduplication in media datasetNumber of Upload Requests#10800.511.522.5Deviation in Deduplication Percentage %#10-30123456789Deviation from perfect deduplication in enterprise datasetFile Size (KB)Time Usage (Millisecond)216214212210282624222021221421621028262422Our protocol (with 30 PAKE runs)Upload encrypted filesUpload plain filesFile Size (KB)Bandwidth Usage (Byte)2162142122102826242220218220222224226228216214212210Our protocol (with 30 PAKE runs)Upload encrypted filesUpload plain files883encryption, they introduce another secret which is provided
by IS and identical for all Cs. The OPRF enables Cs to
generate their keys without revealing their ﬁles to IS, and
without learning anything about IS’s secret. To prevent
the online brute-force attacks from compromised active S.
DupLESS uses a per-client rate limiting strategy to limit
the number of requests that a C can send to IS during each
epoch. We have identiﬁed the limitations for this strategy in
Section 5.2. In addition, if A compromises both S and IS,
it can get the secret from IS, and the scheme is reduced to
normal convergent encryption.
Duan proposes a scheme that uses the same idea as Du-
pLESS, but distributes the task of IS [11], where a C must
interact with a threshold of other Cs to generate the key.
So this scheme is only suitable for peer-to-peer paradigm:
a threshold number of Cs must be online and interact with
one another. While improving availability and security com-
pared to DupLESS, this scheme is still susceptible to online
brute-force attacks by compromised active S, and it is un-
clear how to apply any rate-limiting strategy to it.

In Table 1, we summarize the resilience of these schemes

with respect to the design goals from Section 3.3.

XXXXXXXXX

Schemes

Threat

[10], [4]

[21]
[23]
[3]
[11]

Our work

C
√
√
√
√
√
√

Compromised

S (pas.)

S (act.)

√
X
√
√
√
√

X
√
X

X
√
X

ISs
−
X
√
X
−
−

S, ISs

−
X
X
X
−
−

Table 1: Resilience of deduplication schemes.

9. DISCUSSION
Incentives. In our scheme Cs have to run several PAKE in-
stances as both uploaders and checkers. This imposes a cost
on each C. S is the direct beneﬁciary of deduplication. Cs
may indirectly beneﬁt in that eﬀective deduplication makes
the storage system more eﬃcient and can thus potentially
lower the cost incurred by each C. Nevertheless, it is desir-
able to have more direct incentive mechanisms to encourage
Cs to do do PAKE checks. For example, if a ﬁle uploaded
by C is found to be shared by other Cs, S could reward the
Cs owning that ﬁle by giving them small increases in their
respective storage quotas.
User involvement. A simple solution to the problem of
deduplication vs. privacy is to have the user identify sen-
sitive ﬁles. The client-side program can then use conver-
gent encryption for non-sensitive ﬁles and semantically se-
cure encryption for sensitive ﬁles. This approach has three
drawbacks:
it is too burdensome for average users, it re-
veals which ﬁles are sensitive, and it foregoes deduplication
of sensitive ﬁles altogether.
Deduplication eﬀectiveness. We can improve dedupli-
cation eﬀectiveness by introducing additional checks. For
example, an uploader can indicate the (approximate) ﬁle
size (which will be revealed anyway) so that S can limit
the selection of checkers to those whose ﬁles are of a similar
size. Similarly, S can keep track of similarities between Cs
based on the number of ﬁles they share and use this infor-
mation while selecting checkers by prioritizing checkers who

are similar to the uploader. Nevertheless, as discussed in
Figure 10, our scheme exceeds what is considered as realistic
levels of deduplication early in the life of the storage system.
Whitehouse [25] reported that when selecting a deduplica-
tion scheme, enterprise administrators rated considerations
such as ease of deployment and of use being more important
than deduplication ratio. Therefore, we argue that the very
small sacriﬁce in deduplication ratio is oﬀset by the signif-
icant advantage of ensuring user privacy without having to
use independent third party servers.
Block-level deduplication. Our scheme can be applied
for both ﬁle-level and block-level deduplication. Applying it
for block-level deduplication will incur more overhead.
Datasets. Deduplication eﬀectiveness is highly dependent
on the dataset. Analysis using more realistic datasets can
shed more light on the eﬃcacy of our scheme.
In our analysis
Realistic modeling of oﬄine status.
of how deduplication eﬀectiveness is aﬀected by the oﬄine
rate (Figure 9), we assumed a simple model where the oﬄine
status of clients is distributed uniformly at a speciﬁed rate.
In practice the oﬄine status is inﬂuenced by many factors
like geography and time of day.

10. CONCLUSIONS

In this paper, we dealt with the dilemma that cloud stor-
age providers want to use deduplication to save cost, while
users want their data to be encrypted on client-side. We de-
signed a PAKE-based protocol that enables two parties to
privately compare their secrets and share the encryption key.
Based on this protocol, we developed the ﬁrst single-server
scheme that enables cross-user deduplication of client-side
encrypted data.

Acknowledgments
This work was supported in part by the “Cloud Security Ser-
vices” project funded by the Academy of Finland (283135),
the EU 7th Framework Program (FP7/2007-2013) under
grant agreement n. 609611 (PRACTICE) and a grant from
the Israel Ministry of Science and Technology. We thank
Ivan Martinovic for suggesting the analogy between our sys-
tem and web-caching proxies. We thank Billy Brumley,
Kaitai Liang, and the reviewers for their valuable feedback.

11. REFERENCES
[1] M. Abdalla and D. Pointcheval. Simple

password-based encrypted key exchange protocols. In
A. Menezes, editor, CT-RSA, volume 3376 of LNCS,
pages 191–208. Springer, 2005.

[2] A. Afshar, P. Mohassel, B. Pinkas, and B. Riva.

Non-interactive secure computation based on
cut-and-choose. In P. Q. Nguyen and E. Oswald,
editors, EUROCRYPT, volume 8441 of LNCS, pages
387–404. Springer, 2014.

[3] M. Bellare, S. Keelveedhi, and T. Ristenpart.

DupLESS: Server-aided encryption for deduplicated
storage. In USENIX Security, pages 179–194. USENIX
Association, 2013.

[4] M. Bellare, S. Keelveedhi, and T. Ristenpart.

Message-locked encryption and secure deduplication.
In EUROCRYPT, volume 7881 of LNCS, pages
296–312. Springer, 2013.

884[5] M. Bellare, D. Pointcheval, and P. Rogaway.

[22] S. Quinlan and S. Dorward. Venti: A new approach to

Authenticated key exchange secure against dictionary
attacks. In Preneel [20], pages 139–155.

archival storage. In USENIX FAST, pages 7–7.
USENIX Association, 2002.

[6] S. M. Bellovin and M. Merritt. Encrypted key

exchange: password-based protocols secure against
dictionary attacks. In IEEE Computer Society
Symposium on Research in Security and Privacy,
pages 72–84. IEEE Computer Society, 1992.

[7] V. Boyko, P. D. MacKenzie, and S. Patel. Provably

secure password-authenticated key exchange using
diﬃe-hellman. In Preneel [20], pages 156–171.

[8] L. Breslau, P. Cao, L. Fan, G. Phillips, and

S. Shenker. Web caching and zipf-like distributions:
evidence and implications. In INFOCOM, volume 1,
pages 126–134, Mar 1999.

[9] R. Canetti, S. Halevi, J. Katz, Y. Lindell, and P. D.
MacKenzie. Universally composable password-based
key exchange. In EUROCRYPT, pages 404–421, 2005.
[10] J. R. Douceur, A. Adya, W. J. Bolosky, P. Simon, and
M. Theimer. Reclaiming space from duplicate ﬁles in a
serverless distributed ﬁle system. In ICDCS, pages
617–624. IEEE, 2002.

[11] Y. Duan. Distributed key generation for encrypted
deduplication: Achieving the strongest privacy. In
CCSW, pages 57–68. ACM, 2014.

[12] M. Dutch. Understanding data deduplication ratios.

SNIA Data Management Forum, 2008.
http://storage.ctocio.com.cn/imagelist/2009/
222/l3pm284d8r1s.pdf.

[13] T. ElGamal. A public key cryptosystem and a

signature scheme based on discrete logarithms. In
G. Blakley and D. Chaum, editors, CRYPTO, volume
196 of LNCS, pages 10–18. Springer, 1985.

[14] O. Goldreich. Foundations of Cryptography: Volume 2,
Basic Applications. Cambridge University Press, 2004.

[15] D. Harnik, B. Pinkas, and A. Shulman-Peleg. Side
channels in cloud services: Deduplication in cloud
storage. IEEE Security & Privacy, 8(6):40–47, Nov
2010.

[16] C. Hazay and Y. Lindell. Eﬃcient Secure Two-Party

Protocols - Techniques and Constructions. Information
Security and Cryptography. Springer, 2010.

[17] J. Liu, N. Asokan, and B. Pinkas. Secure

deduplication of encrypted data without additional
servers. Technical Report 455, ePrint archive, May,
2015. https://eprint.iacr.org/2015/455.

[18] D. T. Meyer and W. J. Bolosky. A study of practical

deduplication. In USENIX FAST, pages 1–1. USENIX
Association, 2011.

[19] P. Paillier. Public-key cryptosystems based on

composite degree residuosity classes. In J. Stern,
editor, EUROCRYPT, volume 1592 of LNCS, pages
223–238. Springer, 1999.

[20] B. Preneel, editor. EUROCRYPT, volume 1807 of

LNCS. Springer, 2000.

[21] P. Puzio, R. Molva, M. ¨Onen, and S. Loureiro.

Cloudedup: Secure deduplication with encrypted data
for cloud storage. In CloudCom, pages 363–370. IEEE
Computer Society, 2013.

[23] J. Stanek, A. Sorniotti, E. Androulaki, and L. Kencl.
A secure data deduplication scheme for cloud storage.
In N. Christin and R. Safavi-Naini, editors, FC,
volume 8437 of LNCS, pages 99–118. Springer, 2014.

[24] J. M. Wendt. Getting Real About Deduplication

Ratios. http://www.dcig.com/2011/02/
getting-real-about-deduplication.html, 2011.
[25] L. Whitehouse. Understanding data deduplication
ratios in backup systems. TechTarget article, May
2009.
http://searchdatabackup.techtarget.com/tip/
Understanding-data-deduplication-ratios-in\
-backup-systems.

[26] G. K. Zipf. Relative frequency as a determinant of

phonetic change. Harvard studies in classical philology,
pages 1–95, 1929.

APPENDIX
A. NOTATION TABLE

A table of notations is shown in Table 2.

Notation

EntitiesC

S
A
IS

Description

Client
Server

Adversary

Independent Server

Cryptographic Notations

E()
D()

k

Enc()
Dec()

⊕
(cid:9)
H()

h

SH()

sh
PAKE
Fpake
Fdedup
Parameters

F
m
n

RLu
RLc
tF
cF
ρ

Symmetric key encryption
Symmetric key decryption

Symmetric encryption/decryption key
Additively homomorphic encryption
Additively homomorphic decryption

Additively homomorphic addition

Additively homomorphic subtraction

Cryptographic hash function

Cryptographic hash
Short hash function

Short hash

Password Authenticated Key Exchange

Ideal functionality of PAKE

Ideal functionality of deduplication protocol

File

Entropy of a predictable ﬁle

Length of the short hash
Rate limit by uploaders
Rate limit by checkers

Random threshold for a ﬁle

Counter for a ﬁle

Deduplication Percentage

Table 2: Summary of notations

885