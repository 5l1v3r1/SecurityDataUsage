Subsampled Exponential Mechanism:

Differential Privacy in Large Output Spaces

Eric Lantz

Kendrick Boyd

Department of Computer

Department of Computer

Sciences

University of

Sciences

University of

David Page

Department of Biostatistics

and Medical Informatics

University of

Wisconsin-Madison

lantz@cs.wisc.edu

Wisconsin-Madison

boyd@cs.wisc.edu

Wisconsin-Madison

page@biostat.wisc.edu

ABSTRACT
In the last several years, diﬀerential privacy has become
the leading framework for private data analysis.
It pro-
vides bounds on the amount that a randomized function
can change as the result of a modiﬁcation to one record
of a database. This requirement can be satisﬁed by using
the exponential mechanism to perform a weighted choice
among the possible alternatives, with better options receiv-
ing higher weights. However, in some situations the number
of possible outcomes is too large to compute all weights eﬃ-
ciently. We present the subsampled exponential mechanism,
which scores only a sample of the outcomes. We show that
it still preserves diﬀerential privacy, and fulﬁlls a similar ac-
curacy bound. Using a clustering application, we show that
the subsampled exponential mechanism outperforms a pre-
viously published private algorithm and is comparable to the
full exponential mechanism but more scalable.

1.

INTRODUCTION

Diﬀerential privacy has been explored as a framework for
data privacy in many scenarios, including data release [4,12,
21], machine learning [10, 19, 22], auctions [18], and graph
analysis [3, 14, 15]. For any two databases diﬀering in one
record, the ability of an adversary to determine the eﬀect of
the diﬀering record is tightly bounded.

In this paper, we examine an extension to the standard ex-

ponential mechanism for enforcing diﬀerential privacy. Rough-
ly speaking, in the exponential mechanism the possible out-
comes are scored by their utility, and instead of choosing
the one with the highest score, a noisy choice is made in
such a way that all solutions have non-zero probability of
being selected. There are many cases in which the number
of possible outcomes to a query are exponential, factorial,
or even inﬁnite. The exponential mechanism would typically
require us to evaluate the quality of every possible outcome
in order to calculate an appropriately-weighted distribution
from which to select an outcome. This limits diﬀerential pri-

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
AISec’15, October 16, 2015, Denver, Colorado, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3826-4/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2808769.2808776 .

vacy to cases where the outcome is numeric (falling under a
special case called the Laplace mechanism) or is a member
of a relatively small set. Nonetheless, it may be quite easy
to sample an outcome from a very large distribution. We
propose to sample from the space of possible outcomes, and
choose an outcome from among this sample of the outcome
space. We prove that the mechanism still preserves diﬀeren-
tial privacy, since the action is independent of the database.
In addition, following a result from [18], we show that sam-
pling the output space results in only a moderately weaker
accuracy bound.

While the proposed mechanism is quite simple,

it can
prove surprisingly eﬀective. We examine the private k-median
problem, in which a set of cluster centers must be selected
taking into account a private subset of data points. There
are a factorial number of k-subsets, so approximate solutions
are typically used even in a non-private setting. An existing
diﬀerentially private solution has been proposed [11], which
uses a local search algorithm to ﬁnd possible cluster centers.
We show that our algorithm performs signiﬁcantly better
than theirs, and with a much faster runtime.

In addition, we explore an aspect of diﬀerential privacy
that is often overlooked. The exponential mechanism is de-
ﬁned using a base measure over the outcome space. This is
typically assumed to be uniform, but it need not be. It need
only be independent of the data itself. We show how utilizing
a non-uniform base distribution improves the eﬀectiveness of
our algorithm when applied to the private k-median task.

In the next section, we present formal background on dif-
ferential privacy.
In Section 3 we present the subsampled
exponential mechanism, as well as proofs about its privacy
and accuracy. Section 4 contains experiments examining the
eﬀectiveness of our mechanism.

2. BACKGROUND

In diﬀerential privacy, the presence or absence of a record
in the database is guaranteed to have a small eﬀect on the
output of an algorithm. As a result, the amount of informa-
tion an adversary can learn about a single record is limited.
For any databases D, D′ ∈ D, let D and D′ be considered
neighbors if they diﬀer by exactly one record (denoted by
D′ ∈ nbrs(D)). Diﬀerential privacy requires that the prob-
ability an algorithm outputs the same result on any pair of
neighboring databases is bounded by a constant ratio.

Definition 1. (ǫ-diﬀerential privacy [8]): For any input
database D ∈ D, a randomized algorithm f : D → Z where

25Z = Range(f ) is ǫ-diﬀerentially private iﬀ for any S ⊆ Z
and any database D′ ∈ nbrs(D).

Pr(f (D) ∈ S) ≤ eǫ Pr(f (D′) ∈ S)

(1)

Mechanisms for ensuring diﬀerential privacy rely on the
sensitivity of the function we want to privatize. Sensitivity
is the largest diﬀerence between the output on any pair of
neighboring databases.

Definition 2. (Sensitivity [8]): Given a function f :

D → Rd, the sensitivity (∆) of f is:

∆f = max

D′ ∈nbrs(D)

kf (D) − f (D′)k1

(2)

A sequence of diﬀerentially private computations also en-
sures diﬀerential privacy. This is called the composition
property of diﬀerential privacy as shown in Theorem 1.

Theorem 1. (Composition [8]): Given a sequence of com-
putations f = f1,. . .,fd, with fi meeting ǫi-diﬀerential pri-

i=1 ǫi)-diﬀerentially private.

vacy, then f is (Pd

When the outcome domain is real-valued, it is possible
to add noise directly to the non-private value. Using noise
drawn from the Laplace distribution (sometimes called the
double exponential distribution) to perturb any real-valued
query gives the following result:

q1

q2

0.0

0.5
q

1.0

Figure 1: Two example distributions for the q func-
tion. Of these two distributions, the subsampled
exponential mechanism will produce more useful
(higher q value) outputs when applied to q1 than
to q2 because of the long right tail on the q2 distri-
bution.

3. SUBSAMPLED EXPONENTIAL

MECHANISM

The subsampled exponential mechanism is an extension to
the exponential mechanism in which all possible outcomes
do not need to be evaluated. Instead we require some way
to sample outcomes from the outcome space, and run the
exponential mechanism among a limited sample. Note that
the sampling is done over the output space, not the records
in the database, as has been discussed in [6, 7, 9, 13, 17, 20].
Since the exponential mechanism itself could be considered a
weighted sampling over the output space, we call our mech-
anism the subsampled exponential mechanism.

Theorem 2. (Laplace noise [8]): Given a function f :

Definition 3

(Subsampled Exponential Mechanism).

D → R, the computation

ǫ (cid:19)
f ′(D) = f (D) + Laplace(cid:18) ∆f

(3)

guarantees ǫ-diﬀerential privacy.

Given a quality function q : D × Z → R that assigns a score
to each outcome z ∈ Z and a database-independent base dis-
tribution on outputs µ(Z), the subsampled exponential mech-
anism draws m independent samples z1, z2, ..., zm ∼ µ(Z)
and uses the exponential mechanism (with equal weight on
the m samples). That is, after selecting the m samples,

For domains that are not real-valued, the exponential mech-

anism can be used to select among outputs.

P r(M (D) = z) =

Theorem 3. (Exponential mechanism [18]): Given a qual-

ity function q : (D × Z) → R that assigns a score to each
outcome z ∈ Z for a given database, and base measure µ
over Z, a randomized algorithm, M : D → Z, that outputs
z∗ with probability

P r(M (D) = z∗) =

is 2ǫ∆q-diﬀerentially private.

eǫq(D,z∗)µ(z∗)

RZ eǫq(D,z)µ(z′)dz

(4)

The exponential mechanism subsumes the Laplace mech-
anism as a special case where q(D, z) = −|f (D) − z| [18].
However, this only holds when µ(z) is uniform.

One problem with the exponential mechanism is that it
requires the quality function to be computed for every pos-
sible outcome. For example, if the domain of the outcomes
is combinatorial, enumeration of all outcomes becomes im-
practical. In this paper we explore the case where the size
of the output domain limits the utility of the exponential
mechanism.

eǫq(D,z)
i=1 eǫq(D,zi)

Pm

(5)

The success of the subsampled exponential mechanism
will be related to the distribution over outcomes of the qual-
ity function q as applied to a particular database. Figure 1
shows two theoretical distributions of q, where q1 has more
probability mass shifted towards the maximum value of 1.
The subsampled exponential mechanism will be more likely
to sample an output with a high value of q if the distribution
looks more like q1 than q2.

Before looking at the properties of the subsampled expo-
nential mechanism, we will start by motivating an example
of a problem for which it might be useful. When a patient
gets cancer, a wide variety of mutations occur in the nu-
clear DNA of the tumor. Recent technologies have allowed
scientists to perform genetic sequencing on tumors.
If we
collected tumor samples from patients, we could look at the
diﬀerence between the sequence of a given gene (for example,
the tumor suppressor p53) in our patients and a reference
genome. We might want to publish a consensus sequence
that approximates an average of the mutated sequences, but
without releasing any of the patient’s sequences. Can we do
this in a diﬀerentially private manner?

26Bounding each of the three terms in Equation (9) indepen-
dently: the ﬁrst by the deﬁnition of sensitivity, the second
by independence, and the third by Equation (11),

P r(M (D) = z∗)
P r(M (D′) = z∗)

=

eǫq(D,z∗)P r(z∗ ∈ ˆz) Eˆz [ φ(D, ˆz)| z∗ ∈ ˆz]
eǫq(D′,z∗)P r(z∗ ∈ ˆz) Eˆz [ φ(D′, ˆz)| z∗ ∈ ˆz]

≤ eǫ∆q · 1 · eǫ∆q = e2ǫ∆q

(12)

(13)

which is precisely the deﬁnition for 2ǫ∆q-diﬀerentially pri-
vacy.

3.2 Proof of Accuracy

The paper that introduced the exponential mechanism [18]
also established a bound on its accuracy. The theorem de-
pends on a lemma relating St, the set of outcomes within t
of the optimal q value, and ¯S2t, the set of outcomes that are
more than 2t away from the optimal value.

Lemma 1

(From [18]). Assuming the sensitivity of q
is ∆ = 1, for the exponential mechanism, where St = {z ∈
Z : q(D, z) > OP T − t} and OP T = maxz q(D, z) is the
true optimal value among the set of outcomes,

P r(M (D) ∈ ¯S2t) ≤

e−ǫt
µ(St)

.

(14)

where µ(St) =RSt

Theorem 5

µ(z)dz.

(From [18]). For the exponential mecha-

nism, where t satisﬁes t ≥ ln( OP T

tµ(St) )/ǫ, we have

E[q(D, M (D))] ≥ OP T − 3t

(15)

We will use a corollary to Lemma 1 that provides a similar

bound, but gives more ﬂexibility.

Lemma 2. For the exponential mechanism and w > 0,

P r(M (D) ∈ ¯S(1+w)t) ≤

e−ǫtw
µ(St)

.

(16)

The proof is essentially the same as for Lemma 1.

In the subsampled exponential mechanism, OP T might

If the sequences were all the same length, one approach
could be to look individually at each position, and use the
exponential mechanism and the proportion of each base pair
at that position. This approach is straightforward, but has
two important limitations. One, due the composition mech-
anism we must divide our ǫ by the number of positions in
the sequence, increasing the resulting randomness at each
base pair. Two, because of insertions and deletions, DNA
sequences are rarely exactly the same length, and a variety
of alignment algorithms and scoring functions are used.

Another approach would be to generate all possible se-
quences of length n (or for a small range of n) and use the
exponential mechanism to pick the best one. Our quality
function could be the average pairwise distance to the pa-
tient sequences using any DNA sequence alignment function
for which we could calculate a sensitivity. For the exponen-
tial mechanism, the number of possible sequences is O(4n),
making it impractical for all but the smallest n.

However, we can leverage the fact that we have a refer-
ence sequence that is close to the patient sequences to pro-
duce randomized sequences that are close to the reference
and choose among them with the subsampled exponential
mechanism. For example, we could generate samples by
performing a series of random mutations on the reference
sequence. Note that we need not be able to explicitly cal-
culate the probability distribution over sequences that this
process generates, we only need be able to draw from it.

3.1 Proof of Differential Privacy

Theorem 4. The subsampled exponential mechanism de-

ﬁned in Deﬁnition 3 is 2ǫ∆q-diﬀerentially private.

Proof. Let ˆz = z1, ..., zm be a vector-valued random
variable for the m samples from base distribution µ. The
sampled exponential mechanism is a two-step process. For
a particular z∗ ∈ Z to be selected by the mechanism, it must
ﬁrst be included in the sample, and then selected from that
sample. This gives the following probability for returning a
particular z∗:

P r(M (D) = z∗) = Eˆz"

eǫq(D,z∗)
i=1 eǫq(D,zi)

I[z∗ ∈ ˆz]#

Pm

where I[expr] is the indicator function that evaluates to 1 if
expr is true and 0 otherwise. To reduce the mathematical
clutter, we represent the normalization term in Equation (6)
by φ(D, ˆz) =

1

i=1 eǫq(D,zi ) .

Pm

P r(M (D) = z∗) = Eˆzhφ(D, ˆz) eǫq(D,z∗) I[z∗ ∈ ˆz]i

(7)

= eǫq(D,z∗) Eˆz [φ(D, ˆz) I[z∗ ∈ ˆz]]
(8)
= eǫq(D,z∗)P r(z∗ ∈ ˆz) Eˆz [ φ(D, ˆz)| z∗ ∈ ˆz]
(9)

Provided the expectation in Equation (9) is ﬁnite, we can
now consider neighboring databases D and D′. Since ∆q
bounds the change in q between D and D′,

Eˆz(cid:2) φ(D′, ˆz)(cid:12)(cid:12) z∗ ∈ ˆz(cid:3) =Xˆz
≥Xˆz

φ(D′, ˆz)P r(ˆz|z∗ ∈ ˆz)

(10)

φ(D, ˆz)P r(ˆz|z∗ ∈ ˆz)

eǫ∆q

(11)

(6)

not be sampled, so we must modify the lemma as follows.

Lemma 3. Assuming the sensitivity of q is 1, for the sub-

sampled exponential mechanism we have

2e−ǫt
µ(St)
2 eǫt.

P r(M (D) ∈ ¯S2t) ≤

.

(17)

provided the number of samples m ≥ 1

Proof. Assuming points are drawn according to base dis-
tribution µ, we need to bound the probability that the se-
lected z∗ ∈ ¯S2t. However, for the subsampled exponential
mechanism, we have the additional step of taking the sam-
ple ˆz, and only items in ˆz can actually be selected. Let
a = µ(St)
µ(Z) be the probability mass in µ of the outputs in St.
There are two possibilities for the sample ˆz, either it con-
tains at least one output that is in St or it contains none. In
the former case, which occurs with probability 1 − (1 − a)m,
we will bound the probability of selecting a z∗ ∈ ¯S2t.
If
ˆz contains no elements of St, we make no assumptions on
what the mechanism does. Since this occurs with probabil-
ity (1 − a)m, we will need to add this term to our overall
bound on P r( ¯S2t).

27Let \OP T = maxzi ∈ˆz q(D, zi) be the score of the best out-
put that is selected in ˆz. Assuming the case that ˆz ∩ St 6= ∅,
OP T ≥ \OP T ≥ OP T − t. Let ˆt = \OP T − (OP T − t). So ˆt
is the amount that \OP T is above the threshold OP T − t.

The subsampled exponential mechanism performs an ex-
ponential mechanism selection, but over just the sample ˆz.
So we are going to use the bound in Lemma 1 that applies
to ˆz and then show how that leads to the desired bound in
Z.
In applying the exponential mechanism to ˆz, we have
analogous version of St, but we will use

ˆSˆt = {zi ∈ ˆz : q(D, zi) ≥ \OP T − ˆt}

(18)

since it means

\OP T − ˆt = OP T − t

(19)

However, we cannot directly apply Lemma 1 because it only
gives a bound for choosing an output where q(D, z∗) ≤
OP T − t − ˆt, but ˆt ≤ t so this is not suﬃcient to bound
the probability of ¯S2t which requires all outputs have score
less than OP T −2t. Instead we use the corollary in Lemma 2
with w = t/ˆt. Using ˆµ for the uniform distribution on ˆz, we
have

c0 and c1.

P r( ¯S2t) ≤

=

≤

≤

(26)

2e−ǫtm

2e−ǫtm

ci! − c0 − c1#

a(m + 1) "  n
Xi=0
a(m + 1) (cid:2)1 − (1 − a)m+1 − a(m + 1)(1 − a)m(cid:3)(27)
(cid:2)1 − (1 − a)m+1 − a(m + 1)(1 − a)m(cid:3) (28)

2e−ǫt

a

2e−ǫt

a

− 2e−ǫt(m + 1)(1 − a)m

(29)

We drop the −(1 − a)m+1 term from Equation (28) to Equa-
tion (29) in order to get a looser, but simpler, bound at the
end. Equation (29) is the bound for when ˆz ∩ St 6= ∅. If that
intersection is ∅, then we cannot make any claims about the
probability of choosing a poor output. So we additionally
have an additive term of (1−a)m from when the intersection
is ∅.

P r( ¯S2t) ≤

2e−ǫt

a

− 2e−ǫt(m + 1)(1 − a)m + (1 − a)m (30)

P r( ˆ¯S(1+w)ˆt) ≤

e−ǫˆtw
ˆµ( ˆSˆt)

=

e−ǫt
ˆµ( ˆSˆt)

.

(20)

If m ≥ 1
than the additive third term, and we get ﬁnal bound of

2 eǫt, then the second term in Equation (30) is larger

The bound in Equation (20) is for the probability of selecting
an output with score less than \OP T − (1 + w)ˆt. Substituting
for w and using Equation (19) means

P r( ¯S2t) ≤

2e−ǫt

a

=

2e−ǫt
µ(St)

(31)

\OP T − (1 + w)ˆt = \OP T − ˆt − t

= OP T − t − t = OP T − 2t

(21)

(22)

Using Lemma 3 leads to a modiﬁed version of the accuracy

theorem from [18].

So all outputs in ˆ¯S(1+w)ˆt have scores less than OP T −2t, and
thus ˆ¯S(1+w)ˆt ⊆ S2t. For a particular ˆz, no other elements
from S2t can possibly be chosen, so Equation (20) implies
that when ˆz ∩ St 6= ∅,

Theorem 6. For the subsampled exponential mechanism,
2tµ(St) , we have

tµ(St) )/ǫ and m ≥ OP T

where t satisﬁes t ≥ ln( OP T

E[q(D, M (D))] ≥ OP T − 5t

(32)

The proof is identical to Theorem 5 as proven in [18],

except using Lemma 3 rather than Lemma 1.

P r( ¯S2t) ≤

e−ǫt
ˆµ( ˆSˆt)

.

(23)

4. EXPERIMENTS

If we let k = |ˆz ∩ St|, then ˆµ( ˆSˆt) = k

m . Separating out
each possible value of k using a binomial expansion when
ˆz ∩ St 6= ∅, we have

P r( ¯S2t) ≤ e−ǫtm

m

Xk=1

k

ak(1 − a)m−k(cid:0)m
k(cid:1)
ak+1(1 − a)m−k m + 1
Xk=1

m

≤

2e−ǫtm
a(m + 1)

(24)

k + 1! (25)

Equation (25) results from multiplying by 2ka(m+1)
is ≥ 1 when k ≥ 1.

(k+1)a(m+1) which

Let i = k + 1 and n = m + 1. The binomial theorem

for 0 ≤ a ≤ 1 can be written as ci = ai(1 − a)n−i(cid:0)n
Pn

i=0 ci = 1. The sum in Equation (25) contains all but two
of the terms in sum, so we subtract oﬀ those missing terms:

i(cid:1) and

To investigate the practical aspects of using the exponen-
tial and subsampled exponential mechanisms, we perform
several experiments on a clustering task. Our primary in-
vestigation is the utility of the outputs produced by the vari-
ous methods, in comparison to other private and non-private
methods. Additionally, we explore the empirical behavior of
the subsampled exponential mechanism as the number of
samples varies and with alternative base distributions.

4.1 K-median Task

We use the k-median (or, perhaps more accurately, the
k-medoid) problem setting described in [11]. This is a clus-
tering task similar to k-means where the cluster centers must
be chosen from among the data points, hence the name me-
dian. The centers are to be chosen among the public set of
data points, V , with a distance metric, d : V × V → R, over
those points. The private data may be a subset, D ⊆ V , or
may be a separate set of points in the same space (with d
also deﬁned between public and private points), the exact
identity of which we want to keep private. The task then

281.0

0.5

y

0.0

−0.5

−1.0

G

G
G

G
G
G
G
G

G
G

G
G
G

G

G
G
G
G
G
G

G

G

G
G
G
G
G
G

G
G
G

G
G

G
G
G
G
G
G
G

G

G

G

G
G
G

G
G
G
G

G

G
G

G

G

G
G
G

G
G
G
G
G
G
G
G
G

G
G

G

G

G
G
G

G

G

G
G
G
G
G

G
G
G

G

G
G

G
G
G

G

G

G

G

G
G

G
G
G
G
G

G

G

G
G
G
G
G
G

G
G
G
G
G

G
G

G

G
G
G
G
G
G
G
G
G
G
G
G
G

G

G

G
G
G
G

G

G

G
G
G
G
G
G
G

G

G

G
G
G

G

G
G

G
G
G

G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G

G
G

G
G
G
G
G

G

G

G
G
G
G
G
G
G
G
G
G
G
G
G

G
G
G

G

G

G
G
G
G
G
G
G
G

G
G
G

G

G
G
G
G

G
G

G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G

G
G
G

G
G
G
G

G

G
G
G
G

G

G
G
G
G

G
G
G
G
G
G
G

G
G
G
G
G

G

G
G
G
G
G
G
G
G

G

G

G
G
G

G

G

G
G
G

G
G
G

G

G

G
G
G
G
G
G
G
G
G
G
G
G

G

G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G

G
G
G
G
G
G
G

G

G

G
G
G
G
G
G
G

G

G

G
G
G

G
G
G
G
G
G
G
G

G
G
G
G

G

−1.0

−0.5

G

0.0
x

G

G

G
G
G
G
G
G

G
G
G
G
G
G

G

G
G
G
G
G
G
G
G
G

G

G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G

G

G

G
G

G
G
G G
G
G
G
G
G
G
G
G

G
G
G
G
G

G

G
G

G
G

G
G
G
G

G
G

G
G

G

G
G

G

G
G

G
G
G
G

G
G
G
G
G
G
G
G

G
G
G

G

G
G
G
G
G
G
G
G
G
G
G

G
G
G

G
G
G
G
G
G

G
G
G

G

G

G
G
G
G
G

G

G
G

G
G

G

G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G

G
G

G

G
G
G

G
G

G
G
G

G
G
G
G
G

G
G

G
G
G
G
G

G
G
G
G
G
G
G
G
G
G
G

G
G

G

G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G

G

G
G
G
G
G
G
G
G
G

G
G

G
G
G
G

G
G

G

G
G

G

G
G
G

G
G
G
G
G
G
G

G

G
G
G
G
G

G

G
G
G

G

G

G

G
G

G
G
G
G

G

G
G
G

G

G
G
G
G

G
G
G
G
G
G
G
G
G
G

G

G
G
G
G

G
G
G
G
G
G
G
G
G
G
G
G
G
G

G

G
G
G
G
G
G
G
G
G
G
G
G
G

G

private

G no

yes

t

n
u
o
c

600

400

200

0

6e+05

4e+05

2e+05

0e+00

k
=
2

k
=
4

0.5

1.0

10

(a)

30

20
cost (q)
(b)

Figure 2: (a) Public data points (V ) for n = 1000 with the s = 300 private data points in D denoted by red
triangles. (b) Histogram of costs, the negative of the q function distribution, for our k-median task with
n = 100 and k = 2, 4. Note that since the q function is actually the negative cost, the best outputs are on the
left and the long right tail contains very poor clusterings.

is to ﬁnd the set of centers F ⊆ V with |F | = k that mini-
mizes the distances to the private subset. That is, we want

While the brute-force approach of evaluating the cost of

to minimize cost(F ) =Pv∈D minf ∈F d(v, f ).
all (cid:0)n

k(cid:1) subsets of size k will ﬁnd the optimal solution, it is

not practical for any but the smallest problems. A variety of
non-private, heuristic algorithms exist for this problem, typ-
ically based around some sort of (greedy) local search. The
prototypical approach is the partitioning around medoids
(PAM) algorithm, an expectation-maximization-like algo-
rithm made popular by [16]. Another method that includes
theoretical guarantees about the quality of the solutions in
relation to the optimal is the local search with 1-swaps inves-
tigated in [2]. Private approaches seek to output a reason-
ably low-cost solution while ensuring the presence or absence
of any particular point in D is suﬃciently obfuscated. Gupta
et al. discuss using the exponential mechanism on all sub-
sets as well as their modiﬁed 1-swap search that sequentially
composes a number of private steps to create a diﬀerentially
private version of local search [11]. After dividing up their
budget ǫ to make a series of local search steps, the algorithm
then uses the exponential mechanism once more to select the
ﬁnal set of centers.

The exponential mechanism and subsampled exponential
mechanism require a q function that indicates the quality
of a particular solution F for a particular dataset D. As
in [11], we use the negative of the cost function. The global
sensitivity of this q function is the maximum change in the
cost when removing, adding, or moving a single point in D.
Changing a point in D can change the cost function by at
most the maximum distance between two points in V , also
known as the diameter. i.e., ∆ = maxv,w∈V d(v, w).

4.1.1 Algorithms

Results are presented for a variety of algorithms. These
methods can be separated into three categories: data agnos-
tic, diﬀerentially private, or non-private.

Data agnostic methods do not actually look at the pri-
vate dataset, and instead only use properties of the pub-
lic data to choose the centers. This means the algorithms

are diﬀerentially private by default (for any ǫ), and can be
used as input to other diﬀerentially private methods. The
two data agnostic methods that we investigate are random,
which simply chooses a random subset of size k from the
n public points, and random k++, which uses the initializa-
tion algorithm from k-means++ [1]. The initialization al-
gorithm selects the ﬁrst cluster center uniformly at random,
then chooses each subsequent point weighted by the inverse
square of the distance to the closest existing center. The
result is that the centers tend to be relatively spread out,
particularly for larger values of k.

We provide a baseline of the best solution found by non-
private algorithms for each problem setting. For the smaller

k(cid:1) subsets of the public points

problems we can evaluates all(cid:0)n

to identify the optimal solution with minimum cost. For the
larger problems, we report the smallest solution found after
10 runs of the local 1-swap algorithm from [2]. Though this
is not guaranteed to ﬁnd the optimal solution and the the-
oretical bounds are quite loose, in practice it performs very
well. In our experiments, the local search always ﬁnds an
optimal solution when we are able to calculate the optimal
cost.

We evaluate several versions of the diﬀerentially private al-
gorithms. First is the diﬀerentially private iterative method
described in [11], which we refer to as gupta. Then, we have

k(cid:1) subsets; this

the full exponential mechanism over all (cid:0)n

em method is only feasible on the smaller parameter set-
tings. Finally, we have the subsampled exponential mecha-
nism (ssem) with its additional parameter, m, for how many
samples to take. As a relatively direct comparison to the it-
erative method, we set the number of samples to be the
same as the number of iterations in gupta (6k ln(n)) for the
ssemauto method. These two approaches can be seen as the
two extremes between using the privacy budget for the ﬁnal
choice with random samples (ssemauto) or using the major-
ity of the privacy budget to obtain better samples via an
iterative Monte Carlo method (gupta).

An often overlooked aspect of the exponential mechanism
is the base measure. By changing the base measure to bias
towards more typically useful outputs instead of a uniform

29distribution on outputs, it is possible to improve utility while
preserving diﬀerential privacy. The critical aspect, though,
is that the base distribution must be independent of the
private data.

In the k-median task, for example, one expects good so-
lutions to have centers that are reasonably spaced, and not
bunched up together. But that intuition is diﬃcult to encode
in a probability distribution function. With the subsampled
exponential mechanism, however, it is not necessary to for-
malize this intuition as a fully speciﬁed probability distri-
bution function over the outputs. Instead, a sampling algo-
rithm can be used that implies some probability distribution
function over the output space. Thus, we can use the random
k++ algorithm mentioned above to probabilistically choose
cluster centers for the public points (without reference to
which points are in the private dataset) and use that as the
base distribution for ssem and ssemauto. These versions are
referred to as ssem k++ and ssemauto k++, respectively.

4.1.2 Synthetic Data

We compare and contrast the aforementioned methods in
a simple, synthetic dataset modeled on the problem descrip-
tion from [11]. n = |V | points are uniformly chosen in the
unit circle and a random subset of size s is used as the
private data. See Figure 2a for an example of the points
being clustered. We use n = 100, 1000, 10000 with associ-
ated s = 30, 300, 3000, respectively. The number of centers
ranges over k = 2, 4, 8, 16. The utility of each method is
assessed by the cost of the cluster centers produced. Me-
dian cost and quantiles are shown for 1,000 replicates of the
randomized algorithms.

As discussed in Section 3, the eﬀectiveness of the subsam-
pled exponential mechanism depends on the distribution of
q(Z). If the best output (the smallest cost for the k-median
task) is at the end of a long tail of the distribution of the q
function, then it will be diﬃcult for the subsampling process
to ﬁnd good solutions. On the other hand, if the best output
is near a good percentage of the density of the q function,
then the subsampled exponential mechanism should perform
well. Figure 2b contains the histograms of costs, i.e., the q
function distribution, for two of the smaller k-median set-
tings we consider. Since the goal is a minimum cost solution,
the fact that both distributions are left-shifted and have long
tails to the right, but not to the left, means the subsampled
exponential mechanism should be eﬀective at getting sam-
ples that are close to the optimal cost. If, for some unknown
reason, the goal was to maximize the cost of the cluster cen-
ters, our method would not perform as well because it would
be unlikely to sample any outputs in the long right tail.

4.2 Results

The ﬁrst question we address is how the basic diﬀeren-
tially private methods compare to each other and to the
best solutions found via non-private means. In Figure 3a,
we show results for the em, gupta, and ssemauto methods
when only choosing 2 cluster centers (k = 2). With only
O(n2) possible centers, we are able to run the em algorithm
and can also ﬁnd the optimal cost solution (denoted by the
black horizontal line). The median cost of a randomly cho-
sen subset is given by the dashed, gray horizontal line. The
box plots show the 25th and 75th percentiles with middle
line showing the median. The whiskers extending above and
below to the minimum and maximum cost for any solution

in the 1,000 replicates.
If the notches for two algorithms
do not overlap, this suggests their medians are statistically
signiﬁcantly diﬀerent [5].

When ǫ = 0.1 and n = 100, none of the private algorithms
do much better than random. Though they occasionally
produce good solutions, they also can produce terrible solu-
tions as evidence by the wide range of the whiskers, and the
median is only slightly less than the median for a random so-
lution. As ǫ or n increases, however, it is possible to protect
privacy while providing lower cost solutions. We show re-
sults for ǫ of 10 and 100 to see if the algorithms improve with
additional budget despite most discussion of diﬀerentially
privacy focusing on ǫ of around 1 or lower. Indeed, as n in-
creases, the utility improves considerably, and for n = 10000
and ǫ = 1.0, the em method consistently produces solutions
with nearly optimal cost. Figure 3a shows that the iterative
gupta method requires large ǫ’s to eﬀectively select a good
set of cluster centers. Except for a few cases when ǫ = 100,
gupta is worse than both the em and ssemauto methods.
Despite using a subsample of the output space, our more
computationally eﬃcient ssemauto method (see discussion
of runtimes in Section 4.3 and Figure 5) performs similarly
to the full exponential mechanism, and is typically much
better than the gupta method. The few times gupta is pro-
ducing lower cost solutions than ssemauto, the ssem method
can obtain comparable results by using more samples, while
still running more quickly.

Next, we look at what happens as the number of cluster
centers is varied. Figure 3b provides results for k = 2, 4, 8, 16
and has a similar structure to Figure 3a discussed above.
Instead of the boxplots, we plot the median cost of solutions
and the error bars show the 2.5% and 97.5% percentiles.

With k > 2, the full exponential mechanism is not feasible
and the gupta method becomes prohibitively expensive for
the largest parameter settings as well. These results are
all for the same set of public/private points for each value
of n, so the total cost for a particular n gets smaller as k
increases and each private data point is closer to a center.
We continue to see our ssemauto method performing about
as well as em and similar to or better than gupta.

A critical parameter for our subsampled exponential mech-
anism is m: the number of samples to take. To explore the
eﬀect of m on performance in the k-medians task, Figure 3c
shows the median cost of the solution produced by ssem as
m varies. Additionally, we show the results for the ssem
k++ method which uses non-uniform sampling to bias the
solutions.

The performance of the subsampled exponential mecha-
nism improves or remains the same as m increases. When
the median cost plateaus for ssem as m increases, it is most
likely because it has attained as much utility as the em
method. In the settings where we can run em, we demon-
strate this in Figure 3d.

In the previous experiments, we assume that the private
data points are drawn from the same distribution as the
public points. What if this is not the case? In Figure 4, we
show an experiment where the public points remain uniform
but the private points are drawn from a gaussian distribu-
tion centered at (0.5,0.5). With the public points drawn
from a unit circle centered at (0,0), this provides both a
diﬀerent variance and mean for the private points. When
the variance parameter of the gaussian is small, all private
points are tightly packed, and the optimal solution has low

30t
s
o
c

35

30

25

20

15

  100

 1000

10000

300

250

200

150

3000

2500

2000

1500

method

em

gupta

ssemauto

0.1

1.0

10.0 100.0

0.1

1.0

10.0 100.0

0.1

1.0

10.0 100.0

ε

(a)

k = 2

k = 4

k = 8

k = 16

200

G
G
G

t
s
o
c

100

G

G

G

G
G

G

GGG

G

G

G

G

G

G

GG

n
=
1
0
0
0

method

G

G

G

em

gupta

ssemauto

GG

G

G

G

G

G

G

G

G

G

G

0.1 1.0 10.0 100.0

0.1 1.0 10.0 100.0

0.1 1.0 10.0 100.0

0.1 1.0 10.0 100.0

ε

(b)

k = 2

k = 4

k = 8

k = 16

200

t
s
o
c

150

100

50

t
s
o
c

190

180

170

160

GG

GG

GG

GG

G

G

GG

GG

GG

G

G

G

G

G

G

G

G

G

G

G

G

G

G

G

G

ε
=
1

method

G

G

ssem

ssem k++

1
0

1
0
0

1
0
0
0

1
0
0
0
0

1
0

1
0
0

1
0
0
0

1
0
0
0
0

1
0

1
0
0

1
0
0
0

1
0
0
0
0

1
0

1
0
0

1
0
0
0

1
0
0
0
0

m

(c)

ε = 0.1

ε = 1

ε = 10

ε = 100

G

G

G

G

G

G

G

G

1
0

1
0
0

1
0
0
0

1
0
0
0
0

1
0

1
0
0

1
0
0
0

1
0
0
0
0

G

1
0

G

1
0
0

G

1
0
0
0

G

1
0
0
0
0

G

1
0
0
0

G

1
0
0
0
0

G

1
0

G

1
0
0

m

(d)

n
=
1
0
0
0
,
k
=
2

method

G

G

em

ssem

Figure 3: Median cost of diﬀerentially private solutions. Error bars are 2.5% and 97.5% quantiles. Missing
points and error bars for em and gupta occur when runtimes or memory requirements were excessive. Cost of
best solution found using non-private local search is shown by solid black line and the median cost of random
solutions by the gray, dashed line. (a) Box and whisker plots comparing em, gupta, and ssemauto versus the
privacy budget, ǫ. (b) Comparison of em, gupta, and ssemauto versus the privacy budget, ǫ for n = 1000. (c)
Comparison of ssem and ssem k++ versus the number of samples, m, used in ssem for ǫ = 1. (d) Comparison of
ssem with diﬀerent number of samples, m, versus em shown as green horizontal line. n = 1000 and k = 2.

31cost. As the variance increases, the optimal solution has
higher cost, though the cost of a random solution remains
constant. In all cases, ssemauto is aﬀected by the changing
private density similarly to local.

t
s
o
c

150

100

50

0

G

G

G

G

G
G

G
G

0.01

0.02

G

G

G

G

0.1
0.05
std. dev.

G

G

G

G

G

G

0.2

0.5

method G

local

G

ssemauto

G

random

Figure 4: Median cost when the private distribution
is drawn from a gaussian centered at (0.5,0.5) as
the standard deviation of the gaussian changes. n =
1000, k = 4, and ssemauto run with ǫ = 1.

)
c
e
s
(
 
e
m

i
t

1e+02

1e+00

1e−02

G

G

G

100

G

G

G

G

G

G

1000

n

10000

method G

em

G

ssemauto

G

gupta

Figure 5: Log-scale runtime (in seconds) versus n for
k = 2. The minimum time for a single replicate of
an algorithm and parameter setting is plotted due to
the heterogeneous set of machines upon which our
experiments were executed.

4.3 Runtimes

In the previous section, we frequently alluded to where
it was feasible to run certain algorithms and the relative
computational eﬃciency of ssem. A rough presentation of
selected algorithm runtimes is presented in Figure 5. We ran
our experiments via a high throughput computing system1
on a heterogeneous set of hardware, so precise time compar-
isons are not possible. However, by plotting the minimum
time it took for any single replicate of a particular algorithm

1HTCondor: http://research.cs.wisc.edu/htcondor/

and parameter setting, we can approximately see what the
performance is on the fastest machines in the pool. On the
log-log scale in Figure 5, we see that the algorithms are
generally scaling linearly in n (when k is held constant).
Our subsampled exponential method is consistently faster
than gupta ans scales better than em. Similar runtime re-
sults hold for the variations on the subsampled exponential
method and as k is increased.

5. CONCLUSION

The subsampled exponential mechanism allows for diﬀer-
entially private queries to be made eﬃciently over large out-
put spaces. In addition to the proof of privacy, we provided
a theorem related to the accuracy of the subsampled expo-
nential mechanism similar to the existing accuracy bounds
for the exponential mechanism. This expands the range of
problems that can be evaluated with the single application
of a private mechanism. The gupta method is an iterative,
private algorithm that involves many uses of the exponen-
tial mechanism, while our ssem approach consists of one pri-
vate selection. As demonstrated in our experiments on the
k-median task, the single private selection required in the
subsampled exponential mechanism obtains better or simi-
lar utility solutions than the gupta method and runs more
quickly. This is a vastly diﬀerent approach than is typically
used in non-private learning, but in private learning has the
advantage of not needing to divide the privacy budget while
allowing operation in extremely large output spaces.

6. REFERENCES

[1] D. Arthur and S. Vassilvitskii. k-means++: The

advantages of careful seeding. In Proceedings of the
eighteenth annual ACM-SIAM symposium on Discrete
algorithms, pages 1027–1035. Society for Industrial
and Applied Mathematics, 2007.

[2] V. Arya, N. Garg, R. Khandekar, A. Meyerson,

K. Munagala, and V. Pandit. Local search heuristics
for k-median and facility location problems. SIAM
Journal on Computing, 33(3):544–562, 2004.

[3] J. Blocki, A. Blum, A. Datta, and O. Sheﬀet.

Diﬀerentially private data analysis of social networks
via restricted sensitivity. In Proceedings of the 4th
conference on Innovations in Theoretical Computer
Science, pages 87–96. ACM, 2013.

[4] A. Blum, K. Ligett, and A. Roth. A learning theory
approach to noninteractive database privacy. Journal
of the ACM (JACM), 60(2):12, 2013.

[5] J. Chambers. Graphical methods for data analysis.

Chapman & Hall statistics series. Wadsworth
International Group, 1983.

[6] K. Chaudhuri and D. J. Hsu. Convergence rates for

diﬀerentially private statistical estimation. In
Proceedings of the 29th International Conference on
Machine Learning (ICML-12), pages 1327–1334, 2012.
[7] K. Chaudhuri and N. Mishra. When random sampling
preserves privacy. In Advances in Cryptology-CRYPTO
2006, pages 198–213. Springer, 2006.

[8] C. Dwork, F. McSherry, K. Nissim, and A. Smith.

Calibrating noise to sensitivity in private data
analysis. In Theory of Cryptography, pages 265–284.
Springer, 2006.

32[9] C. Dwork, G. N. Rothblum, and S. Vadhan. Boosting
and diﬀerential privacy. In Foundations of Computer
Science (FOCS), 2010 51st Annual IEEE Symposium
on, pages 51–60. IEEE, 2010.

[10] A. Friedman and A. Schuster. Data mining with

diﬀerential privacy. In Proceedings of the 16th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 493–502. ACM,
2010.

[16] L. Kaufman and P. J. Rousseeuw. Finding groups in
data: an introduction to cluster analysis. John Wiley
& Sons, 1990.

[17] N. Li, W. Qardaji, and D. Su. On sampling,

anonymization, and diﬀerential privacy or,
k-anonymization meets diﬀerential privacy. In
Proceedings of the 7th ACM Symposium on
Information, Computer and Communications Security,
pages 32–33. ACM, 2012.

[11] A. Gupta, K. Ligett, F. McSherry, A. Roth, and

[18] F. McSherry and K. Talwar. Mechanism design via

K. Talwar. Diﬀerentially private combinatorial
optimization. In Proceedings of the Twenty-First
Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 1106–1125. Society for Industrial
and Applied Mathematics, Jan. 2010.

[12] M. Hardt, K. Ligett, and F. McSherry. A simple and

practical algorithm for diﬀerentially private data
release. In Advances in Neural Information Processing
Systems, pages 2339–2347, 2012.

[13] Z. Ji and C. Elkan. Diﬀerential privacy based on

importance weighting. Machine learning,
93(1):163–183, 2013.

[14] V. Karwa, S. Raskhodnikova, A. Smith, and

G. Yaroslavtsev. Private analysis of graph structure.
Proceedings of the VLDB Endowment,
4(11):1146–1157, 2011.

[15] S. P. Kasiviswanathan, K. Nissim, S. Raskhodnikova,

and A. Smith. Analyzing graphs with node diﬀerential
privacy. In Theory of Cryptography, pages 457–476.
Springer, 2013.

diﬀerential privacy. In Foundations of Computer
Science, 2007. FOCS’07. 48th Annual IEEE
Symposium on, pages 94–103. IEEE, 2007.

[19] B. I. Rubinstein, P. L. Bartlett, L. Huang, and

N. Taft. Learning in a large function space:
Privacy-preserving mechanisms for svm learning.
arXiv preprint arXiv:0911.5708, 2009.

[20] L. Wasserman and S. Zhou. A statistical framework

for diﬀerential privacy. Journal of the American
Statistical Association, 105(489):375–389, 2010.

[21] X. Xiao, G. Wang, and J. Gehrke. Diﬀerential privacy

via wavelet transforms. Knowledge and Data
Engineering, IEEE Transactions on, 23(8):1200–1214,
2011.

[22] J. Zhang, X. Xiao, Y. Yang, Z. Zhang, and

M. Winslett. Privgene: diﬀerentially private model
ﬁtting using genetic algorithms. In Proceedings of the
2013 international conference on Management of data,
pages 665–676. ACM, 2013.

33