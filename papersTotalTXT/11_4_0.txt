Secure Computation on Floating Point Numbers

Mehrdad Aliasgari, Marina Blanton, Yihua Zhang, and Aaron Steele

Department of Computer Science and Engineering
{maliasga,mblanton,yzhang16,asteele2}@nd.edu

University of Notre Dame

Abstract—Secure computation undeniably received a lot of
attention in the recent years, with the shift
toward cloud
computing offering a new incentive for secure computation and
outsourcing. Surprisingly little attention, however, has been paid
to computation with non-integer data types. To narrow this
gap, in this work we develop efﬁcient solutions for computation
with real numbers in ﬂoating point representation, as well
as more complex operations such as square root,
logarithm,
and exponentiation. Our techniques are information-theoretically
secure, do not use expensive cryptographic techniques, and can
be applied to a variety of settings. Our experimental results also
show that the techniques exhibit rather fast performance and in
some cases outperform operations on integers.

I. INTRODUCTION

As today’s society is becoming increasingly connected with
a growing amount of information available in the digital
form, the amount of personal, sensitive, or otherwise private
information collected and stored is constantly growing. When
dealing with such information, one of the most technically
challenging issues arises when private or sensitive information
needs to be used in computation without revealing unnecessary
information. This applies to both personal and proprietary data.
The desire to carry out computation in a privacy-preserving
manner without revealing any information about the private
inputs throughout the computation has been a topic of research
since the ﬁrst general secure function evaluation result was
introduced by Yao in his seminal work [43]. Despite a large
amount of attention from the research community, secure com-
putation techniques are not commonly used in practice because
of their complexity and overhead. The recent progress in the
performance of secure multi-party computation techniques,
however, shows that secure computation can be very fast (e.g.,
millions of operations performed on the order of seconds by
Sharemind on a LAN [7]).

Furthermore, the shift toward cloud computing and storage
offers a new major incentive for further development of such
techniques. Cloud computing enables on-demand access to
computing and data storage resources, which can be conﬁgured
to meet unique constraints of the clients and utilized with
minimal management overhead. Such services are attractive
and economically sensible for clients with limited computing
or storage resources who are unwilling or unable to procure
and maintain their own computing infrastructure and can
therefore outsource their computational tasks to the cloud.
It has been suggested, however, that the top impediment on
the way of harnessing the beneﬁts of cloud computing to
the fullest extent is security and privacy considerations that

prevent clients from placing their data or computations on
the cloud (see, e.g., [3]). Any progress in the performance of
secure computation techniques or extending their scope toward
general-purpose computing over private data can therefore
make a substantial impact on the ﬁeld of computing.

While it is well known that any computable function can
be evaluated securely (e.g., as a Boolean or arithmetic circuit),
recently a substantial amount of literature (see, e.g., [19], [20],
[32] among many others) has been dedicated to optimizing
secure realizations of commonly used operations. This is not
surprising considering that, for instance, a substantial perfor-
mance gain in secure implementation of a comparison opera-
tion (be it based on garbled circuits, homomorphic encryption,
or secret sharing techniques) leads to a signiﬁcant performance
improvement of a very large number of functions which might
be used in secure computation. To date, however, the great
majority of techniques treat computation on the integers, while
there are undeniably limitations to integer arithmetic. This
work makes a step toward expanding the available techniques
to other data types, and in particular develops techniques for
secure multi-party computation on real numbers in the ﬂoating
point representation. While prior literature [23], [17], [24]
offers a limited number of techniques for real number compu-
tation, we are not aware of any work that develops a suite of
techniques for ﬂoating point computation or secure multi-party
techniques for complex functions such as logarithm, square
root, and exponentiation. Such techniques are therefore the
focus of this work.

Because performance of secure computation techniques is
crucial, our techniques are designed to optimize both the
overall and round complexity of the solutions. In addition,
in order to provide a fair assessment of the performance of
operations on different numeric data types in the multi-party
framework, we implement a number of operations for integer,
ﬁxed point, and ﬂoating point real values and evaluate their
performance. To the best of our knowledge, these are the ﬁrst
experimental results not only for ﬂoating point representation,
but also for ﬁxed point operations, which we carry out in a
standard threshold linear secret sharing setting. To summarize,
our contributions are as follows:

• Design of efﬁcient secure multi-party techniques for
ﬂoating point computation in a standard linear secret
sharing framework.

• Design of efﬁcient (and fast converging) protocols for
complex operations over real numbers (square root, log-

arithm, and exponentiation).

• Evaluation of the developed and existing techniques for

integer, ﬁxed point, and ﬂoating point arithmetic.

Security of our protocols is shown in both passive (also known
as semi-honest or honest-but-curious) and active (malicious)
adversarial models.

II. PRELIMINARIES

A. Framework

In this work we use the multi-party setting in which n > 2
parties P1, . . ., Pn jointly execute a prescribed functionality on
private inputs and outputs. We utilize a linear secret sharing
scheme (such as Shamir secret sharing scheme [41]) for
representation of and secure computation on private values.
To ensure composability of our protocols, we assume that
prior to the computation, the parties P1 through Pn hold their
respective shares of the input and also compute shares of the
output. Then any party holding a private input will produce
shares of its values before the computation starts, and upon
computation completion the computational parties P1 through
Pn send their shares to the entities that are entitled to learn
the result. This gives ﬂexibility to the problem setting in that
the parties holding the inputs may be disjoint from the parties
carrying out the computation (as in the case with outsourcing).
Similarly,
the parties receiving the output do not have to
coincide with the input parties or computational parties.

Throughout this work we assume that parties P1, . . ., Pn are
connected by pair-wise secure authenticated channels. Each
input and output party also establishes secure channels with P1
through Pn. With a (n, t)-secret sharing scheme, any private
value is secret-shared among n parties such that any t + 1
shares can be used to reconstruct it, while t or fewer shares
reveal no information about the shared value, i.e., it is perfectly
protected in the information-theoretic sense. Therefore, the
values of n and t should be chosen such that an adversary
is unable to corrupt more than t computational parties.

In a secret sharing scheme that we utilize, any linear
combination of secret-shared values can be performed by each
computational party locally, without any interaction, but mul-
tiplication of two secret-shared values requires communication
between all of them. In other words, if we let [x] denote that
value x is secret-shared among P1, . . ., Pn, operations [x]+[y],
[x]+c, and c[x] are performed by each Pi locally on its shares
of x and y, while computation of [x][y] is interactive. We also
use notation Output([a]) to denote that all parties broadcast
their shares of a which allows them to reconstruct the value
of a. All operations are assumed to be performed in a ﬁeld
Fq for a small prime q greater than the maximum value that
needs to be used in the computation (deﬁned later).

Performance of secure computation techniques is of grand
signiﬁcance, as protecting secrecy of data throughout
the
computation often incurs substantial computational costs. For
that reason, besides security, efﬁcient performance of the
developed techniques is one of our prime goals. Normally,
performance of a protocol in the current setting is measured

in terms of two parameters: (i) the number of interactive
operations (multiplications, distributing shares of a private
value or opening a secret-shared value) necessary to perform
the computation, and (ii) the number of sequential interactions,
or rounds. We employ the same metrics throughout this work.

B. Related work

Prior work on general techniques for secure two- and multi-
party computation is extensive, and its review is beyond the
scope of this work. Prior work more closely related to ours
primarily concentrates on techniques for integer computation
and includes work on bit decomposition [19], [36], [42], [39],
where a secret-shared value is converted to shares of the bits
in its binary representation; comparison [19], [36], [26], [38],
where the operands may or may not have to be given in the bit-
decomposed form; addition and subtraction of bit decomposed
values [19], [8], [15]; and division [9], [13], [16], [30], [17],
[12]. The majority of the above techniques concentrate on
constant-round protocols (i.e., independent of the bit length of
their operands), and the researchers over time gained notable
performance improvement for some of the operations.

Techniques for secure two-party computation of logarithm
are also known [35], but they do not immediately gener-
alize to the multi-party setting. Techniques for computing
on real values are starting to appear. In particular, Catrina
and Saxena [17] use ﬁxed point representation and develop
techniques for several operations. Franz et al. [24] propose to
use quantized logarithmic representation, which enables the
relative error to be bounded for both very large and very
small values, in the two-party setting (using homomorphic
encryption and other tools). In addition, recently Franz et al.
[25] proposed a solution for four basic operations on ﬂoating
point numbers in the two-party setting with no implementation.
Fouque et al. [23] provide two-party techniques for addition
and multiplication using rational values. Also of interest, Setty
et al. [40] developed GINGER, a system for veriﬁcation
of outsourced computation, including support for (limited)
operations on ﬂoating point numbers. Given a sheer volume
of applications that use real values and have been considered
in prior literature in the secure computation context (e.g.,
scientiﬁc computing, biometric and signal processing, and data
mining), additional work in standard frameworks is needed.

Recent implementations of secure multi-party techniques
include [6], [5] and two-party techniques include [31], [33].

C. Building blocks

Our solutions rely on the following existing building blocks:
• [r] ← RandInt(k) allows the parties to generate shares of
a random k-bit value [r] without any interaction (see, e.g.,
[17]) using what can be viewed as a distributed pseudo-
random function.1

1More precisely, to produce a random integer without interaction, the parties
generate it as the sum of random integers from [0, 2k), producing a value
longer than k bits long, the length of which depends on the number of parties
n and the threshold value t.

each yi = (cid:81)i

yn such that each yi = (cid:87)i

• [r] ← RandBit() allows the parties to produce shares of
a random bit [r] using one interactive operation.
• [c] ← XOR([a], [b]) computes exclusive OR of bits a and
b as [a] + [b] − 2[a][b] using one multiplication.
• [c] ← OR([a], [b]) computes OR of bits a and b as [a] +
[b] − [a][b].
• [b] ← Inv([a]) computes b = a−1 (in Fq). For a
non-zero a this operation can be implemented using a
single interaction, where the parties create a random [r],
compute and open c = r · a and set [b] = c−1[r].
• ([y1], . . ., [yn]) ← PreMul([x1], . . ., [xn]) computes
preﬁx-multiplication, where on input a sequence of inte-
gers x1, . . ., xn, the output consists of y1, . . ., yn, where
j=1 xj. The most efﬁcient implementation
of this operation in our framework that we are aware of is
due to Catrina and de Hoogh [15] that uses 2 rounds and
3n− 1 interactive operations, but works only on non-zero
elements of a ﬁeld. Most of the cost is input independent
and can be performed ahead of time (after precomputation
the cost becomes n interactive operations in 1 round).
• ([y1], . . ., [yn]) ← PreOR([x1], . . ., [xn])
computes
preﬁx-OR of n input bits x1, . . ., xn and outputs y1, . . .,
j=1 xj. One possible imple-
mentation from [15] uses 5n− 1 interactive operations in
3 rounds, which after input-independent precomputation
becomes 2n − 1 interactive operations in 2 rounds.
• [b] ← EQ([x], [y], (cid:96)) is an equality protocol that on input
two secret-shared (cid:96)-bit values x and y outputs a bit b
which is set to 1 iff x = y. Secure multi-party imple-
mentation of this operation in [15] uses another protocol
[b] ← EQZ([x(cid:48)], (cid:96)), which outputs bit b = 1 iff x(cid:48) = 0,
by calling EQZ([x]− [y], (cid:96)). EQ and EQZ use (cid:96) + 4 log (cid:96)
interactive operations2 in 4 rounds; after precomputation
this becomes log((cid:96))+2 interactive operations in 3 rounds.
• [b] ← LT([x], [y], (cid:96)) is a comparison protocol that on
input two secret-shared (cid:96)-bit values x and y outputs a
bit b which is set to 1 iff x < y. Efﬁcient implemen-
tations of this function also exist, e.g., we can use the
comparison protocol from [15] with 4 rounds and 4(cid:96) − 2
interactive operations (or (cid:96) + 1 interactive operations in 3
rounds after precomputation). It similarly utilizes protocol
[b] ← LTZ([x(cid:48)], (cid:96)) which outputs 1 iff x(cid:48) < 0 by calling
LTZ([x] − [y], (cid:96)).
• [y] ← Trunc([x], (cid:96), m) computes (cid:98)[x]/2m(cid:99), where (cid:96)
is the length of x. This operation can be efﬁciently
implemented, e.g., using the techniques of [15], which
require 4 rounds and 4m + 1 interactive operations for
this functionality, or m + 2 interactive operations in 3
rounds after input-independent precomputation.
• [xm−1], . . ., [x0] ← BitDec([x], (cid:96), m) performs bit de-
composition of m least signiﬁcant bits of x, where (cid:96)
is the size of x. An efﬁcient
implementation of this
functionality can be found in [17] that uses log(m)
rounds and m log(m) interactive operations (note that the

2In this work, we use log(a) to mean logarithm to the base 2, log2(a).

complexity is independent of the size of its argument x).
• [a] ← FPDiv([x], [y], γ, f ) performs division of ﬂoating
point values x and y and produces ﬂoating point value
x/y. It can be found in [17]. Here γ is the total length
of ﬂoating point representation and 2−f is the precision
(i.e., there are γ − f and f bits before and after the radix
point, respectively). The complexity of this function for
γ = 2f is 3 log(γ) + 2θ + 12 rounds and 1.5γ log(γ) +
2γθ + 10.5γ + 4θ + 6 interactive operations, where θ is
the number of iterations equal to (cid:100)log(γ/3.5)(cid:101).

D. Security model

For each presented protocol, we deﬁne its secure function-
ality such that the parties carrying out the computation do not
provide any input and do not receive any output. Instead, it
is assumed that prior to the beginning of the computation the
parties with inputs will secret-share their values among the
parties carrying out the computation. Likewise, if the result
of a computation is to be revealed to one or more parties,
the computational parties will send their shares to the output
parties who reconstruct the result. This allows for arbitrary
composition of the protocols and their suitability for use in
outsourced environments.

We next formally deﬁne security using the standard deﬁni-
tion in secure multi-party computation for semi-honest adver-
saries, i.e., those that follow the computation as prescribed,
but might attempt to learn additional information about the
data from the intermediate results. We prove our techniques
secure in the semi-honest model and then show that standard
techniques for making the computation robust to malicious
behavior apply to our protocols as well.

Deﬁnition 1: Let parties P1, . . ., Pn engage in a protocol
π that computes function f (in1, . . ., inn) = (out1, . . ., outn),
where ini and outi denote the input and output of party Pi,
respectively. Let VIEWπ(Pi) denote the view of participant
Pi during the execution of protocol π. More precisely, Pi’s
view is formed by its input and internal random coin tosses
ri, as well as messages m1, . . ., mk passed between the parties
during protocol execution:

VIEWπ(Pi) = (ini, ri, m1, . . ., mk).

Let I = {Pi1, Pi2, . . ., Pit} denote a subset of the participants
for t < n and VIEWπ(I) denote the combined view of
participants in I during the execution of protocol π (i.e., the
union of the views of the participants in I). We say that
protocol π is t-private in presence of semi-honest adversaries
if for each coalition of size at most t there exists a probabilistic
polynomial time simulator SI such that

denotes computational indistinguishability.

III. NEW BUILDING BLOCKS

Before proceeding with describing our solution for secure
ﬂoating point arithmetic, we present new building blocks

{SI (inI , f (in1, . . ., inn)} ≡ {VIEWπ(I), outI},

where inI = (cid:83)

Pi∈I{ini}, outI = (cid:83)

Pi∈I{outi}, and ≡

which are used in several of our protocols and can also be
of independent interest. Such building blocks are:

• [b] ← Trunc([a], (cid:96), [m]) that performs truncation of its
ﬁrst argument [a] by an unknown number of bits m,
where (cid:96) is the bitlength of a. This operation is the
same as division by an unknown power of 2. Note
that a straightforward approach to implementing this
functionality is to run Trunc([a], (cid:96), m) on all possible
values of m < (cid:96) and then obliviously choose one of
them. This approach, however, results in O((cid:96)2) interactive
operations, while we are able to achieve notable O((cid:96))
in O(log log (cid:96)) rounds. This, in particular, substantially
outperforms general-purpose division. Our protocol is of
independent interest and is used in this work in several
types of operations on ﬂoating point numbers.
• [a0], . . ., [a(cid:96)−1] ← B2U([a], (cid:96)) is a conversion procedure
from a binary to unary representation. It converts the ﬁrst
argument a < (cid:96) from a binary to unary bitwise represen-
tation, where the output is (cid:96) bits, a least signiﬁcant bits of
which are set to 1 and all others are set to 0. In this work,
B2U constitutes a signiﬁcant component of our truncation
protocol by an unknown number of bits, and its efﬁciency
helps us to achieve the claimed performance of Trunc.
• [2a] ← Pow2([a], (cid:96)) raises 2 in the (unknown) power a
supplied as the ﬁrst argument, where the second argument
(cid:96) speciﬁes the length of the values. In particular, the
bitlength of 2a cannot exceed (cid:96), which means that a
should be in the range [0, (cid:96)). Pow2 is called from many
protocols including B2U, and is described next.

be represented) and computing the result as(cid:81)m−1

We implement [2a] ← Pow([a], (cid:96)) as follows: The logic behind
it is rather simple and consists of ﬁrst computing m = (cid:100)log (cid:96)(cid:101)
least signiﬁcant bits [am−1], . . ., [a0] of [a] (since the result of
raising 2 in the power of a longer than log((cid:96))-bit value cannot
[ai] +
1 − [ai]). This gives us complexity sub-linear in (cid:96).
[2a] ← Pow2([a], (cid:96))
1) m ← (cid:100)log (cid:96)(cid:101);
2) [am−1], . . ., [a0] ← BitDec([a], m, m);
3) ([x0], . . ., [xm−1]) ← PreMul(220

[a0] + 1 − [a0], . . .,

i=0 (22i

22m−1

[am−1] + 1 − [am1 ]);

4) return [xm−1];

In addition to using this Pow2 protocol as a building block of
Trunc and B2U, we utilize it to implement other functionalities
in this work and it is thus of independent interest.

In the next protocol, binary-to-unary conversion B2U([a], (cid:96))
below, we ﬁrst call Pow2([a], (cid:96)). We then create a random
((cid:96)+κ)-bit value r, the (cid:96) least signiﬁcant bits of which are secret
shared by the parties, and broadcast c = [2a] + [r] (steps 2 and
3). Here κ is a statistical security parameter, which is also used
in many building blocks listed in section II-C. The function
Bits(c, (cid:96)) in step 4 returns (cid:96) least signiﬁcant bits of (public)
c. Given the bits ci’s of 2a blinded with a random value r,
in step 5 we compute XOR of ci and the corresponding bit

ri of r and store the resulting bits as xi’s. As a result of this
operation, we obtain that because a least signiﬁcant bits of 2a
are 0, ci = ri and xi = 0 for i = 0, . . ., a − 1. Also, because
the ath bit of 2a is 1, ca (cid:54)= ra and xa = 1. The remaining most
signiﬁcant bits xi can contain arbitrary values because of carry
bit propagation. What matters to us, however, is the position
of the ﬁrst (smallest numbered) non-zero value among the bits
xi. As the next step in the computation, we execute PreOR
on the computed bits xi’s, as a result of which we obtain that
a least signiﬁcant bits y0, . . ., ya−1 are 0, while the remaining
(cid:96)− a bits ya, . . ., y(cid:96)−1 are all set to 1. This allows us to easily
obtain bits of the unary representation of a by returning the
complement of each computed bit.
[a0], . . ., [a(cid:96)−1] ← B2U([a], (cid:96))
1) [2a] ← Pow2([a], (cid:96));
2) for i = 0 to (cid:96) − 1 in parallel [ri] ← RandBit();
4) (c(cid:96)−1, . . ., c0) ← Bits(c, (cid:96));
5) for i = 0 to (cid:96) − 1 in parallel [xi] ← ci + [ri] − 2ci[ri];
6) ([y0], . . ., [y(cid:96)−1]) ← PreOR([x0], . . ., [x(cid:96)−1]);
7) for i = 0 to (cid:96) − 1 in parallel [ai] ← 1 − [yi];
8) return [a0], . . ., [a(cid:96)−1];

3) c ← Output([2a] + 2(cid:96) · RandInt(κ) +(cid:80)(cid:96)−1

i=0 2i[ri]);

Lastly, we describe truncation Trunc([a], (cid:96), [m]) and provide
its protocol below. In the solution, we ﬁrst produce bit-wise
unary representation of its argument [m] using B2U. Because
B2U also computes [2m] at an intermediate step, we store
that value as well instead of recomputing it. The computation
starting from line 3 uses the structure of the regular truncation
by a number of bits given in the clear as the starting point. In
detail, we ﬁrst choose (cid:96) random bits, form an m-bit random
r(cid:48) and an (κ+(cid:96)−m)-bit random r(cid:48)(cid:48) (without knowledge of m
in the clear), and broadcast the value of a protected with an
(κ+(cid:96))-bit random r = 2mr(cid:48)(cid:48)+r(cid:48) (lines 3–6). After performing
modulo reduction of the result (using all possible powers of
2 as the moduli), we obtain c(cid:48)(cid:48) = (a + r) mod 2m (line 8).
Because a mod 2m = c(cid:48)(cid:48) − r(cid:48) + 2md, where d is a bit which
equals to 1 if a mod 2m + r(cid:48) > 2m, we need to compute the
value of d and compensate for the error. This is done on lines
9–10, where the returned result is (a−(a mod 2m))2−m. Note
that the output of Trunc is meaningful only when 0 ≤ m < (cid:96).
[b] ← Trunc([a], (cid:96), [m])
1) (cid:104)[x0], . . ., [x(cid:96)−1](cid:105), [2m] ← B2U([m], (cid:96));
2) [2−m] ← Inv([2m]);
3) for i = 0 to (cid:96) − 1 in parallel [ri] ← RandBit();
i=0 2i(1 − [xi])[ri];
i = c mod 2i;

4) [r(cid:48)] ←(cid:80)(cid:96)−1
5) [r(cid:48)(cid:48)] ← 2(cid:96) · RandInt(κ) +(cid:80)(cid:96)−1
8) [c(cid:48)(cid:48)] ←(cid:80)(cid:96)−1

6) c ← Output([a] + [r(cid:48)(cid:48)] + [r(cid:48)]);
7) for i = 1 to (cid:96) − 1 in parallel c(cid:48)
i([xi−1] − [xi]);
9) [d] ← LT([c(cid:48)(cid:48)], [r(cid:48)], (cid:96));
10) [b] ← ([a] − [c(cid:48)(cid:48)] + [r(cid:48)])[2−m] − [d];
11) return [b];

i=0 2i[xi][ri];

i=1 c(cid:48)

Note that because the protocol already computes a mod 2m,
we can use its steps to easily realize Mod2m([a], (cid:96), [m])
function for computing modulo reduction for 2m, where m
is secret shared. To accomplish this, all that is needed is to
replace line 10 in Trunc with [b] ← [c(cid:48)(cid:48)] − [r(cid:48)] + [2m][d].

The round complexity and number of interactive operations
of the above protocols and other protocols for ﬂoating point
computation that we develop are provided in Table I.

IV. BASIC OPERATIONS IN FLOATING POINT

REPRESENTATION

A. Floating point representation

Today the ﬂoating point format is the most common rep-
resentation for real numbers on computers due to its obvious
precision advantages over ﬁxed point or integer representation.
Floating point representation consists of a ﬁxed-precision
signiﬁcand v and an exponent p which speciﬁes how the value
is to be scaled (using a speciﬁed base), e.g., the value of
the form v · 2p when base is 2. To maximize precision and
the number of representable values, ﬂoating point numbers
are typically stored in normalized form, where the signiﬁcand
stores the leading non-zero bit and a ﬁxed number of bits that
follows. Then because 0 does not have such a representation,
to preserve correctness of the computation, we choose to
explicitly store with each value a bit which indicates whether
the value is 0. For performance reasons we also store the
sign bit separately; this prevents us from explicitly testing for
0 values and the sign during basic operations. We therefore
represent each real value u as a 4-tuple (v, p, z, s) with base 2,
where v is an (cid:96)-bit signiﬁcand, p is a k-bit exponent, z is a bit
which is set to 1 iff u = 0, and s is a sign bit which is set when
the value is negative. We obtain that u = (1− 2s)(1− z)v· 2p.
When u = 0, we also maintain that z = 1, v = 0, and p = 0.
Each non-zero value is normalized, which in our represen-
tation means that the most signiﬁcant bit of v is always set
to 1 and therefore v ∈ [2(cid:96)−1, 2(cid:96)). This is done by adjusting
the (signed) exponent p accordingly, which is from the range
(−2k−1, 2k−1) that we denote by Z(cid:104)k(cid:105). The mapping from
negative values to elements of the ﬁeld is straightforward:
for any a ∈ Z(cid:104)k(cid:105), we deﬁne it as ﬂd(a) = a mod q.
This representation is guaranteed to work correctly with both
ﬁeld operations (i.e., addition and multiplication), and other
protocols (for instance, truncation) are also designed to work
with this representation.

While error detection is not a common topic in secure
multi-party literature, and error indicators can leak information
about private data, for the type of operations developed in
this work we consider error detection an important enough
topic to address. This mechanism is optional, and if for
certain computations the fact
that an error occurred may
result in a privacy leak, error detection can be avoided with
the understanding that in the case of an error a valid, but
meaningless result will be returned.

Among the ﬁve IEEE exceptions for ﬂoating point numbers,
we only consider four main error types: invalid operation (e.g.,
square root or logarithm of a negative input), division by zero

(e.g., division by 0 or logarithm of 0), overﬂow, and underﬂow
(the absolute value of the output of an operation is either too
large or too small to be represented using ﬁnite precision). The
last IEEE exception for ﬂoating point numbers, inexact (the
rounded result of a valid operation requires higher precision
than what is used), is usually masked, and we therefore omit
detection of such an error. Among all operations on ﬂoating
point numbers that we consider in this work, the ﬁrst two
types of exceptions (i.e., invalid operation and division by
zero) can only occur in division, square root, and logarithm.
For such cases, we utilize a special ﬂag, Error, which is set
when one of these exceptions is triggered. For example, in the
logarithm protocol, if the input is either 0 (division by zero
exception) or negative (invalid operation exception), then the
error ﬂag is set. The other two types of exceptions, underﬂow
and overﬂow, can occur in division, multiplication, addition,
subtraction, and exponentiation operations. To check for these
exceptions, one only needs to perform two comparisons at the
end of protocols: If the exponent of the result is either greater
than the maximum exponent value 2k−1 − 1 or it is less than
the minimum exponent value −2k−1 + 1, then overﬂow or
underﬂow, respectively, has occurred. For simplicity of pre-
sentation, we do not explicitly include these two comparisons
in our protocols. Furthermore, we leave them optional and do
not include them in the implementation, as the probability of
such errors occurring in practice with the adequate choice of
parameters (cid:96) and k is extremely small.

As mentioned above, the error ﬂag is set as part of operation
execution. If it is not immediately revealed, the parties will
need to maintain a global error indicator. This global error
indicator is ORed with the error ﬂag returned from an op-
eration after each operation in which an exception can take
place. The computational parties can handle errors in one of
three ways as described in [25]. Namely, either the parties can
open the error ﬂag after each operation, the ﬂag can be opened
periodically after a certain number of operations, or it can be
returned to the parties entitled to learn the result at the end of
all of operations. What strategy is adopted is guided by the
amount of information that the error ﬂag may leak about the
inputs. It is also trivial to extend our exception detection to a
more ﬁne-grained approach by using a dedicated ﬂag for each
type of exception.

In our protocols we require that Fq is such that integers of
length max(2(cid:96) + 1, k) can be represented (to accommodate
expansion during multiplication or division). Also, if over-
ﬂow and underﬂow detection is used, the above changes to
max(2(cid:96) + 1, k + 1).

B. Basic operations on ﬂoating point numbers

We next show how we can securely realize basic operations

on real values in the ﬂoating point representation.
1) Multiplication and division: Multiplication of two ﬂoat-
ing point numbers (cid:104)v1, p1, z1, s1(cid:105) and (cid:104)v2, p2, z2, s2(cid:105) is per-
formed by ﬁrst multiplying their signiﬁcands v1 and v2 to
obtain a 2(cid:96)-bit product v. The product
then needs to be
truncated by either (cid:96) or (cid:96) − 1 bits depending on whether the

most signiﬁcant bit of v is 1 or not. In the protocol below
this is accomplished obliviously on lines 2–4. Partitioning this
truncation into two steps allows us to reduce the number of
interactive operations (at a slight increase in the number of
rounds). After computing the zero and sign bits of the product
(lines 5–6), we obliviously adjust the exponent for non-zero
values by the amount of previously performed truncation.
(cid:104)[v], [p], [z], [s](cid:105) ← FLMul((cid:104)[v1], [p1], [z1], [s1](cid:105),(cid:104)[v2], [p2], [z2],
[s2](cid:105))
1) [v] ← [v1][v2];
2) [v] ← Trunc([v], 2(cid:96), (cid:96) − 1);
3) [b] ← LT([v], 2(cid:96), (cid:96) + 1);
4) [v] ← Trunc(2[b][v] + (1 − [b])[v], (cid:96) + 1, 1);
5) [z] ← OR([z1], [z2]);
6) [s] ← XOR([s1], [s2]);
7) [p] ← ([p1] + [p2] + (cid:96) − [b])(1 − [z]);
8) return (cid:104)[v], [p], [z], [s](cid:105);

The complexity of this and other protocols in this section are
given in Table I. The numbers listed correspond to the minimal
number of rounds and interactive operations, which often re-
quires rearranging computation in the protocols. For example,
computing 2[b][v] + (1 − [b])[v] on line 4 of FLMul uses only
one multiplication; computation on lines 1, 5, and 6 of FLMul
can be carried out in parallel, etc. When multiple possibilities
exist for the building blocks, we use the complexities listed in
section II-C. Also, in certain computation we might need to
multiply a ﬂoating point number by a constant. Then if one of
the operands (or even only some components of one operand)
is given in the clear, the complexity of this protocol reduces.
In particular, multiplications on lines 1, 5, and 6 become local,
which also reduces the number of rounds.

The main component of our division protocol is a simpliﬁed
division SDiv given in Appendix A, which we modiﬁed from
the ﬁxed point division FPDiv in [17]. Because FPDiv uses
Goldschmidt’s method for division, it pays the price of normal-
izing the divisor in order to compute the initial approximation.
With our ﬂoating point representation, however, all values
are already normalized, and the initial approximation can be
computed at very low cost. In fact, since the divisor in our
format is between 2(cid:96)−1 and 2(cid:96)−1, our initial approximation of
one over the divisor is always set to 2−(cid:96). We provide additional
details about SDiv in Appendix A.

As the ﬁrst step of our ﬂoating point division protocol
FLDiv, we execute simple division SDiv on the signiﬁcands
(line 1). Note that, in order to avoid executing it on a zero
divisor, we ensure that the second argument to SDiv is non-
zero. The output of SDiv is a normalized either ((cid:96) + 1)-bit
or (cid:96)-bit value. The truncation on line 3 assures that the ﬁnal
value of division is always a normalized (cid:96)-bit value. We then
adjust the exponent p of the result accordingly. When z2 is
set, we need to also set the error ﬂag (division by 0 if z1 is
not set and invalid operation when z1 is also set).
((cid:104)[v], [p], [z], [s](cid:105), [Error]) ← FLDiv((cid:104)[v1], [p1], [z1], [s1](cid:105),

(cid:104)[v2], [p2], [z2], [s2](cid:105))
1) [v] ← SDiv([v1], [v2] + [z2], (cid:96));
2) [b] ← LT([v], 2(cid:96), (cid:96) + 1);
3) [v] ← Trunc(2[b][v] + (1 − [b])[v], (cid:96) + 1, 1);
4) [p] ← (1 − [z1])([p1] − [p2] − (cid:96) + 1 − [b]);
5) [z] ← [z1];
6) [s] ← XOR([s1], [s2]);
7) [Error] ← [z2];
8) return ((cid:104)[v], [p], [z], [s](cid:105), [Error]);
2) Addition and subtraction: Addition and subtraction
of ﬂoating point values
is more involved due to the
need to align the arguments to the same exponents. Also,
we do not explicitly provide a subtraction protocol as
subtraction FLSub((cid:104)[v1], [p1], [z1], [s1](cid:105),(cid:104)[v2], [p2], [z2], [s2](cid:105))
can be performed by calling FLAdd((cid:104)[v1], [p1], [z1],
[s1](cid:105),
(cid:104)[v2], [p2], [z2], 1 − [s2](cid:105)).

The intuition behind our approach is to shift the signiﬁcand
of the larger input by the appropriate number of bits to align
the inputs,
then perform addition using the inputs’ signs,
followed by truncation and normalization.

In the protocol below, we ﬁrst compute shares of the larger
and the smaller of both the signiﬁcands and exponents, denoted
by vmax, vmin, pmax, and pmin, respectively (lines 1–7).
Computation of the exponents is straightforward, while for
signiﬁcands, if the exponents p1 and p2 differ, the larger value
vmax is set based on the summand with the larger exponent,
otherwise, it is set based on the larger signiﬁcand.

The logic used in the protocol depends on the signs of
inputs (i.e., if they are the same, the values are being added;
otherwise, they are being subtracted), and the difference s3
(line 8) indicates which case takes place. We also need to look
at the difference between the exponents, denoted by ∆. There
are two cases to consider: (i) ∆ > (cid:96) and (ii) 0 ≤ ∆ ≤ (cid:96). When
∆ > (cid:96) and s3 = 0 (addition), there is no need to perform
addition and the signiﬁcand and exponent of the output are
set to vmax and pmax, respectively.3 When ∆ > (cid:96) and s3 = 1
(subtraction), how the algorithm proceeds depends on whether
vmax > 2(cid:96)−1 or otherwise vmax = 2(cid:96)−1. In the former case,
to obtain the result, we need to subtract 1 from vmax, while
setting the exponent to pmax. This is because when we subtract
vmin from vmax the result will have the same exponent as
pmax and its signiﬁcand will be vmax−1 because we only keep
(cid:96) most signiﬁcant bits. In the latter case, vmax = 2(cid:96)−1, and
the ﬁnal result’s signiﬁcand will be (cid:96) − 1 bits long consisting
of all 1s, which we can make (cid:96) bits long by appending a 1 and
setting the exponent to pmax − 1. To accommodate both cases
in one statement, we use v3 = 2(vmax − s3) + 1 on line 11. If
the former case occurs, then v3 will have (cid:96) + 1 bits in which
case the extra least signiﬁcant bit will be later removed. If the
latter case occurs, v3 will contain the correct (cid:96)-bit value, but
the exponent will later need to be set by decrementing pmax.

3As with all other protocols, we store the most signiﬁcant (cid:96) bits of the result
by truncating all remaining bits when the intermediate results are longer than
(cid:96)-bits long. This corresponds to rounding down the absolute value.

When ∆ ≤ (cid:96), we keep the minimum exponent and shift the
maximum signiﬁcand by ∆ bits. This is done by multiplying
vmax by 2∆, where 2∆ is obtained using Pow2 function (line
10). Therefore v4 = vmax2∆+(1−2s3)vmin will have (cid:96)+∆±
1 bits, which is at most 2(cid:96). Next, we combine the two cases
above (line 13) using the result of the comparison on line 9:
v is either v3 (when ∆ > (cid:96)) or v4 (when ∆ ≤ (cid:96)). To make v
of a ﬁxed length we also multiply it by 2(cid:96)−∆. Note that (cid:96)− ∆
is always positive. We obtain that now v will have 2(cid:96) + δ bits
where δ is either 0 or ±1. Because we can only keep (cid:96) most
signiﬁcant bits of v, we truncate (cid:96)−1 least signiﬁcant bits from
the result to ensure that at least (cid:96) most signiﬁcant bits of v are
always kept (line 14). Now we need to normalize the result
and remove the extra bits. To do so, we ﬁrst bit-decompose
v and using preﬁx-OR calculate the location p0 of the most
signiﬁcant bit set to 1 in v’s bit decomposition (lines 15–17).
Then we shift v to left by p0 bits by multiplying the value
by 2p0. Now v has exactly (cid:96) + 2 bits and is normalized, from
which two least signiﬁcant bits are removed (line 18–19).

We also need to adjust the exponent of the result to reﬂect
the shiftings and truncations performed. If ∆ > (cid:96), then p0 = 0
when v > 2(cid:96)−1 and p0 = 1 otherwise when v = 2(cid:96)−1. Thus
the ﬁnal exponent should be pmax − p0. If ∆ ≤ (cid:96), then since
we truncate the result by ∆ + 1 and then we shift it by p0, the
original exponent pmin needs to be adjusted accordingly. This
means that the ﬁnal exponent p will be pmin + ∆ + 1 − p0 =
pmax − p0 + 1 (line 20).

Lastly, we check whether any of the original summands are
0, in which case the computed sum can be incorrect, and the
result instead is set to equal the non-zero summand (line 21).
The rest of the algorithm computes the remaining elements
of the result. In particular, the zero bit is set based on whether
the computed signiﬁcand is 0. The exponent is adjusted similar
to the adjustment of the signiﬁcand on line 21 and takes
into account zero values. The computation of the sign of the
result on line 24 is similar to the computation of the larger
signiﬁcand on line 7 using the sign of the larger summand.
The sign is then adjusted (similar to v and p) to take into
account zero values (line 25).
(cid:104)[v], [p], [z], [s](cid:105) ← FLAdd((cid:104)[v1], [p1], [z1], [s1](cid:105),(cid:104)[v2], [p2], [z2],
[s2](cid:105))
1) [a] ← LT([p1], [p2], k);
2) [b] ← EQ([p1], [p2], k);
3) [c] ← LT([v1], [v2], (cid:96));
4) [pmax] ← [a][p2] + (1 − [a])[p1];
5) [pmin] ← (1 − [a])[p2] + [a][p1];
6) [vmax] ← (1 − [b])([a][v2] + (1 − [a])[v1]) + [b]([c][v2] +
7) [vmin] ← (1 − [b])([a][v1] + (1 − [a])[v2]) + [b]([c][v1] +
8) [s3] ← XOR([s1], [s2]);
9) [d] ← LT((cid:96), [pmax] − [pmin], k);
10) [2∆] ← Pow2((1 − [d])([pmax] − [pmin]), (cid:96) + 1);
11) [v3] ← 2([vmax] − [s3]) + 1;
12) [v4] ← [vmax][2∆] + (1 − 2[s3])[vmin];

(1 − [c])[v1]);
(1 − [c])[v2]);

13) [v] ← ([d][v3] + (1 − [d])[v4])2(cid:96)Inv([2∆]);
14) [v] ← Trunc([v], 2(cid:96) + 1, (cid:96) − 1);
15) [u(cid:96)+1], . . . , [u0] ← BitDec([v], (cid:96) + 2, (cid:96) + 2);
16) [h0], . . . , [h(cid:96)+1] ← PreOR([u(cid:96)+1], . . . , [u0]);

17) [p0] ← (cid:96) + 2 −(cid:80)(cid:96)+1
18) [2p0 ] ← 1 +(cid:80)(cid:96)+1

i=0 [hi];

[c])[s1]);

i=0 2i(1 − [hi]);
19) [v] ← Trunc([2p0][v], (cid:96) + 2, 2);
20) [p] ← [pmax] − [p0] + 1 − [d];
21) [v] ← (1 − [z1])(1 − [z2])[v] + [z1][v2] + [z2][v1];
22) [z] ← EQZ([v], (cid:96));
23) [p] ← ((1−[z1])(1−[z2])[p]+[z1][p2]+[z2][p1])(1−[z]);
24) [s] ← (1− [b])([a][s2] + (1− [a])[s1]) + [b]([c][s2] + (1−
25) [s] ← (1− [z1])(1− [z2])[s] + (1− [z1])[z2][s1] + [z1](1−
26) return (cid:104)[v], [p], [z], [s](cid:105);
As in the case of FLMul, complexity of FLAdd can be
reduced when one operand is a constant given in the clear.
In this case, not only a number of multiplications can be
performed locally, but also depending on the operand’s value,
some other operations might become unnecessary (e.g., the
comparison on line 3).

[z2])[s2];

3) Additional operations: We conclude description of basic
ﬂoating point operations by providing two other commonly
used operations: comparisons and rounding.

The comparison operation below outputs bit b, which is
set to 1 iff the ﬁrst operand is less than the second operand.
Based on the inputs’ signs and zero indicators, their exponents
determine which input is larger. When neither of the operands
is 0 and they have the same sign, the result can be determined
by comparing the exponents (and the signiﬁcands as well when
the exponents are equal). This computation appears on lines 1–
5. In the protocol, b+ and b− denote the result of comparison if
both inputs are positive and if both are negative, respectively.
Then line 6 adjusts the result to cover the cases when at least
one operand is 0 or when the signs are different ([s1](1− [s2])
is used for opposite signs on line 6). Note that when both
inputs are negative with different exponents, the one with the
smaller exponent is larger. Also note that when both operands
are 0 or equal, the output of this function is 0.
[b] ← FLLT((cid:104)[v1], [p1], [z1], [s1](cid:105),(cid:104)[v2], [p2], [z2], [s2](cid:105))
1) [a] ← LT([p1], [p2], k);
2) [c] ← EQ([p1], [p2], k);
3) [d] ← LT((1 − 2[s1])[v1], (1 − 2[s2])[v2], (cid:96) + 1);
4) [b+] ← [c][d] + (1 − [c])[a];
5) [b−] ← [c][d] + (1 − [c])(1 − [a]);
6) [b] ← [z1](1 − [z2])(1 − [s2]) + (1 − [z1])[z2][s1] + (1 −
[z1])(1 − [z2])([s1](1 − [s2]) + (1 − [s1])(1 − [s2])[b+] +
[s1][s2][b−]);

7) return [b];

Other relations (such as less than or equal, greater than, equal-
ity, etc.) can be easily constructed from the above protocol. It
is not difﬁcult to construct FLEQ protocol that avoids calls to

Protocol
B2U
Trunc
FLMul
FLDiv
FLAdd
FLLT
FLRound

Rounds

log log (cid:96) + 4
log log (cid:96) + 9

11

2 log (cid:96) + 7

Interactive operations

6(cid:96) + 3 log (cid:96) + (log (cid:96)) log log (cid:96) − 1
12(cid:96) + (log (cid:96)) log log (cid:96) + 3 log (cid:96)

8(cid:96) + 10

2 log (cid:96)((cid:96) + 2) + 3(cid:96) + 8

log (cid:96) + log log (cid:96) + 27

6

14(cid:96) + 9k + (log (cid:96)) log log (cid:96) + ((cid:96) + 9) log (cid:96) + 4 log k + 37

4(cid:96) + 5k + 4 log k + 13

log log (cid:96) + 30

15(cid:96) + (log (cid:96)) log log (cid:96) + 15 log (cid:96) + 8k + 10

COMPLEXITY OF FLOATING POINT AND SUPPLEMENTAL PROTOCOLS.

TABLE I

LT and is more efﬁcient.

The rounding function FLRound that we provide operates
in two different modes: if mode = 0, it computes ﬂoor, and if
mode = 1, it computes ceiling.

The logic behind our approach is simple: The input has a
fractional part only if its exponent is negative. In this case,
we need to truncate the input’s signiﬁcand to remove only the
fractional part.
As the ﬁrst step of our protocol (lines 1–2), we look at the
input’s exponent p1. If p1 < −(cid:96) + 1, then the input value is
from the range (−1, 1) and thus the output should be either
0 or ±1 depending on mode. If, however, p1 is positive, the
output of the rounding function is equal to the input (i.e., the
input is an integer). The only case that requires non-trivial
computation is when p1 is between 0 and −(cid:96). In this case,
rounding amounts to setting −p1 least signiﬁcant bits of the
signiﬁcand v1 to 0. Furthermore, if those bits are not all 0,
then based on the input’s sign and mode we need to add 2−p1
to the signiﬁcand, the effect of which will be incrementing the
integer value by 1. For instance, if we are to compute the ﬂoor
of input −3.54, then the output should be −(3 + 1) = −4.
This computation is performed on lines 3–5.
Now, if the addition on line 5 results in an overﬂow, then
the result v needs to be adjusted. Because 2(cid:96)−1 ≤ v1 < 2(cid:96)
and we are adding either 0 or 1 to it, the overﬂow happens
only when v = 2(cid:96). In this case, we set the value to 2(cid:96)−1 (line
7) and increment the exponent by 1. To properly set the sign
of the output, notice that the sign does not change unless the
input belongs to the range (−1, 0) and the function computes
the ceiling. In those circumstances, we output is 0 and we to
set the sign s to 0 as well (line 9). Lastly, we compute the
zero bit of the result and adjust the exponent if necessary.
(cid:104)[v], [p], [z], [s](cid:105) ← FLRound((cid:104)[v1], [p1], [z1], [s1](cid:105), mode)
1) [a] ← LTZ([p1], k);
2) [b] ← LT([p1],−(cid:96) + 1, k)
3) (cid:104)[v2], [2−p1](cid:105) ← Mod2m([v1], (cid:96),−[a](1 − [b])[p1]);
4) [c] ← EQZ([v2], (cid:96));
5) [v] ← [v1] − [v2] + (1 − [c])[2−p1 ](XOR(mode, [s1]));
6) [d] ← EQ([v], 2(cid:96), (cid:96) + 1);
7) [v] ← [d]2(cid:96)−1 + (1 − [d])[v];
8) [v] ← [a]((1 − [b])[v] + [b](mode − [s1])) + (1 − [a])[v1];
9) [s] ← (1 − [b]mode)[s1];
10) [z] ← OR(EQZ([v], (cid:96)), [z1]);
11) [v] ← [v](1 − [z]);

12) [p] ← ([p1] + [d][a](1 − [b]))(1 − [z]);
13) return (cid:104)[v], [p], [z], [s](cid:105);

Above we use a slightly modiﬁed Mod2m([a], (cid:96), [m]), which
in addition to computing [a mod 2m] also outputs [2m], which
is computed a part of the protocol. This allows us to avoid
redundancy in computing [2−p1 ].

We also note that our rounding functionality can be easily
realized when mode is secret-shared. This allows the par-
ties to execute input-based rounding operations. Furthermore,
rounding to the nearest integer FLRound((cid:104)[v1], [p1], [z1], [s1](cid:105),
mode = 2) can be achieved by ﬁrst adding 0.5 in the
ﬂoating point representation to the input and then executing the
ﬂoor function (i.e., FLRound(FLAdd((cid:104)[2(cid:96)−1], [−(cid:96)], 0, 0(cid:105),(cid:104)[v1],
[p1], [z1], [s1](cid:105)), 0)).

V. TYPE CONVERSIONS

In this section we provide methods to convert a value
from one representation (i.e., ﬂoating point, ﬁxed point, or
integer) to another. We use (cid:96) and k to denote bitlengths of
signiﬁcands and exponents, respectively, in a ﬂoating point
representation. We also use γ to denote is the total bitlength
of values in either integer or ﬁxed point representation, and
f to denote the bitlength of the fractional part in ﬁxed point
values (in other words, ﬁxed point representation uses γ bits,
from which f bits are dedicated to the fractional and γ − f
bits are dedicated to the integer part). Note that
if x is
represented as a ﬁxed point value, then x = x2−f , where
x ∈ Z(cid:104)γ(cid:105). Thus representing |x| requires γ − 1 bits. Similarly,
integer values x ∈ Z(cid:104)γ(cid:105) have |x| < 2γ−1. Although the
publicly known bitlengths (cid:96), k, γ, f can be passed to protocols,
we assume that k > max((cid:100)log((cid:96) + f )(cid:101) ,(cid:100)log(γ)(cid:101)) and the
modulus q > max(22(cid:96), 2γ, 2k).
Integer to ﬂoating point number. The ﬁrst protocol that we
provide converts a signed γ-bit integer a into a ﬂoating point
value, and is given next. It proceeds by ﬁrst determining the
sign of the γ-bit input, normalizes the input, and then ﬁts it
into an (cid:96)-bit representation.

In the protocol, we ﬁrst determine whether the input is 0
or is negative (lines 2 and 3 below) and turn a into a positive
value on line 4. Because we need to normalize the value of a,
we bit-decompose it into λ = γ − 1 bits (recall that one bit of
its γ-bit representation was used for the sign). Preﬁx OR on
line 6 allows us to compute the number of 0 bits in a starting
from the most signiﬁcant bit until the ﬁrst 1 is observed. If

the number of such bits is k, then our goal is to compute
normalized value 2ka. We note that by using preﬁx OR, we
set all bits after the most signiﬁcant 1. Thus, if we ﬂip all bits
(i.e., 1− bi), we have k 1s and the rest of bits become 0. Now

if we compose all the bits as(cid:80) 2i(1 − bi), we obtain 2k − 1.

This computation and the ﬁnal multiplication (a2k) occur on
line 7. The output’s exponent needs to be set to −k (line 8)
since a is now shifted k times to the left.
What remains is to represent the value as an (cid:96)-bit number
instead of using λ = γ − 1 bits and adjust the exponent
if γ − 1 > (cid:96), we select (cid:96) most
accordingly. Therefore,
signiﬁcant bits (line 9); otherwise, we shift
the value by
(cid:96) − γ + 1 to the left to ensure that its most signiﬁcant bit
is 1 (line 10). The exponent is adjusted by the amount of shift
on line 11 and reset to 0 if necessary.
(cid:104)[v], [p], [z], [s](cid:105) ← Int2FL([a], γ, (cid:96))
1) λ ← γ − 1;
2) [s] ← LTZ([a], γ);
3) [z] ← EQZ([a], γ);
4) [a] ← (1 − 2[s])[a];
5) [aλ−1], . . . , [a0] ← BitDec([a], λ, λ);
6) [b0], . . . , [bλ−1] ← PreOR([aλ−1], . . . , [a0]);

7) [v] ← [a](1 +(cid:80)λ−1
8) [p] ← −(λ −(cid:80)λ−1

i=0 2i(1 − [bi]));
i=0 [bi]);

9) if (γ − 1) > (cid:96) then [v] ← Trunc([v], γ − 1, γ − (cid:96) − 1);
10) else [v] ← 2(cid:96)−γ+1[v];
11) [p] ← ([p] + γ − 1 − (cid:96))(1 − [z]);
12) return (cid:104)[v], [p], [z], [s](cid:105);

Floating point number to integer. Our next protocol converts
a ﬂoating point input with an (cid:96)-bit signiﬁcand to the nearest
integer representable in γ bits. Our approach is to round the
input to the nearest integer and then, based on the exponent
of the outcome, ﬁt the result into a γ-bit representation.

As a ﬁrst step in our solution, we round the input to the
nearest integer using FLRound with the result still being in the
ﬂoating point format. The resulting integer can be represented
using (cid:96) + p(cid:48) bits (plus the sign), where p(cid:48) is the exponent of
the output, and our goal is to produce an γ-bit signed integer.
We divide all possible options into the following categories:
If p(cid:48) ≥ γ − 1, the input is too large to be represented using
γ bits. In other words, its last p(cid:48) ≥ γ − 1 bits are 0 and we
output 0. This condition is checked on line 2 of our protocol,
but setting the output to 0 is deferred until the end (this allows
us not to worry about this case for most of the computation).
Otherwise, if p(cid:48) + (cid:96) > γ − 1, the value is still too large to be
represented using γ − 1 bits, and we have to truncate (cid:96) + p(cid:48) −
(γ − 1) most signiﬁcant bits from the signiﬁcand v(cid:48). In other
words, the integer will consist of γ−1−p(cid:48) least signiﬁcant bits
of v(cid:48) followed by p(cid:48) 0 bits. This condition is checked on line
3. Now, if p(cid:48) < 0, some of the signiﬁcand’s bits correspond
to the fractional part (while being set to 0) and we need to
truncate v(cid:48) by p(cid:48) bits. This condition is checked on line 4.
When p(cid:48) < 0, we, however, can still have that p(cid:48) + (cid:96) > γ − 1
(namely, when (cid:96) is much larger than γ), which means that a

number of most signiﬁcant bits of v(cid:48) need to be truncated.
Assuming that p(cid:48) < γ − 1 we thus obtain four different cases
based on conditions p(cid:48) > 0 and p(cid:48) + (cid:96) > γ − 1.
The computation that follows treats the cases above. When
0 < p(cid:48) < γ − 1 and p(cid:48) + (cid:96) > γ − 1, we keep γ − 1 − p(cid:48) least
signiﬁcant bits of v(cid:48) and otherwise keep v(cid:48) unchanged (lines
5–7) (note that 1− b− bc is the complement of b(1− c)). The
next 5 lines treat the cases when p(cid:48) is negative. In particular,
on lines 8–10, we remove p(cid:48) least signiﬁcant bits of v(cid:48) (which
are all 0) if p(cid:48) is negative and keep v(cid:48) unchanged otherwise.
Also, p(cid:48) < 0 and p(cid:48) + (cid:96) > γ − 1, we keep only γ − 1 least
signiﬁcant bits of the output (lines 11–12). What remains is
to append p(cid:48) 0 bits for positive p(cid:48), include the sign, and set
the value to 0 if necessary.

As an important performance issue, we note that two calls
to Mod2m can be combined into a single call and, similarly,
two calls to Pow2 in the protocol can be combined into one,
thus noticeably improving the complexity of the algorithm.
This is possible because either function is called on mutually
exclusive arguments on two occasions (i.e., multiplied by bit
c in one case and by bit 1− c in the other), and the result can
be updated as before. For readability of this work we do not
include them in the protocol.
[g] ← FL2Int((cid:104)[v], [p], [z], [s](cid:105), (cid:96), k, γ)
1) (cid:104)[v(cid:48)], [p(cid:48)], [z(cid:48)], [s(cid:48)](cid:105) ← FLRound((cid:104)[v], [p], [z], [s](cid:105), 2);
2) [a] ← LT([p(cid:48)], γ − 1, k);
3) [b] ← LT(γ − (cid:96) − 1, [p(cid:48)], k);
4) [c] ← LTZ([p(cid:48)], k);
5) [m] ← [a][b](1 − [c])(γ − 1 − [p(cid:48)]);
6) [u] ← Mod2m([v(cid:48)], (cid:96), [m]);
7) [v(cid:48)] ← [b](1 − [c])[u] + (1 − [b] + [b][c])[v(cid:48)];
] ← Pow2(−[c][p(cid:48)], (cid:96));
8) [2−p(cid:48)
] ← Inv([2−p(cid:48)
9) [2p(cid:48)
10) [v(cid:48)] ← ([c][2p(cid:48)
] + 1 − [c])[v(cid:48)];
11) [w] ← Mod2m([v(cid:48)], (cid:96), [b][c](γ − 1));
12) [v(cid:48)] ← [b][c][w] + (1 − [b][c])[v(cid:48)];
] ← Pow2([a](1 − [c])[p(cid:48)], γ − 1);
13) [2p(cid:48)
14) [g] ← (1 − [z(cid:48)])(1 − 2[s(cid:48)])[2p(cid:48)
][a][v(cid:48)];
15) return [g];

]);

Fixed point to ﬂoating point numbers. We next describe a
protocol that converts a value g · 2−f in ﬁxed point notation
to a ﬂoating point representation. We have that g is a signed
γ-bit integer, and the resulting ﬂoating point number has a (cid:96)-
bit signiﬁcand and k-bit exponent. To perform the conversion,
we call our integer-to-ﬂoating-point function Int2FL on the
integer g and subtract f from the exponent of the output.
(cid:104)[v], [p], [z], [s](cid:105) ← FP2FL([g], γ, f, (cid:96), k)
1) (cid:104)[v], [p], [z], [s](cid:105) ← Int2FL([g], γ, (cid:96));
2) [p] ← ([p] − f )(1 − [z]);
3) return (cid:104)[v], [p], [z], [s](cid:105);

Floating point to ﬁxed point numbers. Our last protocol
converts a ﬂoating point number with a (cid:96)-bit signiﬁcand and

k-bit exponent to to a ﬁxed point number represented using
γ bits with precision 2−f . This is achieved by converting the
input to a γ-bit integer after adding the ﬁxed point precision to
the exponent. Note that if the input’s exponent is close enough
to the maximum so that if it overﬂows when f is added to it,
the output should be set to zero.
[g] ← FL2FP((cid:104)[v], [p], [z], [s](cid:105), (cid:96), k, γ, f )
1) [b] ← LT([p], 2k−1 − f, k);
2) [g] ← FL2Int((cid:104)[v], [p] + f, [z], [s](cid:105), (cid:96), k, γ)[b];
3) return [g];

VI. COMPLEX OPERATIONS

We next present several complex operations on numeric
data types. While the approach we take is suitable for secure
processing of different data representations, our protocols
focus on ﬂoating point numbers.

A. Square root

As the ﬁrst operation, we compute x =

y for any positive
real number y. To achieve this, one method is to use the old
Babylonian formula that has very fast convergence rate

√

(cid:19)

(cid:18)

xi+1 =

1
2

xi +

y
xi

where the initial point x0 is always set to 1. The rate of
convergence in this method is quadratic in the size of the
input, which means that the number of bits of accuracy roughly
doubles on each iteration. Then, if the precision is (cid:96) bits,
the number of iterations is (cid:100)log (cid:96)(cid:101) + 1. As evident from the
formula, this method requires one division, one addition, and
one multiplication in each iteration. Because division is an
√
expensive operation, there are other methods to approximate
x using only multiplication and addition with the same rate
of logarithmic convergence. One of these methods is Newton-
Raphson’s method. As described in [34], this method tries to it-
eratively approximate the roots of a continuous and at least one
time differentiable function. We can apply Newton-Raphson’s
method to the function f (R) = 1
x and
ultimately approximate
x by multiplying the approximation
of

1√
x by x. The iterative equation in this method is

R2 −x to approximate 1√

√

Ri+1 =

Ri(3 − xR2
i )

1
2

(1)

This method requires three multiplications and one addition
inside the iterative equation. All these operations have to be
executed sequentially, thus the round complexity is high.

The most efﬁcient (both in terms of round and com-
munication complexity) approach for computing the square
√
root of a positive number is to use the iterative method of
Goldschmidt [28]. It provides an approximation of both
x
and 1√
x with quadratic convergence. This method, as shown
1√
in [34], needs an initial approximation of
x (denoted by
y0) that satisﬁes 0.5 < xy2
0 < 1.5. The value of y0 is
usually approximated using a linear equation y0 = αx + β,

where α and β are predetermined constants that can be found
x and h is an
in [34]. Then, if g is an approximation of
approximation of

x, the iterative equations are:

√
1
2

√

gi+1 = gi(1.5 − gihi)

hi+1 = hi(1.5 − gihi)

where h0 = y0/2 and g0 = xy0. One of the advantages of
this approach is that since gi+1 and hi+1 can be computed
in parallel (independently) after gihi has been computed,
the round complexity is reduced (as opposed to Newton-
Raphson’s method discussed above). This method is not self-
correcting and the error can accumulate. To eliminate the
accumulated errors, we take advantage of the self-correcting
Newton-Raphson’s approximation method and replace the
last iteration of Goldschmidt’s method with one iteration of
Newton-Raphson’s method. By replacing Ri/2 with hi
in
equation 1, we obtain hi+1 = hi(3/2 − 2xh2
i ) as the new
√
x. Ultimately, at the end of
formula for approximating
1
2
√
iterations, we need to multiply the last computed hi by 2x
to obtain an approximation for

x.

√

u =

We next provide a protocol that implements Goldschmidt’s
approximation (similar to the ﬁxed point protocol of [34])
for the ﬂoating point representation. In our representation, the
signiﬁcand of the input 2(cid:96)−1 ≤ v < 2(cid:96) is already normalized
and therefore 1/2 ≤ x = v2−(cid:96) < 1. In fact, if we are to
calculate the square root of u = v · 2p, we can run our
approximation on u = v2−(cid:96)2p+(cid:96) = x2p+(cid:96). Furthermore,
√
ax2(cid:98)(p+(cid:96))/2(cid:99), where a is either 1 (when p+(cid:96) is even)
or 2 (when p + (cid:96) is odd). In the protocol, we ﬁrst determine
whether p + (cid:96) is even or odd (lines 1–2) and set the ﬁnal
exponent to (cid:98)(p + (cid:96))/2(cid:99) (line 3). Here (cid:96)0 denotes the least
signiﬁcant bit of (cid:96). Next, we compute the initial approximation
y0 using the linear equation mentioned above (lines 4–5). We
use (cid:104)vα, pα, zα, sα(cid:105) and (cid:104)vβ, pβ, zβ, sβ(cid:105) to denote the ﬂoating
point representation of α and β, respectively. The value of g0
is computed on line 6 by multiplying x by the approximation
1√
of
x. To compute h0, we only need to subtract 1 from
the exponent of y0 (line 7). In the loop (lines 8–12) we
follow Goldschmidt’s algorithm; but since our initial approx-
imation has accuracy of 5.4 bits [34], we execute the loop
(cid:100)log((cid:96)/5.4)(cid:101) − 1 times followed by one iteration of Newton-
Raphson’s method (lines 13–17). Note that it is not necessary
to compute line 11 in the last
iteration of Goldschmidt’s
√
algorithm since gi is not used after the loop. Finally, we need
√
to multiply the result by the factor
v
(line 18). In the protocol, v√
2
and p√
2 is its exponent in the ﬂoating point representation.
Our ﬂoating point implementation of approximating square
root has a substantially smaller complexity than the ﬁxed point
protocol of [34] for the same precision. This is due to the
fact that in [34] quadratic communication complexity is used
to normalize the input while the input in our ﬂoating point
representation is already normalized.
((cid:104)[v], [p], [z], [s](cid:105), [Error]) ← FLSqrt((cid:104)[v1], [p1], [z1], [s1](cid:105))
1) [b] ← BitDec([p1], (cid:96), 1);

a above to obtain
2 is the (cid:96)-bit signiﬁcand of

√

10)

11)

zα, sα(cid:105));
pβ, zβ, sβ(cid:105));
[z0], [s0](cid:105));

2) [c] ← XOR([b], (cid:96)0);
3) [p] ← ([p1] − [b])2−1 + (cid:98)(cid:96)/2(cid:99) + OR([b], (cid:96)0);
4) (cid:104)[v2], [p2], [z2], [s2](cid:105) ← FLMul((cid:104)[v1],−(cid:96), 0, 0(cid:105),(cid:104)vα, pα,
5) (cid:104)[v0], [p0], [z0], [s0](cid:105) ← FLAdd((cid:104)[v2], [p2], [z2], [s2](cid:105),(cid:104)vβ,
6) (cid:104)[vg], [pg], [zg], [sg](cid:105) ← FLMul((cid:104)[v1],−(cid:96), 0, 0(cid:105),(cid:104)[v0], [p0],
7) (cid:104)[vh], [ph], [zh], [sh](cid:105) ← (cid:104)[v0], [p0] − 1, [z0], [s0](cid:105);
8) for i = 1 to (cid:100)log((cid:96)/5.4)(cid:101) − 1
9)

(cid:104)[v2], [p2], [z2], [s2](cid:105) ← FLMul((cid:104)[vg], [pg], [zg], [sg](cid:105),
(cid:104)[vh], [ph], [zh], [sh](cid:105));
(cid:104)[v2], [p2], [z2], [s2](cid:105) ← FLSub((cid:104)3 · 2(cid:96)−2,−((cid:96) − 1), 0,
0(cid:105),(cid:104)[v2], [p2], [z2], [s2](cid:105));
(cid:104)[vg], [pg], [zg], [sg](cid:105) ← FLMul((cid:104)[vg], [pg], [zg], [sg](cid:105),
(cid:104)[v2], [p2], [z2], [s2](cid:105));
(cid:104)[vh], [ph], [zh], [sh](cid:105) ← FLMul((cid:104)[vh], [ph], [zh], [sh](cid:105)
12)
,(cid:104)[v2], [p2], [z2], [s2](cid:105));
13) (cid:104)[vh2 ], [ph2], [zh2 ], [sh2](cid:105) ← FLMul((cid:104)[vh], [ph], [zh], [sh](cid:105),
(cid:104)[vh], [ph], [zh], [sh](cid:105));
14) (cid:104)[v2], [p2], [z2], [s2](cid:105) ← FLMul((cid:104)[v1],−(cid:96), 0, 0(cid:105),(cid:104)[vh2],
[ph2 ], [zh2], [sh2](cid:105));
15) (cid:104)[v2], [p2], [z2], [s2](cid:105) ← FLSub((cid:104)3 · 2(cid:96)−2,−((cid:96) − 1), 0, 0(cid:105),
(cid:104)[v2], [p2] + 1, [z2], [s2](cid:105));
16) (cid:104)[vh], [ph], [zh], [sh](cid:105) ← FLMul((cid:104)[vh], [ph], [zh], [sh](cid:105),
(cid:104)[v2], [p2], [z2], [s2](cid:105));
17) (cid:104)[v2], [p2], [z2], [s2](cid:105) ← FLMul((cid:104)[v1],−(cid:96), 0, 0(cid:105),(cid:104)[vh],
[ph] + 1, [zh], [sh](cid:105));
18) (cid:104)[v2], [p2], [z2], [s2](cid:105) ← FLMul((cid:104)[v2], [p2], [z2], [s2](cid:105),
2, 0, 0(cid:105));
(cid:104)(1− [c])2(cid:96)−1 + [c]v√
19) [p] ← ([p2] + [p])(1 − [z1]);
20) [v] ← [v2](1 − [z1]);
21) [Error] ← [s1];
22) return ((cid:104)[v], [p], [z1], [s1](cid:105), [Error]);

2,−(1− [c])((cid:96)− 1) + [c]p√

B. Exponentiation

We next treat exponentiation for the case that when the
base is 2. Recall that exponentiation with an arbitrary base
b can be computed as ba = 2a log b, and we subsequently
provide a solution for computing log b when b is a ﬂoating
point number. In our solution below we try to avoid any use of
series since ﬂoating point addition is quite expensive. In fact,
out approach does not rely on any ﬂoating point operations
introduced above.

As any real number has an integer and fractional parts (e.g.,
11001.01011), when 2 is raised to the power of that number,
we break it into two components: 2 to the power of the integer
part (in this example 211001) times 2 to the power of fractional
part (20.01011). In fact, if we have the fractional part in a bit
decomposed form u1,··· , un, we can compute
+ (1 − ui)) (2)
20.u1···un =
For our ﬂoating point format, the smallest number of the form
of 22−i
that we can represent is for i = (cid:96). We therefore
use a set of ﬂoating point constants 22−i for i = 1, . . ., (cid:96).

(cid:89)n

(cid:89)n

(ui22−i

2ui2−i

i=1

=

i=1

The above intuition leads to FLExp2 protocol for computing
exponentiation on a secret-shared ﬂoating point exponent that
we describe next.

Given a ﬂoating point input, we would like to represent it
as w.u, where w and u are the integer and fractional parts,
respectively. We only need to consider the (cid:96) most signiﬁcant
bits of u to achieve our desired precision. The computation
that we perform to produce the integer w and fractional u
parts depends on the sign of the input, since we demand the
fractional part to be positive. Therefore, in some cases we may
need to subtract 1 from the integer part to guarantee that the
fractional part is positive, but regardless of the input, the output
of the exponentiation will be 2w multiplied by 20.u. We follow
the approach above to compute 20.u using bit decomposition.
To determine w and u, we divide the range of the input’s
exponent p1 (for non-zero inputs) into four regions: (i) when
p1 ≤ −2(cid:96) (i.e., the value is so small that effectively w = 0
and u = 0), (ii) when −2(cid:96) < p1 ≤ −(cid:96) (i.e., the input can
occupy only the fractional part u), (iii) when −(cid:96) < p1 ≤ max
(i.e., w (cid:54)= 0 and u (cid:54)= 0), and (iv) when max < p1 (i.e., the
input is too large and results in an overﬂow or underﬂow).
Here max represents the value exceeding which for the
exponent guarantees either an overﬂow (for a positive input) or
underﬂow (for a negative input) in the output of the function.
We compute it as follows: the maximum value that we can
represent is (2(cid:96) − 1)22k−1−1. An overﬂow for this function
occurs whenever 2v1·2p1 exceeds that value for a positive input
with signiﬁcand v1 and exponent p1. Because v1 ≥ 2(cid:96)−1,
an overﬂow is guaranteed when 22(cid:96)−12p1 > (2(cid:96) − 1)22k−1−1,
which allows us to compute the maximum value p1 can take.
The same reasoning applies to underﬂow for negative inputs.
Note the the purpose of max is not to detect all possible
cases of overﬂow and underﬂow (which should be done at the
end of the computation if error checking is used), but rather
ensure correct operation of the protocol. Thus, in our protocol
FLExp2 we ﬁrst compute this max value and proceed with
processing all four regions in parallel.

The region in which the input falls is determined using three
comparisons on lines 2–4. Let us ﬁrst consider the third region
with −(cid:96) < p1 ≤ max, which corresponds to values a = 1,
b = 0, and c = 0 in the protocol. In this case, the input has
non-zero integer and fractional parts. We know that the integer
part is the truncation of v1 by p2 = −p1 and thus we separate
the integer and fractional parts and store them in variables x
and y, respectively (lines 5–7). To use the bit-decomposition
idea above, we need to make sure that the fractional part u
that will be bit-decomposed is always positive. In this region
it can be negative if after the initial computation on line 7
its value is non-zero and the input is negative. Thus, when
both conditions are true, we subtract 1 from the integer part
(i.e., w = x − 1) and subtract the fractional part from 1 (i.e.,
0.u = 1 − 0.y). To accomplish this in the protocol, we ﬁrst
test whether y (cid:54)= 0 (line 8), subtract 1 from x when s1 = 1
and y (cid:54)= 0 (line 9), and subtract y from 2p2 when s1 = 1 and
y (cid:54)= 0 (line 10). For inputs in this region (with a = 1, b = 0,
and c = 0) we keep w unchanged on line 11, but normalize

u on line 12.
If the input falls into the second region with −2(cid:96) < p1 ≤
−(cid:96), we have that a = 1, b = 1, and c = 0. In this
case, the input is too small to contain any integer part (i.e.,
the integer part w is 0), and we need to compute only the
fractional part. In this case, we truncate the input’s signiﬁcand
by p2 = −(p1 + (cid:96)) and store the result, which now represents
the fractional part, in x (lines 5–6). Because we only keep (cid:96)
most signiﬁcant bits of the fractional part, in this region we
can throw away the leftover of the truncation (i.e., y is not
used). If the input is positive, we will have w = 0 and u = x.
Otherwise, if the input (i.e., the fractional part) is non-zero,
we will need to set w to −1 and subtract the fractional part
from 1. To accomplish this, the truncated part y is compared
to 0 on line 8. Because our fractional part has exactly (cid:96)
bits, the subtraction is performed by subtracting x from 2(cid:96).
Furthermore, because our protocols round down, for negative
inputs we set x = 2(cid:96)−x−1 if y (cid:54)= 0 and x = 2(cid:96)−x otherwise.
In the protocol, this computation is performed on lines 9 and
11, and u is set to x on line 12 (with a = 1, b = 1, and c = 0).
If the input falls into the ﬁrst region, we have p1 ≤ −2(cid:96) and
a = 1, b = 1, and c = 1 on lines 2–4. In this case, we consider
two sub-cases based on the sign of the input. In this region the
input is too small to be representable with (cid:96)-bit precision and
when the input is positive (i.e., s1 = 0), we set the output to
20 = 1. If the input is negative, we need to make the fractional
part positive. We accomplish this by setting the integer part
of the input to −1 and subtracting the fractional part from 1.
Because we only keep the ﬁrst (cid:96) bits of the fractional part,
for this region all bits will be 1 for non-zero inputs because
rounding down is used and we can set u = 2(cid:96) − 1. In our
protocol this computation is accomplished on lines 11 and 12
(with a = b = c = 1), i.e., x and y are not used.

Lastly, if the input falls into the fourth region, we have
max < p1 and a = b = c = 0. In this case, the output’s
exponent will be too large and outside of the representable
range, which means that we have an error. To address this,
we set w and u to 0 on lines 11 and 12. At the end of the
protocol, we set the exponent to 2k−1 (or −2k−1 for negative
inputs), which is outside of the representable range to indicate
an overﬂow or underﬂow.
The loop on lines 14–16 computes the terms in the product
in equation 2. We use constants (cid:104)cvi, cpi, 0, 0(cid:105) to represent
22−i in the ﬂoating point format, and thus ai and bi correspond
+ (1− ui)), respec-
to the signiﬁcand and exponent of (ui22−i
tively. Multiplication of all of these terms is accomplished
on line 17. The notation FLProd denotes the computation of
the product of a number of ﬂoating point values. Instead of (cid:96)
sequential multiplications in our case, it can be implemented
in (cid:100)log (cid:96)(cid:101) rounds by constructing a tree of pairwise multi-
plications (i.e., (cid:96)/2 multiplications at the lowest level, (cid:96)/4
multiplications at the next level, etc.). With the availability of
constant round preﬁx multiplication for ﬂoating point numbers,
the round complexity of this operation could be reduced to
constant rounds. We leave this as a direction for future work.
After computing the product as in equation 2, we multiply

the result by 2w, which is performed on line 18 by increasing
the exponent of the result by w. On line 18 we also set the
exponent to a value outside of the representable range for
inputs falling into the fourth region (with a = 0) to indicate
an overﬂow or underﬂow.

[d] − [x]);

Finally, the remainder of the protocol (lines 19–20) ad-
dresses the case when the input is 0 and sets the result to be 1
in the normalized form (i.e., with v = 2(cid:96)−1 and p = −((cid:96)−1)).
(cid:104)[v], [p], [z], [s](cid:105) ← FLExp2((cid:104)[v1], [p1], [z1], [s1](cid:105))

1) max ←(cid:6)log(2k−1 − 1 + (cid:96)) − (cid:96) + 1(cid:7);

(2(cid:96) − 1)[c][s1];

2) [a] ← LT([p1], max, k);
3) [b] ← LT([p1],−(cid:96) + 1, k);
4) [c] ← LT([p1],−2(cid:96) + 1, k);
5) [p2] ← −[a](1 − [c])([b]([p1] + (cid:96)) + (1 − [b])[p1])
6) (cid:104)[x], [2p2](cid:105) ← Trunc([v1], (cid:96), [p2]);
7) [y] ← [v1] − [x][2p2];
8) [d] ← EQZ([y], (cid:96));
9) [x] ← (1 − [b][s1])([x] − (1 − [d])[s1]) + [b][s1](2(cid:96) − 1 +
10) [y] ← (1 − [d])[s1]([2p2] − [y]) + (1 − [s1])[y];
11) [w] ← [a](1−[c])((1−[b])[x]+[b][s1])(1−2[s1])−[c][s1];
12) [u] ← [a](1 − [c])([b][x] + (1 − [b])(2(cid:96) · Inv([2p2]))[y]) +
13) [u(cid:96)], . . . , [u1] ← BitDec([u], (cid:96), (cid:96));
14) for i = 1 to (cid:96) do in parallel
15)
16)
17) (cid:104)[vu], [pu], 0, 0(cid:105) ← FLProd((cid:104)[a1], [b1], 0, 0(cid:105), . . .,(cid:104)[a(cid:96)], [b(cid:96)],
18) [p] ← [a]([w] + [pu]) + 2k−1(1 − [a])(1 − 2[s1]);
19) [v] ← 2(cid:96)−1[z1] + (1 − [z1])[vu];
20) [p] ← −[z1]((cid:96) − 1) + (1 − [z1])[p];
21) return (cid:104)[v], [p], [0], [0](cid:105);
The protocol above works under the assumption that k < (cid:96),
which is normally the case. If, however, it is desirable to use
k ≥ (cid:96), we provide a modiﬁed (and slightly more expensive)
solution in Appendix A.

[ai] ← 2(cid:96)−1(1 − [ui]) + (cvi)[ui];
[bi] ← −((cid:96) − 1)(1 − [ui]) + (cpi)[ui];

0, 0(cid:105));

C. Logarithm

The last operation that we present is computing logarithm
to the base 2. Recall that for other bases b the function can
be computed as logb(x) = log2(x)/ log2(b). In the following
description, we use e to represent the Euler’s constant.

To compute the logarithm of a positive number in any

representation, we can utilize the formula

arctan x ≈(cid:80)M

and approximate arctan x by bounding its Taylor’s series as

x2i+1

(2i+1) for |x| < 1. This gives us

i=0

log x = 2 log e arctan

x − 1
x + 1

M(cid:88)

i=0

log x ≈ 2 log e

z2i+1
(2i + 1)

,

x+1 . In general, z ≤ zmax = xmax−1

where z = x−1
xmax+1 , where
xmax is the maximum value of input x, and to achieve (cid:96)-bit
precision, we need to use M > (cid:100)(cid:96)/(−2 log zmax) − 1/2(cid:101). For
practical values of (cid:96), this approach ﬁnishes faster than using
Taylor’s series, since not only the latter approach requires more
interactive rounds of computation but also ﬁnding the initial
approximation is not trivial.
In our ﬂoating point representation, a positive value is
represented as x = v · 2p, where 2(cid:96)−1 ≤ v < 2(cid:96). We can
therefore write v as 2(cid:96)u where 1/2 ≤ u < 1 − 2−(cid:96). In this
representation, log x = (cid:96) + p + log u, and we use the above
approximation to derive the formula:

log x = (cid:96) + p − M(cid:88)

i=0

2 log e

y2i+1
2i + 1

(3)

where y = (1− v2−(cid:96))/(1 + v2−(cid:96)). This computation requires
one division, 2(M + 1) multiplications, and M + 2 additions,
where M = (cid:100)(cid:96)/(2 log 3) − 1/2(cid:101) to achieve (cid:96) bits of precision.
Note that one can apply Pad´e approximants [10] to the Taylor’s
series above to reduce the round complexity at the cost of
(noticeably) increasing the communication complexity of the
protocol. We omit the details of such a technique.

2 log e

Throughout most of our protocol FLLog2 below for com-
puting the logarithm of ﬂoating point values, we assume that
the input is positive. If the input is either 0 (the log of which
is −∞, division by zero exception) or negative (the log of
which is a complex number, invalid operation exception), the
error ﬂag will be set (line 16).
The ﬁrst 3 lines of the protocol compute y = (1 −
v12−(cid:96))/(1 + v12−(cid:96)) in the ﬂoating point notation, followed
by the computation of y2 on line 4. Because the powers of y
in the series above are all odd, in each iteration, we multiply
the previous y2(i−1)+1 by y2 to obtain y2i+1 (line 9). In the
protocol, we use constants (cid:104)cvi, cpi, 0, 0(cid:105) for i = 0, . . ., M
to denote the ﬂoating point representation of
(2i+1). The
variable (v, p) holds the current value of the sum in equation 3
(initialized on line 5 to the 0th constant times y) and the
variable (vy, py) holds the current power of y. (The variables
with subscripts 2 and 3 hold intermediate results.) Then inside
the loop we update the current power of y, multiply it by the
appropriate constant, and add the result to the sum (lines 7–9).
After exiting the loop, we need to subtract the computed
sum from p + (cid:96). To do so, we ﬁrst convert p + (cid:96) into the
ﬂoating point format (line 10) and then perform the subtraction
(line 11) to obtain the ﬁnal result. The rest of the function
sets the output to 0 in case the input was 1 (represented as
(cid:104)2(cid:96)−1,−((cid:96) − 1), 0, 0(cid:105)) and performs error checking.
((cid:104)[v], [p], [z], [s],(cid:105)[Error]) ← FLLog2((cid:104)[v1], [p1], [z1], [s1](cid:105))
1) (cid:104)[v2], [p2], 0, 0(cid:105) ← FLSub((cid:104)2(cid:96)−1,−((cid:96) − 1), 0, 0(cid:105),(cid:104)[v1],
2) (cid:104)[v3], [p3], 0, 0(cid:105) ← FLAdd((cid:104)2(cid:96)−1,−((cid:96) − 1), 0, 0(cid:105),(cid:104)[v1],
3) ((cid:104)[vy], [py], 0, 0(cid:105), 0) ← FLDiv((cid:104)[v2], [p2], 0, 0(cid:105),(cid:104)[v3], [p3],

−(cid:96), 0, 0(cid:105));
−(cid:96), 0, 0(cid:105));

0, 0);

8)

0(cid:105));

0, 0(cid:105));
0(cid:105));

4) (cid:104)[vy2 ], [py2 ], 0, 0(cid:105) ← FLMul((cid:104)[vy], [py], 0, 0(cid:105),(cid:104)[vy], [py], 0,
5) (cid:104)[v], [p], 0, 0(cid:105) ← FLMul((cid:104)[vy], [py], 0, 0(cid:105), (cid:104)cv0, cp0, 0, 0(cid:105));
6) for i = 1 to M do in parallel
(cid:104)[vy], [py], 0, 0(cid:105) ← FLMul((cid:104)[vy], [py], 0, 0(cid:105),(cid:104)[vy2], [py2],
7)
(cid:104)[v2], [p2], 0, 0(cid:105) ← FLMul((cid:104)[vy], [py], 0, 0(cid:105),(cid:104)cvi, cpi, 0,
(cid:104)[v], [p], 0, 0(cid:105) ← FLAdd((cid:104)[v], [p], 0, 0(cid:105),(cid:104)[v2], [p2], 0, 0(cid:105));
9)
10) (cid:104)[v2], [p2], [z2], [s2](cid:105) ← Int2FL((cid:96) − [p], (cid:96), (cid:96));
11) (cid:104)[v], [p], [z], [s](cid:105) ← FLSub((cid:104)[v2], [p2], [z2], [s2](cid:105),(cid:104)[v], [p], 0,
12) [a] ← EQ([p1],−((cid:96) − 1), k);
13) [b] ← EQ([v1], 2(cid:96)−1, (cid:96));
14) [z] ← [a][b];
15) [v] ← [v](1 − [z]);
16) [Error] ← OR([z1], [s1]);
17) [p] ← [p](1 − [z]);
18) return ((cid:104)[v], [p], [z], [s](cid:105), [Error]);

0(cid:105));

VII. SECURITY ANALYSIS

Correctness of the computation has been discussed with
each respective protocol and we show their security in Ap-
pendix B.

VIII. EXPERIMENTAL RESULTS

In order to compare performance of arithmetic using dif-
ferent data types, we implement functions for integer, ﬁxed
point, and ﬂoating point operations. All implemented ﬂoating
point operations — namely, multiplication, division, addition,
comparison, exponentiation, and logarithm — were tested for
correctness as described in Appendix C.

In the experiments,

for ﬂoating point values we use
bitlengths (cid:96) = 32 for signiﬁcands and k = 9 for (signed)
exponents. This allows us to represent real values with pre-
cision 2−256. For (signed) ﬁxed point representation, we use
bitlength γ = 64 with f = 32 (i.e., 32 bits before and after
the radix point). This gives us precision 2−32 while covering
values with the integer part up to 31 bits. Lastly, for (signed)
integer computation we use length γ = 64, which makes it
easier to compare performance with that of ﬁxed point values
(recall that a ﬁxed point value g is represented as integer g·2f ).
We set statistical hiding parameter κ = 48 for all data types.
For integer arithmetic, the size of the ﬁeld is determined by
the bitlengths necessary for division and we obtain |q| > 2γ +
κ + 1 = 177 (if division is known to not be used, the ﬁeld
size can be reduced to use |q| > γ + κ = 112). For ﬁxed point
arithmetic, the ﬁeld size is also determined by the length of the
values used during division and we obtain |q| > γ + 3f + κ =
208 (once again, if division is not used, the size of the ﬁeld can
be reduced to use |q| > γ + f + κ = 144). Lastly, for ﬂoating
point arithmetic, we need to have |q| > 2(cid:96) + κ + 1 = 113.

For our experiments, all protocols were implemented in
C/C++ using the GMP library [2] for large-number arithmetic.
We used (3, 1)-Shamir secret sharing with n = 3 participants

(a) addition

(b) multiplication

(c) comparison

(d) division

Fig. 1. Performance of basic operations for different data types.

in the semi-honest setting. The implementation also used the
Boost libraries [1] for communication and OpenSSL [4] for
securing the communication. The computational parties holds
each other’s public RSA keys and establish pair-wise secure
channels using 128-bit AES keys prior to any computation.
The computational parties were run on 2.2 GHz Linux ma-
chines on a 1Gbps LAN. While this LAN setting represents
the best-case scenario, it provides valuable insights on the
performance of our techniques. We leave WAN experiments
and experiments in the cloud environment for future work.

The ﬂoating point operations were implemented using pro-
tocols FLAdd, FLMul, FLLT, and FLDiv. Integer and ﬁxed
point addition are the same as addition in Fq. Integer multi-
plication is the same as multiplication in Fq, while ﬁxed point
multiplication uses FPMul from [17]. Comparison for both
integers and ﬁxed point values uses LT. Finally, ﬁxed point
division uses FPDiv from [17] and integer division utilizes
the same FPDiv, but without fractional bits used.

Our current implementation supports only a limited degree
of parallel execution. In particular, we execute a number of
identical operations in a single batch saving on communication
and round complexity, but no support for fully parallelized
execution is implemented. This in particular means that two
different types of operations which could be executed in-
dependently in a protocol have to run sequentially in our
implementation. In addition, although initial rounds and a
number of interactive operations in some building blocks are
input-independent and could be executed in parallel with the
computation that precedes them, in our implementation they
are run sequentially. This means that our implementation does
not achieve the round complexity reported in Table I and the
runtime of our protocols can be improved, especially when
a small number of instances of an operation needs to be
executed. This also implies that computational overhead of

our implementation could be improved by utilizing multiple
cores in an implementation with full support for parallelism.
While we attempted to optimize the implementation, there
are a number of optimizations which, if implemented, would
likely improve the performance of the protocols. They include
using a smaller ﬁeld size (e.g., F28) for computation on bits,
which are subsequently converted to elements of Fq for the
remaining computation. This is expected to make a greater
performance difference for large moduli q since the conversion
process adds overhead. We also note that for some of our
ﬂoating point protocols (namely, FLAdd and FLLT) combined
execution of comparisons which are run on exactly the same
input (e.g., LT and EQ executed on [p1], [p2]) can be optimized
by generating fewer random bits compared to when these
algorithms are run separately. This is due to the fact that
these algorithms ﬁrst generate a number of random values,
broadcast the result of adding the random values to the input,
and use the broadcasted value to proceed with the rest of the
computation. When such functions are executed on the same
input, we can safely use a single broadcasted value for two
different purposes.

Figure 1 provides the results of the experiments for all three
data types, and the results are also listed in Table II. The
table is included to provide performance numbers with higher
precision than what can be observed from the plots. The timing
results are given per operation when a number of them were
run in parallel. As the plots suggest, addition is very fast for
both integer and ﬁxed point data types as it is non-interactive,
while for ﬂoating point values, the algorithm is quite involved
(note that with conventional, non-secure ﬂoating point com-
putation, addition is also more costly than multiplication).
The difference in the performance of integer and ﬁxed point
addition is due to the different ﬁeld sizes. The multiplication
operation is substantially faster for integers than the two other

lllll10110310510−410−2100101OperationsTime / Operation (ms)lIntegerFixed PointFloating Pointlllll10110310510−210−1100101OperationsTime / Operation (ms)lIntegerFixed PointFloating Pointlllll1011031050123456OperationsTime / Operation (ms)lIntegerFixed PointFloating Pointlllll101103105020406080OperationsTime / Operation (ms)lIntegerFixed PointFloating PointOperation

Addition

Multiplication

Comparison

Division

Integer
Fixed point
Floating point
Integer
Fixed point
Floating point
Integer
Fixed point
Floating point
Integer
Fixed point
Floating point

Floating point logarithm
Floating point exponentiation

10

4.0 × 10−4
5 × 10−4

19

0.088
3.5
4.2
5.0
6.1
5.4
60
70
15
88
159

11

0.021
2.6
3.4
4.2
5.1
3.2
43
55
12
80
103
TABLE II

9.3
0.014
2.6
3.2
3.8
4.9
2.3
41
53
12
75
97

Number of operations in a batch
100

1, 000

1.1 × 10−4
1.4 × 10−4

9.0 × 10−5
1.2 × 10−4

10, 000
9.0 × 10−5
1.2 × 10−4

100, 000
9.0 × 10−5
1.2 × 10−4

9.0
0.012
2.5
3.1
3.7
4.7
2.2
40
50
11
74
96

9.1
0.013
2.5
3.1
3.8
4.8
2.2
41
51
12
75
97

PERFORMANCE OF IMPLEMENTED OPERATIONS FOR DIFFERENT DATA TYPES FOR A SINGLE OPERATION MEASURED IN MILLISECONDS.

the improvement is barely noticeable (≈ 3%). This complies
with the fact that the round complexity is the bottleneck for a
small number of parallel operations, while for a larger number
of operations computation becomes the bottleneck.

IX. CONCLUSIONS

This work introduces a number of secure and efﬁcient
multi-party protocols for ﬂoating point arithmetic including
complex operations and conversion between different numeric
data types. Our solutions are information-theoretically secure
in a standard framework against passive and active adversaries.
Implementation results also conﬁrm suitability of the proposed
techniques for a large variety of computations, and even show
that secure computation over ﬂoating point numbers can in
some cases outperform computation on integer or ﬁxed point
data types. While we treat a number of complex functions such
as logarithm, exponentiation, and square root, this work can be
extended to other operations such as trigonometric functions.
Lastly, additional optimizations as well as precise evaluation
and bounding of errors introduced by the algorithms are also
venues for future work.

X. ACKNOWLEDGMENTS

We thank anonymous reviewers for their valuable com-
ments. Portions of this work were sponsored by grant AFOSR-
FA9550-09-1-0223 from the Air Force Ofﬁce of Scientiﬁc
Research and grant 1223699 from the National Science Foun-
dation.

REFERENCES

[1] Boost C++ Libraries. http://www.boost.org/.
[2] GMP – The GNU Multiple Precision Arithmetic Library. http://gmplib.

org.

org/.

[3] IT cloud services user survey, pt. 2: Top beneﬁts & challenges. http:

//blogs.idc.com/ie/?p=210.

[4] OpenSSL: The Open Source toolkit for SSL/TLS. http://www.openssl.

[5] Sharemind. http://sharemind.cyber.ee/.
[6] VIFF – the Virtual Ideal Functionality Framework. http://viff.dk/.
[7] Sharemind
December
sharemind-20-reaches-a-million-operations-per-second.

second,
http://sharemind.cyber.ee/news-blog/

2.0
2010.

a million

operations

reaches

per

Fig. 2. Performance of ﬂoating point exponentiation and logarithm.

data types, as it requires one interactive operation for integer
values, while both ﬁxed point and ﬂoating point multiplication
involves truncation of the result. Comparison (less than), on
the other hand, does not have drastic performance differences
for all three data types, with the ﬂoating point comparisons
outperforming two other types. The difference between integer
and ﬁxed point comparisons (which use the same algorithm)
can be explained by the difference in the ﬁeld sizes, while
faster performance of ﬂoating point comparisons is due to the
need to compare shorter values despite using a more complex
algorithm. Note that from all of these operations, each protocol
takes at most a few msec. The remaining division operation
takes the longest to execute with the ﬂoating point algorithm
being several times faster than the other two. This can be
explained by the need in both integer and ﬁxed point division
to normalize the input for the initial approximation, which is
not needed with (normalized) ﬂoating point representation.

Figure 2 provides the timing results for exponentiation
and logarithm of ﬂoating point numbers. The overhead of
exponentiation is dominated by FLProd operation (line 17 of
FLExp2) that accounts for 90% of the overall time. Therefore,
any direct improvement in the performance of FLProd will
result in the same improvement to FLExp2. We also observed
that for a small number of operations (e.g., 10), the log-round
implementation of FLProd results in signiﬁcant savings in
performance (≈ 80%) compared to its sequential implemen-
tation, while for a large number of operations (e.g., 10,000),

lllll10110310504080120160OperationsTime / Operation (ms)lLogarithmExponentiation[8] SecureSCM Project Deliverable D9.2.

http://pi1.informatik.

uni-mannheim.de/index.php?pagecontent=site/Research.menu/
SecureSCM.page, University of Mannheim, July 2009.

[9] M. Atallah, M. Bykova, J. Li, K. Frikken, and M. Topkara. Private
In ACM Workshop on

collaborative forecasting and benchmarking.
Privacy in the Electronic Society (WPES), pages 103–114, 2004.

[10] G. Baker and P. Graves-Morris.

Pad´e approximants. Cambridge

International Conference on Cryptology and Network Security (CANS),
pages 1–20, 2009.

[33] B. Kreuter, A. Shelat, and C.H. Shen. Billion-gate secure computation
with malicious adversaries. In USENIX Security Symposium, pages 285–
300, 2012.
[34] M. Liedel.

Secure distributed computation of the square root and

applications. In ISPEC, pages 277–288, 2012.

[35] Y. Lindell and B. Pinkas. Privacy preserving data mining. Journal of

Cryptology, 15(3):177–206, 2002.

[36] T. Nishide and K. Ohta. Multiparty computation for interval, equality,
and comparison without bit decomposition protocol. In Conference on
Theory and Practice of Public Key Cryptography (PKC), pages 343–360,
2007.

[37] K. Peng and F. Bao. An efﬁcient range proof scheme.

In IEEE
International Conference on Information Privacy, Security, Risk and
Trust (PASSAT), pages 826–833, 2010.

[38] T. Reistad. Multiparty comparison – An improved multiparty protocol
for comparison of secret-shared values. In International Conference on
Security and Cryptography (SECRYPT), pages 325–330, 2009.

[39] T. Reistad and T. Toft. Linear, constant-rounds bit-decomposition.
In International Conference on Information, Security and Cryptology
(ICISC), pages 245–257, 2009.

[40] S. Setty, V. Vu, N. Panpalia, B. Braun, A.J. Blumberg, and M. Walﬁsh.
Taking proof-based veriﬁed computation a few steps closer to practical-
ity. In USENIX Security Symposium, pages 253–268, 2012.

[41] A. Shamir. How to share a secret. Communications of the ACM,

22(11):612–613, 1979.

[42] T. Toft. Constant-rounds, almost-linear bit-decomposition of secret
shared values. In Topics in Cryptology – CT-RSA, pages 357–371, 2009.
[43] A. Yao. How to generate and exchange secrets. In IEEE Symposium on

Foundations of Computer Science, pages 162–167, 1986.

APPENDIX A

ADDITIONAL PROTOCOLS

We describe an additional protocol which we call simple
division SDiv. It is called as a sub-protocol from ﬂoating
point division FLDiv. SDiv follows the idea of Goldschmidt’s
method and FPDiv in [17]. The main difference here is that
since our ﬂoating point representation maintains signiﬁcands
normalized between 2(cid:96)−1 and 2(cid:96) − 1, there is no need to
call complex initial approximation AppRcr used in FPDiv, the
main overhead of which comes from normalization. Instead,
we set the initial approximation w to 2−(cid:96). We then multiply
all values used in Goldschmidt’s method by 2(cid:96) to scale
them up. We need later to adjust the output’s exponent to
reﬂect this change. The algorithm below is iterative (with θ
iterations), and in each iteration we compute y = y(2− y) and
x = x(2−y) from the Goldschmidt’s division algorithm where
y stores the result and x stores the current error term. Because
as a result of this computation the size of the intermediate
values grows, we need to truncate the resulting values to (cid:96) + 1
bits. After the last iteration, y is updated in the same way as
before and returned as the result. The ﬁnal y is a normalized
either (cid:96) + 1-bit or (cid:96)-bit value. FLDiv will remove the extra bit
if necessary and adjust the output’s exponent accordingly to
ﬁt the ﬁnal signiﬁcand in a normalized (cid:96)-bit value.
[y] ← SDiv([a], [b], (cid:96))
1) θ ← (cid:100)log (cid:96)(cid:101);
2) [x] ← [b];
3) [y] ← [a];
4) for i = 1 to θ − 1
5)

[y] ← [y](2(cid:96)+1 − [x]);

University Press, 1995.

[11] Z. Beerliova-Trubiniova and M. Hirt. Perfectly-secure MPC with linear
In Theory of Cryptography Conference

communication complexity.
(TCC), pages 213–230, 2008.

[12] M. Blanton. Achieving full security in privacy-preserving data mining.
In IEEE International Conference on Information Privacy, Security, Risk
and Trust (PASSAT), pages 925–934, 2011.

[13] P. Bunn and R. Ostrovsky. Secure two-party k-means clustering.

In
ACM Conference on Computer and Communications Security (CCS),
pages 486–497, 2007.

[14] R. Canetti.

Security and composition of multiparty cryptographic

protocols. Journal of Cryptology, 13(1):143–202, 2000.

[15] O. Catrina and S. de Hoogh. Improved primitives for secure multiparty
integer computation. In Security and Cryptography for Networks (SCN),
pages 182–199, 2010.

[16] O. Catrina and C. Dragulin. Multiparty computation of ﬁxed-point
multiplication and reciprocal. In International Workshop on Database
and Expert Systems Application (DEXA), pages 107–111, 2009.

[17] O. Catrina and A. Saxena. Secure computation with ﬁxed-point numbers.
In Financial Cryptography and Data Security (FC), pages 35–50, 2010.
[18] R. Cramer, I. Damg˚ard, and U. Maurer. General secure multi-party
In Advances in

computation from any linear secret-sharing scheme.
Cryptology – EUROCRYPT, pages 316–334, 2000.

[19] I. Damg˚ard, M. Fitzi, E. Kiltz, J. Nielsen, and T. Toft. Unconditionally
secure constant-rounds multi-party computation for equality, compari-
In Theory of Cryptography Conference
son, bits and exponentiation.
(TCC), volume 3876 of LNCS, pages 285–304, 2006.

[20] I. Damg˚ard, M. Geisler, and M. Krøigaard. Homomorphic encryption
and secure comparison. International Journal of Applied Cryptography
(IJACT), 1(1):22–31, 2008.

[21] I. Damg˚ard, Y. Ishai, and M. Krøigaard. Perfectly secure multiparty
In

computation and the computational overhead of cryptography.
Advances in Cryptology – EUROCRYPT, pages 445–465, 2010.

[22] I. Damg˚ard, Y. Ishai, M. Krøigaard, J. Nielsen, and A. Smith. Scalable
In

multiparty computation with nearly optimal work and resilience.
Advances in Cryptology – CRYPTO, pages 241–261, 2008.

[23] P.-A. Fouque, J. Stern, and J.-G. Wackers. Cryptocomputing with

rationals. In Financial Cryptography (FC), pages 136–146, 2002.

[24] M. Franz, B. Deiseroth, K. Hamacher, S. Jha, S. Katzenbeisser, and
In IEEE
H. Schr¨oder. Secure computations on non-integer values.
International Workshop on Information Forensics and Security (WIFS),
pages 1–6, 2010.

[25] M. Franz and S. Katzenbeisser. Processing encrypted ﬂoating point
signals. In ACM Workshop on Multimedia and Security (MMSEC’11),
pages 103–108, 2011.

[26] J. Garay, B. Shoenmakers, and J. Villegas. Practical and secure solutions
for integer comparison. In Conference on Theory and Practice of Public
Key Cryptography (PKC), pages 330–342, 2007.

[27] R. Gennaro, M. Rabin, and T. Rabin. Simpliﬁed VSS and fast-track
multiparty computations with applications to threshold cryptography.
In ACM Symposium on Principles of Distributed Computing (PODC),
pages 101–111, 1998.

[28] R.E. Goldschmidt. Applications of division by convergence. Master’s

thesis, M.I.T, 1964.

[29] M. Hirt and U. Maurer. Robustness for free in unconditional multi-party
computation. In Advances in Cryptology – CRYPTO, pages 101–118,
2001.

[30] T. Hoens, M. Blanton, and N. Chawla. A private and reliable recommen-
dation system using a social network. In IEEE International Conference
on Information Privacy, Security, Risk and Trust (PASSAT), pages 816–
825, 2010.

[31] Y. Huang, D. Evans, and J. Katz. Private set intersection: Are garbled
circuits better than custom protocols. In Network and Distributed System
Security Symposium (NDSS), 2012.

[32] V. Kolesnikov, A.-R. Sadeghi, and T. Schneider. Improved garbled circuit
building blocks and applications to auctions and computing minima. In

[y] ← TruncPr([y], 2(cid:96) + 1, (cid:96));
[x] ← [x](2(cid:96)+1 − [x]);
[x] ← TruncPr([x], 2(cid:96) + 1, (cid:96));

6)
7)
8)
9) [y] ← [y](2(cid:96)+1 − [x]);
10) [y] ← TruncPr([y], 2(cid:96) + 1, (cid:96));
11) return [y];

Note that in the protocol above truncation is performed using
function TruncPr([x], (cid:96), m) from [17] instead of previously
described Trunc([x], (cid:96), m). The difference is that TruncPr
has a signiﬁcantly faster performance (2 rounds and m + 1
interactive operations, from which all but 1 round and 1
interactive operation can be precomputed), but the output is
randomized. TruncPr rounds the result to the nearest integer
with probability 1 − δ, where δ is the distance between x/2m
and the nearest integer.

A. Exponentiation

In this section we describe a ﬂoating point exponentiation
protocol that works for arbitrary parameters k and (cid:96) and is
therefore suitable for the case of k ≥ (cid:96). The protocol is similar
to the one presented in section VI-B, and the differences are
speciﬁc to handling exponents longer than signiﬁcands. From
all four regions of the exponents, only the third region is
affected in which case p1 may be positive. In the protocol,
lines 6, 7, and 14 are new, and line 6 tests whether the value
of p2, by which the signiﬁcand is to be truncated, is negative.
The result of this comparison is used on line 8 to guarantee that
the truncation is always performed by a non-negative amount
and on line 14 to normalize the value by shifting it if p1 is
positive. Positive exponent p1 that falls into the third region
will result in an overﬂow or underﬂow.
(cid:104)[v], [p], [z], [s](cid:105) ← FLExp2((cid:104)[v1], [p1], [z1], [s1](cid:105))

1) max ←(cid:6)log(2k−1 − 1 + (cid:96)) − (cid:96) + 1(cid:7);

2) [a] ← LT([p1], max, k);
3) [b] ← LT([p1],−(cid:96) + 1, k);
4) [c] ← LT([p1],−2(cid:96) + 1, k);
5) [p2] ← −[a](1 − [c])([b]([p1] + (cid:96)) + (1 − [b])[p1])
6) [e] ← LTZ([p2], k);
7) [2p1] ← Pow2([e][p1], 2k−(cid:96) + 1);
8) (cid:104)[x], [2p2](cid:105) ← Trunc([v1], (cid:96), (1 − [e])[p2]);
9) [y] ← [v1] − [x][2p2 ];
10) [d] ← EQZ([y], (cid:96));
11) [x] ← (1 − [b][s1])([x] − (1 − [d])[s1]) + [b][s1](2(cid:96) − 1 +
12) [y] ← (1 − [d])[s1]([2p2] − [y]) + (1 − [s1])[y];
13) [w] ← [a](1−[c])((1−[b])[x]+[b][s1])(1−2[s1])−[c][s1];
14) [w] ← (1 − [e])[w] + [e](1 − 2[s1])[v1][2p1 ];
15) [u] ← [a](1 − [c])([b][x] + (1 − [b])(2(cid:96) · Inv([2p2 ]))[y]) +
16) [u(cid:96)], . . . , [u1] ← BitDec([u], (cid:96), (cid:96));
17) for i = 1 to (cid:96) do in parallel
18)
19)

[ai] ← 2(cid:96)−1(1 − [ui]) + (cvi)[ui];
[bi] ← −((cid:96) − 1)(1 − [ui]) + (cpi)[ui];

(2(cid:96) − 1)[c][s1];

[d] − [x]);

0, 0(cid:105));

20) (cid:104)[vu], [pu], 0, 0(cid:105) ← FLProd((cid:104)[a1], [b1], 0, 0(cid:105), . . .,(cid:104)[a(cid:96)], [b(cid:96)],
21) [p] ← [a]([w] + [pu]) + 2k−1(1 − [a])(1 − 2[s1]);
22) [v] ← (1 − [z1])[vu] + 2(cid:96)−1[z1];
23) [p] ← (1 − [z1])[p] − ((cid:96) − 1)[z1];
24) return (cid:104)[v], [p], [0], [0](cid:105);

APPENDIX B

SECURITY ANALYSIS

First, we note that the linear secret sharing scheme achieves
perfect secrecy in presence of collusions of size at most t (i.e.,
no information can be learned about secret-shared values by t
or fewer parties) in the case of passive adversaries. Similarly,
the multiplication protocol does not reveal any information,
as the only information transmitted to the participants are the
shares. Furthermore, because other building blocks used in this
work (e.g., EQ, PreMul, etc.) have been previously shown to
be secure, information is not revealed during their execution as
well. The only value that is revealed in all of our protocols is c
in B2U and Trunc. It, however, statistically hides any sensitive
information, and the parties will not be able to learn any in-
formation about private values with non-negligible probability.
We obtain that our protocols combine only secure building
blocks without revealing any information to the computational
parties. By Cannetti’s composition theorem [14], we have
that composition of secure sub-protocols results in security
of the overall solution. More formally, to comply with the
security deﬁnition, we can build a simulator for our protocols
by invoking simulators for the corresponding building blocks
to result in the environment that will be indistinguishable from
the real protocol execution by the participants.

To show security in presence of malicious adversaries, we
need to ensure that (i) all participants prove that each step of
their computation was performed correctly and that (ii) if some
dishonest participants quit, others will be able to reconstruct
their shares and proceed with the rest of the computation. The
above is normally achieved using a veriﬁable secret sharing
scheme (VSS), and a large number of results have been
developed over the years (e.g., [27], [29], [11], [22], [21] and
others). In particular, because any linear combination of shares
is computed locally, each participant is required to prove that
it performed each multiplication correctly on its shares. Such
3 in the information theoretic or
results normally work for t < n
computational setting with different communication overhead
and under a variety of assumptions about the communication
channels. Additional proofs associated with this setting include
proofs that shares of a private value were distributed correctly
among the participants (when the dealer is dishonest) and
proofs of proper reconstruction of a value from its shares
(when not already implied by other techniques). In addition,
if at any point of the computation the participants are required
to input values of a speciﬁc form, they would have to prove
that the values they supplied are well formed. Such proofs are
not necessary for the computation that we use to construct our
protocols, but are needed by the implementations of some of

the building blocks (e.g., RandInt).

Thus, security of our protocols in the malicious model can
be achieved by using standard VSS techniques, e.g., [27], [18],
where a range proof, e.g., [37] will be additionally needed for
the building blocks. These VSS techniques would also work
with malicious input parties (who distribute inputs among the
computational parties), who would need to prove that they
generate legitimate shares of their data.

APPENDIX C

CORRECTNESS TESTING

We tested all

implemented ﬂoating point protocols —
namely, FLMul, FLDiv, FLAdd, FLLT, FLExp2, and FLLog2
— for correctness. The tests were designed to cover special

cases of relevance to the protocols. In particular, inputs to all
of the above functions were formed using each combination
from the following list:

• signiﬁcand: 2(cid:96)−1, 2(cid:96)−1 + 1, 2(cid:96) − 1
• exponent: 0, −(cid:96), +(cid:96), −2(cid:96), +2(cid:96), and a low positive

constant

• sign bit: 0, 1
• zero bit: 0, 1

This gives 37 possibilities for each operand. In addition, for
the exponentiation we use additional
tests speciﬁc to the
implementation of that function, which divides all inputs into
four regions. In particular, we add tests with exponents equal
to −2(cid:96) + 1, −(cid:96) + 1, max − 1, and max.

