Upgrading HTTPS in Mid-Air:

An Empirical Study of Strict Transport Security and Key Pinning

Michael Kranch
Princeton University

mkranch@princeton.edu

Joseph Bonneau
Princeton University

jbonneau@princeton.edu

Abstract—We have conducted the ﬁrst in-depth empirical
study of two important new web security features: strict transport
security (HSTS) and public-key pinning. Both have been added to
the web platform to harden HTTPS, the prevailing standard for
secure web browsing. While HSTS is further along, both features
still have very limited deployment at a few large websites and a
long tail of small, security-conscious sites. We ﬁnd evidence that
many developers do not completely understand these features,
with a substantial portion using them in invalid or illogical ways.
The majority of sites we observed trying to set an HSTS header
did so with basic errors that signiﬁcantly undermine the security
this feature is meant to provide. We also identify several subtle
but important new pitfalls in deploying these features in practice.
For example, the majority of pinned domains undermined the
security beneﬁts by loading non-pinned resources with the ability
to hijack the page. A substantial portion of HSTS domains and
nearly all pinned domains leaked cookie values, including login
cookies, due to the poorly-understood interaction between HTTP
cookies and the same-origin policy. Our ﬁndings highlight that
the web platform, as well as modern web sites, are large and
complicated enough to make even conceptually simple security
upgrades challenging to deploy in practice.

I.

INTRODUCTION

HTTPS [1], which consists of layering HTTP trafﬁc over
the TLS/SSL encrypted transport protocols [2] to ensure
conﬁdentiality and integrity, is the dominant protocol used
to secure web trafﬁc. Though there have been many subtle
cryptographic ﬂaws in TLS itself (see [3] for an extensive
survey), the most signiﬁcant problem has been inconsistent and
incomplete deployment of HTTPS. Browsers must seamlessly
support a mix of HTTP and HTTPS connections, enabling
stripping attacks [4] whereby network attackers attempt to
downgrade a victim’s connection to insecure HTTP despite
support for HTTPS at both the server and client.

The primary countermeasure to HTTPS stripping is strict
transport security (HSTS) [5] through which browsers learn
that speciﬁc domains must only be accessed via HTTPS. This
policy may be speciﬁed dynamically by sites using an HTTP
header or preloaded by browsers for popular domains. While
HSTS is conceptually simple, there are subtle interactions with

Permission to freely reproduce all or part of this paper for noncommercial
purposes is granted provided that copies bear this notice and the full citation
on the ﬁrst page. Reproduction for commercial purposes is strictly prohibited
without the prior written consent of the Internet Society, the ﬁrst-named author
(for reproduction of an entire paper only), and the author’s employer if the
paper was prepared within the scope of employment.
NDSS ’15, 8-11 February 2015, San Diego, CA, USA
Copyright 2015 Internet Society, ISBN 1-891562-38-X
http://dx.doi.org/10.14722/ndss.2015.23162

other browser security features, namely the same-origin policy
and protections for HTTP cookies. This leads to a number of
deployment errors which enable an attacker to steal sensitive
data without compromising HTTPS itself.

Beyond HTTPS stripping,

there are growing concerns
about weaknesses in the certiﬁcate authority (CA) system.
The public discovery of commercial software to use compelled
certiﬁcates [6], as well as high-proﬁle compromises of several
trusted CAs, have spurred interest
in defending against a
new threat model in which the attacker may obtain a rogue
certiﬁcate for a target domain signed by a trusted CA.

While many protocols have been proposed to maintain
security against an attacker with a rogue certiﬁcate for a
target domain signed by a trusted CA [3], the only defense
deployed to date is public-key pinning (or just key pinning),
by which a browser learns to only connect to speciﬁc domains
over HTTPS if one of a designated set of keys is used. This
policy can be used to “pin” a domain to a whitelist of keys
of which at least one must appear somewhere in the server’s
certiﬁcate chain. This can be used to pin a domain to speciﬁc
end-entity keys, certiﬁcate authorities, or a mix of both. Key
pinning is currently deployed only as a browser-preloaded
policy with Chrome and Firefox, although support is planned
for dynamically declared header-based pins [7].

While both technologies are still in the early stages, there
is signiﬁcant enough deployment to draw some meaningful
lessons about their use in practice. In this work, we report
on the ﬁrst comprehensive survey of HSTS and key pinning.
We use OpenWPM [8] to perform realistic crawling (see
Section III) of both the list of domains with preloaded security
policies in Firefox and Chrome and the top million most
highly-visited domains as provided by Alexa [9], examining
both sites’ static code and dynamically generated trafﬁc.

We catalog several common security bugs observed in
practice (see Sections IV–VI). A summary of these errors is
provided in Table I. To the best of our knowledge, we provide
the ﬁrst published evidence of these bugs’ appearance in the
wild. Knowledge of these errors is useful for any administrator
seeking to deploy HSTS and/or pinning securely.

In summarizing our ﬁndings (Section VIII), we highlight
several underlying causes of these errors. In particular, we
believe the speciﬁcation of HSTS and pinning suffers from
both insufﬁcient ﬂexibility and a lack of sensible defaults.
These lessons are timely given the considerable amount of
ongoing research and development of new proposals for up-
grading HTTPS security.

TABLE I.

SUMMARY OF MAIN VULNERABILITIES FOUND

Error

Preloaded HSTS without dynamic HSTS
Erroneous dynamic HSTS conﬁguration
Pinned site with non-pinned active content

§
IV-E
IV-E
V

Pinned site with non-pinned passive content

V

Cookies scoped to non-pinned subdomains

Cookies scoped to non-HSTS subdomains

VI-C
VI-B
VI-B

%

34.6%
59.5%
3.0%
55.6%
3.0%
44.4%
1.8%
44.4%
23.8%
47.8%

#

349/1,008
7,494/12,593
8/271
5/9
8/271
4/9
5/271
4/9
182/765
2,460/5,099

Prevalence

studied domains
domains with preloaded HSTS
top 1M domains attempting to set HSTS
base domains with preloaded pins
non-Google base domains with preloaded pins
base domains with preloaded pins
non-Google base domains with preloaded pins
base domains with preloaded pins
non-Google base domains with preloaded pins
base domains with preloaded HSTS
base domains with dynamic HSTS

Security implications

HTTPS stripping possible on old browsers
HTTPS stripping possible
data theft with a rogue certiﬁcate

page modiﬁcations with a rogue certiﬁcate

cookie theft with a rogue certiﬁcate
cookie theft by active network attacker
cookie theft by active network attacker

II. OVERVIEW OF WEB SECURITY TECHNOLOGIES

The core protocols of the World Wide Web, namely HTTP
and HTML, were not designed with security in mind. As
a result, a series of new technologies have been gradually
tacked on to the basic web platform. All of these are to some
extent weakened by backwards-compatibility considerations.
In this section we provide an overview of relevant web security
concepts that we will study in this paper.

A. HTTPS and TLS/SSL

HTTPS is the common name for “HTTP over TLS” [1]
which combines normal HTTP trafﬁc with the TLS [2], [10],
[11] (Transport Layer Security) protocol
instead of basic
(insecure) TCP/IP. Most HTTPS implementations will also
use the older SSL v3.0 (Secure Sockets Layer) protocol [12]
for backwards compatibility reasons although it contains a
number of cryptographic weaknesses. TLS and SSL are often
used interchangeably to describe secure transport; in this paper
we will strictly refer to TLS with the understanding that our
analysis applies equally to SSL v3.0.

The goals of TLS are conﬁdentiality against eavesdroppers,
integrity against manipulation by an active network adversary,
and authenticity by identifying one or both parties with a
certiﬁcate. The main adversary in TLS is typically called a man
in the middle (MitM) or active network attacker, a malicious
proxy who can intercept, modify, block, or redirect all trafﬁc.
Formally, this adversary is referred to as a Dolev-Yao attacker
model [13]. We will not discuss cryptographic issues with TLS;
Clark and van Oorschot provide a thorough survey [3].

1) Certiﬁcates and Certiﬁcation Authorities: The ultimate
goal of HTTPS is to bind the communication channel to the
legitimate server for a given web domain, and is achieved with
the use of server certiﬁcates in TLS.1 Names are bound at the
domain level and are sometimes referred to as “hostname,”
“host,” or “fully-qualiﬁed domain name”. In this paper, we’ll
always use “domain” to refer to a fully-qualiﬁed domain name.
We’ll use the term “base domain” to refer to the highest-
level non-public domain in a fully-qualiﬁed domain name,
also sometimes referred to as “public sufﬁx plus 1” (PS+1).2

1TLS also supports mutual authentication in which both client and server
are identiﬁed with a long-term certiﬁcate. However, in nearly all use on the
web only the server is authenticated and the client authentication is left for
higher-level protocols (typically passwords submitted over HTTPS).

2Detecting base domains is not as simple as ﬁnding the domain immediately
below the top-level domain (TLD+1) due to the existence of public sufﬁxes
of more than one domain, such as .ca.us or .ac.uk. We use Mozilla’s
public sufﬁx list (https://publicsufﬁx.org/) as the canonical list.

For example, for www.example.com the base domain is
example.com.

HTTPS clients will check that the “common name” ﬁeld in
the server’s presented certiﬁcate matches the domain for each
HTTP request. The exact rules are somewhat complicated [14]
enabling the use of wildcards and the “subject alternative
name” extension to allow certiﬁcates to match an arbitrary
number of domains.

If name matching fails or a certiﬁcate is expired, mal-
formed, or signed by an unknown or untrusted certiﬁcate
authority, HTTPS clients typically show the user a warning.
Browser vendors have made the warnings more intrusive over
time and click-through rates have declined signiﬁcantly [15],
[16] to below 50% for Firefox. This decrease is generally
considered a positive development as the vast majority of
HTTPS warning messages represent false positives due to
server misconﬁgurations or expired certiﬁcates [17], [18].

B. Strict transport security

Because a large portion of web sites only support insecure
HTTP, user agents must support both HTTP and HTTPS.
Many domains serve trafﬁc over both HTTP and HTTPS. This
enables an active network attacker to attempt to downgrade
security to plain HTTP by intercepting redirects from HTTP
to HTTPS or rewriting URLs contained in an HTTP page to
change the protocol from HTTPS to HTTP. Such an attack
is called an SSL stripping or HTTPS stripping attack. While
the threat has long been known [19],
it gained increased
attention in 2009 when the popular sslstrip software package
was released to automate the attack [4].

Browsers use graphical indicators to tell the user whether
a connection is made over HTTP or HTTPS. Typically, this
UI includes showing https in the browser’s address bar and
a padlock icon. However, user studies have indicated that the
vast majority of users (upwards of 90%) do not notice if these
security indicators are missing and are still willing to transmit
sensitive data such as passwords or banking details [20]. Thus,
it is insufﬁcient to rely on users to detect if their connection
to a normally-secure server has been downgraded to HTTP by
an attacker.

To counter the threat of HTTPS stripping, Jackson and
Barth proposed “ForceHTTPS” [21] to enable servers to re-
quest clients only communicate over HTTPS. Their proposal
was ultimately renamed “HTTP strict transport security” and

2

standardized in RFC 6797 [5].3

1) HSTS security model: HSTS works as a binary (per
domain) security policy. Once set, the user agent must refuse
to send any trafﬁc to the domain over plain HTTP. Any
request which would otherwise be transmitted over HTTP (for
example, if the user clicks on a link with the http scheme
speciﬁed) will be upgraded from HTTP to HTTPS.

In addition to upgrading all trafﬁc to HTTPS, the HSTS
speciﬁcation recommends two other changes. First, any TLS
error (including certiﬁcate errors) should result in a hard fail
with no opportunity for the user to ignore the error. Second, it
is recommended (non-normatively) that browsers disable the
loading of insecure resources from an HSTS-enabled page; this
policy has since been adopted by Chrome and Firefox for all
HTTPS pages even in the absence of HSTS (see Section V).
By default, HSTS is declared for a speciﬁc fully-
an optional
qualiﬁed domain name,
to all
includeSubDomains directive which applies
the domain setting the policy. For ex-
subdomains of
ample,
an HSTS policy with
includeSubDomains, then all trafﬁc to example.com as
well as a.example.com and b.a.example.com must be
over HTTPS only.4

if example.com sets

though there

is

Note that while HSTS requires a valid TLS connection,
it places no restrictions on the set of acceptable certiﬁcates
beyond what the user agent would normally enforce. That
is, HSTS simply requires any valid TLS connection with a
trusted certiﬁcate. It
is not designed as a defense against
insecure HTTPS due to rogue certiﬁcates (see Section II-C),
only against the absence of HTTPS completely.

2) HSTS headers: The primary means

for a server
to establish HSTS is by setting the HTTP header
[5]
Strict-Transport-Security. Compliant user agents
will apply an HSTS policy to a domain once the header has
been observed over an HTTPS connection with no errors. Note
that setting the header over plain HTTP has no effect although
a number of sites do so anyway (see Section IV-E).

In addition to the optional includeSubDomains di-
rective, an HSTS header must specify a max-age directive
instructing the user agent on how long to cache the HSTS
policy. This value is speciﬁed in seconds and represents a
commitment by the site to support HTTPS for at least that
time into future. It is possible to “break” this commitment by
serving an HSTS header with max-age=0 but this directive
must be served over HTTPS.

For regularly-visited HSTS domains (at

least once per
max-age period), the policy will be continually updated and
prevent HTTP trafﬁc indeﬁnitely. This can be described as a
continuity policy.5 A known shortcoming of HSTS is that it can
not protect initial connections or connections after extended
inactivity or ﬂushing of the browser’s HSTS state. HSTS policy

3Jackson and Barth [21] originally proposed that servers would request
HSTS status by setting a special cookie value, but this was ultimately changed
to be an HTTP header.

4Note that includeSubDomains covers subdomains to arbitrary depth
unlike wildcard certiﬁcates, which only apply to one level of subdomain [14].
5HSTS can also be described as a “trust-on-ﬁrst-use” (TOFU) scheme. We
prefer the term continuity which does not imply permanent trust after ﬁrst use.

3

caching may also be restricted by browser privacy concerns.
For example, policies learned during “private browsing” ses-
sions should be discarded because they contain a record of
visited domains.

HSTS is also vulnerable to an attacker capable of manipu-
lating the browser’s notion of time so that it thinks a policy has
expired, for example by manipulating the NTP protocol [22].
3) HSTS preloads: To address the vulnerability of HTTPS
stripping before the user agent has visited a domain and
observed an HSTS header, Chrome and Firefox both now ship
with a hard-coded list of domains receiving a preloaded HSTS
policy.This approach reduces security for preloaded domains
to maintaining an authentic, up-to-date browser installation.

receive

an

domains

Preloaded

automatic HSTS
policy from the browser
and may optionally specify
includeSubDomains.6 There is no per-domain max-age
speciﬁcation, however in Chrome’s implementation, the entire
preload list has an expiration date if the browser is not
regularly updated.

4) HTTPS Everywhere: The EFF’s HTTPS Everywhere
browser extension7 (available for Chrome and Firefox) pro-
vides similar protection for a much larger list (currently over
5,000 domains). It has been available since 2011. The HTTPS
Everywhere extension relies on a large group of volunteers
to curate a preload list in a distributed manner. Because it
is an optional (though popular) browser extension, HTTPS
Everywhere is willing to tolerate occasional over-blocking
errors in return for increased coverage compared to the more
conservative preload lists. Because HTTPS Everywhere is
crowd-sourced, errors are due to the developers and not site
operators themselves. Hence we do not study it in this work.

C. Key pinning

HSTS is useful for forcing trafﬁc to utilize HTTPS; how-
ever, it has no effect against an attacker able to fraudulently
obtain a signed certiﬁcate for a victim’s domain (often called
a rogue certiﬁcate) and use this certiﬁcate in a man-in-the-
middle attack. Because every trusted root in the browser can
sign for any domain, an attacker will succeed if they are
able to obtain a rogue certiﬁcate signed by any trusted root
(of which there are hundreds [23]–[25]). This vulnerability
has long been known and security researchers have obtained
several rogue certiﬁcates by exploiting social engineering and
other ﬂaws in the certiﬁcate authority’s process for validating
domain ownership [3].

However, in 2010 it was reported publicly for the ﬁrst time
that commercial software was available for sale to government
agencies to utilize rogue certiﬁcates to intercept trafﬁc en
masse [6]. This report raised the concern of governments
using compelled certiﬁcates obtained by legal procedures or
extralegal pressure to perform network eavesdropping attacks.
In addition to the risk of government pressure, a number
of high-proﬁle CA compromises have been detected since

6The syntax between HSTS preloads and HSTS headers is unfortunately
incompatible, with includeSubDomains speciﬁed in the former and
include_subdomains used in the latter along with other minor details
we will omit for clarity.

7https://www.eff.org/https-everywhere

20118 [26] including security breaches at Comodo and Dig-
iNotar (which has since been removed as a trusted CA from all
browsers) and improperly issued subordinate root certiﬁcates
from TrustWave and T¨urkTrust. Collectively, these issues have
demonstrated that simply requiring HTTPS via HSTS is not
sufﬁcient given the risk of rogue certiﬁcates.

1) Pinning security model: Key pinning speciﬁes a limited
set of public keys which a domain can use in establishing
a TLS connection. Speciﬁcally, a key pinning policy will
specify a list of hashes (typically SHA-1 or SHA-256) each
covering the complete Subject Public Key Info ﬁeld of an
X.509 certiﬁcate. To satisfy a pinning policy, a TLS connection
must use a certiﬁcate chain where at least one key appearing
in the chain matches at least one entry in the pin set. This
enables site operators to pin their server’s end-entity public
key, the key of the server’s preferred root CA, or the key of any
intermediate CA. Pinning makes obtaining rogue certiﬁcates
much more difﬁcult, as the rogue certiﬁcate must also match
the pinning policy which should greatly reduce the number of
CAs which are able to issue a usable rogue certiﬁcate.

In fact, the browsers’ default policy can be viewed as “pin-
ning” all domains with the set of all trusted root certiﬁcate au-
thorities’ keys. Explicit pinning policies further reduce this set
for speciﬁc domains. Much like HSTS, pinning policies apply
at the domain level but an optional includeSubDomains
directive extends this protection to all subdomains.

The risk of misconﬁgured pinning policies is far greater
than accidentally setting HSTS. HSTS can be undone as long
as the site operator can present any acceptable certiﬁcate,
whereas if a site declares a pinning policy and then can’t obtain
a usable certiﬁcate chain satisfying the pins (for example, if
it loses the associated private key to a pinned end-entity key),
then the domain will effectively be “bricked” until the policy
expires. For this reason, pinning policies often require a site
to specify at least two pins to mitigate this risk.

2) Pinning preloads: Chrome has deployed preloaded
pinning policies since 2011, although only a handful of
non-Google domains currently participate. Firefox shipped
preloaded pinning in 2014 with policies for a subset of
Chrome’s pinned domains plus several Mozilla domains. Like
with preloaded HSTS, preloaded pinning policies have no
individual expiration date but the entire set expires if the
browser is not frequently updated. No other browsers have
publicly announced plans to implement preloaded pinning.

3) Pinning headers (HPKP): A draft RFC speciﬁes HTTP
Public Key Pinning (HPKP) by which sites may declare pin-
ning policies via the Public-Key-Pins HTTP header [7].
The syntax is very similar
to HSTS, with an optional
includeSubDomains directive and a mandatory max-age
directive. Pinning policies will only be accepted when declared
via a valid TLS connection which itself satisﬁes the declared
policy.

Unlike HSTS,

the HPKP standard adds an additional
Public-Key-Pins-Report-Only header. When this
policy is declared, instead of failing if pins aren’t satisﬁed,

8It should be noted that most of these issues were detected due to Chrome’s
deployment of key pinning. It is possible a large number of CA compromises
occurred before pinning was deployed but evaded detection.

the user agent will send a report to a designated address. This
is designed as a step towards adoption for domains unsure
if they will cause clients to lose access. Additionally, the
standard recommends that user agents limit policies to 60 days
of duration from the time they are declared, even if a longer
max-age is speciﬁed, as a hedge against attackers attempting
to brick a domain by declaring a pinning policy which the
genuine owner can’t satisfy.

Currently, no browsers support HPKP and the standard
remains a draft. Chrome and Firefox have both announced
plans to support the standard when it is ﬁnalized.

III. MEASUREMENT SETUP

To study how HSTS and pinning are deployed in practice,
we made use of an automated web measurement platform. Our
goal was to measure web sites by simulating real browsing
as accurately as possible while also collecting as much data
as possible. To this end we used a version of the Firefox
browser modiﬁed only for automation and data capture. Our
approach combined static analysis of downloaded page content
with dynamic analysis of all connections actually made by the
browser on behalf of rendered page content.

OpenWPM: We utilized OpenWPM as the backbone for
our testing. OpenWPM is an open-source web-crawling and
measurement utility designed to provide a high level of repro-
ducibility [8]. OpenWPM is itself built on top of the well-
known Selenium Automated Browser [27] which abstracts
away details such as error handling and recovery.

Static Resources: Selenium provides an interface for in-
specting the parsed DOM after pages have completed loading.
We utilized this interface to extract all tags of interest to check
for potential mixed content errors that were not triggered by
our browsing.

Dynamic Resources: By itself, neither Selenium nor Open-
WPM contain an API to instrument the browser as it executes
scripts and loads resources on behalf of the page. We utilized
the Firefox add-on system to build a custom Firefox extension
to instrument and record all resource calls as the page was
loaded to capture dynamic resource loading. Our extension
implements the nsIContentPolicy interface in the Firefox
extension API, which is called prior to any resource being
loaded. This interface is designed to allow extensions to
enforce arbitrary content-loading restrictions. In our case, we
allowed all resource loads but recorded the target URL, the
page origin URL, as well as the context in which that request
was made, for example if it was the result of an image being
loaded or an XMLHttpRequest (Ajax).

Sites tested: We conducted three main crawls in our study.
The ﬁrst was a depth-one9 sputtering of every domain listed
in Chrome and Firefox’s preloaded HSTS/pinning lists. We
tried fetching both the exact domain and the standard www
subdomain. Some domains were not accessible, as we discuss
in Section IV. We did not attempt to ﬁnd alternate pages for
domains without an accessible home page. Manual inspection
suggested that all domains which failed to resolve were either

9By “depth-one”, we mean we crawled the original page and followed every

link on that page.

4

content-delivery networks without a home page by design or
domains which have ceased operation.

The second crawl was an expanded crawl of the sites with
a preloaded pinning policy. For Twitter, Dropbox, Facebook,
and Google domains, we performed this crawl while “logged
in,” that is, using valid session cookies for a real account. The
other pinned domains did not implement a login system.

Finally, we performed a HTTP and HTTPS header-only
crawl of the exact domain and www subdomain of all sites in
the Alexa top million [9] list to test for the presence of HSTS
or HPKP. We performed this crawl using ZMap [28], a tool
designed for efﬁcient probing of a large number of hosts. For
the domains attempting to set HSTS or HPKP headers, we also
ran our full OpenWPM crawl to extract cookies and test for
various additional issues.

IV. CURRENT DEPLOYMENT

In this section we provide an overview of current deploy-
ment of HSTS and pinning using crawling and inspection of
the preload lists used by Chrome and Firefox. Our statistics
are all based on observations from November 2014.

A. Preload implementations

Chrome’s preload list is canonically stored10 in a single
JSON ﬁle11 containing both pinning and HSTS policies.There
is also a supporting ﬁle specifying named keys referenced in
pinning policies. Some are speciﬁed as complete certiﬁcates
(although only the keys are germane for pinning) and other
are speciﬁed as SHA-1 hashes of keys.

Until 2014,

the criteria for inclusion in Chrome’s list
were not formally documented and inclusion required email
communication with an individual engineer responsible for
maintaining the list [29], [30]. In mid-2014, Google rolled
out an automated web form12 for requesting inclusion into
Chrome’s preload list. In addition to submitting this form,
domains must redirect from HTTP to HTTPS and set an HSTS
header with includeSubDomains, a max-age value of at
least 10,886,400 (one year), and a special preload directive.
As seen in Figure 1,
this automated process has led to
a signiﬁcant increase in the numbers of preloaded entries.
However, the new requirements have not been retroactively
enforced on old entities and new sites can be added at
the discretion of Google without meeting these requirements
(e.g. Facebook and Yahoo! were recently added but do not
set includeSubDomains). Additionally, while requests for
inclusion can now be submitted automatically, they are still
manually reviewed and take months to land in a shipping
version of the browser and there still is not an automated
method of submitting preloaded pinsets.

During our crawl, we observed 456 entries on the preload
list which present the preload token, accounting for 1.0%
of the list. Since this token was only recently introduced, this
number is a clear indication of both the effect of Google’s

10This ﬁle is not shipped with the browser but is parsed at build-time into

a machine-generated C++ ﬁle storing the policies

11Technically,

the ﬁle is not valid JSON because it

includes C-style

comments.

12https://hstspreload.appspot.com

Fig. 1. Growth of preloaded HSTS in Google Chrome.

automated entry and the growth of the HSTS list. We also
observed 127 additional sites setting the preload token in
our top million crawl that are not yet in the preloaded list.
Several of these sites (32.3%) are presenting invalid headers
so were most likely rejected by Google for inclusion in the
list; however, the remaining sites do set valid HSTS header
and must be awaiting manual conﬁrmation.

Mozilla’s implementation uses two separate ﬁles for
preloaded HSTS and pinning policies. The preloaded HSTS
ﬁle is compiled as C code. Currently, the list is a strict subset
of Chrome’s list based on those domains in Chrome’s list
which also set an HSTS header with a max-age of at least
18 weeks [31]. The includeSubDomains parameter must
also be set in the header to be preloaded, so 45 domains are
preloaded in Chrome with includeSubDomains set but
preloaded in Firefox without it. Because Mozilla’s list is a
strict subset of Chrome’s list, we perform all testing for the
remainder of the paper on the larger Chrome list.

Firefox also has a preloaded pin list which is implemented
as a JSON ﬁle with an identical schema to Chrome’s. Cur-
rently, the only entries are 9 Mozilla-operated domains, 3
test domains, and twitter.com. We perform testing only on
Chrome’s preloaded pinning lists.

B. Preloaded HSTS

Chrome’s preload list, launched in 2010, currently con-
sists of 1258 domains from 1004 unique base domains as
summarized in Table II. Roughly a quarter of the current list
represents Google-owned properties. Aside from the Google
domains, there are a large number of relatively small websites.
Figure 2 shows the distribution of preloaded sites’ Alexa trafﬁc
rankings. The median site rank is about 100,000 and the mean
is 1.5M, with 294 base domains (29%) too small to be ranked
(and thus not included in our computed mean). Additionally,
only 7 of the top 100 and 19 of the top 1000 non-Google
sites are included in the preloaded HSTS list, suggesting that
uptake is primarily driven by sites’ security interest and not by
their size. At least 15 sites on the list appear to be individual
people’s homepages.

5

Jun2012Sep2012Dec2012Mar2013Jun2013Sep2013Dec2013Mar2014Jun2014Sep2014Date0200400600800100012001400Entries024681012PinsetsTotalentriesBasedomainsGooglebasedomainsNon-GooglebasedomainsPinsetsTABLE IV.

DYNAMIC HSTS ERRORS

Alexa top 1M
%
—

Preloaded domains
Domains

Attempts to set dynamic HSTS
Doesn’t redirect HTTP→HTTPS
Sets HSTS header only via HTTP
Redirects to HTTP domain
HSTS Redirects to non-HSTS
Malformed HSTS header
max-age = 0
0 < max-age <= 1 day
Sets HSTS securely w/o errors

Domains
12,593
5,554
517
774
74
322
665
2,213
5,099

44.1%
4.1%
6.1%
0.6%
2.6%
5.3%
17.6%
40.5%

751
23
3
9
3
12
0
5
659

%
—
3.1%
0.4%
3.1%
0.4%
1.6%
0%
0.7%
87.7%

found of 100,000,000,000 (over 3,000 years). The max-age
values also vary widely amongst the major players with Paypal
only setting a header with an age of 4 hours and Twitter, Vine
and Etsy all setting max-age over 20 years.

Very short values, such as the 17.6% of domains setting
a value of a day or less, arguably undermine the value of
HSTS. While they provide some protection against a passive
eavesdropper in the case of developer mistakes (such as using
the HTTP protocol for an internal link), they are dangerous
against an active attacker as many users will be repeatedly
making untrusted initial connections if they do not visit the
site very frequently.

It is an open question if very large max-age values will
cause problems in the future if a small number of clients cache
very old policies. Unlike the proposal for HPKP, there is no
standard maximum for max-age and user agents really are
meant to cache the policy for 80 years if instructed to do so,
although most browser instances in practice will not be around
for that long.

Finally, we saw 665 sites setting a max-age of 0 including
several big domains like LinkedIn and t.co, a Twitter content
delivery domain. Yahoo actually redirects from HTTP to
HTTPS on every tested Yahoo subdomain but still speciﬁcally
avoids using HSTS and intentionally sets a max age of 0. This
is valid according to the speciﬁcation [5]. It’s intended use is
to force clients to forget their cached policy for the domain
in case the domain wishes to revert to using HTTP trafﬁc. Of
these, 121 redirect from HTTPS to HTTP, clearly using the
speciﬁcation as intended.

E. HSTS errors

Of the 12,593 sites setting dynamic HSTS, 7,494 did so
in erroneous ways. A summary of errors we observed is in
Table IV. It is striking that overall, of the non-preloaded sites
attempting to set HSTS dynamically, nearly 59.5% had a major
security problem which undermined the effectiveness of HSTS.
The rate of errors was signiﬁcantly lower among sites with a
preloaded HSTS policy.

HSTS sites failing to redirect: The HSTS speciﬁcation [5]
states that HSTS domains should redirect HTTP trafﬁc to
HTTPS. However, of the 12,593 sites attempting to set dy-
namic HSTS, 5,554 do not redirect. In addition, 65 preloaded
HSTS sites do not redirect from HTTP to HTTPS. In addition
to violating the standard this represents a security hole as ﬁrst-
time visitors may never transition to HTTPS and therefore
never learn the HSTS policy.

HSTS headers set over HTTP: Another sign of confusion
about the technology is sites setting an HSTS header over
HTTP. Although the HSTS standard speciﬁes that this has no
effect and should not be done [5], we observed 4,094 domains
setting an HSTS header on the HTTP version of their sites.
For sites otherwise implementing HSTS properly, this is a
harmless mistake; however, we found 517 domains set HTTP
HSTS headers without specifying a HTTPS header, a strong
indication that the sites misunderstood the speciﬁcation. 1,735
of the total HTTP HSTS domains (including popular tech
sites like blockchain.info and getﬁrebug.com) also failed to
redirect to HTTPS, indicating they might not understand that
HSTS does not achieve redirection on its own. In addition,
206 preloaded HSTS domains set HSTS headers over HTTP
including the pinned site CryptoCat, but these sites all also set
valid HTTPS HSTS headers.

These sites are clearly attempting to improve the security
of their connection via HSTS but perhaps misunderstanding
the nature of the technology. The relatively high proportion
of this error compared to sites successfully deploying HSTS
(32.5%) is a clear sign of signiﬁcant developer confusion.

key-value

sites

value

and www.gov.uk.
including more

including notable
The most
than

one

Malformed HSTS: We also found 322 sites

setting
like
common
for
includes
pair strict-transport-security:

malformed HSTS headers
paypal.com14
mistake was
max-age. For example, www.gov.uk’s header
the
max-age=31536000, max-age=31536000,
max-age=31536000;. In all cases of multiple max-ages,
the max-age value was the same. We conﬁrmed Chrome
this mistake without harm,
and Firefox will
but
the speciﬁcation. We also
found 3 setting simply a value without
the required key
max-age=. For example, www.pringler.com sets the header
strict-transport-security: 2678400).
This
results in the header being ignored by the browser. We saw a
further 3 sites setting a negative value for max-age which
causes the header to be ignored.

this

technically violates

tolerate

HSTS redirection to HTTP: We also observed 774 sites
which correctly set an HSTS header via HTTPS but then
immediately redirected to HTTP (where several
then set
a meaningless HSTS header). For example, https://www.
blockchain.info sets an HSTS header while redirecting to
http://blockchain.info. HSTS-compliant browsers handle this
error and instead to go to https://blockchain.info which re-
sponds successfully, but a non-HSTS-complaint browser would
be redirected back to HTTP which is clearly not the site’s
intention. In addition, 9 preloaded HSTS domains use 30x
redirects from the listed preloaded domain to HTTP, 5 of which
redirect back to HTTP versions of the same base domain which
is still protected by the preload list. The Minnesota state health
insurance exchange website, mnsure.org, is both preloaded as
HSTS and sets a dynamic HSTS header but still responds with
a 301 code (moved permanently) to http://www.mnsure.org/.
HSTS-compliant browsers will follow this redirect but then
still upgrade the connection to https://www.mnsure.org/.

14The error at PayPal was particularly interesting as a PayPal engineer was

the lead author of the HSTS standard [5].

8

F. Dynamic pinning (HPKP) deployment

Dynamic pins, as speciﬁed by the HPKP protocol, are
not yet
implemented by any major browser. Nevertheless,
we observed many sites already setting valid HPKP headers
indicating they would like to use pinning.

We found 18 domains attempting to set dynamic key pins
(of which 7 also had a preloaded HSTS policy). We found
a comparable rate of errors with dynamic key pins as with
dynamic HSTS with only 12 of 18 (67%) setting pins securely.
As with dynamic HSTS, short validity periods were a major
issue: 5 set key-pin values of 10 minutes or less (1 at only
60 seconds) and 1 was incorrectly formatted without any
max-age. It appears most of the domains experimenting with
the header are small security-focused sites, with none ranked
in the Alexa top 10,000 most popular sites.

In addition, we found two new errors not present with
HSTS. Amigogeek.net dynamically pins to a hash that is not
found with SHA-1 or SHA-256 in any of the certiﬁcates
presented by that domain. Segu-info.com.ar mislabels a SHA-
1 hash as SHA-256. Both these issues would result
in a
standards-compliant user agent ignoring these policies.

Most of the key pins we observed were speciﬁed as SHA-
256 hashes although the standard allows either SHA-1 or
SHA-256. This was somewhat surprising as SHA-1 hashes
are smaller and therefore more efﬁcient to transmit and all
preloaded pins are speciﬁed as SHA-1 hashes.

Finally, we observed little use of the HPKP standard’s
error reporting features. Only one domain (www.mnot.net)
set a Public-key-pins-report-only header to de-
tect (but not block) pinning errors and only one domain
(freenetproject.org) speciﬁed the report-uri directive to
receive error reports.

V. MIXED CONTENT

Browsers isolate content using the same-origin policy,
where the origin is deﬁned as the scheme, host, and port
of the content’s URL. For example, the contents of a page
loaded with the origin example.com should not be acces-
sible to JavaScript code loaded by the origin b.com. This
is a core principle of browser security dating to the early
development of Netscape 2.0 [33] and formally speciﬁed in
the years since [34]. Because HTTP and HTTPS are dis-
tinct schemes, the same-origin policy means content delivered
over HTTPS is isolated from any insecure HTTP content
an attacker injects with the same host and port. Therefore,
an attacker cannot simply inject a frame with an origin of
http://example.com into the browser to attempt to read
data from https://example.com.

However, subresources such as scripts or stylesheets in-
herit the origin of the encapsulating document. For example,
if example.com loads a JavaScript library from b.com,
the code has an origin of example.com regardless of the
protocol used to load it and can read user data (such as
cookies) or arbitrarily modify the page contents. When an
HTTPS page loads resources from an HTTP origin, this is
referred to as mixed content. Mixed content is considered dan-
gerous as the attacker can modify the resource delivered over
HTTP and undermine both the conﬁdentiality and integrity

of the HTTPS page, signiﬁcantly undermining the beneﬁts of
deploying HTTPS. For this reason, Internet Explorer has long
blocked most forms of mixed content by default, with Chrome
in 2011 and Firefox in 2013 following suit [35], although the
details vary and there is no standard. Other browsers (such as
Safari) allow mixed content with minimal warnings.

Not all mixed content is equally dangerous. While termi-
nology is not standardized, mixed content is broadly divided
into active content such as scripts, stylesheets, iframes, and
Flash objects, which can completely modify the contents of the
encapsulating page’s DOM or exﬁltrate data [36], and passive
or display content such as images, audio, or video which
can only modify a limited portion of the rendered page and
cannot steal data. All browsers allow passive mixed content by
default (usually modifying the graphical HTTPS indicators as a
warning). The distinction between active and passive content is
not standardized. For example, XMLHttpRequests (Ajax) and
WebSockets are considered passive content by Chrome and not
blocked but are blocked by Firefox and IE.

A. Pinning and mixed content

Unfortunately, the mixed content problem repeats itself
with pinned HTTPS (as it has for HTTPS with Extended
Validation certiﬁcates [37] and other versions of HTTPS with
increased security). If a website served over a pinned HTTPS
connection includes active subresources served over traditional
(non-pinned) HTTPS, then, just as with traditional mixed con-
tent, an attacker capable of manipulating the included resources
can hijack the encapsulating page. In the case of non-pinned
mixed content, manipulation requires a rogue certiﬁcate instead
of simply modifying HTTP trafﬁc. It should be noted that
this risk is not exactly analogous to traditional mixed content
because an attacker’s ability to produce a rogue certiﬁcate
may vary by the target domain whereas the ability to modify
HTTP trafﬁc is assumed to be consistent regardless of domain.
Still, including non-pinned content substantially undermines
the security beneﬁts provided by pinning.

A further potential issue with pinned content is that sub-
resources may be loaded over pinned HTTPS with a different
pinned set. This also represents a potential vulnerability, as
an attacker may be able to obtain a rogue certiﬁcate satisfying
the subresource’s pin set but not the encapsulating page. Thus,
the effective pin set of the encapsulating page is the union of
the pin sets of all (active) subresources loaded by the page. If
any of the subresources are not pinned, security of the page is
reduced to the “implicit” set of pins consisting of all trusted
root CAs, negating the security beneﬁts of pinning completely.
Empirical results: Overall, from the homepages of 271
total base domains with a pinning policy, we observed a total
of 66,537 non-pinned resources being included across 10 do-
mains. Of these, 24,477 resources at 8 domains (dropbox.com,
twitter.com, doubleclick.net, crypto.cat, and torproject.org)
were active content. As noted above, this effectively negates
the security goals of pinning for these domains.

While only 8 of 271 pinned base domains having active
mixed-pinning content appears low at ﬁrst glance, recall that
262 of the pinned domains are operated by Google. Google has
been diligent to avoid mixed content and has the advantage
of using its own content-delivery and advertising networks.

9

TABLE V.

SELECTED TYPES OF PINNED MIXED CONTENT RESOURCES

Active

Passive

Content type

script
stylesheet
link (rel=“stylesheet”)
xmlhttprequest
subdocument
font
total
image
link (rel=“shortcut icon”)
link (rel=“apple-touch-icon”)
media
link (rel=“image-src”)
total

#

15,540
4,725
2,470
1,515
170
49
24,477
41,702
146
132
45
36
42,061

The fact that 5 of the other 9 suffered from fatal active-mixed
content problems suggests this will be a serious problem as
pinning is incrementally deployed, especially because these
sites are on the cutting edge of security awareness.

Sources of mixed content: A summary of the types of
resources involved in mixed content errors is provided in
Table V. The errors generally arise from including active
web analytics or advertising resources. For example, crypto.
cat loads 27 scripts on 4 pages (including crypto.cat) from
get.clicky.com, a Web analytics site.

Content delivery networks were another major source of
errors. At dev.twitter.com we observed 85 loads from vari-
ous subdomains of the content-delivery network akamai.net.
Dropbox was responsible for 921 of the mixed-content loads
we observed, including loading scripts, stylesheets (CSS), and
fonts (considered active content) from cloudfront.net, another
content-delivery network. They load these resources multiple
times on essentially every page we visited within the domain.
We also observed interesting errors in “widget” iframes for
pinned sites which we happened to observe embedded in other
pages in our crawl. For example, Twitter’s embeddable gad-
get twitter.com/settings/facebook/frame loads (3 times) scripts
from connect.facebook.net. Similarly, we observed the adver-
tising network DoubleClick loading an assortment of advertis-
ing scripts from various locations within an iframe embedded
in other sites. While this is meant to be included as an iframe at
other sites, the non-pinned scripts it loads could still in some
cases steal cookies and read user data. In particular, all of
DoubleClick’s cookies and many of Twitter’s are not marked
httponly and can therefore be read by malicious scripts.

Impact of subdomains: A large number of these mixed
content errors were due to resources loaded from subdomains
of pinned domains without includeSubDomains set. Of
the 9 pinned non-Google base domains, 4 domains had mixed
content issues from loading a resource from a non-pinned
subdomain of an otherwise pinned domain. Overall, 99.96%
of the unpinned active content loads were “self-inﬂicted” in
that they were loaded from the same base domain.

Twitter had perhaps the most

issues including loading
scripts from syndication.twitter.com. Although they did set a
dynamic HSTS Header to protect this resource load from this
non-preloaded subdomain, this doesn’t ﬁx the fact that the
domain isn’t pinned. Tor also included content from numerous
non-pinned subdomains. Dropbox and CryptoCat both link to
their blog and forum subdomain without an HSTS header, and
dropbox.com loads images and other passive resources from

photo-*.dropbox.com without HSTS being set. The blog.x.com
subdomain was the most frequent subdomain with this issue
with two of the ﬁve domains introducing “self-imposed”
mixed content on this subdomain. These ﬁndings suggest that
confusion over the relationship between subdomains owned by
the same entity is a major source of errors and that developers
may be forgetting when includeSubDomains is in effect.
Some of the problems may also simply come from modern
websites’ complicated structure making it difﬁcult to remember
what is pinned and what isn’t. To this end we observed many
cases of content included from non-pinned domains owned by
the same domain in practice, though not strictly subdomains.
For example, Google loads images from *.ggpht.com on many
of the major Google domains including play.google.com.

Expanded pin set mixed content: We observed 3,032
references to resources protected by a different pin set from
8 domains. As discussed above, this expands the effective
pin set to the union of the top-level page and all resources
loaded. Of these, 42 were loaded as active content by 2
domains: Twitter and Dropbox. Twitter accounts for over 85%
of the expanded pin-set resources, primarily through self-
expansion. Since Twitter has two separately listed pin sets, it
frequently increases its effective pin set size by loading content
from the twitterCDN pin set (e.g. platform.twitter.com and
api.twitter.com) on a twitterCom pin set domain. Both Twitter
and Dropbox also include a script from ssl.google-analytics.
com in multiple places. While this is a lower risk than includ-
ing unpinned content, these ﬁndings support our expectation
that mixed content handling will be more complicated for
pinned content due to the multiple levels of potential risk..
Plain HTTP resources loaded by pinned domains:

We observed a further 30,642 references to resources over
plain HTTP from 205 pinned domains. Only one domain,
(doubleclick.net), made the mistake (observed 3 times) of
including active content over HTTP by including a script from
http://bs.serving-sys.com/. Again, this script was only loaded
in a doubleclick.net iframe we observed within another page.
These numbers serve as a useful baseline for comparison
and suggest that errors due to mixed pinning, particularly
active content, are more common than mixed HTTP content.
This suggests that this problem is not yet widely understood
or appreciated, although it can completely undermine pinning.

B. HSTS Mixed Content

We also brieﬂy consider the existence of mixed content
between HSTS-protected HTTPS pages and non-HSTS re-
sources loaded over HTTPS. Unlike the case of pinning, this
is not currently a signiﬁcant security risk because resources
referenced via a URL with the https scheme must be
accessed over HTTPS, even if they are not protected by HSTS.
There is an edge case which is not clearly deﬁned by the
speciﬁcation [5] related to error messages. The HSTS standard
requires hard failure with no warning if a connection to an
HSTS domain has a certiﬁcate error but doesn’t specify if
warnings can be shown for non-HSTS resources loaded by
the page. This is likely a moot point, as modern browsers
now typically block active content which would produce a
certiﬁcate error even from non-HSTS pages.

10

Still, we found references to non-HSTS resources from
HSTS pages were widespread, with 171,533 references from
349 base domains, of which 87,465 from 311 domains were
active content. As with the pinned mixed content errors, the
vast majority were “self-inﬂicted” in that they were resources
loaded from a common base domain, accounting for 84.73%
of all mixed content and 71.96% of the active mixed con-
tent. Resources from explicit subdomains were again a major
source of mixed policy, with 20,913 references from 115 base
domains, of which 10,577 were active content.

VI. COOKIE THEFT

A long-standing problem with the web has been the in-
consistency between the same-origin policy deﬁned for most
web content and the one deﬁned for cookies [38]–[40]. Per
the original cookie speciﬁcation [38], cookies are isolated
only by host and not by port or scheme. This means cookies
set by a domain via HTTPS will be submitted back to
the same domain over HTTP [41]. Because cookies often
contain sensitive information, particularly session identiﬁers
which serve as login credentials, this poses a major secu-
rity problem. Even if a domain secure.com only serves
content over HTTPS, an active attacker may inject script
into any page in the browser triggering an HTTP request to
http://secure.com/non-existent and the outbound
request will contain all of the users cookie’s for the domain.

A. Secure cookies

To address this problem, the secure attribute for cookies
was added in 2000 by RFC 2965 [39], the ﬁrst update to
the cookie speciﬁcation. This attribute speciﬁes that cookies
should only be sent over a “secure” connection. While this was
left undeﬁned in the formal speciﬁcation, all implementations
have interpreted this to limit the cookie to being sent over
HTTPS [40]. A persistent issue with the secure attribute is
that it protects read access but not write access. HTTP pages
are able to overwrite (or “clobber”) cookies even if they were
originally marked secure.15

B. Interaction of secure cookies and HSTS

At ﬁrst glance, it might appear that HSTS obviates the
secure cookie attribute, because if a browser learns of an
HSTS policy and will refuse to connect to a domain over
plain HTTP at all it won’t be unable to leak a secure cookie
over HTTPS. Unfortunately, there the different treatment of
subdomains which means that cookies can still be leaked.

Cookies may include a domain attribute which speciﬁes
which domains the cookie should be transmitted to. By default,
this includes all subdomains of the speciﬁed domain, unlike
HSTS which does not apply to subdomains by default. Even
more confusingly, the only way to limit a cookie to a single
speciﬁc domain is to not specify a domain parameter at all, in
which case the cookie should be limited to exactly the domain
of the page that set it. However, Internet Explorer violates
the standard [39] in this case and scopes the cookie to all
subdomains anyway [41].

15In fact, HTTP connections can set cookies and mark them as secure,

in which case they won’t be able to read them back over HTTP.

11

TABLE VI.

VULNERABLE COOKIES FROM HSTS DOMAINS

Condition

Domains with HSTS hole
Domains with vulnerable cookies
Cookies not marked secure

Preloaded HSTS

domains

#

230/765
182/765
782/823

%

30.1%
23.8%
95.0%

Dynamic HSTS

domains

#

3,637/5,099
2,460/5,099
10,174/10,398

%

70.7%
47.8%
97.8%

is

is

set). For

example,

accessible

to subdomains

the default) and the cookie

Thus, a well-intentioned website may expose cook-
ies by setting HSTS but not
the secure attribute if
the HSTS policy does not specify includeSubDomains
(which is
scoped to
(which occurs whenever
be
any domain attribute
suppose
example.com, a domain which successfully sets HSTS with-
out includeSubDomains, sets a cookie session_id=x
with domain=example.com but does not set secure. This
cookie will now be transmitted over HTTP to any subdomain
of example.com. The browser won’t connect over HTTP
to example.com due to the HSTS policy, but will connect
to http://nonexistent.example.com and leak the
cookie value x over plain HTTP.
can

to
http://nonexistent.example.com into any page
in the browser, making this cookie effectively accessible to
any network attacker despite the domain’s efforts to enforce
security via HSTS. Thus, we consider this to be a bug as
it very likely undermines the security policy the domain
administrator is hoping to enforce. HSTS does not serve as
an effective replacement for secure cookies for this reason
and it is advisable that HSTS sites generally mark all cookies
as secure unless they are speciﬁcally needed by an HTTP
subdomain.

reference

An

active

inject

a

attacker

Empirical results: This vulnerability requires three con-
ditions: an HSTS domain with a non-HSTS subdomain (a
“hole”), cookies scoped to that subdomain, and those cookies
to not be marked with secure. Table VI summarizes the
number of domains vulnerable to the attack, broken down by
these three conditions.

This issue was present on several important domains like
Paypal, Lastpass, and USAA, and the cookies included nu-
merous tracking and analytics cookies, user attributes cookies
like county code and language, and unique session identiﬁca-
tion cookies like “guest id,” “VISITORID”, and “EndUserId”.
Stealing these cookies can be a violation of user’s privacy
and may be used to obtain a unique identiﬁer for users
browsing over HTTPS. Encouragingly, however, all authen-
tication cookies we were able to identify for these sites were
marked as secure and hence could not be leaked over HTTP.
This suggests that the secure attribute is relatively well-
understood by web developers.

C. Interaction of cookies and pinning

A similar

if example.com,

issue exists for pinned domains whereby
a cookie may leak to unprotected subdomains. For ex-
ample,
a pinned domain without
includeSubDomains, sets a cookie session_id=x with
domain=example.com, the cookie will be transmitted over
unpinned HTTPS to any subdomain of example.com. Note
that even setting the secure ﬂag doesn’t help here—this will

TABLE VII.

LEAKABLE PINNED COOKIES

Domain
crypto.cat

dropbox.com
facebook.com
twitter.com

Domain Hole
*.crypto.cat

*.dropbox.com
*.facebook.com
*.twitter.com

www.gmail.com

*.www.gmail.com

total

Insecure Cookies

3
3
17
35
5
63

Total Cookies

3
8
21
38
5
75

only require the cookie to be sent over HTTPS but an attacker
able to compromise HTTPS with a rogue certiﬁcate will still
be able to observe the value of the cookie.

Because there is no equivalent attribute to secure which
would require a cookie to be sent over a pinned connection,
there is currently no good ﬁx for this problem. The only way to
securely set cookies for a pinned domain is either to limit them
to a speciﬁc domain (by not setting a domain parameter) or
to specify includeSubDomains for the pinning policy.

Empirical results: We checked for cookies vulnerable to
theft over non-pinned HTTPS from all pinned domains in the
Chrome preload list. We observed 75 cookies on 5 pinned
domains which are accessible by non-pinned subdomains,
as summarized in Table VII. As mentioned above, there is
no equivalent of the secure attribute to limit cookies to
transmission over a pinned connection, meaning all of these
cookies are vulnerable.

Interestingly, the majority of these cookies are also in fact
vulnerable to theft over plain HTTP as 63 of these cookies
(84.0%) did not set the secure attribute.16 This suggests
that even if an attribute existed to limit cookies to a pinned
connection, the semantics of this problem are complex enough
that developers may not always deploy it.

Unlike our results for HSTS domains, we crawled 4 of
the pinned sites with login cookies and we did observe
several authentication cookies vulnerable to theft. Notably,
authentication cookies17 for both Twitter (with it’s critical
auth_token cookie) and Facebook (with c_user and xs)
are both vulnerable. Both are scoped to .twitter.com and
.facebook.com, respectively, meaning they are visible to all
subdomains even though neither Twitter nor Facebook set
includeSubDomains for their base domain. Thus an at-
tacker can steal valid authentication cookies for either website
without triggering the pinning policy.

in both cases it

We responsibly disclosed this vulnerability to both
is considered unﬁx-
sites. Unfortunately,
able at
the moment as neither site is capable of setting
includeSubDomains for their preloaded HSTS policy. A
ﬁx has been proposed which will allow these sites to specify
includeSubDomainsForPinning in the preload ﬁle.

Dropbox, by contrast, sets pins for dropbox.com with-
out includeSubDomains but scoped its login cookies to
.www.dropbox.com, for which includeSubDomains is set,
preventing the cookies from being vulnerable.

16Note

that because

requires only one
line per domain for both pinning and HSTS policies, every domain
with a non-includeSubDomains pinning policy also has a non-
includeSubDomains HSTS policy (if any HSTS policy at all).

the Chrome preload ﬁle

17Identifying exactly which set of cookies is sufﬁcient to hijack a user’s
session can be a difﬁcult problem [42] but we conﬁrmed manually for Twitter
and Facebook that the vulnerable cookies were sufﬁcient to log in.

Google’s case is considerably more complex but

in-
structive. While the majority of Google’s pinning entries
set includeSubDomains including google.com and thus
would appear to avoid this error, until August 2014 play.
google.com did not set includeSubDomains.18 For sub-
domains *.play.google.com,
the play.google.com this entry
overrode the less speciﬁc google.com entry as per RFC 6797
[5]. As a result, any subdomain of play.google.com like
evil.play.google.com was not be bound by the Google pin
set and an adversary with a rogue certiﬁcate for one of
these domains would have access to all of cookies scoped
for *.google.com there. However, Google limits its “mas-
ter” authentication cookies’ scope to accounts.google.com,
which cannot be accessed by *.play.google.com, but assigns
per-subdomain authentication cookies as needed. Thus this
vulnerability was limited to login cookies giving access to
play.google.com only. Google was aware of this vulnerability
when we initially disclosed it and has since ﬁxed it by
extending includeSubDomains to cover play.google.com.
Recommendation to browsers: As a result of our ﬁndings
with pinned cookies, we recommend that browser vendors
extend the semantics of the secure attribute for cookies as
follows: if a cookie is set by a domain with a pinning policy
and marked secure, the cookie should only be transmitted
over HTTPS connections which satisfy the pinning policy of
the domain setting the cookie. This is a relatively simple ﬁx
which would close the security holes we found without intro-
ducing any new syntax. This is also a reasonable interpretation
of the original speciﬁcation for secure, which never limited
the syntax to mean simply HTTPS. Given that a large number
of developers have successfully learned to mark important
cookies as secure, it makes to extend this in a natural way
as pinning and other HTTPS upgrades are deployed.

VII. RELATED WORK

A. Empirical web security studies

Our work ﬁts nicely into a long and fruitful line of measure-
ment studies of web security, covering a wide variety web se-
curity topics such as authentication cookies [43], http-only
cookies [42], password implementations [44], third-party script
inclusions [45], [46],
third-party trackers [47], [48], Flash
cross-domain policies [49], and OpenID implementations [50],
[51]. A classic problem is detecting cross-site scripting vulner-
abilities which is practically its own sub-ﬁeld of research [52]–
[55]. A common model for this research is exploration and
analysis of an emerging threat on the web, followed by mea-
surement and crawling to detect its prevalence and character.
A desirable research outcome is for automated detection to be
built-in to web application security scanners [56]–[58].

Ultimately, browsers themselves have come to absorb some
of this logic. For example, Chrome and Firefox both now warn
developers about basic mixed-content errors in their built-in
developer consoles. Several issues identiﬁed in our research
are solid candidates for inclusion in future automated tools:
cookies vulnerable to theft, pinning mixed content, and some
types of erroneous or short-lived HSTS headers.

18Chrome’s preload list previously included the comment “play.google.com
doesn’t have include subdomains because of crbug.com/327834;” however,
this link was never valid and it isn’t clear what the original bug was.

12

B. Empirical studies of HTTPS and TLS

A signiﬁcant amount of research has also focused speciﬁ-
cally on empirical errors with HTTPS and TLS, of which Clark
and van Oorschot provide the deﬁnitive survey [3]. Important
studies of cryptographic vulnerabilities have included studies
of key revocation after the Debian randomness bug [59], stud-
ies of factorable RSA keys due to shared prime factors [60],
[61], studies of elliptic curve deployment errors in TLS [62],
forged TLS certiﬁcates in the wild [18] and multiple studies of
key sizes and cipher suites used in practice [23], [63], [64]. Our
work is largely distinct from these in that we focus on two new
aspects of HTTPS (pinning and strict transport security) which
are vulnerabilities at the HTTPS (application) level rather than
the TLS (cryptographic) level.

Perhaps the most similar study to ours was by Chen
et al. [65] which focused on mixed-content and stripping
vulnerabilities. This study was performed prior to the advent
of HSTS, pinning, and mixed content blocking. Hence, that
work can be viewed as a ﬁrst-generation study compared
to our second-generation study based on newly introduced
security measures (although no doubt many of the original
vulnerabilities are still present on the web today).

Other empirical studies of the TLS ecosystem have fo-
cused on certiﬁcate validation libraries [66], non-browser TLS
libraries [67], the interaction of HTTPS with content-delivery
networks [68], and TLS implementations in Android apps [69],
[70]. Again, these efforts all found widespread weaknesses. A
common theme is developers not correctly understanding the
underlying technology and using them in an insecure manner.

C. Other proposals for improving HTTPS

We brieﬂy overview several noteworthy proposals for fur-
ther improving HTTPS. These proposals are mainly aimed
at
limiting the risk of rogue certiﬁcates and hence could
complement or supplant key pinning. We exclude other web
security proposals such as Content Security Policy (CSP) [71].
1) DANE: DNS-based Authentication of Named Entities
(DANE) [72] is a proposal for including the equivalent of
public key pins in DNS records, relying on DNSSEC [73]
for security. This has the advantage of avoiding the scalability
concerns of preloaded pins and potentially being easier19
and more efﬁcient20 to conﬁgure than pins set in headers.
Unfortunately, DANE adoption is delayed pending widespread
support for DNSSEC, which common browsers do not cur-
rently implement.

DANE does not currently contain support for declaring
policies applicable to all subdomains. Based on our study, we
would strongly advise such support be added (and possibly
turned on by default) to avoid the type of mixed content and
cookie vulnerabilities we observed with pinning today.

2) Out-of-chain key pinning: An alternative to pinning to
keys within a cite’s certiﬁcate chain is to specify a separate
self-managed public key which must sign all end-entity public

19It is an open question whether web developers are more comfortable

conﬁguring HTTP headers or DNS TXT records.

20Unlike for HSTS which operates with relatively small headers, there is
signiﬁcant concern about the efﬁciency of setting pins in headers which may
be hundreds of bytes long for complicated pin sets as seen in our study.

13

keys, in addition to requiring a certiﬁcate chain leading to
a trusted CA. This avoids fully trusting any external CA
while offering more ﬂexibility than pinning to an enumerated
set of end-entity public keys. Conceptually, it is similar to
pinning to a domain-owned key in a CA-signed, domain-bound
intermediate certiﬁcate.21 TACK [74] proposes distributing
out-of-chain public keys using continuity,22 while Sovereign
Keys [75] proposes using a public append-only log.

instead recommending that

TACK explicitly does not contain support for extending
policies to subdomains,
imple-
menters omit the domain parameter to limit cookies to one
domain and/or set the secure ﬂag to avoid cookie theft. Of
course, the secure ﬂag is inadequate for defending against
rogue certiﬁcates given that one cannot set a TACK policy for
non-existent subdomains. Sovereign Keys, by contrast, does
support wildcards to extend support to subdomains.

3) Public logging: Due to the risk of improperly conﬁgured
pinning policies causing websites to be inaccessible, some
proposals aim simply to require that all valid certiﬁcates
be publicly logged to ensure rogue certiﬁcates are detected
after the fact. Certiﬁcate Transparency [76] (CT) is the most
prominent of these efforts, recording all valid end-entity cer-
tiﬁcates in a publicly veriﬁable, append-only log. As currently
proposed, clients will begin to reject certiﬁcates lacking proof
of inclusion in the log after universal adoption23 by all pub-
lic CAs.24 A somewhat-related proposal is Accountable Key
Infrastructure [77].

As proposed, Certiﬁcate Transparency would avoid most
of the subtle issues identiﬁed in this work in that the burden
on web developers is extremely low (the only requirement is
to start using a CT-logged certiﬁcate prior to some future
deadline). Given the number of errors we observed in our
study, this seems like a major design advantage. However,
if CT is not able to be adopted via the proposed “ﬂag day”
strategy, it may be necessary to distribute policies to browsers
specifying which domains require CT protection. This problem
would be largely similar to distributing HSTS today. Thus,
the lessons from our paper would apply to designing such a
mechanism, which most likely would be implemented as an
extra directive in HSTS itself.

VIII. CONCLUDING DISCUSSION

HSTS is still in the early stages of adoption and pinning
is in the very early stages of adoption. Nevertheless, both
technologies have already greatly enhanced security for a
number of important websites. It is a tribute to pinning that it
has been responsible for the detection of most of the known
CA compromises since it was deployed in 2010. Still our
research shows that a large number of misconﬁguration errors
are undermining the potential security in many cases with its
early deployment.

21Domain-bound

X.509’s
nameConstraints extension. However, this extension is not universally
supported and non-supporting clients will reject such certiﬁcates.

intermediates

using

are

possible

22Continuity in TACK is distinct from HSTS/HPKP in that clients only
retain a TACK policy for as long into the future as the policy has been
consistently observed in the past, subject to a maximum of 30 days.

23Even after universal adoption, clients must wait until all extant legacy

certiﬁcates have expired to require CT proofs.

24Private CAs, such as those used within an enterprise, are excluded.

Some of these issues can be attributed to developer unfa-
miliarity with the new tools as previous studies have shown
that developers traditionally make critical mistakes in the early
implementation of new techniques. It is worth emphasizing,
however, that many of the errors we observed were made
by large websites that are, in many ways, at the forefront of
web security, the very developers who should be in the best
position to understand these new tools. While familiarity may
increase with time, less security-aware developers will begin
implementing these techniques which may increase the number
of errors.

Our work should serve as a reminder that simplicity for
developers is a critical aspect of any web security technology.
Considering the root causes of some of the errors we found,
better defaults might have helped. For example, we would
advocate for a default value of perhaps 30 days for HSTS
policies set without an explicit max-age. Forcing all developers
to choose this value has probably led to unwise choices on both
ends of the spectrum in addition to malformed headers. Setting
sensible defaults for pinning is far more challenging—there is
no clear way to choose a default “backup pin” besides the
values currently being used. This issue suggests that pinning
may never be a simple “on switch” for developers unless TLS
certiﬁcate management can be abstracted away completely.

Another important takeaway from our work is that many
developers appear to not fully understand the same-origin
policy and the relation of subdomains to one another. This is
particularly true with cookies. For both HSTS and pinning,
having policies apply to subdomains by default, with an
option to disable this or turn it off for speciﬁc subdomains,
would be a safer design. This recommendation is already
reﬂected in Chrome’s new policy for preloaded inclusion which
requires sites to set includeSubDomains. Extending cook-
ies’ secure attribute to require pinning (where applicable) as
well as HTTPS, as discussed in Section VI-C, also appears
to be a simple step towards making the technology match
developer expectations more closely.

Finally, we advocate for streamlining HTTPS security
features to make conﬁguration as simple as possible. HSTS
and pinning will almost certainly not be the last HTTPS en-
hancements ever added. Already there are two distinct syntaxes
being standardized to set them in HTTP headers and browser
preloads. It would be beneﬁcial to combine dynamic HSTS and
pinning declarations into a more ﬂexible and extensible syntax
that developers can declare once, preferably with sensible
defaults, rather than expect developers to learn new syntax
and subtleties as each new patch is applied.

REFERENCES

[1] E. Rescorla, “HTTP over TLS,” RFC 2818, Internet Engineering Task

Force, 2000.

[2] T. Dierks and E. Rescorla, “The Transport Layer Security (TLS)
Protocol Version 1.2,” RFC 5246, Internet Engineering Task Force,
2008.
J. Clark and P. C. van Oorschot, “SSL and HTTPS: Revisiting past
challenges and evaluating certicate trust model enhancements,” IEEE
Symposium on Security and Privacy, 2013.

[3]

[4] M. Marlinspike, “New Tricks For Defeating SSL In Practice,” in Black

Hat DC, 2009.
J. Hodges, C. Jackson, and A. Barth, “HTTP Strict Transport Security
(HSTS),” RFC 6797, Internet Engineering Task Force, 2012.

[5]

[6] C. Soghoian and S. Stamm, “Certiﬁed Lies: Detecting and Defeating
Government Interception Attacks Against SSL,” Financial Cryptogra-
phy and Data Security, 2012.

[7] C. Evans, C. Palmer, and R. Sleevi, “Internet-Draft: Public Key Pinning

Extension for HTTP,” 2012.

[8] S. Englehardt, C. Eubank, P. Zimmerman, D. Reisman,

and
A. Narayanan, “Web Privacy Measurement: Scientiﬁc principles, en-
gineering platform, and new results,” 2014.
[9]
“Alexa: The Web Information Company,” http://www.alexa.com, 2014.
[10] T. Dierks and c. Allen, “The Transport Layer Security (TLS) Protocol

Version 1.0,” RFC 2246, Internet Engineering Task Force, 1999.

[11] T. Dierks and E. Rescorla, “The Transport Layer Security (TLS)
Protocol Version 1.1,” RFC 4346, Internet Engineering Task Force,
2006.

[12] A. Freier, P. Karlton, and P. Kocher, “The Secure Sockets Layer (SSL)
Protocol Version 3.0,” RFC 6101, Internet Engineering Task Force, May
2011.

[13] D. Dolev and A. C. Yao, “On the security of public key protocols,”

IEEE Transactions on Information Theory, vol. 29, no. 2, 1983.

[14] R. Housley, W. Ford, W. Polk, and D. Solo, “Internet X.509 Public
Key Infrastructure Certiﬁcate and CRL Proﬁle,” RFC 2459, Internet
Engineering Task Force, 1999.
J. Sunshine, S. Egelman, H. Almuhimedi, N. Atri, and L. F. Cranor,
“Crying Wolf: An Empirical Study of SSL Warning Effectiveness,”
USENIX Security Symposium, 2009.

[15]

[16] D. Akhawe and A. P. Felt, “Alice in Warningland: A Large-Scale Field
Study of Browser Security Warning Effectiveness,” USENIX Security
Symposium, 2013.

[17] D. Akhawe, B. Amann, M. Vallentin, and R. Sommer, “Here’s my cert,
so trust me, maybe?: Understanding TLS errors on the web,” 22nd
International Conference on World Wide Web (WWW), 2013.

[18] L.-S. Huang, A. Rice, E. Ellingsen, and C. Jackson, “Analyzing Forged
SSL Certiﬁcates in the Wild,” IEEE Symposium on Security and
Privacy, 2014.

[19] A. Ornaghi and M. Valleri, “Man in the middle attack demos,” Blackhat

Security, 2003.

[20] S. E. Schechter, R. Dhamija, A. Ozment, and I. Fischer, “The Emperor’s
New Security Indicators,” IEEE Symposium on Security and Privacy,
2007.

ACKNOWLEDGMENTS

[22]

We would like to thank Steven Englehardt, Christian Eu-
bank, and Peter Zimmerman for assistance using OpenWPM.
We thank David Adrian, Zakir Durmeric and Alex Halderman
for assistance obtaining header data using ZMap. We thank
Arvind Narayanan, Jennifer Rexford, Harry Kalodner, and
Laura Roberts for feedback on our write-up and presenting
our results. We thank Eric Lawrence, Adam Langley, Jeffrey
Walton, Tanvi Vyas, and Devdatta Akhawe for volunteering
helpful technical ﬁxes to earlier drafts.

[26] A. Niemann and J. Brendel, “A Survey on CA Compromises,” 2013.
J. Huggins and P. e. a. Hammant, “Selenium browser automation
[27]
framework,” http://code.google.com/p/selenium, 2014.

14

[21] C. Jackson and A. Barth, “Beware of Finer-Grained Origins,” Web 2.0

Security and Privacy, 2008.
Jose Selvi, “Bypassing HTTP Strict Transport Security,” Black Hat
Europe, 2014.
I. Ristic, “Internet SSL Survey 2010,” Black Hat USA, 2010.

[23]
[24] P. Eckersley and J. Burns, “The (decentralized) SSL observatory (Invited

[25]

Talk),” 20th USENIX Security Symposium, 2011.
J. Kasten, E. Wustrow, and J. A. Halderman, “Cage: Taming certiﬁcate
authorities by inferring restricted scopes,” Financial Cryptography and
Data Security, 2013.

[28] Z. Durumeric, E. Wustrow, and J. A. Halderman, “Zmap: Fast internet-
wide scanning and its security applications.” USENIX Security Sympo-
sium, 2013.

[56] S. Kals, E. Kirda, C. Kruegel, and N. Jovanovic, “Secubat: a web
vulnerability scanner,” 15th International Conference on World Wide
Web (WWW), 2006.

[33]

[35]

[29] A. Langley, “Strict Transport Security,” Imperial Violet (blog), January

2010.

[30] ——, “Public Key Pinning,” Imperial Violet (blog), May 2011.
[31] D. Keeler, “Preloading HSTS,” Mozilla Security blog, November 2012.
[32] Google Support, “Block adult content at your school with SafeSearch,”

retrieved 2014.
J. Ruderman, “The same origin policy,” http://www.mozilla.org/projects/
security/components/same-origin.html, 2001.

[34] A. Barth, “The Web Origin Concept,” RFC 6454, Internet Engineering

Task Force, 2011.
I. Ristic, “HTTPS Mixed Content: Still the Easiest Way to Break SSL,”
Qualys Security Labs Blog, 2014.

[36] T. Vyas, “Mixed Content Blocking Enabled in Firefox 23!” Mozilla

Security Engineering—Tanvi’s Blog, 2013.

[37] M. Zusman and A. Sotirov, “Sub-prime PKI: Attacking extended

validation SSL,” Black Hat Security Brieﬁngs, 2009.

[38] D. Kristol, “HTTP State Management Mechanism,” RFC rfc2109,

Internet Engineering Task Force, 1997.

[39] D. Kristol and L. Montulli, “HTTP State Management Mechanism,”

RFC 2965, Internet Engineering Task Force, 2000.

[40] A. Barth, “HTTP State Management Mechanism,” RFC 6265, Internet

Engineering Task Force, 2011.

[41] M. Zalewski, The Tangled Web: A Guide to Securing Modern Web

Applications. No Starch Press, 2012.

[42] Y. Zhou and D. Evans, “Why Aren’t HTTP-only Cookies More Widely

Deployed?” Web 2.0 Security and Privacy, 2010.

[43] K. Fu, E. Sit, K. Smith, and N. Feamster, “Dos and don’ts of client
authentication on the web,” in USENIX Security. Berkeley, CA, USA:
USENIX Association, 2001.
J. Bonneau and S. Preibusch, “The password thicket: technical and
market failures in human authentication on the web,” Workshop on the
Economics of Information Security (WEIS), 2010.

[44]

[45] C. Yue and H. Wang, “Characterizing insecure JavaScript practices on
the web,” 18th International Conference on World Wide Web (WWW),
2009.

[46] N. Nikiforakis, L. Invernizzi, A. Kapravelos, S. Van Acker, W. Joosen,
C. Kruegel, F. Piessens, and G. Vigna, “You are what you include:
Large-scale evaluation of remote JavaScript inclusions,” in ACM Con-
ference on Computer and Communications Security (CCS), 2012.
J. Mayer and J. Mitchell, “Third-party web tracking: Policy and
technology,” in IEEE Symposium on Security and Privacy, 2012.

[47]

[48] F. Roesner, T. Kohno, and D. Wetherall, “Detecting and defending
against third–party tracking on the web,” 9th USENIX Symposium on
Networked Systems Design and Implementation (NSDI), 2012.

[49] A. Venkataraman, “Analyzing the Flash crossdomain policies,” Master’s

thesis, 2012.

[50] S.-T. Sun and K. Beznosov, “The devil is in the (implementation) de-
tails: an empirical analysis of OAuth SSO systems,” in ACM Conference
on Computer and Communications Security (CCS). ACM, 2012.

[51] R. Wang, S. Chen, and X. Wang, “Signing me onto your accounts
through Facebook and Google: A trafﬁc-guided security study of
commercially deployed single-sign-on web services,” IEEE Symposium
on Security and Privacy, 2012.

[52] G. A. Di Lucca, A. R. Fasolino, M. Mastoianni, and P. Tramontana,
“Identifying cross site scripting vulnerabilities in web applications,” in
Int. Telecommunications Energy Conference (INTELEC), 2004.

[53] N. Jovanovic, C. Kruegel, and E. Kirda, “Pixy: A static analysis tool for
detecting web application vulnerabilities,” IEEE Symposium on Security
and Privacy, 2006.

[54] G. Wassermann and Z. Su, “Static detection of cross-site scripting
vulnerabilities,” 30th International Conference on Software Engineering
(ICSE), 2008.

[55] A. E. Nunan, E. Souto, E. M. dos Santos, and E. Feitosa, “Automatic
classiﬁcation of cross-site scripting in web pages using document-based
and URL-based features,” in 2012 IEEE Symposium on Computers and
Communications (ISCC), 2012.

[57] M. Curphey and R. Arawo, “Web application security assessment tools,”

IEEE Symposium on Security & Privacy, 2006.

[58] M. Vieira, N. Antunes, and H. Madeira, “Using web security scanners
to detect vulnerabilities in web services,” in IEEE/IFIP International
Conference on Dependable Systems & Networks (DSN), 2009.

[59] S. Yilek, E. Rescorla, H. Shacham, B. Enright, and S. Savage, “When
private keys are public: results from the 2008 debian openssl vulnera-
bility,” ACM SIGCOMM Internet Measurement Conference, 2009.

[60] A. K. Lenstra, J. P. Hughes, M. Augier, J. W. Bos, T. Kleinjung, and
C. Wachter, “Ron was wrong, Whit is right,” IACR Cryptology ePrint
Archive, 2012.

[61] N. Heninger, Z. Durumeric, E. Wustrow, and J. A. Halderman, “Mining
Your Ps and Qs: Detection of Widespread Weak Keys in Network
Devices,” USENIX Security Symposium, 2012.
J. W. Bos, J. A. Halderman, N. Heninger, J. Moore, M. Naehrig, and
E. Wustrow, “Elliptic curve cryptography in practice.” IACR Cryptology
ePrint Archive, 2013.

[62]

[63] R. Holz, L. Braun, N. Kammenhuber, and G. Carle, “The SSL land-
scape: a thorough analysis of the X.509 PKI using active and passive
measurements,” ACM SIGCOMM Internet Measurement Conference,
2011.

[64] B. Amann, M. Vallentin, S. Hall, and R. Sommer, “Revisiting SSL:
A large-scale study of the internets most trusted protocol,” Technical
report, ICSI, Tech. Rep., 2012.

[65] S. Chen, Z. Mao, Y.-M. Wang, and M. Zhang, “Pretty-bad-proxy:
An overlooked adversary in browsers’ HTTPS deployments,” IEEE
Symposium on Security and Privacy, 2009.

[66] C. Brubaker, S. Jana, B. Ray, S. Khurshid, and V. Shmatikov, “Using
Frankencerts for Automated Adversarial Testing of Certiﬁcate Valida-
tion in SSL/TLS Implementations,” IEEE Symposium on Security and
Privacy, 2014.

[67] M. Georgiev, S.

Iyengar, S. Jana, R. Anubhai, D. Boneh, and
V. Shmatikov, “The most dangerous code in the world: validating SSL
certiﬁcates in non-browser software,” ACM Conference on Computer
and Communications Security, 2012.
J. Liang, J. Jiang, H. Duan, K. Li, T. Wan, and J. Wu, “When https meets
cdn: A case of authentication in delegated service,” IEEE Symposium
on Security and Privacy, 2014.

[68]

[69] S. Fahl, M. Harbach, T. Muders, L. Baumg¨artner, B. Freisleben, and
M. Smith, “Why Eve and Mallory love Android: An analysis of Android
SSL (in) security,” ACM Conference on Computer and Communications
Security (CCS), 2012.

[70] S. Fahl, M. Harbach, H. Perl, M. Koetter, and M. Smith, “Rethinking
SSL development in an appiﬁed world,” ACM Conference on Computer
and Communications Security (CCS), 2013.

[71] S. Stamm, B. Sterne, and G. Markham, “Reining in the web with content
security policy,” in 19th International Conference on World Wide Web
(WWW), 2010.

[72] P. Hoffman and J. Schlyter, “RFC 6698: The DNS-based authentication
of named entities (DANE) transport layer security (TLS) protocol:
TLSA,” 2012.

[73] G. Ateniese and S. Mangard, “A new approach to DNS security
(DNSSEC),” ACM Conference on Computer and Communications Se-
curity (CCS), 2001.

[74] M. Marlinspike and T. Perrin, “Internet-Draft: Trust Assertions for

Certiﬁcate Keys,” 2012.

[75] P. Eckersley, “Internet-Draft: Sovereign Key Cryptography for Internet

Domains,” 2012.

[76] B. Laurie, A. Langley, and E. K¨asper, “Internet-Draft: Certiﬁcate

Transparency,” 2013.

[77] T. H.-J. Kim, L.-S. Huang, A. Perrig, C. Jackson, and V. Gligor,
“Accountable key infrastructure (AKI): A proposal for a public-key
validation infrastructure,” 22nd International Conference on World Wide

Web (WWW), 2013.

15

