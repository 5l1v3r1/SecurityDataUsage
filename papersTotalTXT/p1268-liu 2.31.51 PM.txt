MiddlePolice: Toward Enforcing Destination-Deﬁned

Policies in the Middle of the Internet

Zhuotao Liu∗

Hao Jin†

Yih-Chun Hu∗

Michael Bailey∗

∗ University of Illinois at Urbana-Champaign, † Nanjing University
∗ {zliu48, yihchun, mdbailey}@illinois.edu, † jinhaonju@gmail.com

ABSTRACT
Volumetric attacks, which overwhelm the bandwidth of a
destination, are amongst the most common DDoS attacks
today. One practical approach to addressing these attacks is
to redirect all destination traﬃc (e.g., via DNS or BGP) to
a third-party, DDoS-protection-as-a-service provider (e.g.,
CloudFlare) that is well provisioned and equipped with ﬁl-
tering mechanisms to remove attack traﬃc before passing
the remaining benign traﬃc to the destination. An alterna-
tive approach is based on the concept of network capabili-
ties, whereby source sending rates are determined by receiver
consent, in the form of capabilities enforced by the network.
While both third-party scrubbing services and network ca-
pabilities can be eﬀective at reducing unwanted traﬃc at
an overwhelmed destination, DDoS-protection-as-a-service
solutions outsource all of the scheduling decisions (e.g., fair-
ness, priority and attack identiﬁcation) to the provider, while
capability-based solutions require extensive modiﬁcations to
existing infrastructure to operate. In this paper we intro-
duce MiddlePolice, which seeks to marry the deployability of
DDoS-protection-as-a-service solutions with the destination-
based control of network capability systems. We show that
by allowing feedback from the destination to the provider,
MiddlePolice can eﬀectively enforce destination-chosen poli-
cies, while requiring no deployment from unrelated parties.

1.

INTRODUCTION

Attacks against availability, such as distributed denial of
service attacks (DDoS), continue to plague the Internet. The
most common of these attacks, representing roughly 65% of
all DDoS attacks last year [40], are volumetric attacks. In
these attacks, adversaries seek to deny service by exhaust-
ing a victim’s network resources and causing congestion.
Such attacks are diﬃcult for a victim network to mitigate
as the largest of these attacks can exceed the available up-
stream bandwidth by orders of magnitude. For example,
Internet service providers (ISP) reported attacks in excess
of 500 Gbps in 2015 [40].

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978306

One common solution to this problem is the use of DDoS-
protection-as-a-service providers, such as CloudFlare. These
providers massively over-provision data centers for peak at-
tack traﬃc loads and then share this capacity across many
customers as needed. When under attack, victims use DNS
or BGP to redirect traﬃc to the provider rather than their
own networks. The DDoS-protection-as-a-service provider
applies a variety of techniques to scrub this traﬃc, sepa-
rating malicious from benign, and then re-injects only the
benign traﬃc back into the network to be carried to the vic-
tim. Such methods are appealing, as they require no modi-
ﬁcation to the existing network infrastructure and can scale
to handle very large attacks. However, these cloud-based
systems use proprietary attack detection algorithms and ﬁl-
tering which limit the ability of customers to prioritize traﬃc
kinds or choose preferred scheduling policies. Further, exist-
ing cloud-based systems assume that all traﬃc to the victim
will be routed ﬁrst to their infrastructure, an assumption
that can be violated by a clever attacker [38, 47].

A second approach to solving volumetric DDoS attacks is
network capability-based solutions [8,11,12,34,41,42,50,51].
Such systems require a source to receive explicit permission
before being allowed to contact the destination. Such ca-
pabilities are enforced by the network infrastructure itself
(i.e., routers) and capabilities range from giving the victim
the ability to block traﬃc from arbitrary sources to giving
the victim control over the bandwidth allowed for each ﬂow.
A major advantage, then, of these capability-based systems
is the ability of the victim to control precisely what and how
much traﬃc it wants to receive. However, these capability-
based systems are not without challenges, and most face sig-
niﬁcant deployment hurdles. For instance, approaches such
as TVA [50] and NetFence [34] require secret key manage-
ment and router upgrades across diﬀerent Autonomous Sys-
tems (ASes). Yet other approaches require clients to modify
their network stack to insert customized packet headers, cre-
ating additional deployment hurdles.

In this paper, we present MiddlePolice, which seeks to
combine the deployability of cloud-based solutions with the
destination-based control of capability-based systems. Mid-
dlePolice is built on a set of traﬃc policing units (referred
as mboxes) which rely on a feedback loop of self-generated
capabilities to guide scheduling and ﬁltering. MiddlePolice
also includes a mechanism to ﬁlter nearly all traﬃc that
tries to bypass the mboxes, using only the ACL conﬁgura-
tion already present on commodity routers. We implement
MiddlePolice as a Linux Kernel Module, and evaluate it ex-
tensively over the Internet using cloud infrastructures, on

1268our private testbed, and via simulations. Our results show
that MiddlePolice can handle large-scale DDoS attacks, and
eﬀectively enforce the destination-chosen policies.
2. PROBLEM FORMULATION
2.1 MiddlePolice’s Desirable Properties
Readily Deployable and Scalable. MiddlePolice is de-
signed to be readily deployable in the Internet and suﬃ-
ciently scalable to handle large scale attacks. To be readily
deployable, a system should only require deployment at the
destination, and possibly at related parties on commercial
terms. The end-to-end principle of the Internet, combined
with large numbers of end points, is what gives rise to its
tremendous utility. Because of the diversity of administra-
tive domains, including end points, edge-ASes, and small
transit ASes, ASes have varying levels of technological so-
phistication and cooperativeness. However, some ASes can
be expected to help with deployment; many ISPs already
provide some sort of DDoS-protection services [2], so we
can expect that such providers would be willing to deploy
a protocol under commercially reasonable terms. We con-
trast this with prior capability-based work, which requires
deployment at a large number of unrelated ASes in the In-
ternet and client network stack modiﬁcation, that violates
the deployability model.

The goal of being deployable and scalable is the major
reason that MiddlePolice is designed to be built into existing
cloud-based DDoS defense systems.
Destination-driven Policies. MiddlePolice is designed to
provide the destination with ﬁne-grained control over the
utilization of their network resources. Throughout the pa-
per, we use “destination” and “victim” interchangeably. Ex-
isting cloud-based systems have not provided such function-
ality. Many previously proposed capability-based systems
are likewise designed to work with a single scheduling pol-
icy. For instance, CRAFT [28] enforces per-ﬂow fairness,
Portcullis [41] and Mirage [37] enforce per-compute fair-
ness, NetFence [34] enforces per-sender fairness, SIBRA [12]
enforces per-steady-bandwidth fairness, and SpeakUp [48]
enforces per-outbound-bandwidth fairness. If any of these
mechanisms is ever deployed, a single policy will be enforced,
forcing the victim to accept the choice made by the defense
approach. However, no single fairness regime can satisfy
all potential victims’ requirements.
Ideally, MiddlePolice
should be able to support victim-chosen policies. In addi-
tion to these fairness metrics, MiddlePolice can implement
ideas such as ARROW’s [42] special pass for critical traﬃc,
and prioritized services for premium clients.
Fixing the Bypass Vulnerability. Existing cloud-based
systems rely on DNS or BGP to redirect the destination’s
traﬃc to their infrastructures. However, this model opens up
the attack of infrastructure bypass. For example, a majority
of cloud-protected web servers are subject to IP address ex-
posure [38,47]. Larger victims that SWIP their IP addresses
may be unable to keep their IP addresses secret from a de-
termined adversary. In such cases, the adversary can bypass
the cloud infrastructures by routing traﬃc directly to the
victims. MiddlePolice includes a readily deployable mecha-
nism to address this vulnerability.

MiddlePolice is designed to augment the existing cloud-
based DDoS prevention systems with destination-selectable
policies. The literature is replete with capability-based sys-

tems that provide a single fairness guarantee with extensive
client modiﬁcation and deployment at non-aﬃliated ASes.
The novelty and challenge of MiddlePolice is therefore ar-
chitecting a system to move deployment to the cloud while
enforcing a wide variety of destination-selectable fairness
metrics. Built atop a novel capability feedback mechanism,
MiddlePolice meets the challenge, thereby protecting against
DDoS more ﬂexibly and deployably.
2.2 Adversary Model and Assumptions
Adversary Model. We consider a strong adversary owning
large botnets that can launch strategic attacks and amplify
its attack [29]. We assume the adversary is not on-path be-
tween any mbox and the victim, since otherwise it could drop
all packets. Selecting routes without on-path adversaries is
an orthogonal problem and is the subject of active research
in next-generation Internet protocols (e.g., SCION [51]).
Well-connected mboxes. MiddlePolice is built on a dis-
tributed and replicable set of mboxes that are well-connected
to the Internet backbone. We assume the Internet backbone
has suﬃcient capacity and path redundancy to absorb large
volumes of traﬃc, and DDoS attacks against the set of all
mboxes can never be successful. This assumption is a stan-
dard assumption for cloud-based systems.
Victim Cooperation. MiddlePolice’s defense requires the
victim’s cooperation. If the victim can hide its IP addresses
from attackers, it simply needs to remove a MiddlePolice-
generated capability carried in each packet and return it
back to the mboxes. The victim needs not to modify its
layer-7 applications as the capability feedback mechanism is
transparent to applications. If attackers can directly send
or point traﬃc (e.g., reﬂection) to the victim, the victim
needs to block the bypassing traﬃc. MiddlePolice includes
a packet ﬁltering mechanism that is immediately deployable
on commodity Internet routers.
Cross-traﬃc Management. We assume that bottlenecks
on the path from an mbox to the victim that is shared
with other destinations are properly managed, such that
cross-traﬃc targeted at another destination cannot cause
unbounded losses of the victim’s traﬃc. Generally, per-
destination-AS traﬃc shaping (e.g., weighted fair share) on
these links will meet this requirement.
3. SYSTEM OVERVIEW

MiddlePolice’s high-level architecture is illustrated in Fig-
ure 1. A MiddlePolice-protected victim redirects its traﬃc
to the mboxes. Each mbox polices traversing traﬃc to en-
force the bandwidth allocation policy chosen by the victim.
The traﬃc policing relies on a feedback loop of MiddlePolice-
generated capabilities to eliminate the deployment require-
ments on downstream paths. When the victim keeps its
IP addresses secret, a single deploying mbox can secure the
entire path from the mbox to the victim.

For victims whose IP addresses are exposed, attackers can
bypass the mboxes and direct attack traﬃc to the victim.
MiddlePolice designs a packet ﬁltering mechanism relying
on the ACL on commodity routers or switches to eliminate
the traﬃc that does not traverse any mbox. As long as each
bottleneck link is protected by an upstream ﬁlter, the bypass
attack can be prevented.
4. DETAILED DESIGN OF mboxes

MiddlePolice’s traﬃc policing algorithm (i) probes the

1269(and thus WR) is underestimated, the mbox can still fur-
ther deliver packets as long as the downstream path is not
congested.
Fairness Regimes. Each mbox allocates its bandwidth
estimate amongst its senders based on the sharing policies
chosen by the victim. For policies enforcing global fairness
among all senders, all mboxes sharing the same bottleneck
share their local observations.
4.1 Information Table

The basis of MiddlePolice’s traﬃc policing is an informa-
tion table (iTable) maintained by each mbox. Each row of
the iTable corresponds to a single sender. The contents of
the iTable depend on the victim-selected sharing policy; this
section describes iTable elements needed for per-sender fair-
ness, and §4.3.5 extends the iTable to other fairness regimes.
In §6, we describe a mechanism to ﬁlter source spooﬁng at
the mbox, so this section ignores source spooﬁng.

f
64

TA Pid NR ND WR WV
128
32

16

32

32

32

LR
64

Table 1. Fields of an iTable entry and their sizes (bits).
Each sender si has one row in the iTable, identiﬁed by
a unique identiﬁer f . The table contents are illustrated in
Table 1. Other than f , the remaining ﬁelds are updated in
each detection period. The timestamp TA records the cur-
rent detection period. The capability ID Pid is the maximum
number of distinct capabilities generated for si. NR stores
the number of packets received from si. ND indicates the
number of best-eﬀort packets dropped by the mbox. WR de-
termines the maximum number of privileged packets allowed
for si. The veriﬁcation window WV is designed to compute
si’s packet loss rate, whereas LR stores the LLR for si.
4.2 Capability Computation

For si, the mbox generates two types of capabilities: dis-
tinct capabilities and common capabilities. The CHM can
use either capability to authenticate that the packet has tra-
versed the mbox, though only distinct capabilities are used
to infer downstream packet losses.

A distinct capability for si is computed as follows:

C = IPMP || ts || Pid || f || TA ||

MACKs (IPMP || ts || Pid || f || TA),

(1)
where IPMP is the IP address of the mbox issuing C and ts is
the current timestamp (included to mitigate replay attack).
The combination of Pid||f||TA ensures the uniqueness of C.
The MAC is computed based on a secret key Ks shared by
all mboxes. The MAC is 128 bits, so the entire C consumes
∼300 bits. A common capability is deﬁned as follows

Cc = IPMP || ts || MACKs (IPMP || ts).

(2)

The design of capability incorporates a MAC to ensure
that attackers without secure keys cannot generate valid ca-
pabilities, preventing capability abuse.
4.3 Trafﬁc Policing Logic
4.3.1 Populating the iTable
We ﬁrst describe how to populate the iTable. At time
ts, the mbox receives the ﬁrst packet from si. It creates an
entry for si, with f computed based on si’s source address,
and initializes the remaining ﬁelds to zero. It then updates

Figure 1. The architecture of MiddlePolice. The mboxes
police traﬃc to enforce victim-chosen policies. The packet
ﬁltering discards all traﬃc bypassing the mboxes.

available downstream bandwidth from each mbox to the vic-
tim and (ii) allocates the bandwidth to senders according to
the policies chosen by the victim.
Bandwidth Probe. The fundamental challenge of estimat-
ing downstream bandwidth is that MiddlePolice requires no
deployment at downstream links. Such a challenge is two-
sided: an overestimate will cause downstream ﬂooding, ren-
dering traﬃc policing useless, while an underestimate will
waste downstream capacity, reducing performance.

To solve the overestimation problem, MiddlePolice relies
on a capability feedback mechanism to make senders self-
report how many packets they have successfully delivered
to the victim. Speciﬁcally, upon a packet arrival, the mbox
stamps an unforgeable capability in the packet. When the
packet is delivered to the victim, MiddlePolice’s capability
handling module (CHM) deployed on the victim returns the
carried capability back to the mbox. If the capability is not
returned to the mbox after a suﬃciently long time interval
(compared with the RTT between the mbox and victim), the
mbox will consider the packet lost. Thus, the feedback en-
ables the mbox to infer a packet loss rate (hereinafter, LLR)
for each sender. Then the mbox estimates the downstream
capacity as the diﬀerence between the number of packets re-
ceived from all senders and packets lost on the downstream
path. As the estimation is based on the traﬃc volume deliv-
ered to the victim, this approach solves the overestimation
problem.

However, the above technique does not overcome the un-
derestimation problem. Speciﬁcally, since the traﬃc de-
mand may be less than downstream capacity, simply using
the volume of delivered traﬃc may cause underestimation.
To prevent underestimation, the mbox categorizes packets
from each sender as privileged packets and best-eﬀort pack-
ets. Speciﬁcally, the mbox maintains a rate window WR for
each sender to determine the amount of privileged packets
allowed for the sender in each period (hereinafter, detection
period ). WR is computed based on the above downstream
capacity estimation as well as victim-chosen policies. Pack-
ets sent beyond WR are classiﬁed as best-eﬀort packets. The
mbox forwards all privileged packets to the victim, whereas
the forwarding decisions for best-eﬀort packets are subject
to a short-term packet loss rate (hereinafter, SLR). The SLR
reﬂects downstream packet loss rates (congestion) at a RTT
granularity. That is, if the downstream is not congested
upon an arrival of a best-eﬀort packet, the mbox will for-
ward the packet. Thus, even when the downstream capacity

The Internet BackboneCloudCloudThe VictimISPISPThe mboxPacket Filtering1270Deﬁnition

Symb.
Dp
Thcap
Thrtt Maximum waiting time for cap. feedback
Thdrop
SLR thres. for dropping best-eﬀort pkts

The length of the detection period
The upper bound of capability ID

slr
β

Thlpass

Sslr

The weight of historical loss rates
The threshold for calculating LLR

The length limit of the cTable

Table 2. System parameters.

Value

4s
128
1s
0.05
0.8
5

100

TA to ts, increases both NR and Pid by one to reﬂect the
packet arrival and computes a capability using the updated
Pid and TA.
Upon receiving a packet from si with arrival time ta−TA >
Dp (Dp is the length of the detection period), the mbox starts
a new detection period for si by setting TA = ta. The mbox
also updates the remaining ﬁelds based on the traﬃc policing
algorithm (as described in §4.3.4). The algorithm depends
on si’s LLR and the mbox’s SLR, the computation of which
is described in the following two sections.
Inferring the LLR for Source si

4.3.2
Capability Generation. For each packet from si, the
mbox generates a distinct capability for the packet if (i) its
arrival time ta − TA < Dp − Thrtt, and (ii) the capability
ID Pid < Thcap. The ﬁrst constraint ensures that the mbox
allows at least Thrtt for each capability to be returned from
the CHM. By setting Thrtt well above the RTT from the
mbox to the victim, any missing capabilities at the end of
the current detection period correspond to lost packets. Ta-
ble 2 lists the system parameters including Thrtt and their
suggested values. MiddlePolice’s performance with diﬀerent
settings are studied in §8.3. The second constraint Thcap
bounds the number of distinct capabilities issued for si in
one detection period, and thus bounds the memory require-
ment. We set Thcap = 128 to reduce the LLR sampling error
while keeping memory overhead low.

Packets violating either of the two constraints, if any, will
carry a common capability (Equation (2)), which is not re-
turned by the CHM or used to learn si’s LLR. However, it
can be used for packet authentication.
Capability Feedback Veriﬁcation. Let Kth denote the
number of distinct capabilities the mbox generates for si,
with capability ID ranging from [1, Kth]. Each time the mbox
receives a returned capability, it checks the capability ID to
determine which packet (carrying the received capability)
has been received by the CHM. WV represents a window
with Thcap bits. Initially all the bits are set to zero. When a
capability with capability ID i is received, the mbox sets the
ith bit in WV to one. At the end of the current detection
period, the zero bits in the ﬁrst Kth bits of WV indicate
the losses of the corresponding packets. To avoid feedback
reuse, feedback is processed only for capabilities issued in
the current period.
LLR Computation. LLR in the kth detection period is
computed at the end of the period, i.e., the time when the
mbox starts a new detection period for si. si’s lost pack-
ets may contain downstream losses and best-eﬀort packets
dropped by the mbox (ND). The number of packets that si
sent to downstream links is NR − ND, and the downstream
packet loss rate is V0Pid
, where V0 is the number of zero bits
in the ﬁrst Pid bits of WV . Thus, the estimated number

of downstream packet losses is N dstream
Then we have LLR = (N dstream

+ ND)/NR.

loss

loss

= (NR − ND) V0Pid

.

Our strawman design is subject to statistical bias, and
may have negative eﬀects on TCP timeouts. In particular,
assume one legitimate TCP source recovers from a timeout
and sends one packet to probe the network condition.
If
the packet is dropped again, the source will enter a longer
timeout. However, with the strawman design, the source
would incorrectly have a 100% loss rate. Adding a low-pass
ﬁlter can ﬁx this problem: if si’s NR in the current period
is less than a small threshold Thlpass, the mbox sets its LLR
in the current period to zero. Attackers may exploit this
design using on-oﬀ attacks [30]. In §4.3.4, we explain how
to handle such attacks. Formally, we write LLR as follows

(cid:40)0,

if NR < Thlpass

, otherwise

(3)

+ND

LLR =

N dstream
NR
Inferring the SLR

loss

4.3.3
SLR is designed to reﬂect downstream congestion on a per-
RTT basis, and is computed across all ﬂows from the mbox to
the victim. Like LLR, SLR is learned through capabilities
returned by the CHM. Speciﬁcally, the mbox maintains a
hash table (cTable) to record capabilities used to learn its
SLR. The hash key is the capability itself and the value is a
single bit value (initialized to zero) to indicate whether the
corresponding key (capability) has been returned.
As described in §4.3.2, when a packet arrives (from any
source), the mbox stamps a capability on the packet. The
capability will be added into cTable if it is not a common ca-
pability and cTable’s length has not reached the predeﬁned
threshold Sslr. The mbox maintains a timestamp Tslr when
the last capability is added into cTable. Then, it uses the
entire batch of capabilities in cTable to learn the SLR. We
set Sslr = 100 to allow fast capability loading to the cTable,
while minimizing sampling error from Sslr being too small.
The mbox allows at most Thrtt from Tslr to receive feed-
back for all capabilities in cTable. Some capabilities may
be returned before Tslr (i.e., before cTable ﬁlls). Once a
capability in cTable is returned, the mbox marks it as re-
ceived. Upon receiving a new packet with arrival time ta >
, where Z0 is the
number of cTable entries that are not received. The mbox
then resets the current cTable to be empty to start a new
monitoring cycle for SLR.
4.3.4 Trafﬁc Policing Algorithm
We formalize the traﬃc policing logic in Algorithm 1.
Upon receiving a packet P , the mbox retrieves the entry
F in iTable matching P (line 8). If no entry matches, the
mbox initializes an entry for P .
P is categorized as a privileged or best-eﬀort packet based
on F’s WR (line 10). All privileged packets are accepted,
whereas best-eﬀort packets are accepted conditionally. If P
is privileged, the mbox performs necessary capability han-
dling (line 11) before appending P to the privileged queue.
The mbox maintains two FIFO queues to serve all accepted
packets: the privileged queue serving privileged packets and
the best-eﬀort queue serving best-eﬀort packets. The privi-
leged queue has strictly higher priority than the best-eﬀort
queue at the output port. CapabilityHandling (line 16) exe-
cutes the capability generation and cTable updates (line 37),
as detailed in §4.3.2 and §4.3.3.

Tslr + Thrtt, the mbox computes SLR = Z0Sslr

1271Algorithm 1: Traﬃc Policing Algorithm.
1 Input:
2
3 Output:
4
5

iTable updates and possible cTable updates;
The forwarding decision of P ;

Packet P arrived at time ts;

6 Main Procedure:
7 begin
8
9
10

F ← iTableEntryRetrieval(P );
F.NR ← F .NR + 1;
if F.NR < F.WR then

11
12

13

14

15

/* Privileged packets

CapabilityHandling(P , F);
Append P to the privileged queue;

else

/* Best-effort packets

BestEﬀortHandling(P , F);

/* Starting a new detection period if necessary

if ts − F .TA > Dp then iTableHandling(F);

*/

*/

*/

16 Function: CapabilityHandling(P , F):
17 begin

18
19

20

21

22

23

/* Two constraints for distinct-capability generation */

if F.Pid < Thcap and ts−F.TA < Dp−Thrtt then
F.Pid ← F .Pid + 1;
Generate capability C based on Equation (1);
cTableHandling(C);

else

/* Common capability for packet authentication

Generate capability Cc based on Equation (2);

*/

24 Function: BestEﬀortHandling(P , F):
25 begin
26

if SLR < Thdrop

slr and F.LR < Thdrop
CapabilityHandling(P , F);
Append P to the best-eﬀort queue;
Drop P ; F.ND ← F .ND + 1;

slr

else

then

27
28

29
30

34

35
36

31 Function: iTableHandling(F):
32 begin
33

Compute recentLoss based on Equation (3);
/* Consider the historical loss rate
F.LR ← (1 − β) · recentLoss + β · F .LR;
WR ← BandwidthAllocationPolicy(F);
Reset WV , Pid, NR and ND to zero;

37 Function: cTableHandling(C):
38 begin

39
40

41

/* One batch of cTable is not ready

if cTable.length < Sslr then
if cTable.length == Sslr then Tslr ← ts ;

Add C into cTable;

*/

*/

slr

If P is a best-eﬀort packet, its forwarding decision is sub-
ject to the mbox’s SLR and F(cid:48)s LLR (line 24). If the SLR
exceeds Thdrop
, indicating downstream congestion, the mbox
discards P . Further, if F’s LLR is already above Thdrop
, the
mbox will not deliver best-eﬀort traﬃc for F as well since
F already experiences severe losses. Thdrop
is set to be few
times larger than a TCP ﬂow’s loss rate in normal network
condition [46] to absorb burst losses. If the mbox decides to
accept P , it performs capability handling (line 27).

slr

slr

Finally, if P ’s arrival triggers a new detection period for
F (line 15), the mbox performs corresponding updates for
F (line 31). To determine F’s LLR, the mbox incorporates
both the recent LLR (recentLoss) obtained in the current
detection period based on Equation (3) and F’s historical
loss rate LR. This design prevents attackers from hiding
their previous packet losses via on-oﬀ attacks. F’s WR is
updated based on the victim-selected policy (line 35), as
described below.
4.3.5 Bandwidth Allocation Policies
We list the following representative policies that may be
NaturalShare: for each sender, the mbox sets its WR for
the next period to the number of delivered packets from the
sender in the current period. The design rationale is that the
mbox allows a rate that the sender can sustainably transmit
without experiencing a large LLR.

chosen to implement in BandwidthAllocationPolicy.

PerSenderFairshare allows the victim to enforce per-sender
fair share at bottlenecks. Each mbox fairly allocates its es-
timated total downstream bandwidth to the senders that
reach the victim through the mbox. To this end, the mbox
maintains the total downstream bandwidth estimate N total
size ,
which it allocates equally among all senders.

To ensure global fairness among all senders, two mboxes
sharing the same bottleneck (i.e., the two paths connect-
ing the two mboxes with the victim both traverse the bot-
tleneck link) share their local observations. We design a
co-bottleneck detection mechanism using SLR correlation:
if two mboxes’ observed SLRs are correlated, they share a
bottleneck with high probability. In §8.3, we evaluate the
eﬀectiveness of this mechanism.
PerASFairshare is similar to PerSenderFairshare except that
the mbox fairly allocates N total
size on a per-AS basis. This pol-
icy mimics SIBRA [12], preventing bot-infested ASes from
taking bandwidth away from legitimate ASes.
PerASPerSenderFairshare is a hierarchical fairness regime:
the mbox ﬁrst allocates N total
size on a per-AS basis, and then
fairly assigns the share obtained by each AS among the
senders of the AS.

PremiumClientSupport provides premium service to pre-
mium clients, such as bandwidth reservation for upgraded
ASes. The victim pre-identiﬁes its premium clients to Mid-
dlePolice. PremiumClientSupport can be implemented to-
gether with the aforementioned allocation policies.
5. PACKET FILTERING

When the victim’s IP addresses are kept secret, attack-
ers cannot bypass MiddlePolice’s upstream mboxes to route
attack traﬃc directly to the victim. In this case, the down-
stream packet ﬁltering is unnecessary since MiddlePolice can
throttle attacks at the upstream mboxes. However, in case
of IP address exposure [38, 47], the victim needs to deploy
a packet ﬁlter to discard bypassing traﬃc. MiddlePolice
designs a ﬁltering mechanism that extends to commodity
routers the port-based ﬁltering of previous work [20, 21].
Unlike prior work, the ﬁltering can be deployed upstream
of the victim as a commercial service.
5.1 Filtering Primitives

Although the MAC-incorporated capability can prove that
a packet indeed traverses an mbox, it requires upgrades from
deployed commodity routers to perform MAC computation
to ﬁlter bypassing packets. Thus, we invent a mechanism

1272based on the existing ACL conﬁgurations on commodity
routers. Speciﬁcally, each mbox encapsulates its traversing
packets into UDP packets (similar techniques have been ap-
plied in VXLAN and [24]), and uses the UDP source and
destination ports (a total of 32 bits) to carry an authentica-
tor, which is a shared secret between the mbox and the ﬁlter-
ing point. As a result, a 500Gbps attack (the largest attack
viewed by Arbor Networks [40]) that uses random port num-
bers will be reduced to ∼100bps since the chance of a correct
guess is 2−32. The shared secret can be negotiated period-
ically based on a cryptographically secure pseudo-random
number generator. We do not rely on UDP source address
for ﬁltering to avoid source spooﬁng.
5.2 Packet Filtering Points

Deployed ﬁltering points should have suﬃcient bandwidth
so that the bypassing attack traﬃc cannot cause packet
losses prior to the ﬁltering. The ﬁltering mechanism should
be deployed at, or upstream of, each bottleneck link caused
by the DDoS attacks. For instance, for a victim with high-
bandwidth connectivity, if the bottleneck link is an internal
link inside the victim’s network, the victim can deploy the
ﬁlter at the inbound points of its network.
If the bottle-
neck link is the link connecting the victim with its ISP, the
victim can can work with its ISP, on commercially reason-
able terms, to deploy the ﬁlter deeper in the ISP’s network
such that the bypassing traﬃc cannot reach the victim’s net-
work. Working with the ISPs does not violate the deploy-
ment model in §2 as MiddlePolice never requires deployment
at unrelated ASes.
6. SOURCE VALIDATION

MiddlePolice punishes senders even after an oﬀending ﬂow
ends. Such persistence can be built on source authentica-
tion or any mechanism that maintains sender accountability
across ﬂows. As a proof-of-concept, we design a source val-
idation mechanism that is speciﬁc to HTTP/HTTPS traf-
ﬁc. The mechanism ensures that a sender is on-path to its
claimed source IP address. This source veriﬁer is completely
transparent to clients.

Our key insight is that the HTTP Host header is in the
ﬁrst few packets of each connection. As a result, the mbox
monitors a TCP connection and reads the Host header. If
the Host header reﬂects a generic (not sender-speciﬁc) host-
name (e.g., victim.com), the mbox intercepts this ﬂow, and
redirects the connection (HTTP 302) to a Host containing a
token cryptographically generated from the sender’s claimed
source address, e.g., T .victim.com, where T is the token. If
the sender is on-path, it will receive the redirection, and its
further connection will use the sender-speciﬁc hostname in
the Host header. When an mbox receives a request with a
sender-speciﬁc Host, it veriﬁes that the Host is proper for the
claimed IP source address (if not, the mbox initiates a new
redirection), and forwards the request to the victim. Thus,
by performing source veriﬁcation entirely at the mbox, pack-
ets from spoofed sources cannot consume any downstream
bandwidth from the mbox to the victim.

If the cloud provider hosting the mbox is trusted (for in-
stance, large CDNs have CAs trusted by all major browsers),
the victim can share its key such that HTTPS traﬃc can be
handled in the same way as HTTP traﬃc. For untrusted
providers [21, 31], the mbox relays the encrypted connec-
tion to the victim, which performs Host-header-related op-
erations. The victim terminates unveriﬁed senders without

Figure 2. The software stack of the mbox and CHM.

returning capabilities to the mbox, so the additional traﬃc
from the unveriﬁed senders is best-eﬀort under Algorithm 1.
In this case, packets from spoofed sources consume limited
downstream bandwidth but do not rely on the trustworthi-
ness of the cloud provider. We acknowledge that performing
source validation at the victim is subject to the DoC at-
tack [10] in which attackers ﬂood the victim with new con-
nections to slow down the connection-setup for legitimate
clients. This attack is mitigated if the source validation is
completely handled by the mbox.
IMPLEMENTATION
7.

We have a full implementation of MiddlePolice.

7.1 The Implementation of CHM and mboxes
The mboxes and the CHM at the victim are implemented
based on the NetFilter Linux Kernel Module, which com-
bined have ∼1500 lines of C code (excluding the capability
generation code). The software stack of our implementation
is illustrated in Figure 2.

All inbound traﬃc from clients to an mbox is subject to the
traﬃc policing whereas only accepted packets go through the
capability-related processing. Packet dropping due to traﬃc
policing triggers iTable updates. For each accepted packet,
the mbox rewrites its destination address as the victim’s ad-
dress to point the packet to the victim. To carry capabil-
ities, rather than deﬁning a new packet header, the mbox
appends the capabilities to the end of the original data pay-
load, which avoids compatibility problems at intermediate
routers and switches. The CHM is responsible for trimming
these capabilities to deliver the original payload to the vic-
tim’s applications. If the packet ﬁlter is deployed, the mbox
performs the IP-in-UDP encapsulation, and uses the UDP
source and destination port number to carry an authenti-
cator. All checksums need to be recomputed after packet
manipulation to ensure correctness. ECN and encapsula-
tion interactions are addressed in [13].

To avoid packet fragmentation due to the additional 68
bytes added by the mbox (20 bytes for outer IP header, 8
bytes for the outer UDP header, and 40 bytes reserved for a
capability), the mbox needs to be a priori aware of the MTU
Md on its path to the victim. Then the mbox sets its MSS
to no more than Md−68−40 (the MSS is 40 less than the
MTU). We do not directly set the MTU of the mbox’s NIC
to rely on the path MTU discovery to ﬁnd the right packet
size because some ISPs may block ICMP packets. On our
testbed, Md = 1500, so we set the mbox’s MSS to 1360.

Upon receiving packets from upstream mboxes, the CHM
strips their outer IP/UDP headers and trims the capabili-
ties. To return these capabilities, the CHM piggybacks ca-
pabilities to the payload of ACK packets. To ensure that

ClientsThe	VictimRewrite	daddras	the	victim’s	addressAppend	capability	to	the	payloadPerform	IP-in-UDP	encapsulationStrip	the	outer	IP	headers	Trim	capability	feedbackRewrite	saddras	the	mbox’saddressStrip	the	outer	IP	and	UDP	headers	Trim	capabilitiesatpacketfooterAppend	capabilities	in	ACK	payloadPerform	IP	tunneling	to	mboxesTraffic	PolicingUpdate	cTable&	iTableThe	InternetmboxCHMPacket	Filtering1273a capability is returned to the mbox issuing the capability
even if the Internet path is asymmetric, the CHM performs
IP-in-IP encapsulation to tunnel the ACK packets to the
right mbox. We allow one ACK packet to carry multiple
capabilities since the victim may generate cumulative ACKs
rather than per-packet ACKs. Further, the CHM tries to
pack more capabilities in one ACK packet to reduce the
capability feedback latency at the CHM. The number of ca-
pabilities carried in one ACK packet is stored in the TCP
option (the 4-bit res1 option). Thus, the CHM can append
up to 15 capabilities in one ACK packet if the packet has
enough space and the CHM has buﬀered enough capabilities.
Upon receiving an ACK packet from the CHM, the mbox
strips the outer IP header and trims the capability feedback
(if any) at the packet footer. Further, the mbox needs to
rewrite the ACK packet’s source address back to its own
address, since the client’s TCP connection is expecting to
communicate with the mbox. Based on the received capa-
bility feedback, the mbox updates the iTable and cTable
accordingly to support the traﬃc policing algorithm.
7.2 Capability Generation

We use the AES-128 based CBC-MAC, based on the In-
tel AES-NI library, to compute MACs, due to its fast speed
and availability in modern CPUs [6, 23]. We port the capa-
bility implementation (∼400 lines of C code) into the mbox
and CHM kernel module. The mbox needs to perform both
capability generation and veriﬁcation whereas the CHM per-
forms only veriﬁcation.
8. EVALUATION
8.1 The Internet Experiments

This section studies the path length and latency inﬂation

for rerouting clients’ traﬃc to mboxes hosted in the cloud.

8.1.1 Path Inﬂation
We construct the AS level Internet topology based on the
CAIDA AS relationships dataset [4], including 52680 ASes
and their business relationships [16]. To construct the com-
munication route, two constraints are applied based on the
routes export policies in [19, 22]. First, an AS prefers cus-
tomer links over peer links and peer links over provider links.
Second, a path is valid only if each AS providing transit is
paid. Among all valid paths, an AS prefers the path with
least AS hops (a random tie breaker is applied if necessary).
As an example, we use Amazon EC2 as the cloud provider
to host the mboxes, and obtain its AS number based on the
report [5]. Amazon claims 11 ASes in the report. We ﬁrst
exclude the ASes not appearing in the global routing table,
and ﬁnd that AS 16509 is the provider for the remaining
Amazon ASes, so we use AS 16509 to represent Amazon.

We randomly pick 2000 ASes as victims, and for each vic-
tim we randomly pick 1500 access ASes. Among all victims,
1000 victims are stub ASes without direct customers and the
remaining victims are non-stub ASes. For each AS-victim
pair, we obtain the direct route from the access AS to the
victim, and the rerouted path through an mbox. Table 3
summarizes the route comparison. N hop
inﬂa is the average AS-
hop inﬂation of the rerouted path compared with the direct
route. P short
is the percentage of access ASes that can reach
the victim with fewer hops after rerouting and P no
inﬂa is per-
centage of ASes without hop inﬂation.

cut

Victims

Non-stub ASes

Stub ASes

Overall

N hop
inﬂa
1.1
1.5
1.3

cut

P short
P no
inﬂa
10.6% 22.2%
8.4% 18.0%
9.5% 20.1%

Table 3. Rerouting traﬃc to mboxes causes small AS-hop
inﬂation, and ∼10% access ASes can even reach the victim
with fewer hops through mboxes.

Figure 3.
paths under various Internet conditions.

[Internet] FCTs for direct paths and rerouted

Overall, it takes an access AS 1.3 more AS-hops to reach
the victim after rerouting. Even for stub victims, which are
closer the Internet edge, the average hop inﬂation is only
1.5. We also notice that ∼10% ASes have shorter paths due
to the rerouting.

Besides EC2, we also perform path inﬂation analysis when
mboxes are hosted by CloudFlare. The results show that
the average path inﬂation is about 2.3 AS-hops. For any
cloud provider, MiddlePolice has the same path inﬂation as
the cloud-based DDoS solutions hosted by the same cloud
provider, since capability feedback is carried in ACK pack-
ets. As such, deploying MiddlePolice into existing cloud-
based systems does not increase path inﬂation.
8.1.2 Latency Inﬂation
In this section, we study the latency inﬂation caused by
the rerouting. In our prototype running on the Internet, we
deploy 3 mboxes on Amazon EC2 (located in North Amer-
ica, Asia and Europe), one victim server in a US university
and about one hundred senders (located in North America,
Asia and Europe) on PlanetLab [1] nodes. We also deploy
few clients on personal computers to test MiddlePolice in
home network. The wide distribution of clients allows us to
evaluate MiddlePolice on various Internet links. We did not
launch DDoS attacks over the Internet, which raises ethical
and legal concerns.
Instead, we evaluate how MiddlePo-
lice may aﬀect the clients in the normal Internet without
attacks, and perform the experiments involving large scale
DDoS attacks on our private testbed and in simulation.

In the experiment, each client posts a 100KB ﬁle to the
server, and its traﬃc is rerouted to the nearest mbox before
reaching the server. We repeat the posting on each client
10,000 times to reduce sampling error. We also run the
experiment during both peak hours and midnight (based on
the server’s timezone) to test various network conditions. As
a control, clients post the ﬁles to server via direct paths.
Figure 3 shows the CDF of the ﬂow completion times
(FCTs) for the ﬁle posting. Overall, we notice ∼9% av-
erage FCT inﬂation, and less than 5% latency inﬂation in
home network. Therefore, traﬃc rerouting introduces small
extra latency to the clients.

MiddlePolice’s latency inﬂation includes both rerouting-
induced networking latency and capability-induced compu-

 0 20 40 60 80 100 0 500 1000 1500 2000 2500CDF (%)Flow Completion Time (FCT) (ms)Home (direct)Home (reroute)Institute (direct)Institute (reroute)1274[Testbed] Throughput and goodput when Mid-

Figure 4.
dlePolice policies diﬀerent numbers of senders.
tational overhead. As evaluated in §8.2.1, the per-packet
processing latency overhead caused by capability computa-
tion is ∼1.4 µs, which is negligible compared with typical
Internet RTTs. Thus, MiddlePolice has latency almost iden-
tical to the existing cloud-based DDoS mitigation.
8.2 Testbed Experiments
8.2.1 Trafﬁc Policing Overhead
In this section, we evaluate the traﬃc policing overhead
on our testbed. We organize three servers as one sender, one
mbox and one receiver. All servers, shipped with a quad-core
Intel 2.8GHz CPU, run the 3.13.0 Linux kernel. The mbox
is installed with multiple Gigabit NICs to connect both the
sender and receiver. A long TCP ﬂow is established be-
tween the sender and receiver, via the mbox, to measure
the throughput. To emulate a large number of sources, the
mbox creates an iTable with N entries. Each packet from the
sender triggers a table look up for a random entry. We im-
plement a two-level hash table in the kernel space to reduce
the look up latency. Then the mbox generates a capability
based on the obtained entry.

Figure 4 shows the measured throughput and goodput un-
der various N . The goodput is computed by subtracting the
additional header and capability size from the total packet
size. The baseline throughput is obtained without Middle-
Police. Overall, the policing overhead in high speed network
is small. When a single mbox deals with 100,000 sources
sending simultaneously, throughput drops by ∼10%. Equiv-
alently, MiddlePolice adds around 1.4 microseconds latency
to each packet processed. By replicating mboxes, the vic-
tim can distribute the workload across many mboxes when
facing large scale attacks.
8.2.2 Enforce Destination-Deﬁned Policies
We now evaluate MiddlePolice’s performance for enforc-
ing victim-deﬁned policies, along with the eﬀectiveness of
ﬁltering bypassing traﬃc. This section evaluates pure Mid-
dlePolice.
In reality, once built into cloud-based systems,
MiddlePolice needs only to process traﬃc that passes their
pre-deployed defense.
Testbed Topology. Figure 5 illustrates the network topol-
ogy, including a single-homed victim AS purchasing 1Gbps
bandwidth from its ISP, an mbox and 10 access ASes. The
ISP is emulated by a Pronto-3297 48-port Gigabit switch to
support packet ﬁltering. The mbox is deployed on a server
with multiple Gigabit NICs, and each access AS is deployed
on a server with a single NIC. We add 100ms latency at the
victim via Linux traﬃc control to emulate the typical Inter-
net RTT. To emulate large scale attacks, 9 ASes are com-
promised. Attackers adopt a hybrid attack proﬁle: 6 attack

Figure 5. Testbed network topology.

Figure 6. [Testbed] Packet ﬁltering via ACL.

ASes directly send large volumes of traﬃc to the victim,
emulating ampliﬁcation-based attacks, and the remaining
attack ASes route traﬃc through the mbox. Thus, the total
volume of attack traﬃc is 9 times as much as the victim’s
bottleneck link capacity. Both the inbound and outbound
points of the mbox are provisioned with 4Gbps bandwidth
to ensure the mbox is not the bottleneck, emulating that the
mbox is hosted in the cloud.
Packet Filtering. We ﬁrst show the eﬀectiveness of the
packet ﬁlter. Six attack ASes spoof the mbox’s source ad-
dress and send 6Gbps UDP traﬃc to the victim. The attack
ASes scan all possible UDP port numbers to guess the shared
secret. Figure 6 shows the volume of attack traﬃc bypass-
ing the mbox and its volume received by the victim. As the
chance of a correct guess is very small, the ﬁlter can eﬀec-
tively stop the bypassing traﬃc from reaching the victim.
Further, even if the shared secret were stolen by attackers
at time ts, the CHM would suddenly receive large numbers
of packets without valid capabilities. Since packets travers-
ing the mbox carry capabilities, the CHM realizes that the
upstream ﬁltering has been compromised. The victim then
re-conﬁgures the ACL using a new secret to recover from key
compromise. The ACL is eﬀective within few milliseconds
after reconﬁguration. Thus, the packet ﬁltering mechanism
can promptly react to a compromised secret.
NaturalShare and PerASFairshare Policies.
In this sec-
tion, we ﬁrst show that the mbox can enforce the Natural-
Share and PerASFairshare policies. We use the default pa-
rameter setting in Table 2, and defer detailed parameter
study in §8.3. Since MiddlePolice conditionally allows an AS
to send faster than its WR, we use the window size, deﬁned
as the larger value between an AS’s WR and its delivered
packets to the victim, as the performance metric. For clear
presentation, we normalize the window size to the maximum
number of 1.5KB packets deliverable through a 1Gbps link
in one detection period. We do not translate window sizes
to throughput because packet sizes vary.

Attackers adopt two representative strategies:

(i) they
send ﬂat rates regardless of packet losses, and (ii) they dy-
namically adjust their rates based on packet losses (reac-
tive attacks). To launch ﬂat-rate attacks, the attackers keep
sending UDP traﬃc to the victim. The CHM uses a dedi-

 0 200 400 600 800 10 20 30 40 50 60 70 80 90 100Bandwidth (Mbps)The number of senders (K)GoodputThroughputBaselinemboxISPVictimLegitimate ASAttackers…Bottleneck	01234567Traffic	Volume	(Gbps)Attack	Traffic	bypassing	the	mboxBypassingtraffic	received	by	the	victimKey	stolen𝒕𝒔ACLreconfig1275(a) Window sizes in ﬂat-attacks.

(b) LLRs in ﬂat-attacks.

(c) Window sizes in reactive att. (d) LLRs in reactive attacks.

Figure 7. [Testbed] Enforcing the NaturalShare policy. The legitimate AS gradually obtains a certain amount of bandwidth
under ﬂat-rate attacks since attackers’ window sizes drop consistently over time (Figure 7(a)) due to their high LLRs (Figure
7(b)). However, the attack ASes can consume over 95% of the bottleneck bandwidth via reactive attacks (Figure 7(c)) while
maintaining low LLRs similar to the legitimate AS’s LLR (Figure 7(d)).

(a) Window sizes in ﬂat-attacks.

(b) LLRs in ﬂat-attacks.

(c) Window sizes in reactive att. (d) LLRs in reactive attacks.

[Testbed] Enforcing the PerASFairshare policy. The legitimate AS can obtain at least the per-AS fair rate at
Figure 8.
the bottleneck regardless of the attack strategies (Figures 8(a) and 8(c)). Further, the legitimate AS gains slightly more
bandwidth than the attackers under ﬂat-rate attacks as the attack ASes have large LLRs (Figure 8(b)).

cated ﬂow to return received capabilities to the mbox since
no ACK packets are generated for UDP traﬃc. One way
of launching reactive attacks is that the attackers simul-
taneously maintain many more TCP ﬂows than the legiti-
mate AS. Such a many-to-one communication pattern allows
the attackers to occupy almost the entire bottleneck, even
through each single ﬂow seems completely “legitimate”.

The legitimate AS always communicates with the victim

via a long-lived TCP connection.

Figure 7 shows the results for the NaturalShare policy. As
the bottleneck is ﬂooded by attack traﬃc, the legitimate AS
is forced to enter timeout at the beginning, as illustrated in
Figure 7(a). The attackers’ window sizes are decreasing over
time, which can be explained via Figure 7(b). As the volume
of attack traﬃc is well above the bottleneck’s capacity, all
attack ASes’ LLRs are well above Thdrop
. Thus, the mbox
drops all their best-eﬀort packets. As a result, when one
attack AS’s window size is W (t) in detection period t, then
W (t+1) ≤ W (t) since in period t+1 any packet sent beyond
W (t) is dropped. Further, any new packet losses from the
attack AS, caused by an overﬂow at the bottleneck buﬀer,
will further reduce W (t + 1). Therefore, all attack ASes’
window sizes are consistently decreasing over time, creating
spare bandwidth at the bottleneck for the legitimate AS. As
showed in Figure 7(a), the legitimate AS gradually recovers
from timeouts.

slr

The NaturalShare policy, however, cannot well protect the
legitimate AS if the attackers adopt the reactive attack strat-
egy. By adjusting the sending rates based on packet losses,
the attack ASes can keep their LLRs low enough to regain
the advantage of delivering best-eﬀort packets. Meanwhile,

Figure 9. [Testbed] MiddlePolice ensures that the premium
client (AS A) receives consistent bandwidth.

they can gain much more bandwidth by initiating more TCP
ﬂows. Figure 7(c) shows the window sizes when each at-
tack AS starts 200 TCP ﬂows whereas the legitimate AS
has only one. The attackers consume over 95% of the bot-
tleneck bandwidth, while keeping low LLRs similar to that
of the legitimate AS (Figure 7(d)).

Figure 8 shows the results for the PerASFairshare policy.
Figures 8(a) and 8(c) demonstrate that the legitimate AS
receives at least per-AS fair rate at the bottleneck regard-
less of the attack strategies, overcoming the shortcomings
of the NaturalShare policy. Further, under ﬂat-rate attacks,
the legitimate AS has slightly larger window sizes than the
attackers since, again, the mbox does not accept any best-
eﬀort packets from the attackers due to their high LLRs (as
showed in Figure 8(b)).
PremiumClientSupport Policy. This section evaluates the
PremiumClientSupport policy. We consider a legitimate AS
(AS A) that is a premium client which reserves half of the
bottleneck bandwidth. Figure 9 plots AS A’s bandwidth

 0 0.1 0.2 0.3 0 20 40 60 80 100Normalized window sizeTime (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 20 40 60 0 20 40 60 80 100LLR (%)Time (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 0.1 0.2 0.3 0.4 0 20 40 60 80 100Normalized window sizeTime (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 1 2 3 4 5 0 20 40 60 80 100LLR (%)Time (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 0.1 0.2 0.3 0 20 40 60 80 100Normalized window sizeTime (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 20 40 60 80 0 20 40 60 80 100LLR (%)Time (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 0.1 0.2 0.3 0 20 40 60 80 100Normalized window sizeTime (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 1 2 3 4 5 0 20 40 60 80 100LLR (%)Time (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS	010020030040050060070011020100100020003000AS	A's	Bandwidth	(Mbps)The	number	of	senders	from	the	attack	ASesPremium	SupportNo	Protection1276(a) NaturalShare.

(b) PerSenderFairshare.

(c) Jain’s fairness index (FI).

(d) FI for various mbox counts.

Figure 10. [Simulation] Evaluating NaturalShare & PerSenderFairshare in large scale. Figures 10(a) and 10(b) show that the
clients’ average window size is larger than that of the attackers under both ﬂat-rate and shrew attacks. Figure 10(c) proves
that the clients’ window sizes converge to fairness in the PerSenderFairshare policy. Figure 10(d) shows that MiddlePolice can
enforce strong fairness among all senders even without coordination among the mboxes.

when the number of senders from the attack ASes increases.
With the PremiumClientSupport policy, MiddlePolice ensures
AS A receives consistent bandwidth regardless of the number
of senders from the attack ASes. However, without such a
policy, the attack ASes can selﬁshly take away the majority
of bottleneck bandwidth by involving more senders.
8.3 Large Scale Evaluation

In this section, we further evaluate MiddlePolice via large
scale simulations on ns-3 [3]. We desire to emulate real-world
DDoS attacks in which up to millions of bots ﬂood a victim.
To circumvent the scalability problem of ns-3 at such a scale,
we adopt the same approach in NetFence [34], i.e., by ﬁx-
ing the number of nodes (∼5000) and scaling down the link
capacity proportionally, we can simulate attack scenarios
where 1 million to 10 million attackers ﬂood a 40Gbps link.
The simulation topology is similar to the testbed topology,
except that all attackers are connected to the mbox.

Besides the ﬂat-rate attacks and reactive attacks, we also
consider the on-oﬀ shrew attacks [30] in the simulations.
Both the on-period and oﬀ-period in shrew attacks are 1s.
The number of attackers is 10 times larger than that of le-
gitimate clients. In ﬂat-rate attacks and shrew attacks, the
attack traﬃc volume is 3 times larger than the capacity of
the bottleneck. In reactive attacks, each attacker opens 10
connections, whereas a client has one. The bottleneck router
buﬀer size is determined based on [9], and the RTT is 100ms.
NaturalShare & PerSenderFairshare in Scale. Figure 10
shows the results for enforcing NaturalShare and PerSender-
Fairshare policies with default parameter settings. We plot
the ratio of clients’ average window size to attackers’ aver-
age window size for the NaturalShare policy in Figure 10(a).
For ﬂat-rate attacks and shrew attacks, it may be surprising
that the clients’ average window size is larger than that of
the attackers. Detailed trace analysis shows that it is be-
cause that the window sizes of a large portion of attackers
keep decreasing, as we explained in our testbed experiment.
As the number of attackers is much larger than the client
count, the attackers’ average window size turns out to be
smaller than that of the clients, although the absolute vol-
ume of attack traﬃc may be still higher. Under reactive
attacks, the clients’ average window size (almost zero) is
too small to be plotted in Figure 10(a).

Figure 10(b) shows that the clients enjoy even larger win-
dow ratio gains under the PerSenderFairshare policy in ﬂat-
rate and shrew attacks because even more attackers enter
the window dropping mode. Further, the PerSenderFairshare

Figure 11.
reﬂects whether two mboxes share a bottleneck.

[Simulation] The SLR correlation coeﬃcient

ensures that the clients’ average window size is close to the
per-sender fair rate in reactive attacks. Figure 10(c) demon-
strates that each client’s window size converges to per-client
fairness as Jain’s fairness index [15] (FI) is close to 1.
mbox Coordination. To enforce global per-sender fairness,
the mboxes sharing the same bottleneck link share their lo-
cal observations (§4.3.5). We ﬁrst investigate how bad the
FI can be without such inter-mbox coordination. We recon-
struct the topology to create multiple mboxes, and map each
client to a random mbox. The attackers launch reactive at-
tacks. The results, plotted in Figure 10(d), show that the
FI drops slightly, by ∼8%, even if 20 mboxes make local rate
allocations without any coordination among them.

To complete our design, we further propose the following
co-bottleneck detection mechanism. The design rationale
is that if two mboxes’ SLR observations are correlated, they
share a bottleneck with high probability. To validate this, we
rebuild the network topology to create the scenarios where
two mboxes share and do not share a bottleneck, and study
the correlation coeﬃcient of their SLRs. We compute one co-
eﬃcient for every 100 SLR measurements from each mbox.
Figure 11 shows the CDF of the coeﬃcient. Clearly, the
coeﬃcient reﬂects whether the two mboxes share a bottle-
neck. Thus, by continuously observing such correlation be-
tween two mboxes’ SLRs, MiddlePolice can determine with
increasing certainty whether or not they share a bottleneck,
and can conﬁgure their coordination accordingly.
Parameter Study. We evaluate MiddlePolice using dif-
ferent parameters than the default values in Table 2. We
mainly focus on Dp, Thdrop
and β. For each parameter, we
vary its value to obtain the clients’ average window size un-
der the 10-million bot attack. The results showed in Table
5 are normalized to the window sizes obtained using the
default parameters in Table 2.
Under the NaturalShare policy, the shorter Dp produces a

slr

 0 1 2 3 4 5 2 4 6 8 10Average window ratio (log2)The number of attackers (million)Flat-rate attacksShrew attacksWindow gain over flatWindow gain over shrew 0 1 2 3 4 5 2 4 6 8 10Average window ratio (log2)The number of attackers (million)Flat-rate attacksShrew attacksReactive attacksWindow gain over flatWindow gain over shrew 0 0.2 0.4 0.6 0.8 1 2 4 6 8 10Jain’s fairness indexThe number of attackers (million)Flat-rate attacksShrew attacksReactive attacks 0 0.2 0.4 0.6 0.8 1 2 4 6 8 10Jain’s fairness indexThe number of attackers (million)Single mbox10 mboxes20 mboxes 0 20 40 60 80 100-0.4-0.2 0 0.2 0.4 0.6 0.8 1CDF (%)SLR correlation coefficientCo-bottleneckDiff. bottlenecks1277Pushback [36]

TVA [50]

Netfence [34]

Phalanx [17] Mirage [37]

SIBRA [12]

MiddlePolice

Source upgrades
Dest. upgrades
AS deployment

No
No

Unrelated

Router support

O(N ) states

Fairness regimes

Other

requirements

None

None

Yes
Yes

Unrelated

O(N ) states;
Cryptography

None

New header

Yes
Yes

Yes
Yes

Unrelated

Unrelated

O(N ) states

O(N ) states;
Cryptography

Per-sender
New header;
Passport [32]

Yes
Yes

Related

Larger
memory

Puzzle;

Yes
Yes

No
Yes

Unrelated

Related

None

Per-AS
Redesign

None

Victim-chosen

None

None

Per-compute

New header

IPv6 upgrade

the Internet

Table 4. Property comparison with other research proposals. “O(N ) states” means that the number of states maintained by a
router increases with the number of attackers. “Cryptography” means that a router needs to support cryptography operation,
e.g., MAC computation. “Puzzle” means that the mechanism requires computational puzzle distribution.

(a) The NaturalShare Policy

Dp

Thdrop

slr

2s
Flat
1.1
Shrew 1.3

8s
0.17
0.65

0.03
0.78
0.77

0.1
0.39
1.0

β

0.5
1.1
1.2

0.9
0.78
0.80

(b) The PerSenderFairshare Policy

Dp

Thdrop

slr

Flat
Shrew

Reactive

2s
1.0
1.1
1.0

8s
1.1
0.98
0.99

0.03
1.0
0.72
1.0

0.1
0.69
0.83
0.94

β

0.5
0.85
1.0
1.0

0.9
0.81
0.98
1.0

Table 5.
diﬀerent parameter settings.

[Simulation] Clients’ average window size under

larger window size for legitimate clients since each sender’s
WR is updated per-period so that a smaller Dp causes faster
cut in attackers’ window sizes. For Thdrop
, a smaller value
slows down the clients’ recovery whereas a larger value al-
lows larger window sizes for attackers. Both will reduce the
clients’ share. A larger β has negative eﬀects as it takes
more time for the clients to recover to a low LLR.

slr

With the PerSenderFairshare policy, MiddlePolice’s perfor-
mance is more consistent under diﬀerent parameter settings.
The most sensitive parameter is Thdrop
slr because it determines
whether one source can send best-eﬀort traﬃc.
9. RELATED WORK

In this section, we brieﬂy discuss previous academic work.
Previous research approaches can be generally categorized
into capability-based approaches (SIFF [49], TVA [50], Net-
Fence [34]), ﬁltering-based approaches (Traceback [43, 45],
AITF [11], Pushback [25, 36], StopIt [33]), overlay-based
approaches (Phalanx [17], SOS [27]), deployment-friendly
approaches (Mirage [37], CRAFT [28]), approaches based
on new Internet architectures (SCION [51], SIBRA [12],
XIA [39], AIP [7]), and others (SpeakUp [48], SDN-based [18,
44], CDN-based [21]). We summarize the properties of one
or two approaches from each category in Table 4. The com-
parison shows that MiddlePolice requires the least deploy-
ment (no source upgrades, no additional router support and
no deployment from unrelated ASes) while providing the
strongest property (enforcing destination-chosen policies).
10. DISCUSSION

We brieﬂy cover some aspects not previously discussed.

mboxes Mapping. MiddlePolice can leverage the end-user
mapping [14] to achieve better mbox assignment, such as

redirecting clients to the nearest mbox, mapping clients ac-
cording to their ASes, and load balancing.
Incorporating Endhost Defense. MiddlePolice can co-
operate with the DDoS defense mechanism deployed, if any,
on the victim. For instance, via botnet identiﬁcation [26,35],
the victim can instruct the mboxes to block botnet traﬃc
early at upstream so as to save more downstream bandwidth
for clients. Such beneﬁts are possible because the policies
enforced by MiddlePolice are completely destination-driven.
Additional Monetary Cost. As discussed in 8.2.1, Mid-
dlePolice introduces small computational overhead. Com-
pared with basic DDoS-as-a-service solutions, MiddlePolice
oﬀers additional functionalities such as enabling destination-
chosen policies and ﬁltering bypassing traﬃc. In a competi-
tive marketplace, service’s price (the monetary cost) should
scale with the cost of providing that service, which, in the
case of MiddlePolice, is low.
11. CONCLUSION

This paper presents MiddlePolice, a DDoS defense system
that is as deployable as cloud-based systems and has the
same destination-based control of capability-based systems.
In its design, MiddlePolice explicitly addresses three chal-
lenges. First, MiddlePolice designs a capability mechanism
that requires only limited deployment from the cloud, rather
than widespread Internet upgrades. Second, MiddlePolice is
fully destination-driven, addressing the shortcomings of the
existing capability-based systems that can work only with a
single fairness regime. Finally, MiddlePolice addresses the
traﬃc-bypass vulnerability of the existing cloud-based solu-
tions. Extensive evaluations on the Internet, testbed and
large scale simulations validate MiddlePolice’s deployability
and eﬀectiveness in enforcing detestation-chosen policies.
12. ACKNOWLEDGMENTS

We thank the anonymous CCS reviewers for their valu-
able feedback. This material is based upon work partially
supported by NSF under Contract Nos. CNS-0953600, CNS-
1505790 and CNS-1518741. The views and conclusions con-
tained here are those of the authors and should not be in-
terpreted as necessarily representing the oﬃcial policies or
endorsements, either express or implied, of NSF, the Univer-
sity of Illinois, or the U.S. Government or any of its agencies.
13. REFERENCES
[1] PlanetLab. https://www.planet-lab.org/. WebCite archive.
[2] AT&T Denial of Service Protection.

http://soc.att.com/1IIlUec, Accessed in 2016. WebCite
archive.

[3] NS-3: a Discrete-Event Network Simulator.
http://www.nsnam.org/, Accessed in 2016.

1278[4] AS Relationships – CIDR Report.

http://www.caida.org/data/as-relationships/, Accessed in
Dec 2015. WebCite archive.
[5] AS Names - CIDR Report.

http://www.cidr-report.org/as2.0/autnums.html, Dec 2015.

[6] Akdemir, K., Dixon, M., Feghali, W., Fay, P., Gopal,
V., Guilford, J., Ozturk, E., Wolrich, G., and Zohar,
R. Breakthrough AES Performance with Intel R(cid:13) AES New
Instructions. Intel white paper (2010).

[7] Andersen, D. G., Balakrishnan, H., Feamster, N.,

Koponen, T., Moon, D., and Shenker, S. Accountable
Internet Protocol (AIP). In ACM SIGCOMM (2008).

[8] Anderson, T., Roscoe, T., and Wetherall, D.

Preventing Internet Denial-of-Service with Capabilities.
ACM SIGCOMM (2004).

[29] K¨uhrer, M., Hupperich, T., Rossow, C., and Holz, T.

Exit from Hell? Reducing the Impact of Ampliﬁcation
DDoS Attacks. In USENIX Security Symposium (2014).

[30] Kuzmanovic, A., and Knightly, E. W. Low-Rate

TCP-Targeted Denial of Service Attacks: the Shrew vs. the
Mice and Elephants. In ACM SIGCOMM (2003).

[31] Liang, J., Jiang, J., Duan, H., Li, K., Wan, T., and Wu,
J. When HTTPS Meets CDN: A Case of Authentication in
Delegated Service. In IEEE S&P (2014).

[32] Liu, X., Li, A., Yang, X., and Wetherall, D. Passport:
Secure and Adoptable Source Authentication. In USENIX
NSDI (2008).

[33] Liu, X., Yang, X., and Lu, Y. To Filter or to Authorize:

Network-Layer DoS Defense Against Multimillion-node
Botnets. In ACM SIGCOMM (2008).

[9] Appenzeller, G., Keslassy, I., and McKeown, N. Sizing

[34] Liu, X., Yang, X., and Xia, Y. NetFence: Preventing

Router Buﬀers. ACM SIGCOMM, 2004.

[10] Argyraki, K., and Cheriton, D. Network Capabilities:

The good, the Bad and the Ugly. ACM HotNets-IV (2005).

[11] Argyraki, K. J., and Cheriton, D. R. Active Internet

Traﬃc Filtering: Real-Time Response to Denial-of-Service
Attacks. In USENIX ATC (2005).

[12] Basescu, C., Reischuk, R. M., Szalachowski, P.,

Perrig, A., Zhang, Y., Hsiao, H.-C., Kubota, A., and
Urakawa, J. SIBRA: Scalable Internet Bandwidth
Reservation Architecture. NDSS (2016).

[13] Briscoe, B. Tunnelling of Explicit Congestion Notiﬁcation.

RFC 6040, 2010.

[14] Chen, F., Sitaraman, R. K., and Torres, M. End-User
Mapping: Next Generation Request Routing for Content
Delivery. In ACM SIGCOMM (2015).

[15] Chiu, D.-M., and Jain, R. Analysis of the Increase and

Decrease Algorithms for Congestion Avoidance in
Computer Networks. Computer Networks and ISDN
systems (1989).

[16] Dimitropoulos, X., Krioukov, D., Fomenkov, M.,

Huffaker, B., Hyun, Y., Riley, G., et al. AS
Relationships: Inference and Validation. ACM SIGCOMM
CCR (2007).

[17] Dixon, C., Anderson, T. E., and Krishnamurthy, A.

Phalanx: Withstanding Multimillion-Node Botnets. In
NSDI (2008).

[18] Fayaz, S. K., Tobioka, Y., Sekar, V., and Bailey, M.
Bohatei: Flexible and Elastic DDoS Defense. In USENIX
Security Symposium (2015).

[19] Gao, L., Griffin, T. G., and Rexford, J. Inherently Safe

Backup Routing with BGP. In IEEE INFOCOM (2001).
[20] Gilad, Y., and Herzberg, A. LOT: a Defense against IP

Spooﬁng and Flooding Attacks. ACM Transactions on
Information and System Security (TISSEC) (2012).

[21] Gilad, Y., Herzberg, A., Sudkovitch, M., and

Goberman, M. CDN-on-Demand: An Aﬀordable DDoS
Defense via Untrusted Clouds. NDSS (2016).

[22] Goldberg, S., Schapira, M., Hummon, P., and

Rexford, J. How Secure are Secure Interdomain Routing
Protocols. ACM SIGCOMM CCR (2011).

[23] Gueron, S. Intel Advanced Encryption Standard (AES)

Instructions Set. White Paper, Intel (2010).

[24] Herbert, T. UDP Encapsulation in Linux. In The
Technical Conference on Linux Networking (2015).
[25] Ioannidis, J., and Bellovin, S. M. Implementing

Pushback: Router-Based Defense Against DDoS Attacks.
USENIX NSDI (2002).

[26] Karasaridis, A., Rexroad, B., and Hoeflin, D.

Wide-scale Botnet Detection and Characterization. In
USENIX HotBots (2007).

[27] Keromytis, A. D., Misra, V., and Rubenstein, D. SOS:

Secure Overlay Services. ACM SIGCOMM (2002).

[28] Kim, D., Chiang, J. T., Hu, Y.-C., Perrig, A., and

Kumar, P. CRAFT: A New Secure Congestion Control
Architecture. In ACM CCS (2010).

Internet Denial of Service from Inside Out. ACM
SIGCOMM (2011).

[35] Livadas, C., Walsh, R., Lapsley, D., and Strayer,
W. T. Using Machine Learning Technliques to Identify
Botnet Traﬃc. In IEEE LCN (2006).

[36] Mahajan, R., Bellovin, S. M., Floyd, S., Ioannidis, J.,
Paxson, V., and Shenker, S. Controlling High Bandwidth
Aggregates in the Network. ACM SIGCOMM (2002).

[37] Mittal, P., Kim, D., Hu, Y.-C., and Caesar, M. Mirage:

Towards Deployable DDoS Defense for Web Applications.
arXiv preprint arXiv:1110.1060 (2011).

[38] Miu, T. T., Hui, A. K., Lee, W., Luo, D. X., Chung,

A. K., and Wong, J. W. Universal DDoS Mitigation
Bypass. Black Hat USA (2013).

[39] Naylor, D., et al. XIA: Architecting a More Trustworthy

and Evolvable Internet. ACM SIGCOMM (2014).

[40] Networks, A. Worldwide Infrastructure Security Report,

Volume IX. https://www.arbornetworks.com/images/
documents/WISR2016 EN Web.pdf, 2016.

[41] Parno, B., Wendlandt, D., Shi, E., Perrig, A., Maggs,
B., and Hu, Y.-C. Portcullis: Protecting Connection Setup
from Denial-of-Capability Attacks. ACM SIGCOMM
(2007).

[42] Peter, S., Javed, U., Zhang, Q., Woos, D., Anderson,

T., and Krishnamurthy, A. One Tunnel is (often)
Enough. In ACM SIGCOMM (2014).

[43] Savage, S., Wetherall, D., Karlin, A., and Anderson,

T. Practical Network Support for IP Traceback. ACM
SIGCOMM (2000).

[44] Shin, S., Porras, P., Yegneswaran, V., Fong, M., Gu,

G., and Tyson, M. FRESCO: Modular Composable
Security Services for Software-Deﬁned Detworks. In NDSS
(2013).

[45] Song, D. X., and Perrig, A. Advanced and Authenticated
Marking Schemes for IP Traceback. In INFOCOM (2001).

[46] Sundaresan, S., De Donato, W., Feamster, N.,

Teixeira, R., Crawford, S., and Pescap`e, A. Broadband
Internet Performance: a View from the Gateway. In ACM
SIGCOMM (2011).

[47] Vissers, T., Van Goethem, T., Joosen, W., and

Nikiforakis, N. Maneuvering Around Clouds: Bypassing
Cloud-based Security Providers. In ACM CCS (2015).

[48] Walfish, M., Vutukuru, M., Balakrishnan, H.,

Karger, D., and Shenker, S. DDoS Defense by Oﬀense.
In ACM SIGCOMM (2006).

[49] Yaar, A., Perrig, A., and Song, D. SIFF: A Stateless

Internet Flow Filter to Mitigate DDoS Flooding Attacks. In
IEEE S&P (2004).

[50] Yang, X., Wetherall, D., and Anderson, T. A

DoS-limiting Network Architecture. In ACM SIGCOMM
(2005).

[51] Zhang, X., Hsiao, H.-C., Hasker, G., Chan, H., Perrig,
A., and Andersen, D. G. SCION: Scalability, Control, and
Isolation on Next-Generation Networks. In IEEE S&P
(2011).

1279