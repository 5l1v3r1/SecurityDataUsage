2012 IEEE Symposium on Security and Privacy

Abusing File Processing in Malware Detectors for Fun and Proﬁt

Suman Jana and Vitaly Shmatikov

The University of Texas at Austin

Abstract—We systematically describe two classes of evasion
exploits against automated malware detectors. Chameleon at-
tacks confuse the detectors’ ﬁle-type inference heuristics, while
werewolf attacks exploit discrepancies in format-speciﬁc ﬁle
parsing between the detectors and actual operating systems
and applications. These attacks do not rely on obfuscation,
metamorphism, binary packing, or any other changes to
malicious code. Because they enable even the simplest, easily
detectable viruses to evade detection, we argue that ﬁle pro-
cessing has become the weakest link of malware defense. Using
a combination of manual analysis and black-box differential
fuzzing, we discovered 45 new evasion exploits and tested them
against 36 popular antivirus scanners, all of which proved
vulnerable to various chameleon and werewolf attacks.

I. INTRODUCTION

Modern malware detectors employ a variety of detection
techniques: scanning for instances of known viruses, binary
reverse-engineering, behavioral analysis, and many others.
Before any of these techniques can be applied to a suspicious
ﬁle, however, the detector must (1) determine the type of the
ﬁle, and (2) depending on the type, analyze the ﬁle’s meta-
data and parse the ﬁle—for example, extract the contents
of an archive, ﬁnd macros embedded in a document, or
construct a contiguous view of executable code.

The importance of ﬁle processing grows as automated
malware defense moves away from the host, with antivirus
(AV) scanners and intrusion prevention systems installed at
enterprise gateways and email servers, increasing popularity
of cloud-based malware detection services, etc. Network-
and cloud-based deployments protect multiple hosts, provide
early detection capabilities and better visibility into network-
wide trends, and make maintenance easier. To be effective,
however, remotely deployed detectors must be able to predict
how each ﬁle will be processed at its destination by the
operating system and applications.

In this paper, we argue that the “semantic gap” between
how malware detectors handle ﬁles and how the same
ﬁles are processed on the endhosts is the Achilles heel of
malware defense. We use the term detector generically for
signature-based scanners, behavioral analyzers, or any other
tool that parses and analyzes suspicious ﬁles on its own,
independently of the destination endhost. The vulnerabilities
we describe are unrelated to obfuscation, mutation, or any
other way of hiding malicious functionality. They enable
even exact, unmodiﬁed instances of malware—primitive and
(otherwise) trivially detectable, as well as arbitrarily sophis-

ticated—to evade detection simply because the detector fails
to correctly process the infected ﬁle.

We introduce two new classes of evasion exploits against
malware detectors. The ﬁrst is chameleon attacks, which
target
the discrepancies between the heuristics used by
detectors to determine the type of the ﬁle and those used by
the endhosts. Contrary to a common misconception, neither
is based solely on the ﬁle extension, thus our attacks are
more complex than simply renaming the extension (this trick
does not work against modern detectors), nor can they be
solved by forcing a particular extension onto a ﬁle.

The second class is werewolf attacks, which exploit the
discrepancies in the parsing of executables and application-
speciﬁc formats between malware detectors and actual ap-
plications and operating systems.

We evaluated 36 popular AV scanners using a combination
of manual analysis and differential black-box fuzzing, and
discovered 45 different exploits. All tested scanners proved
vulnerable to both chameleon and werewolf attacks. We
stress that the problem is not speciﬁc to AV scanners and
does not depend on known weaknesses of signature-based
scanning such as the inability to handle metamorphic muta-
tions. The actual viruses used in our testing are extremely
simple. They do not employ self-encryption, polymorphism,
or obfuscation, yet chameleon and werewolf attacks enable
them to pass undetected through scanners whose virus
databases contain their exact code. Because ﬁle processing
must
take place before actual malware detection, more
elaborate detectors are vulnerable, too, as long as their ﬁle-
processing logic differs, however slightly, from the OS and
applications on any of the protected endhosts.

The problem is deeper than the anecdotally known in-
ability of AV software to properly process archive formats.
Many modern ﬁle formats are effectively archives in dis-
guise: for example, MS Ofﬁce documents contain executable
macros, Compiled HTML Help (CHM) contains HTML
ﬁles, etc. We discovered chameleon and werewolf attacks
against all ﬁle formats we tested, from relatively simple
archives to executable images and complex MS Ofﬁce docu-
ment formats. Evasion techniques based on code obfuscation
are widely known and many defenses have been proposed. In
contrast, our attacks involve changes only to the meta-data
of infected ﬁles and are thus different, signiﬁcantly simpler,
and complementary to obfuscation-based evasion.

While each individual vulnerability may be easy to ﬁx, ﬁle
processing in malware detectors suffers from thousands of

© 2012, Suman Jana. Under license to IEEE.
DOI 10.1109/SP.2012.15

80

semantic discrepancies. It is very difﬁcult to “write a better
parser” that precisely replicates the ﬁle-parsing semantics of
actual applications and operating systems: (1) many formats
are underspeciﬁed, thus different applications process the
same ﬁle in different and even contradictory ways, all of
which must be replicated by the detector; (2) replicating
the behavior of a given parser is hard—for example, after
many years of testing, there are still hundreds of ﬁle-parsing
discrepancies between OpenOfﬁce and MS Ofﬁce [23, 24]
and between the “functionally equivalent” implementations
of Unix utilities [7]; (3) the detector must be bug-compatible
with all applications; (4) because applications are designed
to handle even malformed ﬁles, their parsing algorithms
are much looser
than the format speciﬁcation, change
from version to version, and have idiosyncratic, difﬁcult-
to-replicate, mutually incompatible semantics for processing
non-compliant ﬁles, all of which must be replicated by the
detector; (5) even seemingly “safe” discrepancies—such as
attempting to analyze ﬁles with invalid checksums when
scanning an archive for malware—enable evasion.
Responsible disclosure. All attacks described in this paper
have been reported to the public through the Common
Vulnerabilities and Exposures (CVE) database.1 In the rest
of this paper, we refer to them by their candidate CVE
numbers (see Tables I and II). These numbers were current
at the time of writing, but may change in the future.

II. RELATED WORK

We introduce chameleon and werewolf attacks as a
generic, pervasive problem in all automated malware de-
tectors and demonstrate 45 distinct attacks on 36 different
detectors, exploiting semantic gaps in their processing of
many archive and non-archive formats. With a couple of
exceptions (e.g., a RAR archive masquerading as a Windows
executable, previously mentioned in [2]), the attacks in this
paper are reported and described for the ﬁrst time.

There is prior evidence of malformed archive ﬁles evading
AV software [2, 3, 10, 18, 38, 39]. These anecdotes are
limited to archive formats only and do not describe concrete
attacks. Alvarez and Zoller brieﬂy mention that modern AV
scanners need to parse a variety of formats [2] and point out
the importance of correct ﬁle parsing for preventing denial
of service [1]. Concrete werewolf attacks on the detectors’
parsing logic for non-archive formats such as executables
and Ofﬁce documents have been reported in neither research
literature, nor folklore. These attacks have especially serious
repercussions because they are not prevented even by host-
based on-access scanning (see Section IX-A).

Buffer overﬂow and other attacks on AV software, unre-

lated to ﬁle processing, are mentioned in [36, 37].
Chameleon attacks. Chameleon attacks on ﬁle-type in-
ference heuristics are superﬁcially similar to attacks on

1http://cve.mitre.org/

content-snifﬁng heuristics in Web browsers [19, 25, 30].
Barth et al. proposed preﬁx-disjoint content signatures as
a browser-based defense against content-snifﬁng attacks [4].
The premise of this defense is that no ﬁle that matches the
ﬁrst few bytes of some format should be parsed as HTML
regardless of its subsequent content.

Preﬁx-disjoint signatures do not provide a robust defense
against chameleon attacks on malware detectors. Detectors
handle many more ﬁle formats than Web browsers and,
crucially, these formats cannot be characterized solely by
their initial bytes (e.g., valid TAR archives can have arbitrary
content in their ﬁrst 254 bytes, possibly including signatures
for other ﬁle types). Therefore, they cannot be described
completely by any set of preﬁx-disjoint signatures.
Other semantic-gap attacks. Chameleon and werewolf
attacks are an instance of a general class of “semantic-gap”
attacks which exploit different views of the same object
by the security monitor and the actual system. The gap
described in this paper—the monitor’s (mis)understanding
of the type and structure of suspicious ﬁles—received much
less attention than other evasion vectors against AV scanners
and intrusion detection systems [16, 27, 31] and may very
well be the weakest link of malware defense.

Other, complementary evasion techniques exploit net-
working protocols (e.g., split malware into multiple packets),
obfuscate malware using mutation or packing [20], or, in
the case of malicious JavaScript, obfuscate it in HTML,
Flash, and PDF content. For example, Porst showed how
to obfuscate scripts so that they are not recognized by AV
scanners but parsed correctly by the Adobe reader [26].
HTML parsing is notoriously tricky [28], and cross-site
scripting can exploit HTML-parsing discrepancies between
browsers and sanitization routines [5, 35]. In contrast, we
show how the most primitive viruses, which are present in
standard virus databases and do not use any obfuscation, can
evade detection by exploiting discrepancies in the processing
of even basic ﬁle formats such as TAR and PE.

An entirely different kind of semantic gap is exploited by
“split-personality” malware, whose behavior varies between
monitored and unmonitored environments [8]. Such malware
contains code that tries to detect virtualization, emulation,
and/or instrumentation libraries. In contrast, our attacks are
completely passive, require no active code whatsoever, and
target a different feature of malware detection systems.

Semantic-gap attacks on system-call interposition exploit
the gap between the monitor’s and the OS’s views of system-
call arguments [12, 34]. These attacks typically involve
concurrency and are fundamentally different from the attacks
described in this paper.
Program differencing. Brumley et al. proposed to auto-
matically detect discrepancies between different implemen-
tations of the same protocol speciﬁcation by converting
execution traces into symbolic formulas and comparing

81

Table I

TESTED AV SCANNERS.

Name
ClamAV 0.96.4
GData 21
Ikarus T3.1.1.97.0
F-Prot 4.6.2.117
Antiy-AVL 2.0.3.7
Kaspersky 7.0.0.125
Sophos 4.61.0
Norman 6.06.12

AV number
1
4
7
10
13
16
19
22
25 McAfee-GW-Edition 2010.1C
28
31
34

BitDefender 7.2
nProtect 2011-01-17.01
Avast 4.8.1351.0

Name
Rising 22.83.00.03
Symantec 20101.3.0.103
Emsisoft 5.1.0.1
VirusBuster 13.6.151.0
K7AntiVirus 9.77.3565
Jiangmin 13.0.900
NOD32 5795

AV number
2
5
8
11
14
17
20
23 McAfee 5.400.0.1158
26
29
32
35

TrendMicro 9.120.0.1004
eSafe 7.0.17.0
AhnLab-V3 2011.01.18.00
Avast5 5.0.677.0

Name
CAT-QuickHeal 11.00
Command 5.2.11.5
PCTools 7.0.3.5
Fortinent 4.2.254.0
TrendMicro-HouseCall 9.120.0.1004

AV number
3
6
9
12
15
18 Microsoft 1.6402
21
24
27
30
33
36

AntiVir 7.11.1.163
Panda 10.0.2.7
Comodo 7424
F-Secure 9.0.16160.0
AVG 10.0.0.1190
VBA32 3.12.14.2

AFFECTED AV SCANNERS FOR EACH REPORTED ATTACK.

Table II

CVE
2012-1419

Vulnerable scanners
1, 3

2012-1422

2, 3, 20, 22

2012-1425

2012-1428

2012-1431

2012-1434
2012-1437
2012-1440

2012-1443

2012-1446

2012-1449
2012-1452

3, 5, 7, 8, 9, 13, 15, 16, 17,
20, 21, 22, 23, 25, 26
3, 19, 22

2, 6, 10, 19, 25, 27, 28, 29,
30, 31
7, 8, 24, 32
27
22, 24, 29

1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
12, 13, 14, 15, 16, 19, 20, 21,
22, 23, 24, 25, 26, 27, 28, 29,
30, 32, 33, 34, 35, 36
2, 3, 5, 9, 13, 16, 19, 22, 23,
24, 25, 29
2, 20
3, 7, 8

2012-1455

2, 20

2012-1458

1, 19

2012-1459

2012-1461

2, 5, 6, 7, 8, 12, 14, 15, 16,
17, 19, 20, 22, 23, 25, 26, 28,
30, 33, 36

2012-1462

CVE
2012-1420

2012-1423

2012-1426

Vulnerable scanners
2, 3, 6, 10, 12, 14, 16, 18, 20,
22, 24
2, 6, 7, 8, 9, 10, 11, 12, 14,
20, 22
2, 3, 6, 10, 14, 22

CVE
2012-1421

Vulnerable scanners
2, 3, 5, 22

2012-1424

3, 9, 13, 17, 19, 22

2012-1427

3, 19, 22

2012-1429

7, 8, 23, 25, 27, 28, 29, 30, 31

2012-1430

2012-1432

7, 8, 24, 29

2012-1435
2012-1438
2012-1441

7, 8, 24, 29, 32
19, 27
29

2012-1444

24, 29

2012-1433

2012-1436
2012-1439
2012-1442

2012-1445

2, 19, 23, 25, 27, 28, 29, 30,
31
7, 8, 24, 29, 32

7, 8, 24, 29, 32
2, 24, 29
2, 3, 13, 16, 19, 23, 24, 25,
29, 30
2, 24, 29

2012-1447

24, 29

2012-1448

3, 7, 8, 26

2012-1450
2012-1453

2012-1456

7, 8, 19
2, 7, 8, 13, 15, 16, 18, 19, 23,
24, 25, 26
2, 3, 5, 7, 8, 10, 12, 15, 16,
17, 19, 20, 22, 23, 24, 25, 26,
27, 29, 33

1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
12, 13, 14, 15, 16, 17, 18, 19,
20, 21, 22, 23, 24, 25, 26, 27,
28, 30, 31, 32, 33, 34, 35, 36
3, 5, 7, 8, 12, 16, 17, 19, 22,
29, 32, 33

2012-1451
2012-1454

7, 8
2, 23, 24, 25, 29

2012-1457

2012-1460

1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
13, 14, 15, 16, 17, 18, 20, 21,
22, 23, 25, 26, 28, 29, 33, 34,
35, 36
3, 6, 10, 13, 14, 17, 29, 36

2012-1463

3, 6, 10, 22, 23, 24, 27, 28,
29, 30, 31, 32

it

i.e.,

them using SMT solvers [6]. Unfortunately, this approach
does not appear to be feasible for automatically discovering
chameleon and werewolf attacks. The programs tested by
Brumley et al. implement simple protocols like HTTP and
their parsing code is very shallow,
lies close to
the program’s entry point. By contrast, malware scanners
and applications do much more than parsing: scanners load
virus signatures, match them against the ﬁle, etc., while
applications perform a wide variety of operations before
and after parsing. Binary differencing must be applied to the
parsing code only, because the non-parsing functionalities of
malware detectors and applications are completely different.
This requires extracting the parsing code from the closed-
source binaries of both detectors and applications, which

is extremely difﬁcult. Furthermore, both parsers must have
the same interface, otherwise their ﬁnal output states cannot
be easily compared. Brumley et al. provide no method
for automatically recognizing, extracting, normalizing, and
comparing individual pieces of functionality hidden deep
inside the binary.

Furthermore, this technique generates formulas from one
execution path at a time and is less likely ﬁnd bugs in
rare paths. By contrast, most of the attacks reported in
this paper—for example,
the attack which concatenates
two separate streams of gzipped data to create a single
ﬁle—exploit bugs in unusual paths through the parsing code.
BEK is a new language and system for writing and
analyzing string-manipulating sanitizers for Web applica-

82

tions [15]. BEK can compare two sanitizers for equivalence
and produce a counter-example on which their outputs differ.
The BEK language is speciﬁcally tailored for expressing
string manipulation operations and is closely related to
regular expressions. It is ill-suited for expressing ﬁle-parsing
logic. For example, it cannot validate data-length ﬁelds in
ﬁle headers and similar content-dependent format ﬁelds.

Development of program analysis techniques for auto-
mated discovery of chameleon and werewolf attacks is an
interesting topic for future research.

III. ATTACKING FILE PROCESSING
Figure 1 shows the main components of

the ﬁle-
processing logic of antivirus scanners. The basic pattern
applies to other automated2 malware detectors, both be-
havioral and signature-based, as long as they process ﬁles
independently of the endhost’s OS and applications.

Input file

File−type   inference

Preprocessing 

    +

Normalization

Select parts 

to scan

File
parsing

Find relevant signatures

Signature

matching

Figure 1. File processing in antivirus scanners.

The ﬁrst step is ﬁle-type inference. The scanner must
infer the ﬁle type in order to (1) parse the ﬁle correctly and
(2) scan it for the correct subset of virus signatures.

The second step is ﬁle parsing. Files in some formats
must be preprocessed before scanning (for example, the con-
tents of an archive must be extracted). Documents in formats
like HTML contain many irrelevant characters (for example,
whitespace) and must be normalized. In most ﬁle formats,
whether executable or application-speciﬁc, blocks of data
are interspersed with meta-data. For higher performance,
malware scanners parse the meta-data in order to identify
and scan only the potentially “interesting” parts of the ﬁle.
For example, a scanner may parse an MS Word document
to ﬁnd embedded macros and other executable objects and
scan them for macro viruses. To detect viruses in Linux
ELF executables, which can contain multiple sections, the
scanner must construct a contiguous view of the executable
code. This requires parsing the meta-data (ELF header) to
ﬁnd the offsets and sizes of code sections.
Chameleon and werewolf attacks. We will refer to at-
tacks that exploit discrepancies in ﬁle-type inference as

2Human operators may be able to manually prevent incorrect parsing and
ﬁle-type inference, but widespread deployment of human-assisted detectors
is not feasible for obvious scalability reasons.

83

chameleon attacks because attack ﬁles appear as one type
to the detector and as a different type to the actual OS or ap-
plication. We will refer to attacks that exploit discrepancies
in parsing as werewolf attacks because attack ﬁles appear
to have different structure depending on whether they are
parsed by the detector or the application.

Chameleon and werewolf attacks only change the meta-
data of the ﬁle; the contents, including the malicious pay-
load, are not modiﬁed (in contrast to code obfuscation and
polymorphism). These attacks (1) start with a ﬁle that is
recognized as malicious by the detector, (2) turn it into a
ﬁle that is not recognized as malicious, yet (3) the modiﬁed
ﬁle is correctly processed by the destination application or,
in the case of executables, loaded by the OS. If the same
ﬁle can be processed by multiple applications or versions of
the same application, we consider an attack successful if at
least one of them processes the modiﬁed ﬁle correctly.
Fingerprinting malware detectors and learning their
logic. Because ﬁle-type inference heuristics and ﬁle-parsing
logic vary from detector to detector, attacks are detector-
speciﬁc and it helps to know which detector is protecting
the target. This knowledge is often public—for example,
Yahoo Mail scans all messages with Symantec’s Norton
antivirus—but even in blind testing against Gmail’s unknown
scanner, two chameleon and one werewolf attacks (CVE-
2012-1438, 2012-1443, and 2012-1457) evaded detection.

Unknown detectors can be identiﬁed by tell-tale signs
in bounced messages [22], or by using chameleon and
werewolf attacks themselves. As Table II shows, different
attacks work against different detectors. By trying several
attacks and seeing which of them evade detection,
the
attacker can infer the make and model of the detector.

The logic of open-source detectors like ClamAV can be
learned by analyzing their code, but the vast majority of
detectors are closed-source and their logic must be learned
by fuzzing and/or binary analysis. Secrecy of the ﬁle-
processing logic is a weak defense, however: we report
dozens of vulnerabilities in commercial scanners for which
we do not have the source code, many of them discovered
automatically by our black-box differential fuzzer.

IV. GENERATING ATTACKS

To test our attacks, we used VirusTotal [32], a free
Web service that checks any ﬁle against multiple antivirus
scanners (43 at the time of our testing). Several scanners
were not available at various times due to crashes, thus for
consistency we present the results for the 36 scanners that
were continuously available.

VirusTotal executes the command-line versions of all
AV scanners with maximum protection and all detection
methods enabled. We argue that this faithfully models the
level of defense provided by network-based detectors. By
design, they do not observe the actual processing of ﬁles
on the host and thus advanced detection techniques—for

Table III

TESTED APPLICATIONS.

File type
CAB
CHM
ELF
GZIP
DOC
PE
RAR
TAR
7Z

Target application(s)
Cabextract 1.2
Microsoft HTML Help 1.x
Linux kernel (2.6.32) ELF loader
Gzip 1.3.12, File Roller 2.30.1.1
MS Ofﬁce 2007, OpenOfﬁce 3.2
Windows Vista SP2 PE loader, Wine 1.2.2 PE loader
RAR 3.90 beta 2
GNU tar 1.22, File Roller 2.30.1.1
7-Zip 9.04 beta

example, monitoring the program’s execution for signs of
malicious behavior—require the detector to accurately rec-
ognize the ﬁle type, parse the ﬁle, and replicate the host’s
execution environment. In Section IX, we explain why this
is challenging to do correctly.

Attacks were also conﬁrmed by testing against the host-

based versions of AV software, where available.

We used ﬁve toy viruses from VX Heavens [33] in our
tests: EICAR, Linux Bliss and Cassini, Windows Cecile, and
MS Word ABC. If an exact, unobfuscated instance of such
a basic virus evades detection, more sophisticated malware
won’t be detected, either. We count an attack as successful
if the detector (1) recognizes the infection in the original
ﬁle, but (2) no longer recognizes it in the modiﬁed ﬁle.

Target applications used in our testing are summarized
in Table III. They were executed on laptops running Linux
Ubuntu 10.04 and Windows Vista SP 2.
Black-box differential fuzzing. To ﬁnd werewolf attacks
automatically, we built a differential fuzzing framework that
ﬁnds discrepancies between the parsing logic of applications
and malware detectors. Because the source code of detectors
is rarely available, our framework is black-box. It is imple-
mented in Python and runs on both Linux and Windows.

The basic framework is format-independent, but format-
speciﬁc components are added as plugins. Each plugin
provides a parser, an output validator, and a fuzzer. The
parser breaks up the format-speciﬁc header into an array
of (name, offset, length) tuples, where name is the unique
name of a header ﬁeld, offset is its location in the ﬁle, and
length is its size. The fuzzer modiﬁes the content of the ﬁelds
using a format-speciﬁc algorithm. The validator checks if the
application still processes the modiﬁed ﬁle correctly.

Our framework takes as input

two seed ﬁles in the
same format. One ﬁle is parsed correctly by the destination
application, the other is an infected ﬁle recognized by the
detector. The framework uses the format-speciﬁc fuzzer to
automatically generate modiﬁcations to the ﬁrst ﬁle and the
output validator to check if the application still accepts the
ﬁle. If a modiﬁcation is validated, the framework applies it
to the second, infected ﬁle and tests whether the detector
still recognizes the infection. This approach is better than
directly modifying the infected ﬁle and accessing it on

an endhost because (1) the host must be isolated (e.g.,
virtualized) in each test
infection,
imposing a signiﬁcant performance overhead on the testing
framework, and (2) determining if the modiﬁed infected ﬁle
is accepted by the destination application is difﬁcult because
applications are opaque and have complex side effects.

to prevent an actual

A modiﬁcation is thus applied to the infected ﬁle only
if the application’s parser tolerates it. If the ﬁle is no
longer recognized as malicious, a discrepancy between the
application’s and the detector’s parsers has been found and
an actual attack can be generated and veriﬁed by accessing
the modiﬁed infected ﬁle on a secure, isolated host. We
consider an infection veriﬁed if the intact malware code is
extracted from the archive and/or loaded as an executable.
As a proof of concept, we implemented sample plugins for
MS Cabinet (CAB), Windows executable (PE), and Linux
executable (ELF) ﬁles. The fuzzer in these plugins tries a
simple modiﬁcation to the ﬁle’s header, one ﬁeld at a time:
it increments the content of each ﬁeld (or the ﬁrst byte if the
ﬁeld spans multiple bytes) by 1; if this results in an overﬂow,
it decrements the content by 1. Output validators execute
destination applications on modiﬁed seed ﬁles and check the
return codes and the application’s output for correctness.

Once integrated with VirusTotal, our fuzzing framework
found dozens of parsing bugs in 21 different detectors
(Table XII). All result in actual werewolf attacks.

Of course, our simple framework cannot ﬁnd all parsing
discrepancies. Some parsing bugs are hidden in rarely ex-
ecuted paths which can only be reached through specially
crafted inputs, requiring manual guidance to the fuzzer. For
example, attacks involving a concatenation of two gzipped
streams or a header with an incorrect checksum whose
length is modiﬁed to point into the following header (see
Section VI) are difﬁcult to discover by automated fuzzing.
Another limitation is that our fuzzer does not fully “under-
stand” the dependencies between different ﬁelds of format-
speciﬁc headers and cannot automatically generate valid ﬁles
if several ﬁelds must be changed consistently. For example,
if ﬁle length is included in a header ﬁeld, the ﬁle must be
truncated or augmented whenever this ﬁeld is modiﬁed.

V. CHAMELEON ATTACKS

Chameleon attacks involve specially crafted ﬁles that
appear as one type to the ﬁle-type inference heuristics used
by the malware detector but as a different type to the OS or
application on the endhost.

The simplest chameleon attack is to hide the infected
ﬁle in an archive of a type not recognized by the detector,
causing it
to apply generic malware signatures without
extracting the contents. Even this primitive attack is sur-
prisingly effective, as shown by Table IV.

In the rest of this section, we focus on more interesting
chameleon attacks that involve a ﬁle of one type masquerad-
ing as a ﬁle of a different type. Masquerade attacks cause

84

SUPPORT FOR 11 ARCHIVE FORMATS: 7ZIP, 7ZIP-SFX, PACK, ISO, RAR, RAR(SFX), TAR.LZOP, TAR.LZMA, TAR.RZ, TAR.XZ, AR

Table IV

Scanner

ClamAV 0.96.4
GData 21
Ikarus T3.1.1.97.0
F-Prot 4.6.2.117
Antiy-AVL 2.0.3.7
Kaspersky 7.0.0.125
Sophos 4.61.0
Norman 6.06.12
McAfee-GW-Edition 2010.1C
BitDefender 7.2
nProtect 2011-01-17.01
Avast 4.8.1351.0

Unsupported
formats

Scanner

Unsupported
formats

Scanner

Unsupported
formats

8
7
9
9
8
5
8
9
10
9
10
7

Rising 22.83.00.03
Symantec 20101.3.0.103
Emsisoft 5.1.0.1
VirusBuster 13.6.151.0
K7AntiVirus 9.77.3565
Jiangmin 13.0.900
NOD32 5795
McAfee 5.400.0.1158
TrendMicro 9.120.0.1004
eSafe 7.0.17.0
AhnLab-V3 2011.01.18.00
Avast5 5.0.677.0

9
10
8
10
9
9
7
10
10
8
10
7

CAT-QuickHeal 11.00
Command 5.2.11.5
PCTools 7.0.3.5
Fortinent 4.2.254.0
TrendMicro-HouseCall 9.120.0.1004
Microsoft 1.6402
AntiVir 7.11.1.163
Panda 10.0.2.7
Comodo 7424
F-Secure 9.0.16160.0
AVG 10.0.0.1190
VBA32 3.12.14.2

9
8
10
9
10
6
7
8
11
8
9
9

harm in several ways. First, for efﬁciency, detectors usually
apply only a subset of analysis techniques and/or malware
signatures to any given ﬁle type. If the detector is confused
about the type, it may apply a wrong analysis. Second, many
ﬁle types require preprocessing (e.g., unpacking) before they
can be analyzed. A confused detector may apply a wrong
preprocessing or no preprocessing at all.

tested scanners ignore the extension and attempt

File-type inference heuristics. File-type inference in mal-
ware scanners is not based on the ﬁle extension. Even if
the endhost runs Windows, which by default relies on the
extension to determine the ﬁle’s type, users may override the
defaults and use any program to open any ﬁle. Therefore,
all
to
determine the actual type of the ﬁle. The simple attack
of renaming the extension thus does not work, but neither
does the simple defense of having the scanner rewrite the
extension to match the ﬁle’s actual type (see Section VII).
To illustrate ﬁle-type inference heuristics, we use Cla-
mAV v0.95.2, a popular open-source scanner [9]. The basic
principles apply to other detectors, too, as evidenced by the
success of chameleon attacks against all of them. For most
ﬁle types, ClamAV uses ﬁxed-offset byte-string signatures,
but for certain types such as HTML or self-extracting ZIP
archives, ClamAV also uses regular-expression signatures,
described later in this section. Fixed-offset signatures are
tuples of the form (offset, magic-content, length) where offset
denotes the offset from the beginning of the ﬁle which is
to be checked for this particular ﬁle type, magic-content is
the sequence of bytes starting from offset that any ﬁle of
this type should have, and length is the length (in bytes)
of that sequence. Some of ClamAV’s ﬁle-type signatures
are shown in Table XIII in the appendix. For example,
ClamAV’s signature for ELF executables is (0, 7f454c46,
4), thus any ﬁle which has 7f454c46 as its ﬁrst four bytes
will be considered as an ELF ﬁle by ClamAV.

Algorithm 1 shows a simpliﬁed version of ClamAV’s
algorithm for inferring the ﬁle type. The order of signatures
in the list matters: once ClamAV ﬁnds a match, it does
not check the list any further. In particular, if a ﬁxed-offset

Algorithm 1 Simpliﬁed pseudocode of ClamAV’s ﬁle-type
inference algorithm.
Read ﬁrst 1024 bytes of input ﬁle into buf
for each ﬁxed-offset ﬁle-type signature s in the speciﬁed
order do

if !memcmp(buf+s.offset, s.magic-content, s.length) then

if s is a ﬁle type to ignore then

return ignore

else

return s.ﬁletype

end if

end if
end for
Check buf for
Corasick algorithm
if buf matches a regex signature r then

regex ﬁle-type signatures using Aho-

return r.ﬁletype

else

return unknown ﬁle type

end if

signature is matched, all regex signatures are ignored. This
is exploited by one of our attacks.

From version 0.96 onward, ClamAV also supports LLVM
bytecode signatures, typically used to detect polymorphic
malware in a ﬁle-format-aware manner. These signatures are
only checked for speciﬁc ﬁle types, e.g., a signature regis-
tering P DF HOOK DECLARE will only get checked
if the inferred ﬁle type is PDF. Therefore, these signatures
are extremely susceptible to chameleon attacks.

Requirements for ﬁle-type masquerade. Let A be the ﬁle’s
actual type and let B be the fake type that the attacker wants
the detector to infer. For the masquerade to be successful,
three conditions must hold for the ﬁle-type signatures SA
(for type A) and SB (for type B):

1) SA and SB do not conﬂict, i.e., there are no i, j
such that 0 ≤ i < SA.length, 0 ≤ j <
SB.length, SA.offset + i = SB.offset + j, and

85

VULNERABLE FILE-TYPE PAIRS IN CLAMAV

CHAMELEON ATTACKS FOR ABC-INFECTED MS OFFICE 97 DOC FILES.

Table V

Table IX

Real type
POSIX TAR
PNG
GIF
BMP
MP3
PNG

Fake type
mirc.ini
POSIX TAR
JPEG
JPEG
POSIX TAR
JPEG

Real type
ELF
ELF
ELF
MPEG
JPEG
BMP

Fake type
POSIX TAR
JPEG
SIS
POSIX TAR
POSIX TAR
JPEG

SA.magic-content[i] (cid:3)= SB.magic-content[j].

2) The detector matches SB before SA.
3) Destination OS or application correctly processes ﬁles

of type A containing both SA and SB.

The ﬁrst condition ensures that the same ﬁle may contain
both SA and SB, the second that the detector infers type B
for the ﬁle before it has a chance to infer type A. In our
testing, we discovered 12 ﬁle-type pairs that satisfy all three
conditions for ClamAV (Table V).

Masquerade alone is not enough. Even if the detector
infers the wrong ﬁle type, it may still detect the infection
by scanning the ﬁle as a “blob” or if the signatures asso-
ciated with the inferred type contain the virus. That said,
masquerade is a good start for exploring chameleon attacks.

CHAMELEON ATTACKS WITH EICAR-INFECTED TAR FILES.

Table VI

Actual ﬁle type
POSIX TAR
POSIX TAR
POSIX TAR
POSIX TAR
POSIX TAR
POSIX TAR
POSIX TAR
POSIX TAR
POSIX TAR
POSIX TAR

Fake ﬁle type
mirc.ini
ELF
CAB
CHM
PE
SIS
PKZIP
BZip2
WinZip
JPEG

No. of vulnerable AVs
2
11
4
4
11
6
16
6
3
3

CVE
2012-1419
2012-1420
2012-1421
2012-1422
2012-1423
2012-1424
2012-1425
2012-1426
2012-1427
2012-1428

CHAMELEON ATTACKS WITH BLISS-INFECTED ELF FILES.

Table VII

Actual ﬁle type
ELF
ELF
ELF

Fake ﬁle type
POSIX TAR
SIS
JPEG

No. of vulnerable AVs
9
9
10

CVE
2012-1429
2012-1430
2012-1431

CHAMELEON ATTACKS WITH CECILE-INFECTED PE FILES.

Table VIII

Actual ﬁle type
PE
PE
PE
PE
PE

Fake ﬁle type
Winzip
JPEG
SIS
PKLITE
LZH

No. of vulnerable AVs
4
5
4
5
5

CVE
2012-1432
2012-1433
2012-1434
2012-1435
2012-1436

Results for chameleon attacks. Our description of ﬁle-
type inference logic focuses on ClamAV because its open-
source code makes it easy to explain the heuristics. File-type

Actual ﬁle type
MS Ofﬁce
MS Ofﬁce

Fake ﬁle type
PKSFX
POSIX TAR

No. of vulnerable AVs
1
2

CVE
2012-1437
2012-1438

inference based on magic strings is by no means unique
to ClamAV, however. All tested scanners proved vulnerable
to masquerade-based chameleon attacks. The results are
summarized in Table X; the masquerade pairs for each attack
and scanner are shown in Tables VI, VII, VIII, and IX.
In all attacks, to masquerade as a particular type we used
ClamAV’s magic string if supported by ClamAV, otherwise
a string from Table XIV in the appendix.
Sample attack: making a TAR archive look like mirc.ini
We describe a sample exploit against ClamAV in which a
POSIX TAR ﬁle masquerades as a ‘mirc.ini’ ﬁle. Their ﬁle-
type signatures are disjoint and the signature of ‘mirc.ini’ is
matched before the signature of TAR. It remains to ensure
that adding the ‘mirc.ini’ signature to a TAR ﬁle does not
affect the processing of the archive by the tar program.

Table XIII says that the signature of ‘mirc.ini’ begins at
offset 0 and ends at offset 9. Thus the 0 − 9 bytes of the
input TAR ﬁle must be changed to ‘5b616c69617365735d’.
Because the initial 100 bytes contain the name of the ﬁrst
ﬁle, the ﬁrst 9 bytes of this name will change as a side
effect. This does not affect the archive’s contents and any
virus infecting any ﬁle in the archive will be free to spread.
We converted this masquerade exploit into a working
chameleon attack using the test EICAR virus [11], which
is detected by all antivirus scanners, including ClamAV. If
the ﬁrst 9 bytes of a TAR archive containing ‘eicar.com’
are directly changed to ‘5b616c69617365735d’ (‘[aliases]’
in ASCII), the tar application considers the archive cor-
rupted because the names of all member ﬁles are part of a
checksum-protected header block. To avoid this issue, it is
sufﬁcient to rename ‘eicar.com’ to ‘[aliases].com’ and put
it inside the TAR archive. ClamAV does not recognize the
ﬁle as an archive and scans it as a “blob,” looking for the
EICAR signature only at offset 0 and failing to detect it in
the middle of the ﬁle. Another approach is to use a TAR
manipulation library to change the checksum, but this is not
necessary here because the fake ﬁle-type signature ‘[aliases]’
only contains ASCII characters.
Sample attack: user-repaired archive. The application on
the endhost is often capable of repairing the modiﬁed ﬁle
(this is common in archiving programs). In some cases, it
may prompt the user whether she wants to repair the ﬁle.
Most users answer ‘Yes’ to these questions.

Given a RAR archive with an EICAR-infected ﬁle, we
changed the ﬁrst two bytes to “MZ,” which is the magic
identiﬁer for Windows executables. None of the tested
scanners detected the infection in the modiﬁed archive, yet

86

Table X

SUCCESSFUL CHAMELEON ATTACKS.

Format type

non-archive

archive

File format
ELF
PE
MS Ofﬁce 97
TAR

No. of attacks
3
5
2
10

RAR

1

CVE
2012-1429, 2012-1430, 2012-1431
2012-1432, 2012-1433, 2012-1434, 2012-1435, 2012-1436
2012-1437, 2012-1438
2012-1419, 2012-1420, 2012-1421, 2012-1422, 2012-1423, 2012-1424,
2012-1425, 2012-1426, 2012-1427, 2012-1428
2012-1443

the RAR program on the endhost repaired it and correctly
extracted the infected ﬁle. This is especially surprising
because this particular attack is anecdotally known [2].
Sample attack: exploiting regex-based ﬁle-type inference.
To recognize certain formats, ClamAV uses regular expres-
sions to match magic strings that can occur anywhere in
a ﬁle,
in addition to looking for magic strings at ﬁxed
offsets. We describe two sample attacks on this combination
of ﬁxed-offset and regex-based inference (tested against
ClamAV only and thus not counted in Table X).

ZIP archives may start with arbitrary bytes. To recog-
nize ZIP ﬁles, ClamAV uses both a ﬁxed-offset signature
‘504b0304’ at offset 0 and a regex signature ‘504b0304’
which can appear at any offset within the input ﬁle. Once
a format has been recognized according to the ﬁxed-offset
signature, ClamAV does not do any further inference—even
if there are regex matches inside the ﬁle. To exploit this, we
created a ZIP ﬁle containing a Cassini-infected executable
and prepended the string ‘504b0304’ to it. ClamAV matched
the ﬁxed-offset signature at offset 0 but failed to notice
the regex signature at offset 4, was thus unable to extract
the contents correctly, and declared the archive clean. The
destination application (unzip) ignored the initial bytes and
extracted the infected ﬁle.

The second attack exploits the fact

that ClamAV ig-
nores ﬁles of certain types (e.g., MPEG video and Ogg
Vorbis audio) because they are not affected by any major
viruses. We created a CAB archive containing a Cassini-
infected ﬁle and prepended the string ‘000001b3’ to it,
which is the ﬁxed-offset MPEG signature. ClamAV’s ﬁle-
type database contains a regex signature for CAB for-
mat—‘4d534346’ anywhere in the ﬁle, which matches CAB
ﬁles even with garbage prepended—but ClamAV does not
apply regex signatures once the ﬁxed-offset signature has
been matched. Therefore, ClamAV infers MPEG type for the
ﬁle and does not scan it, while the destination application
(cabextract) correctly extracts the infected ﬁle.

VI. WEREWOLF ATTACKS

Werewolf attacks tamper with the ﬁle’s meta-data, causing
the detector to parse it
incorrectly and/or incompletely.
In contrast, the OS or application on the endhost usually
“understands” the format much deeper (see Section VIII-A)
and processes the ﬁle correctly.

Table XI shows that every scanner we tested is vulner-
able to multiple ﬁle-parsing attacks. Table XII summarizes
the header-parsing discrepancies found by our differential
fuzzing framework. All of them result in successful were-
wolf attacks; the rest were found by manual analysis.

Below, we explain some of the attacks, using ClamAV as
an example of an open-source detector and McAfee as an
example of a closed-source detector.

Table XI

SUCCESSFUL WEREWOLF ATTACKS.

Format type

non-archive

archive

File format
ELF
CHM
ZIP
TAR
TAR
TGZ
TGZ
ZIP

No. of vulnerable AVs
12
2
20
29
35
8
20
12

CVE
2012-1463
2012-1458
2012-1456
2012-1457
2012-1459
2012-1460
2012-1461
2012-1462

HEADER-PARSING ERRORS (ALL RESULT IN WEREWOLF ATTACKS).

Table XII

Format type

Format

Header ﬁelds

non-archive

ELF

PE

archive

CAB

padding
identsize
class
abiversion
abi
encoding
e version
ei version
e minalloc + 13 others
e ip and e res
cbCabinet
vMajor
reserved3
reserved2
reserved1
coffFiles
vMinor

No.
of
vuln. AVs
4
5
11
4
4
14
4
6
2
1
5
2
3
2
3
14
2

CVE

2012-1439
2012-1440
2012-1442
2012-1444
2012-1445
2012-1446
2012-1447
2012-1454
2012-1441
2012-1441
2012-1448
2012-1449
2012-1450
2012-1451
2012-1452
2012-1453
2012-1455

A. Sample werewolf attacks on archive ﬁles
Wrong checksum. In a POSIX TAR archive, each member
ﬁle has a 512-byte header protected by a simple checksum.
All headers also contain a ﬁle length ﬁeld, which is used by
the extractor to locate the next header in the archive. Most
scanners do not use the checksum ﬁeld when parsing an
archive. This is reasonable because a virus may lurk even
in an archive whose checksum is wrong, but in this case the
scanners are too smart for their own good.

87

Our sample attack uses a TAR archive with two ﬁles: the
ﬁrst one is clean, while the second is infected with the test
EICAR virus. The length ﬁeld in the header of the ﬁrst,
clean ﬁle has been modiﬁed to point into the middle of the
header of the second, infected ﬁle (see Figure 2). Scanners
that do not verify the checksum ﬁeld are unable to ﬁnd the
beginning of the second header. 35 of the 36 tested scanners
fail to detect the infection in the modiﬁed archive (the only
exception is eSafe 7.0.17.0).

In contrast, tar on Linux discovers that the checksum
is invalid, prints out a warning, skips the ﬁrst header, ﬁnds
the second, infected ﬁle by searching for the magic string
“ustar,” and proceeds to extract it correctly.

header 1

header 2

length

chksum

file 1

length

chksum

file 2

regular TAR archive

header 1

header 2

Random garbage at

the end of a valid GZIP archive
does not affect the gzip program, which simply ignores it
when extracting the contents. Yet, given an EICAR-infected
.tar.gz ﬁle with 6 random bytes appended, 8 out of 36
scanners fail to detect the infection.
Ambiguous ﬁles conforming to multiple formats. Flex-
ibility of many ﬁle formats enables an attacker to create
werewolf ﬁles that can be correctly parsed according to more
than one format and produce different results. Given that
zip correctly parses ZIP archives with garbage prepended,
while tar correctly parses TAR archives with garbage
appended, we created a werewolf ﬁle consisting of a TAR
archive followed by a virus-infected ZIP archive. This ﬁle
can be processed either by tar, or by zip and different
contents will be extracted depending on which program is
used. 20 out of 36 scanners fail to detect the infection.

Other werewolf ﬁles that can be parsed according to
multiple formats are CAB-TAR, ELF-ZIP, and PE-CAB.
Some of these pairs include non-archive formats!

length

chksum
(corrupt)

file 1

(benign)

length

chksum

file 2

(infected)

B. Sample werewolf attacks on non-archive ﬁles

crafted TAR archive

Figure 2. A crafted TAR archive with the modiﬁed length ﬁeld in the ﬁrst
header.

Misleading length. If the length ﬁeld in the header of a ﬁle
included into a TAR archive is greater than the archive’s total
length (1, 000, 000+original length in our experiments), 29
out of 36 scanners fail to detect the infection.

One of the vulnerable scanners is McAfee, which has the
default upper limit of 1 MB on memory for loading a ﬁle.
Since the size speciﬁed in the header is greater than 1 MB,
McAfee declares the ﬁle clean. GNU tar prints a warning
but extracts the infected contents correctly.
Multiple streams. GZIP ﬁles can contain multiple com-
pressed streams, which are assembled when the contents are
extracted. This feature can be used to craft a .tar.gz ﬁle
with the EICAR test virus broken into two parts. 20 out of
36 scanners fail to detect the infection. When the contents
are extracted, the infected ﬁle is correctly reassembled.

For example, McAfee simply ignores all bytes after the
ﬁrst stream of compressed data. Even if another infected ﬁle
is appended to a GZIP ﬁle containing multiple compressed
streams, McAfee fails to detect the infection.
Random garbage. If a ZIP archive has garbage bytes in the
beginning, the unzip program skips these bytes and still
extracts the contents correctly (we used Zip 3.0 and UnZip
6.00 in Ubuntu 10.04 for this test). 12 out of 36 scanners
fail to detect the infection in a ﬁle consisting of 1024 bytes
of random garbage followed by an EICAR-infected ZIP ﬁle.
Note that the ﬁle still has the proper .zip extension.

Werewolf attacks are also effective against executables,
Ofﬁce documents, and CHM ﬁles. Many modern ﬁle formats
are similar to archives because they can can contain em-
bedded objects of different types. This makes parsing much
harder and opens the door to werewolf attacks.
Fake endianness. In most ELF ﬁles, the 5th byte of the
header indicates endianness: 01 for little-endian, 02 for big-
endian. Linux kernel, however, does not check this ﬁeld
before loading an ELF ﬁle. If the 5th byte of a Bliss-infected,
little-endian ELF ﬁle is changed to 02, 12 out of 36 scanners
fail to detect the infection.
Empty VBA project names. MS Word ﬁles may contain
embedded objects such as executable macros, images, etc.
Because viruses can exploit the auto-execution feature, de-
tectors try to recognize and scan macros in the document.
In this example, we focus on how ClamAV does this.

In MS documents, macros are generally stored inside
VBA (Visual Basic for Application) projects. A group of
VBA projects is identiﬁed by a two-byte signature, “cc61”;
each project in a group has an unique unicode name. Cla-
mAV ﬁrst iterates through the VBA project names treating
the data as little-endian and checks if the resulting names
are valid (have positive length and begin with “*\g”, “*\h”,
“*\”, or “*\d”). If an invalid name is found, ClamAV stops.
ClamAV stores the number of valid project names it found in
the ﬁrst pass and repeats the same process, but now assuming
that the data are big-endian. Finally, ClamAV compares the
number of strings found during the two passes. If the ﬁrst
number is greater than the second, ClamAV treats the ﬁle as
little-endian, otherwise, as big-endian.

We created an ABC-infected Word ﬁle in which the ﬁrst
VBA project name is empty but the other names are intact.

88

When parsing project names, ClamAV calculated the valid
name count to be 0 in both little-endian and big-endian
cases and failed to detect the infection. On the other hand,
destination applications (MS Ofﬁce 2007 and OpenOfﬁce)
open the document correctly and execute the infected macros
even though the ﬁrst project name is empty.
Incorrect compression reset interval. A Windows Com-
piled HTML Help (CHM) ﬁle is a set of HTML ﬁles, scripts,
and images compressed using the LZX algorithm. For faster
random accesses, the algorithm is reset at intervals instead
of compressing the entire ﬁle as a single stream. The length
of each interval is speciﬁed in the LZXC header.

If the header is modiﬁed so that the reset interval is lower
than in the original ﬁle, the target application (in this case,
Windows CHM viewer hh.exe) correctly decompresses
the content located before the tampered header. On the
other hand, several detectors (including ClamAV) attempt
to decompress the entire CHM ﬁle before scanning it for
malware. When they fail to decompress the contents located
after the tampered header, they declare the ﬁle to be clean.
Bypassing
signatures. ClamAV uses
section-speciﬁc hash signatures when scanning Windows
executables. They contain the offset and the length of a
section of the ﬁle and the value of its MD5 hash. 85% of
signatures in ClamAV’s current database are of this type.
If ClamAV’s parser mistakenly believes that the executable
is corrupt or contains some unsupported features, ClamAV
skips the section-speciﬁc hash signatures, enabling evasion.

section-speciﬁc

VII. DEFENSES AGAINST CHAMELEON ATTACKS

Simplistic solutions such as changing the order in which
magic strings are matched may address the speciﬁc vul-
nerabilities we found but will undoubtedly introduce new
ones. One generic defense against all chameleon attacks is
to recognize ﬁles that match multiple types and process
them for all matching types. This may open the door to
denial of service if the attacker ﬂoods the detector with
ﬁles containing a large number of magic strings. To prevent
this, the detector should reject ﬁles with an abnormally high
number of possible types, but this only works for ﬁxed-offset
magic strings. Checking whether the ﬁle matches more than
a certain number of regular expressions can still impose an
unacceptable overhead on the detector.

Another approach is normalization: the detector can mod-
ify the ﬁle to ensure that the endhost’s interpretation of
its type matches the detector’s. In Windows, ﬁle extension
determines by default which program is used to open it.
In Linux, desktop managers such as KDE and GNOME use
extensions as the ﬁrst heuristic and fall back on magic strings
if the ﬁle has an unknown extension or no extension at all.
Unfortunately, rewriting the extension is not a failproof
defense against chameleon attacks because it does not guar-
antee that the endhost’s behavior matches the detector’s

expectations. First, users may override the default settings
in both Windows and Linux and choose any program to
open any ﬁle. Second, for endhosts running Linux,
the
detector must be aware of the list of known extensions: if the
normalized extension is not on the list, chameleon attacks
may still succeed even with the default settings.

VIII. NETWORK-BASED DEFENSES AGAINST

WEREWOLF ATTACKS

No matter what technique a network-based detector is
using—scanning for virus signatures, emulated execution,
behavioral analysis, etc.—it must ﬁrst recognize the type
of the ﬁle and parse it correctly. Even behavioral detection
does not help if the detector is unable to ﬁnd executable
code in a maliciously crafted ﬁle and thus cannot execute it.
Because network-based detectors do not observe the actual
processing and/or execution of the ﬁle on the endhost, they
must guess how the endhost may process the ﬁle. If a
network-based detector is protecting multiple endhosts, it
must guess correctly for all of them. In the rest of this
section, we argue that this is extremely error-prone.

A. “Write a better parser”

The obvious defense against werewolf attacks is to ensure
that the malware detector parses each ﬁle exactly the same
way as the ﬁle’s destination application or OS, thus elim-
inating the “semantic gap.” Note, however, that detectors
deployed on mail servers, network gateways, as a cloud-
based service, etc. aim to beneﬁt from economies of scale
and typically protect many hosts with a diverse set of
applications installed on them.

To prevent werewolf attacks, the malware detector must
parse each ﬁle in multiple ways, one for every possible
destination application and OS. The detector must (1) know
all applications that may possibly be used on any of the
endhosts to access the ﬁle, (2) know every application’s
parsing logic, (3) precisely replicate this logic within the
detector for all possible inputs, including damaged and non-
compliant ﬁles, (4) replicate every known and unknown bug
in every application’s parsing algorithm, and (5) be promptly
updated with a new algorithm whenever an application is
installed or updated on any endhost.
Format-compliant parsing is not enough. Format speciﬁ-
cations prescribe how to parse correctly formatted ﬁles. In
practice, however, many ﬁles do not fully conform to the
speciﬁcation, thus (1) applications are designed to handle
even non-compliant ﬁles, and (2) blocking “malformed” ﬁles
is likely to render the detector unusable because of false pos-
itives. Many formats do not deﬁne what it means for a ﬁle to
be “well-formed,” causing ﬁles created by legitimate appli-
cations to appear malformed. For example, up to 68% of PE
executable images in the wild have structural anomalies and
do not conform to the PE format speciﬁcation [29]. Formats
like PDF have no universally recognized notion of validity

89

and even conformance benchmarks accept malformed and
corrupt ﬁles [14]. Furthermore, every application parses non-
compliant ﬁles in its own idiosyncratic way, resulting in
different outputs for the same input (see Fig. 3).

format−compliant inputs  

A3

A1

 


         

         

         

         

         

         

         

         

         

         

         

         

         

         

         

A2

Ai: inputs from which

i−th application

produces valid output

Figure 3. Parsing discrepancies.

Let I be the set of all possible ﬁle inputs, O the set of
possible outputs, and Sf : IS → OS the speciﬁcation for
format f , where IS ⊂ I and OS ⊂ O. An ideal program
Pideal would only produce an output for compliant inputs:

(cid:2)

Pideal(x) =

Sf (x)
Error

if x ∈ IS
if x ∈ I − IS

In practice, however, applications have to deal with non-
compliant inputs that lie in I − IS. Any real program Preal
has its idiosyncratic way Sd of parsing non-compliant ﬁles:

⎧⎨
⎩ Sf (x)

Sd(x)
Error

Preal(x) =

if x ∈ IS
if x ∈ Id where Sd : Id → OS
if x ∈ I − (IS ∪ Id)

Suppose there are n programs P1, P2, . . . , Pn for process-
ing format f . The detector AV does not know which of them
will be used on the endhost and must produce:

Sf (x)
Sd1
Error

(x) . . . ∪ Sdn

(x)

if x ∈ IS
if x ∈ Id1
. . . ∪ Idn
if x ∈ I − (IS ∪ Id1
. . . ∪ Idn

)

⎧⎪⎪⎨
⎪⎪⎩

AV (x) =

Building such a parser is very difﬁcult. For example,
speciﬁcations of archive formats usually do not say what to
do when some member ﬁles are damaged or malformed (e.g.,
have invalid checksums). Some applications extract only the
valid ﬁles, others generate an error, yet others attempt to
repair the damage or simply ignore the invalid checksums.
Critically, many applications produce usable outputs even
for the input ﬁles that are invalid according to the format
speciﬁcation.
Detectors do not parse in the same way as applications.
First, the parsing functionality of applications is much richer
than that of malware detectors. Detectors only implement
the bare minimum needed to analyze a ﬁle for malware.
In the above example, many detectors ignore checksums in
archives because their goal is to ﬁnd hidden malware code,
not verify ﬁle integrity. At ﬁrst glance, a parser that ignores

90

checksums seems safe because, in theory, it should accept
strictly more inputs than a parser that veriﬁes checksums. As
we show in Section VI, this is not true! Ignoring checksums
introduces subtle parsing discrepancies between the parser
and the application and enables werewolf attacks.

Second, applications often have bugs in their ﬁle-parsing
code. The detector must replicate every known and unknown
parsing bug in every application that could be used on any
endhost to handle the ﬁle.

Third, many format speciﬁcations are incomplete and/or
nondeterministic. As a consequence, different applications
make different choices and parse even the same compliant
ﬁle in different ways. For example, parsing of PDF ﬁles is
notoriously loose [14, 26].

Fourth, speciﬁcations of proprietary ﬁle formats are often
closed-source and change with every release of the applica-
tion, making it infeasible for the implementors of malware
detectors to keep up.
It is hard to replicate the application’s parsing logic.
Even with access to the application’s parser, it is difﬁcult to
write another parser that exactly replicates its behavior on
all possible inputs. For example, after 12 years of bug-ﬁxing
there are still many ﬁle-parsing discrepancies between the
“functionally equivalent” busybox and coreutil versions of
Unix utilities [7]. 238 out of 743 compatibility bugs between
OpenOfﬁce and MS Ofﬁce are caused by ﬁle processing [24]
and even after a signiﬁcant reverse-engineering effort, faith-
ful replication of parsing remains a challenge [23].

In general, complete replication of the input-output behav-
ior is infeasible for most non-trivial systems. Non-parsing
examples include differences between OS implementations
of the same network protocol stack (exploited by Nmap) and
differences between monitored and unmonitored execution
environments (exploited by split-personality malware [8]).
Same ﬁle can be parsed according to different, contra-
dictory formats. Many ﬁle formats are ﬂexible enough that
an attacker can craft a single ﬁle which is valid according to
multiple ﬁle formats and can be correctly parsed in multiple
ways. For example, as mentioned in Section VI, a werewolf
ﬁle consisting of a valid TAR archive followed by a valid
ZIP archive can be processed either by tar, or by zip and
will produce different results depending on which program
is used. Similar attacks are possible for other format pairs,
such as ELF and ZIP or PE and CAB.

The detector must determine all possible formats with
which the ﬁle may be compatible, and, for each format, parse
the ﬁle in all possible ways supported by all applications
dealing with this format. Even if this were feasible, it is
likely to impose an unacceptable performance overhead.
Detector must keep an up-to-date list of all applications
on all protected endhosts. Even if the detector were capable
of accurately replicating hundreds of different ﬁle-parsing
algorithms, it must know which algorithms to apply. To do

this, the detector must know which applications may handle
the ﬁle of any of the protected endhosts at any given time,
and its parsing logic must be promptly updated whenever a
new version of any application is installed on any endhost.
In many cases—for instance, when the detector is running
on a mail server—the complete set of protected applications
may not even be known.

For example, one of our werewolf attacks involves a TAR
archive with a single ﬁle and a malformed header specifying
a signiﬁcantly larger length than the actual size of the ﬁle.
We tested three different Linux applications: GNU tar 1.22,
7-Zip 9.04 beta, and File Roller 2.30.1.1. 7-Zip was not able
to extract the contents. GNU tar extracted the contents with
an “unexpected EOF” warning. Surprisingly, File Roller,
which is a GUI front-end for GNU tar, failed to extract
the contents. Further examination revealed that even though
GNU tar extracts correctly, it returns 2 instead of the usual
0 because it reached the end of ﬁle much earlier than it
was expecting based on the length ﬁeld of the header. This
causes File Roller to produce an error message.

File parsing is version-dependent even in the same appli-
cation. For example, GNU tar up to version 1.16 supported
ustar type N header logical records, but later versions of
GNU tar no longer do.
It is hard to update parsing code. Adding or modifying a
ﬁle parser is not nearly as simple as adding a new virus sig-
nature. All signatures share a common format, thus a generic
signature-matching engine is usually capable of handling
both old and new signatures. Adding new signatures does not
require any changes to the signature format or the scanning
code. Parsers, on the other hand, must be implemented by
hand and manually updated after any change in the parsing
logic of any of the protected applications.
Normalization is no easier than parsing. Normaliza-
tion—rewriting a non-compliant ﬁle so that it complies with
the format speciﬁcation—may help in defeating werewolf
attacks. Unfortunately, it requires parsing the ﬁle ﬁrst and
thus faces all the problems outlined above.

For example, consider normalizing an archive to remove
invalid ﬁles. The detector must parse the archive to ﬁnd
individual ﬁles and determine their validity according to the
speciﬁcation of each ﬁle’s format. This is extremely error-
prone. Suppose that per speciﬁcation, the 5th byte of the ﬁle
contains the format version number. Now the detector must
keep track of valid version numbers for each format, and so
on. The notion of validity varies dramatically from ﬁle to
ﬁle, with different parts of the header and content used for
this purpose in different formats. This makes normalization
infeasible for all but the simplest formats.

B. Do not parse ﬁles in the detector

An alternative to parsing is to submit each ﬁle to a
lets it be parsed by the actual

virtual environment that

application or loaded by the guest OS, then tries to detect
malware from outside the OS (e.g., using virtual-machine
introspection [13]). This approach defeats chameleon and
werewolf attacks only if all of the following hold: (1) the
guest OS and applications are exact replicas of the protected
endhost; (2) if there are multiple endhost conﬁgurations
(e.g., if different hosts may use different applications or
versions of the same application to access a given ﬁle),
every conﬁguration is replicated exactly; (3) the analysis
environment exactly replicates human behavior, including
user responses to “repair corrupted ﬁle?” messages; and
(4) the environment is not vulnerable to split-personality
evasion [8]. Production deployment of network- or cloud-
based malware detectors that satisfy all of these requirements
is a hard problem beyond the scope of this paper.

C. Defend in depth

Many attacks are detector-speciﬁc, thus applying mul-
tiple detectors to the same ﬁle—as advocated by Clou-
dAV [21]—may provide better protection, at a signiﬁcant
performance cost. Some of our attacks, however, evaded all
36 tested AV scanners. Furthermore, several non-interfering
attacks can be combined in a single ﬁle, enabling it to evade
multiple detectors.

IX. HOST-BASED DEFENSES AGAINST WEREWOLF

ATTACKS

One of the main purposes of network-based deployment
of malware detectors is to reduce the need for host-based
detection. Nevertheless, we discuss host-based defenses for
completeness. Host-based techniques—such as continuously
scanning the memory for signs of malicious behavior—are
effective because the detector operates during or after the ﬁle
has been processed and thus does not need to independently
replicate the results of ﬁle processing. Therefore, host-based
detectors are better equipped to deal with chameleon and
werewolf attacks. In practice, however, many are vulnerable
to the same attacks as their network-based versions.

A. On-access scanning

A typical on-access scanner intercepts ﬁle-open, ﬁle-close,
and ﬁle-execute system calls and scans their targets for
infection. On-access scanners are effective against werewolf
attacks on archive formats only because they do not need
to parse archives. After the user has extracted the contents,
she will try to open and/or execute them. At this point, the
scanner intercepts the open/execute system call and detects
the virus before any harm is done. This is a special case
where the results of parsing (i.e., the extracted ﬁles) are
stored in the ﬁle system and thus accessible to the scanner.
Unfortunately, as we show in this paper, werewolf attacks
affect not only archive formats, but also ELF, PE, and
MS Ofﬁce (among others). For these formats, existing on-
access scanners do not have access to the internal data

91

representation after the OS or application has parsed the
ﬁle and must rely on their own parsing, opening the door
to werewolf attacks. For example, on-access scanning in
ClamAV uses a Linux kernel module called Dazuko that
scans the target ﬁles of open, close, and exec system calls. In
our experiments, ClamAV successfully detected an infected
ﬁle unpacked from a malformed archive into the monitored
directory, but failed to detect an infected Word ﬁle with an
empty VBA project name (see Section VI-B) even when
opened by OpenOfﬁce from the same directory.

B. Tight integration with applications

When running on the host, a malware detector can beneﬁt
from tighter integration with the ﬁle-processing logic of the
OS and applications. One plausible approach is for the OS
and application implementors to refactor their code so that
the detector can be invoked in the middle of ﬁle processing
and given access to the results of parsing. Unfortunately, this
approach is insecure against malware that exploits vulnera-
bilities in the parsing code itself. For example, a detector that
waits until the JPEG library has parsed a JPEG ﬁle before
checking that the ﬁle is safe cannot protect the library from
malicious JPEGs that use bugs to take it over, defeating the
purpose of malware detection. Furthermore, tight integration
between applications and external functionality which is
not integral to the their operation adds complexity and is
contrary to the principles of modular system design.

Figure 4. Application refactoring to mitigate werewolf and chameleon
attacks on host- and cloud-based malware scanners.

Privilege separation can help solve this “chicken and egg”
dilemma if the application is refactored so that the parsing
code runs at lower privilege than the rest of the application.
The parser can invoke a host- or even cloud-based malware
detector and send the results of parsing for scanning, as
shown in Fig. 4. After the detector declares them clean, they
are passed on to the rest of the application. This architecture
avoids the need to replicate application-speciﬁc parsing in
the detector. Even if malware exploits a vulnerability in the
parser, it will only gain the ability to perform low-privilege
operations that the parser is allowed to perform.

Implementing this architecture requires that the antivirus
vendors support a standardized interface through which ap-
plications can submit parsed data for analysis. Some existing

archiving applications such as WinRAR and WinZip support
invocation of command-line antivirus scanners, but this func-
tionality is not yet available in non-archiving applications.
Another approach is for the application and the detector
to use the same parsing code, e.g., by employing the same
parsing library. For instance, multiple-streams and random-
garbage attacks do not work against ClamAV because
ClamAV uses the libz library for parsing GZIP ﬁles.
The origins of libz are similar to gzip, thus ClamAV
effectively uses the same parsing code as the application.
This approach suffers from most of the ﬂaws outlined
in Section VIII-A—the detector must know exactly which
parsing code is used by the application and must be updated
whenever the application’s parsing logic changes—but these
ﬂaws may be easier to mitigate in a host-based deployment.

X. CONCLUSION

We presented two classes of practical attacks against au-
tomated malware detectors. They enable even unobfuscated,
easily recognizable malware to evade detection by placing
it in specially crafted ﬁles that are processed differently by
the detector and the endhost. All 36 antivirus scanners in
our experimental testing proved vulnerable to these attacks,
yielding a total of 45 different exploits, almost all of which
are reported here for the ﬁrst time. The rest have been known
only anecdotally and never been systematically analyzed.

We argue that semantic gaps in ﬁle processing are a funda-
mental ﬂaw of network- and cloud-based malware detectors,
regardless of the actual detection technique they use. As
long as the detector analyzes ﬁles on its own, independently
of the actual operating systems and applications on the
endhosts, it faces the insurmountable challenge of correctly
replicating their ﬁle-processing logic on every possible input.
Development of malware detectors that do not suffer from
this gap—for example,
if they operate on exact virtual
copies of protected systems that process each ﬁle using the
actual applications and faithfully emulate human response,
or if they are integrated with the parsing logic of actual
applications—is an interesting topic for future research.
Acknowledgments. The research described in this paper
was partially supported by the NSF grants CNS-0746888
and CNS-0905602, Google research award, and the MURI
program under AFOSR Grant No. FA9550-08-1-0352.

REFERENCES

[1] S. Alvarez.

Antivirus

http:
//events.ccc.de/camp/2007/Fahrplan/attachments/1324-
AntivirusInSecuritySergioshadownAlvarez.pdf, 2007.

insecurity.

[2] S. Alvarez and T. Zoller.

The death of AV de-
fense in depth? - revisiting anti-virus software. http:
//cansecwest.com/csw08/csw08-alvarez.pdf, 2008.

[3] avast! Anti-virus engine malformed ZIP/CAB archive
virus detection bypass. http://secunia.com/advisories/
17126/, 2005.

92

[4] A. Barth, J. Caballero, and D. Song. Secure content
snifﬁng for web browsers, or how to stop papers from
reviewing themselves. In S&P, 2009.

[5] D. Bates, A. Barth, and C. Jackson. Regular expres-
sions considered harmful in client-side XSS ﬁlters. In
WWW, 2010.

[6] D. Brumley, J. Caballero, Z. Liang, J. Newsome, and
D. Song. Towards automatic discovery of deviations in
binary implementations with applications to error de-
tection and ﬁngerprint generation. In USENIX Security,
2007.

[7] C. Cadar, D. Dunbar, and D. Engler. KLEE: Unassisted
and automatic generation of high-coverage tests for
complex systems programs. In OSDI, 2008.

[8] X. Chen, J. Andersen, Z. Mao, M. Bailey, and
J. Nazario.
Towards an understanding of anti-
virtualization and anti-debugging behavior in modern
malware. In DSN, 2008.

[9] ClamAV. http://www.clamav.net.
[10] http://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=

evasion, 2012.

[11] EICAR — The Anti-Virus or Anti-Malware Test File.

http://www.eicar.org/anti virus test ﬁle.htm.

[12] T. Garﬁnkel. Traps and pitfalls: Practical problems in
system call interposition based security tools. In NDSS,
2003.

[13] T. Garﬁnkel and M. Rosenblum. A virtual machine
introspection based architecture for intrusion detection.
In NDSS, 2003.

[14] M. Gavin.
PDF ﬁles.
recognizing malformed pdf f.pdf.

Recognizing corrupt and malformed
http://labs.appligent.com/presentations/

[15] P. Hooimeijer, B. Livshits, D. Molnar, P. Saxena, and
M. Veanes. Fast and precise sanitizer analysis with
BEK. In USENIX Security, 2011.

[16] M. Hypponen.

Retroviruses - how viruses ﬁght
back. http://www.hypponen.com/staff/hermanni/more/
papers/retro.htm, 1994.

[17] G. Kessler.

File signatures table.

http://www.

garykessler.net/library/ﬁle sigs.html, 2012.

[18] McAfee VirusScan vulnerability. http://www.pc1news.

com/news/0665/mcafeevirusscanvulnerability-allow-
compressed-archives-to-bypass-the-scan-engine.html,
2009.

[19] J. Nazario.

Mime

snifﬁng

and

phishing.

http://http://asert.arbornetworks.com/2009/03/mime-
snifﬁng-and-phishing/, 2009.

[20] J. Oberheide, M. Bailey, and F. Jahanian. PolyPack: An
automated online packing service for optimal antivirus
evasion. In WOOT, 2009.

[21] J. Oberheide, E. Cooke, and F. Jahanian. CloudAV:

N-version antivirus in the network cloud. In USENIX
Security, 2008.

[22] J. Oberheide and F. Jahanian. Remote ﬁngerprinting
and exploitation of mail server antivirus engines. http:
//jon.oberheide.org/ﬁles/umich09-mailav.pdf, 2009.

[23] Microsoft patch breaks Impress/PowerPoint compat-
http://user.services.openofﬁce.org/en/forum/

ibility.
viewtopic.php?t=36515, 2010.

[24] OpenOfﬁce-MS

interoperability

bugs.

http:

//openofﬁce.org/bugzilla/buglist.cgi?keywords=ms
interoperability, 2011.

[25] W. Palant. The hazards of MIME snifﬁng.

http:

//adblockplus.org/blog/the-hazards-of-mime-snifﬁng,
2007.

[26] S. Porst. How to really obfuscate your PDF mal-
http://www.recon.cx/2010/slides/recon 2010

ware.
sebastian porst.pdf, 2010.

[27] T. Ptacek and T. Newsham.

Insertion, evasion, and
denial of service: Eluding network intrusion detection,
1998.

[28] T. Scholte, D. Balzarotti, and E. Kirda. Quo vadis? A
study of the evolution of input validation vulnerabilities
in Web applications. In FC, 2011.

[29] C. Sheehan. Pimp my PE: Parsing malicious and mal-
formed executables. http://research.sunbelt-software.
com/ViperSDK/Pimp%20My%20PE.ppt, 2007.

[30] IE content-type logic.

http://blogs.msdn.com/b/ie/

archive/2005/02/01/364581.aspx, 2005.

[31] P. Sz¨or and P. Ferrie.

Hunting for metamor-
http://www.symantec.com/avcenter/reference/

phic.
hunting.for.metamorphic.pdf.

[32] Virus Total. http://www.virustotal.com.
[33] VX Heavens. http://vx.netlux.org/vl.php.
[34] R. Watson. Exploiting concurrency vulnerabilities in

system call wrappers. In WOOT, 2007.

[35] J. Weinberger, P. Saxena, D. Akhawe, M. Finifter,
R. Shin, and D. Song. A systematic analysis of
XSS sanitization in Web application frameworks.
In
ESORICS, 2011.

[36] A. Wheeler and N. Mehta.

0wning antivirus.

http://www.blackhat.com/presentations/bh-europe-
05/bh-eu-05-wheeler-mehta-up.pdf, 2005.

[37] F. Xue. Attacking antivirus. http://www.blackhat.com/
presentations/bh-europe-08/Feng-Xue/Whitepaper/bh-
eu-08-xue-WP.pdf, 2008.

[38] Anti-virus software may not properly scan malformed
http://www.kb.cert.org/vuls/id/968818,

zip archives.
2005.

[39] Musing on information security. http://blog.zoller.lu/

search/label/Advisory, 2009.

APPENDIX

93

EXAMPLES OF CLAMAV’S FIXED-OFFSET “MAGIC STRINGS” IN THE ORDER THEY ARE CHECKED.

Table XIII

Magic content
5b 61 6c 69 61 73 65 73 5d

Order
25

Offset
0

Length
5

File type
EVS mail

Order
1

2

3
4

5

6
7
8
9

10

11
12
13

14

15

16

17
18
19
20

21

22

23

24

Offset
0

257

0
0

8

6
6
0
0

0

0
0
0

0

0

0

0
0
0
0

0

0

0

0

Length
9

5

5
14

4

4
4
3
3

8

8
4
4

4

14

13

6
2
8
11

9

11

15

17

File type
mirc.ini
TAR-
POSIX
RTF
SIP log

SIS

JPEG
JPEG
MP3
JPEG
OLE2
container
CryptFF
PNG
ELF

7573746172

7b5c727466
5349502d48495420285349502f48

19040010

4a464946
45786966
fffb90
ffd8ff

d0cf11e0a1b11ae1

b6b9acaefeffffff
89504e47
7f454c46

TNEF

789f3e22

VPOP3
(DOS)
VPOP3
(UNIX)
UUencoded
ARJ
Mail
Symantec

Mail

Mail

Mail

Mail

763a0d0a52656365697665643a20

763a0a52656365697665643a20

626567696e20
60ea
582d5549444c3a20
582d53796d616e7465632d

582d53696576653a20

582d5265616c2d546f3a20

582d4f726967696e616c2d546f3a20

582d456e76656c6f70652d46
726f6d3a20

26

27
28

29

30
31
32
33

34

35
36
37

38

39

40

41
42
43
44

45

46

47

48

0

0
0

0

0
0
0
0

0

0
0
0

0

0

0

0
0
0
0

0

0

0

0

17

4
9

4

13
13
10
4

4

4
8
4

4

12

12

2
4
4
3

26

3

6

5

Magic content
582d455653
582d4170706172656e746c792d
546f3a20
546f3a20
5375626a6563743a20

535a4444

52657475726e2d706174683a20
52657475726e2d506174683a20
52656365697665643a20
52617221

52494658

52494646
504b3030504b0304
504b0304

4f676753

4d6573736167652d49643a20

4d6573736167652d49443a20

Mail

Mail
Mail
compress.
exed
Maildir
Maildir
Raw mail
RAR

RIFX

RIFF
ZIP
ZIP
Ogg
Stream

Mail

Mail

4d5a
MS-EXE
MS CAB
4d534346
MS CHM 49545346
MP3
Qmail
bounce
GIF
Exim
mail

46726f6d3a20

494433
48692e205468697320697320746865
20716d61696c2d73656e64
474946

MBox

46726f6d20

“MAGIC STRINGS” FOR FILE TYPES NOT SUPPORTED BY CLAMAV (SOURCE: [17]). MULTIPLE OFFSETS SEPARATED BY ”,” INDICATE THAT THE

MAGIC CONTENT CAN APPEAR AT ANY OF THEM.

Table XIV

Offset
0
0
0
0
0
2
0
526
29,152
30
0
0
4
0,30
32769, 34817, 36865

Length
8
2
2
8
4
3
6
5
6
6
4
4
8
23
5

File type
MS Ofﬁce ﬁles
TAR.Z (LZW)
TAR.Z (LZH)
AR, MS Coff
PACK
LZA,LZH
7Zip
PKZIP SFX
WinZip
PKLITE
PKZIP
Zoo
Quicktime MOV
EPS
ISO

Magic content
D0 CF 11 E0 A1 B1 1A E1
1F 9D
1F A0
21 3C 61 72 63 68 3E 0A
50 41 43 4B
2D 6C 68
37 7A BC AF 27 1C
50 4B 53 70 58
57 69 6E 5A 69 70
50 4B 4C 49 54 45
50 4B 03 04
5A 4F 4F 20
6D 6F 6F 76
25 21 50 53 2D 41 64 6F 62 65 2D 33 2E 30 20 45 50 53 46 2D 33 20 30
43 44 30 30 31

94

