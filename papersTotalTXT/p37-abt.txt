A Plea for Utilising Synthetic Data when Performing
Machine Learning Based Cyber-Security Experiments

Sebastian Abt and Harald Baier

da/sec – Biometrics and Internet Security Research Group

Hochschule Darmstadt, Darmstadt, Germany
{sebastian.abt,harald.baier}@h-da.de

ABSTRACT
Cyber-security research is a challenging venture where re-
searchers especially face the problem of not having broad
access to labelled real-world data sets. This unavailability of
data challenges performing scientiﬁc sound experiments. Es-
pecially, for machine learning based systems this unavailabil-
ity eﬀectively hinders us to assess performance, attributes
and limitations of such systems. One approach to address
this lack of publicly available data is to perform experiments
using synthetic data. However, we experience that synthetic
data is seldom used in our community. This position paper
gives a plea for utilising synthetic data when performing ma-
chine learning based cyber-security experiments. For this,
we collect major challenges our community faces today and
discuss how synthetic data can help solving them. Further-
more, we discuss open questions in the area of data synthesis
and propose directions for future work.

Categories and Subject Descriptors
I.2.6 [Artiﬁcial Intelligence]: Learning; I.5.2 [Pattern
Recognition]: Design Methodology—classiﬁer design and
evaluation; I.6.4 [Simulation and Modeling]: Model Val-
idation and Analysis; I.6.5 [Simulation and Modeling]:
Model Development—modeling methodologies

General Terms
Security, Experimentation

Keywords
Data Synthesis; Ground-Truth; Cyber-Security Research;
Machine Learning

1.

INTRODUCTION

Cyber-security research aims at protecting hosts, services
and users that, respectively, connect to, provide and rely on
Internet resources. To achieve this goal, cyber-security re-
search typically deals with two distinct classes of problems:

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
AISec’14, November 7, 2014, Scottsdale, Arizona, USA.
Copyright 2014 ACM 978-1-4503-3153-1/14/11 ...$15.00.
http://dx.doi.org/10.1145/2666652.2666663.

(i) recognition of known malicious activity and (ii) detec-
tion of previously unknown malicious activity. Recognition
of known malicious activity is commonly referred to as mis-
use detection and detection of previously unknown malicious
behaviour is typically referred to as anomaly detection. The
diﬀerence between these approaches is that misuse detec-
tion relies on a-priori deﬁned models (e.g. simple signatures
or complex models learnt using binary classiﬁers) of mali-
cious activity. Hence, misuse detection recognises previously
learnt models of malicious activity. In contrast, anomaly de-
tection relies on a speciﬁc model of normality and is capable
of detecting deviations from that model according to a spe-
ciﬁc distance measure and threshold. Consequently, misuse
detection is only capable of recognising already known pat-
terns in a data stream under investigation while anomaly
detection is believed to be capable of additionally detect-
ing new and prior unseen malicious events. Typically, both,
models of malicious activity as well as models of normality
are derived by applying machine learning approaches [29, 7].
In order to be able to develop eﬃcient and eﬀective ap-
proaches covering problem instances of either of both classes,
labelled data sets are required. For misuse detection, re-
searchers need to be able to distinguish malicious activity
from benign activity in order to be able to develop charac-
teristic signatures that do not overlap with benign events.
Otherwise, the false positive rate would be high. Similarly,
in order to be able to follow an anomaly detection approach,
researchers need a clean and representative sample of nor-
mality that does not contain (too much) malicious activity.
Otherwise, approaches tend to overlook malicious events in
a data stream and, thus, lead to high false negative rates.
Both eﬀects, i.e. high false positive rates and high false neg-
ative rates, are problematic. Clearly, high false negative
rates lead to large quantities of unnoticed malicious events.
High false positive rates on the other hand either lead to
blocked legitimate activity, i.e. denial of service, or unnec-
essary workload for security operators, depending on the
applied reaction strategy.

This dependency on labelled data underlines the inher-
ent empiricism of many problems faced in cyber-security re-
search. Unfortunately, however, data sets required for con-
ducting research are typically rare. As we have shown by em-
pirical analysis of data sets used in contemporary research in
earlier work [2], data sets used for cyber-security research are
most commonly handcrafted and seldom publicly released.
One approach to overcome this limitation is wide-spread use
of synthetic data and data synthesis toolchains when per-
forming cyber-security research. However, this seems not

37to be best practice in our community.
In order to better
understand how and to what extend synthetic data facili-
tates cyber-security research, we compile a set of open chal-
lenges when applying machine learning approaches to cyber-
security from related work and argue how synthetic data can
be used to solve these challenges. To the end of this posi-
tion paper, we come to the conclusion that our community
could greatly beneﬁt form synthetic data when performing
cyber-security machine learning experiments. However, we
also identify open research questions related to data synthe-
sis and synthetic data that need to be answered in order
to leverage this potential. We hope that our analysis stim-
ulates discussion in our community and fosters research in
the area of data synthesis.

The remainder of this paper is structured as follows: In
Section 2 we clarify our terminology. Section 3 discusses
related work. In Section 4 we compile and present challenges
currently faced in cyber-security research as described in
related work.
In Section 5 we discuss how synthetic data
can be used to address these challenges. Subsequently, in
Section 6 we present open research questions related to data
synthesis. Finally, Section 7 summarises and concludes.

2. PRELIMINARIES

This section is meant to clarify the terminology we use

throughout this paper.
2.1 Learners, Models and Frameworks

Machine learning is commonly deﬁned as a computer pro-
gram’s ability to learn a speciﬁc task from experience, i.e.
without being explicitly programmed. A computer program
is said to actually learn, if its performance in its task im-
proves with experience [16]. In the remainder of this paper,
we refer to a computer program that has the ability to learn
as either learning algorithm or, shorter, learner. Learners
typically take as input instances of a speciﬁc instance space
and output hypotheses of a deﬁned hypothesis space as de-
scription of the concept that is to be learned. Throughout
this paper, we use the term model synonymous with hy-
pothesis and refer to instances from the instance space as
samples. The learners we speciﬁcally consider in this pa-
per belong to the class of supervised learning algorithms.
In contrast to unsupervised learners, supervised learners re-
quire samples to be attributed with class labels in order to
be able to learn.
In the remainder of this paper, we re-
fer to the process and time frame during which a learning
algorithm learns as training. As (learning) framework we
refer to theory that enables us to analyse and categorise
learning algorithms (e.g. Probably Approximately Correct
(PAC) learning framework [27]).
2.2 Synthetic Data

In this paper, with synthetic data, we refer to any data
that, directly or indirectly, serves as input to a learning al-
gorithm and has not been obtained solely by direct measure-
ment in real-world. Speciﬁcally, our deﬁnition of synthetic
data covers data generated by simulation or, more gener-
ally, data generated by computer programs for the purpose
of the data itself (i.e. data generators) as well as real-world
data superposed with data derived by simulation or data
generators.

3. RELATED WORK

While we are not aware of any position paper that dis-
cusses the need of synthetic data in cyber-security research
and proposes open research questions as well as directions
for future work, diﬀerent publications support our line of ar-
gumentation. Most notably, Ringberg et al. [19] discuss the
necessity of simulation for evaluating anomaly detectors. Es-
pecially, they argue that simulation is a prerequisite in order
to maintain experimental control and propose to train and
test systems using synthetic data. Afterwards, they argue
that validation on real data should be performed to under-
stand how an approach performs in reality. While we gen-
erally agree with this line of argumentation of [19], we make
one further step and propose to measure synthetic data’s
quality, as discussed in Section 6.3. This would eliminate
the ﬁnal validation step. In [24], Sonchack et al. especially
discuss the challenge of evaluating performance of large scale
collaborative systems.
In this scenario, the diﬃculties re-
sulting from unavailability of data potentiate as not only
samples from one environment are required, but from many.
Their conclusion is that in such cases, currently, simulation
is the only viable approach in order to perform repeatable
experiments and propose an approach named parameterised
trace scaling. Sommer and Paxson [23] discuss challenges
when applying machine learning to network intrusion detec-
tion. While the authors recognise the general lack of publicly
available labelled real-world data sets, they discourage the
use of synthetic data due to diﬃculties in simulating the In-
ternet [8]. However, they provide no general answer to coun-
tering this challenge. Speciﬁcally, the authors argue that if
one cannot ﬁnd a sound way to obtain ground-truth for the
evaluation, then it becomes questionable to pursue the work
at all [23]. Not disagreeing with this statement, we never-
theless opine that our community should evaluate utility of
synthetic data in greater detail (cf. Section 6). In general,
the lack of available data and the need of ground-truth is
acknowledged throughout many diﬀerent papers. The ne-
cessity to ﬁll this gap reﬂects in a number of papers. For
instance, in [13], a system is proposed that generates insider
threat data by simulating user activity. Another well-known
approach to generating labelled data sets by means of simu-
lation is the DARPA IDEVAL project [14]. In this laudable
eﬀort, a military network has been simulated. However, the
DARPA IDEVAL data sets date back to 1999 and do not
contain any relevant information of today’s networks. Con-
sequently, an approach towards compiling a contemporary
state-of-the-art data set is described by Song et al. [25]. The
resulting Kyoto 2006+ data set has been collected using hon-
eypots deployed in a university network. Benign traﬃc has
been captured from a web and an email server. While this is
a laudable eﬀort towards increasing availability of ground-
truth, we expect the traces contained in this data set to
be very speciﬁc to one environment (cf. discussion of the
consequences in Sections 5.3 and 5.4).

4. CHALLENGES IN CYBER-SECURITY

In this section we present and discuss ﬁve challenges we
typically face today when applying machine learning to cy-
ber-security. The challenges are derived from related work
as discussed in Section 3. These challenges arise under the
general hypothesis that our community is lacking a repre-
sentative set of labelled real-world data. This absence is

38commonly discussed in related work [26, 19, 23]. In earlier
work [2] we have shown by empirical analysis that this hy-
pothesis in fact holds true. Hence, for the remainder of this
position paper we accept this hypothesis and discuss these
challenges under the assumption of general unavailability of
labelled real-world data.
4.1 Performing Sound Experiments

Performing sound experiments is a vital condition for sci-
entiﬁc sound research, especially if the problem domain is in-
herently empirical. Two prerequisites for performing sound
experiments are (i) experimental control and (ii) repeata-
bility of experiments [19, 24]. Experimental control requires
the researcher to be able to map observations to speciﬁc
parameters of an experiment. Especially, it requires the re-
searcher to be able to hypothesise on the outcome of the
experiment given a speciﬁc set of input parameters before
actually performing the experiment. After the experiment
is run, the hypothesis can be accepted or rejected according
to the observed results of the experiment and, consequently,
the researcher will learn speciﬁcs of his problem. In context
of machine learning, this means that a researcher has to be
able to understand which features and auxiliary parameters
aﬀect the speciﬁc outcome of an experiment and how ad-
justing those features and parameters aﬀects the observable
outcome of the experiment. Repeatability of experiments,
on the other hand, requires an experiment to be reproducible
by the same and other researchers and the outcome of multi-
ple runs with equal experimental settings (e.g. equal feature
space, input data, auxiliary parameters and learning algo-
rithm in case of machine learning) to be identical.

Under the assumption of general unavailability of labelled
data, satisfying both preconditions is diﬃcult: when data
is manually compiled and labelled, expert knowledge is re-
quired which is typically not available. Hence, labelling may
be imperfect. Additionally, collected data is typically only
of local scope. Cyber-security, however, is a global challenge
and activity patterns may vary depending on collector local-
ity. Furthermore, data collected usually contains sensitive
information subject to privacy law or other concerns. As
a consequence, collected data is typically subject to non-
disclosure agreements prohibiting further data sharing. All
these circumstances eﬀectively challenge our community’s
ability to perform sound experiments.
4.2 Demonstrating Adaptability

One striking motivation for using machine learning in cy-
ber-security is a learner’s capability to generalise from its
input data and to learn through experience [16, 17]. This
means, models learnt through machine learning approaches
are assumed to be able to correctly cover prior unseen data.
Especially, this also means that approaches successfully ver-
iﬁed in one execution environment are able to operate in
another execution environment, potentially requiring new
training. One prerequisite for this is that samples presented
during training and samples presented during evaluation /
operational conditions are drawn independently and identi-
cally according the same underlying distribution.

However, demonstrating the adaptability of a speciﬁc ap-
proach is challenging: as mentioned in Section 4.1 and un-
derlined by empirical analysis in [2], data sets used to con-
duct research are typically manually compiled and of lim-
ited scope. For instance, in case of network traﬃc analysis,

data is most commonly captured in university or working
group networks [25], using honeypots [1] or crafted in sand-
nets [20]. Researchers rarely have access to live data from
an Internet Service Provider network or even multiple such
networks, which [23] regards ideal for performing convinc-
ing experiments. Consequently, using data of limited scope,
adaptability of an approach cannot be shown. As the dis-
tribution underlying the data set at hand is static, perfor-
mance of an approach in varying environments cannot be
assessed and, thus, adaptability is uncertain. In fact, this
uncertainty of adaptability is assumed to be one major rea-
son for the signiﬁcant gap between machine learning based
cyber-security research, especially anomaly detection, and
successful deployment of such systems in practice [26].
4.3 Determining Operational Limits

Following the line of argumentation of Section 4.2, being
able to determine the operational limits of a learnt model is
a required and challenging eﬀort. As the cyberspace is con-
stantly evolving, patterns of benign activity are assumed to
change as well [8]. Consequently, at a later point in time,
the distribution of samples encountered by machine learning
based systems during live operation may deviate from the
distribution that has been used when generating the model.
As this contradicts the general requirement that training
samples and live samples are drawn identically and inde-
pendently according to the same underlying distribution,
the results of a machine learning based approach relying
on a speciﬁc model are not reliable and the system is sus-
ceptible to false alarms, as discussed in Section 4.4. One
strategy developed by our community to address this issue
is online learning.
In online learning, a system is capable
of constantly adjusting its model according to newly seen
samples. The new samples stem from live data and labels
are inferred from the model present at sample arrival time.
However, such strategies open the gate for adversaries (cf.
Section 4.5) as they can speciﬁcally craft samples to bias
and compromise the model [10, 5].

Being able to determine and understand the operational
limits of a model generated by a learner using a speciﬁc set
of auxiliary parameters and training samples is an important
aspect towards assessing conﬁdence of decisions and, thus,
reliability of a system. Achieving this is challenging, not
only under the assumption of unavailability of data. Data
sets have to be captured for a signiﬁcant period of time in
order to reﬂect changes due to evolution. However, this
typically conﬂicts with career pressure, the competition on
novel ideas and the resulting requirement to rapidly pub-
lish results as faced in our community. On the other hand,
systems have to be capable of reliably running many years
in real-world environments for the implementation eﬀorts to
pay oﬀ commercially. From our point of view, the challenge
and prevalent inability of determining operationally limits is
a signiﬁcant show-stopper for large-scale real-world deploy-
ment of academic approaches.
4.4 Limiting False Alarms

False alarm rates, and derived metrics, are the primary
performance measure of machine learning based systems.
As mentioned earlier, both kind of false alarms, i.e. false
positives and false negatives, are problematic. If a system is
susceptible to high false negative rates, many attacks may go
unnoticed. On the other hand, if a system shows a high false

39positive rate, benign activity may inadvertently be ﬁltered
or security operators may be overwhelmed with alarms to
be processed unnecessarily. Systems showing too high rates
in either case eﬀectively render themselves useless in prac-
tice, depending on the operational requirements. In order
to be able to balance operational requirements and to ad-
just a system’s susceptibility to either one of the cases, an
auxiliary threshold parameter is typically introduced.

To be able to reliably assess a system’s susceptibility to
false alarms and to be able to tune the threshold and other
auxiliary parameters according to a speciﬁc environment,
however, labelled data sets are required. Put in other words,
labelled data sets are a necessary condition in order to reli-
ably assess a system’s performance. Under the assumption
of general unavailability of labelled data, by deﬁnition, this
condition is not met. In practice, our community addresses
this by manually compiling and labelling data sets. Still,
the challenges described in Sections 4.1 – 4.3 apply, leading
to the problem of reliability of performance rates.
4.5 Assessing Adversarial Capabilities

As already mentioned in Section 4.3, online learning stra-
tegies are often employed in cyber-security research.
In
contrast to traditional batch learning where a learning al-
gorithm learns from a ﬁxed set of training samples, online
learning continually reﬁnes a learnt model based on newly
seen samples. While this proceeding allows a learner to
adapt to changing environments, it also opens a new attack
vector to adversaries. For instance, an adversary could try
to compromise an online learner by crafting benign activity
such that it is structurally similar to malicious attempts.
Depending on the mode of operation of a system trained us-
ing such data, this may either lead to a high false negative
rate due to not recognising malicious events, or to high false
positive rate due to wrongly classifying too much legitimate
activity as malicious. In any case, as argued above, this may
render such system practically useless.

Learning in the presence of an adversary can be regarded
as special case of learning from noisy data [11, 21, 22]. How-
ever, one important diﬀerence between learning from noisy
data and learning in presence of an adversary lies in the
fact that an adversary typically does not behave randomly
and independently, as is assumed in most work analysing
learning form noisy data, but explicitly and deliberately ex-
ploits speciﬁc weaknesses of systems and algorithms. This
diﬀerence is crucial and, consequently, adversarial learning
is intensively discussed in our community [5, 10, 4]. Learn-
ing frameworks, based on the PAC framework, that account
for malicious noise have been developed by Kearns et al. [12]
and Bshouty et al. [6]

Assessing the adversary’s capabilities to inﬂuence a learner
and the impact this may have on the performance of the
developed system is challenging – even without assuming
general unavailability of labelled data. When data is man-
ually collected to experiment with a new approach, data
sets by deﬁnition do not contain malicious activity targeting
the approach under development. Consequently, empirical
analysis of attacks against the learning algorithm and the
algorithm’s robustness against such attacks is impossible.

5. A PLEA FOR SYNTHETIC DATA

In Section 4 we discussed ﬁve challenges our community
faces when performing cyber-security research and especially

experiments involving machine learning. Our discussion was
based on the assumption that labelled real-world data sets
are generally unavailable and have to be compiled individu-
ally. In this section, we discuss how synthetic data can help
us to overcome limitations we currently face. Speciﬁcally,
this section is meant to be a plea for synthetic data in order
to stimulate discussion and research in the area of data syn-
thesis. As a preliminary, however, we would like to note that
we understand that completely relying on synthetic data is
not the ideal solution either. In fact, we are convinced that
in an ideal world, both, synthetic and real-world data would
be used to perform experiments and assess characteristics of
approaches. However, access to labelled real-world data is
currently limited.
5.1 Problem Space

In Sections 4.1 – 4.5 we presented diﬀerent challenges our
community is currently facing. These challenges arise due
to distinct data related problems we have to solve using syn-
thetic data. The challenge of performing sound experiments
arises due to the general unavailability of labelled real-world
data sets and the requirement to manually capture and la-
bel data. The problem we face here is that labels may be
imperfect and, as insight in data is generally limited, exper-
imental control is diﬃcult. Unfortunately, sharing manually
compiled data sets is typically not possible due to contrac-
tual obligations with the data sponsor. Hence, one problem
we have to solve concerns the availability of labelled and
comprehensible data and is discussed in Section 5.2. The
challenge of demonstrating adaptability of an approach re-
lates to the problem of typically having only access to data
of limited scope. More speciﬁcally, we typically do not have
access to multiple data sets reﬂecting speciﬁcs of diﬀerent
environments. This problem is discussed in Section 5.3.
The challenge of determining operational limits concerns
the analysis of the decision boundary of a learnt model. In
order to be able to address this challenge, the problem of
typically not having access to data reﬂecting evolution of
environments has to be solved. We discuss this issue in Sec-
tion 5.4. The challenge of limiting false alarms is related
to the above three mentioned challenges. Speciﬁcally, this
problem can be solved if the problems causing the previously
mentioned challenges are solved. Finally, the challenge of
assessing the adversary’s capabilities to inﬂuence a machine
learning based system and the impact of this inﬂuence re-
lates to the intrinsic co-evolutionary nature of attacks and
responses. At the point in time when a response strategy
is developed, no real-world data exist that capture possible
attacks on that strategy. Such traces can at ﬁrst be found
in the wild when a response strategy has been published or
deployed. The resulting circular dependency of attacks and
responses has to be solved using synthetic data as discussed
in Section 5.5.
5.2 Availability of Data

As we mentioned earlier and is backed by the analysis
we performed in [2], labelled real-world data sets are not
broadly publicly available. This general unavailability re-
quires researchers to constantly capture and label new data
sets in order to perform experiments. Interestingly, the re-
sulting data sets are typically not published, too. We be-
lieve that the reason for this is twofold: First, as we have
experienced ourselves, data sponsors usually require to sign

40non-disclosure agreements that prohibit further sharing of
data. The reason for this typically is the data sponsor’s
fear of loss of reputation and risk of revealing compromis-
ing information. If the distance between a data sponsor and
a data consumer is increasing, due to repetitive sharing or
public release, the data sponsor’s possibility to inﬂuence and
observe research performed on the data decreases together
with the data sponsor’s trust in the data consumer. Second,
we expect some researchers to deliberately not share data in
order to gain competitive advantage and to reap the rewards
of initial hard work in data collection.

While the latter phenomenon cannot be solved using syn-
thetic data1, the ﬁrst can. Synthetic data, by deﬁnition, do
not contain sensitive information that may lead to loss of
reputation of a data sponsor. If we utilise data provided by
data sponsors to derive statistical models and generate syn-
thetic data according to these models, both, resulting data
sets as well as data generators can be released without lim-
itations. Developing this concept a step further, we come
to the conclusion that the eﬀort of manual data collection
can greatly be reduced for many problems as data reposito-
ries exist that oﬀer lots of valuable statistical information.
For instance, the Cooperative Association for Internet Data
Analysis (CAIDA)2 continuously performs Internet traﬃc
measurements at varying scales and with varying granular-
ity. The CAIDA repository contains public and semi-public
data sets that can freely be downloaded or requested by
researchers. The CAIDA repository contains, amongst oth-
ers, Internet traﬃc statistics, as well as Internet topology
data sets, backscatter traces and real-world Internet traﬃc
captures. In the latter data sets, IP addresses are typically
anonymised using Crypto-PAn [30] and packet payload is re-
moved. Unfortunately, traﬃc captures provided by CAIDA
are unlabelled. A comparable repository in size, yet dif-
ferent in modus operandi, is the Protected Repository for
the Defense of Infrastructure Against Cyber Threats (PRE-
DICT)3. As mentioned earlier, these repositories are great
resources of network traﬃc statistics and, thus, can espe-
cially be utilised to model benign network traﬃc, even if no
labels are available and malicious traﬃc may not completely
be ﬁltered out. Due to the large volume of traces found in
these repositories and the low base rate of malicious activ-
ity [3] we expect models derived from such repositories to
converge towards the statistically relevant mass. We are
convinced that by following this approach, initial eﬀort to
perform experiments can be reduced and the resulting out-
put of this initial process can be shared without risk and
contribute to the availability of data.
5.3 Speciﬁcs of Environments

As mentioned earlier, the promise of using machine learn-
ing in cyber-security is a learner’s ability to generalise above
training samples and, hence, to both, being able to correctly
handle new and previously unseen samples as well as being
able to adapt to diﬀerent environments. With the latter,
we mean that having a ﬁxed learning algorithm and a ﬁxed
set of auxiliary parameters, a new model showing similar
performance than initially assessed can be trained from new

1We believe that this competitive researcher mindset can
only, and has to, be stopped by deﬁning even stricter re-
quirements for publication of data-driven research.
2http://www.caida.org/
3http://www.predict.org/

training samples. That is, applicability of an approach to a
diﬀerent environment simply depends on the availability of
training data for that environment.

Unfortunately, in most experiments this capability of a
machine learning approach is not evaluated and, in fact,
also cannot. The reason for this, again, lies in the general
unavailability of real-world labelled data and in the fact that
it is diﬃcult and time-consuming enough to capture a single
data set in one speciﬁc environment. Again, this problem
can be addressed by utilising synthetic data. If, as described
in Section 5.2, statistical models of speciﬁc environments are
available, new data can be generated by simply varying pa-
rameters of these models. The degree to which parameters
have to be adjusted can be derived from large-scale reposi-
tories, such as CAIDA and PREDICT for network traces, as
well as from individual measurements. Also, model parame-
ters could be varied randomly in order to broadly assess one
approach’s capability to adapt.

We believe that our community’s inability to demonstrate
an approach’s adaptability is one major reason for the gap
we recognise between the amount of machine learning based
cyber-security experiments published and the number of ma-
chine learning based techniques actually deployed in prac-
tice. Approaching such demonstration using synthetic data,
hence, would greatly increase utility of our proposed ap-
proaches and reputation of our community.

5.4 Evolution of Environments

Non-stationarity of eﬀects in cyberspace has been dis-
cussed and demonstrated in earlier work [8]. Unlike other
disciplines (e.g. natural sciences, such as physics and maths),
phenomenon observables in the Internet can change rapidly.
Unfortunately, this change may negatively aﬀect reliability
of deployed machine learning approaches. The reason for
this lies in the fact that common learning frameworks (e.g.
PAC [27]) assume training and live samples to be drawn
independently and identically according to the same prob-
ability distribution. As career pressure and competition in
our community is high, researchers often enough are able of
collecting data only for a relatively short period of time –
especially in comparison to the desired lifetime of systems
in practice. This results in the collection period typically
not being long enough to reﬂect evolutionary eﬀects. Con-
sequently, as of today, cyber-security research based on ma-
chine learning approaches cannot indicate operational limits
related to a speciﬁc trained model.

We believe that synthetic data can solve this problem as
well. If scientiﬁc work would be evaluated using synthetic
data sets generated according to statistical models, data
could be altered according to custom ageing models. In a
very simplistic model, labels of a subset of samples could
be altered according to a speciﬁc random process. This
proceeding would not change the parameters of the data
generator, i.e. maintaining speciﬁcs of a given environment,
yet would simulate a slight data change. By gradually in-
creasing the size of the subset of samples to be altered, the
sensitivity of the learnt model with regard to the evolved
environment can be evaluated and operational limits can be
deduced. More complex models could be derived from ex-
isting data repositories, as they typically collect and store
data for many consecutive years.

Additionally, when a system is deployed in practice, we
could derive a statistical model of live data similar in the

41way we derived a statistical model during training. By com-
paring parameters of both models, we could try to work to-
wards a measure of conﬁdence of the learner’s output. Such
conﬁdence measure would greatly increase utility of machine
learning based approaches in reality. For instance, if the de-
termined conﬁdence is low, an operationally relevant conclu-
sion could be that security operators should not spent valu-
able time devoting themselves to analysis of alarms raised
by the system. Instead, assuming high false positive rates
due to low conﬁdence, operators should devote time inspect-
ing the root cause of distribution shift and possibly trigger
re-training of the model.
5.5 Attack-Response-Attack Cycle

Cyber-security research is a prevalently empirical disci-
pline, focussing at solving real-world problems. Speciﬁcally,
cyber-security research is an arms race with attackers who
leverage the size and openness of cyberspace in order to
probe and attack systems.
In contrast to researchers or,
more generally, defenders, attackers have a great competi-
tive advantage: attackers can silently evolve their techniques
and actively attack a target once the technique has matured.
Our community refers to the result of this process as zero-
day attack. The striking problem with zero-day attacks is
that defenders can only react and typically do not know
what exactly is going on. That is, attackers face a signif-
icant window of time during which systems are practically
unprotected. In order to recover from that state, research
and defenders typically intensively work towards identifying
the problem, ﬁxing the gap and developing detection tech-
niques. At that point, the attacker is challenged and the
cycle re-iterates.

The problem with this circular dependency of attacks and
responses is that, by deﬁnition, real-world data reﬂecting an
attacker’s activity is not available at the time when the re-
sponse strategy is developed and experiments are conducted,
but at ﬁrst when the system is published or deployed in
real-world. Consequently, it is diﬃcult, if not impossible,
to empirically evaluate an attacker’s capability to impact a
system and, speciﬁcally, the robustness of a system against
such attacks. This even potentiates as anticipating attack
strategies is diﬃcult.

We believe that utilising synthetic data sets is the only
step towards analysing this phenomenon. When designing
a system, we can assume diﬀerent attacker models and can
reason about possible attack vectors corresponding to these
models. Afterwards, we can speciﬁcally craft samples re-
ﬂecting such attacks and inject such samples during training
or evaluation. Injection works both, for manually compiled
real-world samples as well as for data synthesised accord-
ing to speciﬁc statistical models.
In both cases, however,
the data resulting from superposing diﬀerent data sets is
synthetic. Using such synthetic data, we can analyse the
attacker’s capabilities and a system’s robustness under spe-
ciﬁc assumptions in detail and before the system is actually
deployed. Such evaluation is of great practical value as it
allows operators to understand limits and especially facili-
tates selection and implementation of appropriate protection
strategies parallel to system deployment.

6. OPEN RESEARCH QUESTIONS

As argued in Section 5, we are convinced that synthetic
data can be used to solve problems and challenges our com-

munity is currently facing when performing machine learn-
ing based cyber-security experiments. Yet, some problems
related to data synthesis arise as well which need to be solved
ﬁrst. Speciﬁcally, one major problem is related to the scope
and completeness of synthetic data. Our impression is that
a prevalent opinion of our community is that experiments
performed using synthetic data lack relevance. Sommer and
Paxson put this explicitly: Evaluating an anomaly detection
system that strives to ﬁnd novel attacks using only simu-
lated activity will often lack any plausible degree of realism
or relevance [23]. However, we are not aware of any theoret-
ical or formal proof demonstrating limitations of research on
synthetic data. Furthermore, this viewpoint is typically re-
lated to a speciﬁc synthesis process: simulation. And, more
speciﬁcally, to the process of synthesising network traﬃc by
simulation [13, 14]. This, however, is not the only possibility
to data synthesis. An interesting and open research ques-
tion is at what level of detail and representation data has to
be synthesised and, depending on that, what the quality of
synthetic data is. In the following subsections, we examine
these research questions and propose directions for future
work.

6.1 Synthesis Layer

Synthesis of raw data is an approach typically followed by
our community, if data is synthesised at all. However, this
approach is usually not trivial. Considering the example of
network traﬃc synthesis, we not only have to cover many dif-
ferent payload protocols and to correctly implement state-
models of protocols in order to maintain logical order of
events, but also have to account for variability due to diﬀer-
ent operating systems (e.g. varying packet sizes, such as dur-
ing TCP three-way handshake), variability due to diﬀerent
round-trip times, variability due to spontaneous events (e.g.
ﬂash crowds overwhelming speciﬁc services, failures causing
changes in topology, etc.) and many more such eﬀects en-
countered in real-world streams. Accounting for all those
details heavily raises the complexity bar of data synthesis.
What is typically overlooked when following this approach
is the fact that especially machine learning based systems
rarely operate on real-world data streams.
Instead, these
systems employ a feature extraction process in order to crop
reality to that part of information that is believed to be rel-
evant for the problem under investigation. Hence, this pro-
cess leads to parameterised input data whose complexity is
typically much lower when compared to raw data. We are
convinced that not paying attention to this fact unneces-
sarily increases synthesis complexity and especially hinders
utility of synthetic data and experimental control. Hence,
we regard a detailed analysis of alternative synthesis layers
as well as capabilities and limitations of these layers as an
important research question.

Sticking to the example of network traﬃc synthesis, we
can easily identify three distinct synthesis layers we could
operate on. Obviously, as mentioned above, synthesis at
raw packet layer is one approach. Alternatively, data can
be synthesised at Netﬂow [1] layer, providing a much more
condensed view on network activity. Finally, we can syn-
thesise data on the feature space level, independent of the
original input data stream. While it is intuitively obvious
that complexity of synthesis at raw packet layer is higher
than complexity of synthesis at Netﬂow and feature space
layers, it is not intuitively evident how complexity of syn-

42thesis on the latter two layers compares. Nevertheless, we
assume that there’s a natural order among these diﬀerent
layers. Determining this order and understanding the im-
pact, possibilities and limitations of synthesis at one speciﬁc
layer would greatly contribute to our community’s ability to
perform relevant experiments using synthetic data. Further-
more, we see a great contribution in assessing the inherent
complexity of synthesis at any speciﬁc layer as this would
allow us to compare trade-oﬀs between data synthesis and
data collection depending on the problem deﬁnition.
6.2 Synthesis Process

Most commonly, data synthesis is associated with simu-
lation. Indeed, for synthesis of network traﬃc, the DARPA
IDEVAL project [14] was one such prominent approach and
diﬀerent discrete-event [9, 28] and continuous [18] synthesis
frameworks exist, which facilitate research and development.
However, simulating the Internet and crafting real packets
is not trivial.
In fact, the diﬃculty of simulating the In-
ternet has intensively been discussed by Floyd and Paxson
[8]. They conclude that, due to scale, heterogeneity and
dynamics, the results of simulation are questionable. Non-
surprisingly, the DARPA IDEVAL data set has heavily been
criticised [15].

While we acknowledge and understand this diﬃculty, we
would like to point out that simulation is not the end of the
story. Especially, if we decide to synthesise at a much higher
level than on raw packet level (e.g. at ﬂow or feature space
level). An interesting approach not relying on simulation has
been proposed by Sonchack et al. [24] and is termed param-
eterised trace scaling. The idea of this approach is to repeat
and scale traces found in small real-world data sets according
to speciﬁc distributions instead of developing functions that
try to mimic packet content. We would like to propose to
extend this concept towards parameterised data generators,
as this would be in line with our argumentation in Section 5.
Speciﬁcally, our idea of parameterised data generators is not
simply to repeat existing traces at diﬀerent scale, but to en-
tirely generate new traces according to speciﬁc distribution
parameters. Formally, parameterised data generators can be
regarded as multivariate random variables. The data gener-
ators are parameterised in the sense that variables are drawn
according to a speciﬁc probability distribution.

Unfortunately, the authors of [24] do not explain which
distributions to choose and how to assess parameters of the
distribution from real-world data. Both questions, how-
ever, are highly relevant when considering parameterised
data generators. Answers to these questions would give our
community a blueprint for leveraging the sheer amount of
valuable statistical information typically encountered in In-
ternet measurement data repositories. Hence, we regard this
as an important step for future research.
6.3 Quality of Synthetic Data

An open and exceedingly challenging question is how syn-
thetically generated data’s quality can be assessed. We pro-
pose three complementary approaches that might address
this question.

1. Statistical tests may be performed on real-world data
as well as synthetically generated data in order to as-
sess the quality of synthetic data sets. If synthetically
generated data equals real-world captures with regard
to statistical distribution of key aspects of the prob-

lem domain, i.e. features used for learning, probability
is high that (statistical) learners being trained on syn-
thetic data sets work well on real-world data sets as
well. Clearly, in that case statistical equality under-
lines the relevance of synthetic data.

2. Machine learning (ML) techniques, either supervised
or unsupervised, can be used to compare syntheti-
cally generated data sets with real-world captures. In
case of unsupervised learning, algorithms can be ap-
plied to a mixture data set consisting of both, real-
world and synthetically generated traces.
If clusters
found by unsupervised learning approaches eﬀectively
separate real-world captures from synthetically gener-
ated records, chances are high that classiﬁers relying
on this synthetic data set are prone to false alarms.
Inversely, if clusters generated by unsupervised learn-
ing approaches do not naturally separate real-world
records from synthetic records, we can conclude that
synthetic data is structurally similar to real-world data.
The same holds for supervised ML approaches.

3. Existing approaches published in the problem domain
of interest can be used to assess quality of synthetically
generated data. If approaches proposed in literature
are capable of detecting events in synthetic data sets,
i.e. show high true-positive rates, we can conclude that
the data reﬂects current understanding of reality.

Clearly, the disadvantage of the ﬁrst two approaches de-
scribed above is the dependence on real-world data. How-
ever, as argued above, we have observed that several mea-
surement repositories exist that contain valuable statistical
information. Even if not all details of reality (e.g. payload
information, in the case of the CAIDA and PREDICT repos-
itories) are covered in these repositories, they should serve
as starting point for a relevant set of research. The third
approach overcomes this limitation by not relying on the
availability of real-world data when measuring data qual-
ity. However, evaluating data generators this way may be
biased towards a speciﬁc set of approaches showing struc-
tural or methodological similarity, possibly leading to a self-
consistent but closed eco system. Balancing these perspec-
tives, we believe that a combination of all three proposed
approaches reveals potential for good quality assurance and,
thus, we propose to follow all three directions in future work.
However, we would also like to note that comparing as-
pects of synthetic data with aspects of real-world data, as
described above, is not the single required step towards as-
sessing synthetic data’s quality. Instead, we also see the ne-
cessity of work towards a generalised metric for data quality.
Without such metric, comparability of diﬀerent data sets
would be impossible. We expect that such metric cannot
work universally without recognising the scope of a speciﬁc
experiment and the data set’s utility to the experiment. We
believe that any viable approach to assessing quality has to
be feature space speciﬁc and encourage work towards this
direction as well.
6.4 Limitations of Synthetic Data

As mentioned earlier, to the best of our knowledge, no
approach to formally prove general limitations of synthetic
data exists so far. Also, we did not encounter empirical stud-
ies analysing the general limits of synthetic data in cyber-
security research or any other discipline in computer science.

43This is rather astonishing, as on the other hand we recognise
a manifested belief in our community to doubt the relevance
and utility of synthetic data. We back this observation by
our empirical analysis of data sets used in network security
research [2]. This analysis shows that indeed only 9% of
the publications we analysed performed experiments utilis-
ing synthetic data.

From that, we conclude that our community would greatly
beneﬁt from a stronger evaluation of the capabilities and lim-
itations of synthetic data with regard to performing sound
and relevant experiments. This would greatly increase in-
sight into experiments and would allow us to coarsely deﬁne
areas where synthetic data sets indeed can help and where
they cannot.

7. SUMMARY AND CONCLUSION

In this position paper we presented and discussed ﬁve chal-
lenges the cyber-security research community is experienc-
ing when performing machine learning based experiments,
namely:
the inability to perform scientiﬁc sound experi-
ments, the diﬃculty of demonstrating an approach’s adapt-
ability and determining its operational limits, the problem
of false alarms and the threat arising from adversaries. All
challenges can be reduced to problems arising from the gen-
eral lack of publicly available labelled real-world data sets.
Due to this unavailability, our community faces the prob-
lem of not being able to demonstrate and assess fundamen-
tal capabilities and limitations of machine learning based
approaches. This is unfavourable, as being able to would
heavily increase the practical value of our research. Poten-
tially, this would bridge the gap between amount of research
in this area and number of real-world deployments of pro-
posed approaches.

One possibility to overcome this limitation is by utilis-
ing synthetic data sets when performing experiments. Cur-
rently, this seems not to be best practice. Instead, our com-
munity is sceptic about the relevance and utility of synthetic
data. Yet, to the best of our knowledge, possibilities and lim-
itations arising with the use of synthetic data have not been
systematically studied thus far. To stimulate research in
this direction, in this position paper we gave a plea for syn-
thetic data and corresponding synthesis toolchains. Specif-
ically, we identiﬁed key problems that relate to the previ-
ously mentioned ﬁve challenges and elaborated how and why
availability and utility of synthetic data sets can help solv-
ing them. Furthermore, we formulated and discussed four
open research questions in the area of data synthesis and
indicated possible directions for future research.

We are convinced that incorporating synthetic data and
related toolchains in our experiments can greatly contribute
to our community. Especially, we are convinced that utilis-
ing synthetic data can raise credibility in experimental re-
sults and improves repeatability of research. Also, we see
a great advantage for our community in the way incorpo-
rating synthetic data – and especially analysis of the future
research questions we proposed – may aﬀect how we think,
work and perform experiments. Utilising synthetic data re-
quires us to ﬁrst think about relevant and distinguishing
characteristics of the problem under investigation and the
operating environment of the system proposed in order to
be able to synthesise data. Following that step, we can as-
sess characteristics of our approach and ﬁne-tune it. Hence,
utilising synthetic data requires us to initially develop hy-

potheses and react according to experimental results. We
believe that this proceeding even stronger fosters insight and
enforces best scientiﬁc practices.
8. ACKNOWLEDGEMENTS

This work has been funded by the German Federal Min-
istry of Education and Research (BMBF) under grant num-
ber 03FH005PB2 (INSAIN) and has additionally been sup-
ported by CASED. The authors would like to thank Steﬀen
Lange for his comments on an initial version of this paper.
9. REFERENCES
[1] Abt, S., and Baier, H. Towards eﬃcient and

privacy-preserving network-based botnet detection
using netﬂow data. In Proceedings of International
Network Conference 2012 (INC2012) (2012).

[2] Abt, S., and Baier, H. Are we missing labels? A
study of the availability of ground-truth in network
security research. In Proceedings of the 3rd Workshop
on Building Analysis Datasets and Gathering
Experience Returns for Security (BADGERS’14)
(September 2014), IEEE.

[3] Axelsson, S. The base-rate fallacy and the diﬃculty
of intrusion detection. ACM Trans. Inf. Syst. Secur. 3,
3 (Aug. 2000), 186–205.

[4] Barreno, M., Bartlett, P. L., Chi, F. J.,

Joseph, A. D., Nelson, B., Rubinstein, B. I.,
Saini, U., and Tygar, J. D. Open problems in the
security of learning. In Proceedings of the 1st ACM
Workshop on Workshop on AISec (New York, NY,
USA, 2008), AISec ’08, ACM, pp. 19–26.

[5] Barreno, M., Nelson, B., Sears, R., Joseph,

A. D., and Tygar, J. D. Can machine learning be
secure? In Proceedings of the 2006 ACM Symposium
on Information, Computer and Communications
Security (New York, NY, USA, 2006), ASIACCS ’06,
ACM, pp. 16–25.

[6] Bshouty, N. H., Eiron, N., and Kushilevitz, E.
Pac learning with nasty noise. Theoretical Computer
Science 288, 2 (2002), 255–275.

[7] Denning, D. E. An intrusion-detection model.
Software Engineering, IEEE Transactions on, 2
(1987), 222–232.

[8] Floyd, S., and Paxson, V. Diﬃculties in simulating
the internet. Networking, IEEE/ACM Transactions on
9, 4 (Aug 2001), 392–403.

[9] Henderson, T. R., Lacage, M., Riley, G. F.,

Dowell, C., and Kopena, J. Network simulations
with the ns-3 simulator. SIGCOMM demonstration
(2008).

[10] Huang, L., Joseph, A. D., Nelson, B.,

Rubinstein, B. I., and Tygar, J. D. Adversarial
machine learning. In Proceedings of the 4th ACM
Workshop on Security and Artiﬁcial Intelligence (New
York, NY, USA, 2011), AISec ’11, ACM, pp. 43–58.

[11] Jabbari, S., Holte, R. C., and Zilles, S.

Pac-learning with general class noise models. In KI
2012: Advances in Artiﬁcial Intelligence. Springer,
2012, pp. 73–84.

[12] Kearns, M., and Li, M. Learning in the presence of

malicious errors. SIAM Journal on Computing 22, 4
(1993), 807–837.

44[13] Lindauer, B., Glasser, J., Rosen, M., Wallnau,

K., and ExactData, L. Generating test data for
insider threat detectors. Journal of Wireless Mobile
Networks, Ubiquitous Computing, and Dependable
Applications (JoWUA) 5, 2 (2014), 80–94.

[14] Lippmann, R., Haines, J. W., Fried, D. J.,

Korba, J., and Das, K. The 1999 darpa oﬀ-line
intrusion detection evaluation. Computer networks 34,
4 (2000), 579–595.

[15] McHugh, J. Testing intrusion detection systems: A

critique of the 1998 and 1999 darpa intrusion
detection system evaluations as performed by lincoln
laboratory. ACM Trans. Inf. Syst. Secur. 3, 4 (Nov.
2000), 262–294.

[16] Mitchell, T. Machine Learning. McGraw Hill, 1997.
[17] Morel, B. Artiﬁcial intelligence and the future of

cybersecurity. In Proceedings of the 4th ACM
Workshop on Security and Artiﬁcial Intelligence (New
York, NY, USA, 2011), AISec ’11, ACM, pp. 93–98.
[18] Neri, G., Morling, R., Cain, G., Faldella, E.,

Longhi-Gelati, M., Salmon-Cinotti, T., and
Natali, P. Mininet: A local area network for
real-time instrumentation applications. Computer
Networks (1976) 8, 2 (1984), 107–131.

[19] Ringberg, H., Roughan, M., and Rexford, J.

The need for simulation in evaluating anomaly
detectors. SIGCOMM Comput. Commun. Rev. 38, 1
(Jan. 2008), 55–59.

[20] Rossow, C., Dietrich, C. J., Bos, H., Cavallaro,

L., Van Steen, M., Freiling, F. C., and
Pohlmann, N. Sandnet: Network traﬃc analysis of
malicious software. In Proceedings of the First
Workshop on Building Analysis Datasets and
Gathering Experience Returns for Security (2011),
ACM, pp. 78–88.

[21] Schlimmer, J. C., and Granger Jr, R. H.
Incremental learning from noisy data. Machine
learning 1, 3 (1986), 317–354.

[22] Sloan, R. H. Four types of noise in data for pac

learning. Information Processing Letters 54, 3 (1995),
157–162.

[23] Sommer, R., and Paxson, V. Outside the closed

world: On using machine learning for network
intrusion detection. In Security and Privacy (SP),
2010 IEEE Symposium on (2010), IEEE, pp. 305–316.

[24] Sonchack, J., Aviv, A. J., and Smith, J. M.

Bridging the data gap: Data related challenges in
evaluating large scale collaborative security systems.
In Proceedings of the 6th Workshop on Cyber Security
Experimentation and Test (CSET’13) (2013),
USENIX.

[25] Song, J., Takakura, H., Okabe, Y., Eto, M.,
Inoue, D., and Nakao, K. Statistical analysis of
honeypot data and building of kyoto 2006+ dataset
for nids evaluation. In Proceedings of the First
Workshop on Building Analysis Datasets and
Gathering Experience Returns for Security (New York,
NY, USA, 2011), BADGERS ’11, ACM, pp. 29–36.

[26] Symons, C. T., and Beaver, J. M. Nonparametric

semi-supervised learning for network intrusion
detection: Combining performance improvements with
realistic in-situ training. In Proceedings of the 5th
ACM Workshop on Security and Artiﬁcial Intelligence
(New York, NY, USA, 2012), AISec ’12, ACM,
pp. 49–58.

[27] Valiant, L. G. A theory of the learnable.
Communications of the ACM 27, 11 (1984),
1134–1142.

[28] Varga, A., et al. The omnet++ discrete event
simulation system. In Proceedings of the European
Simulation Multiconference (ESM’2001) (2001), vol. 9,
sn, p. 185.

[29] Wurzinger, P., Bilge, L., Holz, T., Goebel, J.,

Kruegel, C., and Kirda, E. Automatically
generating models for botnet detection. In Computer
Security–ESORICS 2009. Springer, 2009, pp. 232–249.

[30] Xu, J., Fan, J., Ammar, M. H., and Moon, S. B.

Preﬁx-preserving ip address anonymization:
Measurement-based security evaluation and a new
cryptography-based scheme. In Network Protocols,
2002. Proceedings. 10th IEEE International
Conference on (2002), IEEE, pp. 280–289.

45