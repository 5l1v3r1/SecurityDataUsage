On the Practicality of Integrity Attacks on Document-Level

Sentiment Analysis

Andrew Newell, Rahul Potharaju, Luojie Xiang, and Cristina Nita-Rotaru

Dept. of Computer Science, Purdue University

West Lafayette, IN, USA

newella@cs.purdue.edu, rpothara@cs.purdue.edu, xiang7@cs.purdue.edu,

crisn@cs.purdue.edu

ABSTRACT
Sentiment analysis plays an important role in the way com-
panies, organizations, or political campaigns are run, mak-
ing it an attractive target for attacks. In integrity attacks
an attacker inﬂuences the data used to train the sentiment
analysis classiﬁcation model in order to decrease its accu-
racy. Previous work did not consider practical constraints
dictated by the characteristics of data generated by a sen-
timent analysis application and relied on synthetic or pre-
processed datasets inspired by spam, intrusion detection, or
handwritten digit recognition. We identify and demonstrate
integrity attacks against document-level sentiment analysis
that take into account such practical constraints. Our at-
tacks, while inspired by existing work, require novel improve-
ments to function in a realistic environment where a victim
performs typical steps such as data cleaning, labeling, and
feature extraction prior to training the classiﬁcation model.
We demonstrate the eﬀectiveness of the attacks on three
datasets – two Twitter datasets and an Android dataset.

Categories and Subject Descriptors
I.5.2 [Computer Methodologies]: Pattern Recognition—
Design Methodology

Keywords
Machine Learning; Security; Sentiment Analysis

1.

INTRODUCTION

Sentiment analysis has emerged as one of the most popular
data analytics applications driving targeted advertising [1],
product recommendations [2], sentiment extraction [3–5],
and opinion mining [6]. According to a recent article in Com-
munications of the ACM [7] “over 7,000 articles have been
written on the topic, hundreds of startups are developing
sentiment analysis solutions and major statistical packages
such as SAS and SPSS include dedicated sentiment anal-
ysis modules”. The most common form is document-level

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
AISec’14, November 7, 2014, Scottsdale, Arizona, USA.
Copyright 2014 ACM 978-1-4503-2953-1/14/11 ...$15.00.
http://dx.doi.org/10.1145/2666652.2666661.

TRAINING PHASE

CLASSIFICATION PHASE

Training Data

@John: iPhone is 
so cooooool!

Data Cleaning

USER iPhone is 
so cool

Labeling
<Text: USER iPhone is 
so cool, Label: POSITIVE>
Feature Extraction
<Features: 10001101100, 
Label: POSITIVE>

Model Training

Testing Data

@Jill: I soooo looooove the 
iPhone... Its very cool!
Data Cleaning

USER I so love the 
iPhone Its very cool

Feature Extraction

<Features: 100011111111100,
Label: ??>
Classiﬁcation Model

Label: POSITIVE>
Figure 1: A typical learning system

<Features: 100011111111100, 

sentiment analysis which assumes that the entire document
conveys a particular sentiment about one main object.

At the core of a sentiment analysis application lies a learn-
ing system as the one shown in Fig. 1. The main goal of the
system is to learn a classiﬁcation model based on a set of
documents referred to as training dataset. Prior to training
the classiﬁer each document in the training dataset must
be: (1) labeled with its true class, and (2) mapped to a nu-
merical vector through feature extraction [8]. By training on
the feature vectors, the learning system associates each class
with particular features. A higher accuracy classiﬁer can be
obtained with less training data by removing features of the
data that are unimportant for classiﬁcation, also known as
data cleaning. The characteristics of the training dataset
as well as the data cleaning, labeling and feature extraction
procedures inﬂuence the accuracy of the obtained classiﬁca-
tion model.

Numerous techniques have been designed to deal with the
noise and errors present in benign environments for senti-
ment analysis. However, the important role played by sen-
timent analysis systems provides incentives for attackers to
inﬂuence them. For example, by inﬂuencing the sentiment
analysis an adversary can promote low quality or untrust-
worthy content for online advertising or can promote his
products/produce incorrect recommendations for competing
products for review applications. As learning systems were
not designed to work in adversarial environments, an at-
tacker can try to compromise the conﬁdentiality or integrity
of a victim’s learning system. Most of the work on attacks

83against machine learning focused on integrity attacks as they
provide more incentives for the attackers. Conﬁdentiality at-
tacks enable an attacker to learn the classiﬁcation model by
querying the classiﬁer with crafted datapoints [9, 10].
In-
tegrity attacks allow an attacker to decrease the accuracy
of the classiﬁcation model by modifying or inserting data at
some step prior to model training. Note that integrity at-
tacks are diﬀerent from evasion attacks. An evasion attack
knows a learned model and aims to create datapoints which
are classiﬁed incorrectly by that model.
in-
tegrity attacks insert malicious datapoints into the training
data inﬂuencing the classiﬁer to have poor accuracy.

In contrast.

Sentiment analysis systems are extremely vulnerable to
integrity attacks as accurate models rely on training data
from public sources. These public sources (e.g., Twitter) of-
fer a high volume of rich information. However, as the source
of training data is public, an attacker may create malicious
data (e.g., Tweets) that forces the learning system to learn
incorrect patterns. Traditional security techniques such as
cryptographic data integrity cannot prevent such an attack
since an attacker is not modifying an existing training set.
The attacker is tarnishing the public data before it is col-
lected by a learning system for training. A learning system
cannot easily distinguish the data created by an attacker.

Previous work on integrity attacks focused on either syn-
thetic datasets that do not capture the characteristics of
real social media data, or on diﬀerent applications such as
spam, intrusion-detection, or hand-written digit recognition,
essentially diﬀerent in the characteristics of the dataset they
operate on, the cleaning, labeling, and feature vector extrac-
tion procedures used. Speciﬁcally, earlier work [11–13] fo-
cuses on integrity attacks against synthetic data from ﬁxed
distributions. By relying on the statistical properties of
the considered distributions for the datasets, such works
identify theoretical bounds on how much of the most er-
roneous data is necessary to harm classiﬁer accuracy. How-
ever, such datasets are not from real-world applications and
thus it is not clear if the identiﬁed attacks will be eﬀective
in practice against real sentiment analysis datasets. Recent
works [14–19] consider real-world datasets with attacks that
alter training data for spam, intrusion detection, and hand-
written digit recognition. None of these applications gen-
erate data that match the characteristics of data generated
by social media based sentiment analysis applications, thus
their techniques can not be applied for sentiment analysis.
All of the existing work on integrity attacks against learn-
ing systems assumes modiﬁcation or insertion of data at the
level of numerical vectors, i.e. altering the feature vector
(see Fig. 1). However, in a realistic scenario, an attacker
can inﬂuence the data collected for training and not the la-
beled numerical representation of this data which is a result
of extraction, cleaning, and labeling. As a result, the ad-
versary may not know the exact details of each step taken
by a learning system, and thus, cannot understand the ex-
act numerical vectors that result from training data or how
constructed malicious training data is converted into nu-
merical vectors. Moreover, the space of possible numerical
values becomes limited since certain values in the numeri-
cal vectors cannot take on arbitrary real numbers, and thus
attacks that assume arbitrary values will not be eﬀective
against such systems. For example, many sentiment analy-
sis systems use bag-of-words as a feature vector mechanism,

where features are booleans (0 or 1) indicating the existence
of particular words.

In this paper, we study integrity attacks against sentiment
analysis. We apply attacks that are motivated by existing
work on adversarial machine learning, but we alter these at-
tacks to consider the real-world scenario in which such an
application would be deployed. Speciﬁcally, we assume that
the victim implements a realistic learning system with all the
steps described in Fig. 1, consider realistic adversarial mod-
els and construct attacks that meet the constraints resulted
from the data processing steps. Additionally, we consider a
weaker but more realistic attacker that has partial knowl-
edge of the victim’s learning system and study the diﬀer-
ences in crafted attack datapoints as well as their impact on
a learning system. We provide a mathematical formulation
to ﬁnd impactful attack data points. Our attack formulation
ﬁnds a feasible data point optimizing an objective which is
the opposite objective of the correct model. For instance, an
attacker inserts positively labeled datapoints with words of
heavy negative sentiment.To our knowledge, our work is the
ﬁrst to demonstrate adversarial machine learning attacks in
such a practical scenario. Recent work has also considered
practical scenarios, but focused on evasion attacks [20, 21].

Our evaluations consider three realistic datasets (two Twit-
ter datasets and an Android dataset), two well-known ma-
chine learning algorithms (SVM and Naive Bayes), and the
bagging ensemble technique. We highlight the following
ﬁndings from our evaluation:
• Our attack with the strongest adversarial assumptions
is able to degrade accuracy of a learning system from
86% to 50% by adding fewer than 2% maliciously crafted
datapoints to a training dataset.

• We gained insights into attack eﬀectiveness against diﬀer-
ent aspects of a learning system: more complex learning
algorithms are more susceptible to attack, larger training
datasets require more attack points to corrupt accuracy,
using data that has more relaxed constraints is more sus-
ceptible to attack, and the learning algorithm ensemble
technique of bagging slightly reduces attack eﬀectiveness.
• We also assume scenarios where an attacker may have
partial knowledge about certain preprocessing steps of
a victim’s classiﬁer. Considering such a scenario brings
insight into the value for a victim to keep information of
their system secret from possible attackers. The attack
eﬀectiveness varied from degrading accuracy from 86% to
50% (same as full knowledge) to degrading only to 68%.
Thus, attacks that craft datapoints can still be eﬀective
even in scenarios of partial knowledge.

We organize our paper as follows. Section 2 describes the
general machine learning model we aim to attack. Section 3
describes attacks against machine learning models. Section
4 describes our experimental methodology for evaluating the
performance of these attacks and Section 5 presents the re-
sults of our evaluation. Section 6 presents results when the
attacker has partial information about the steps used by the
learning system. Section 7 lists related work, and Section 8
concludes our work.

842. MACHINE LEARNING FOR

SENTIMENT ANALYSIS

We describe the learning system for document-level sen-
timent analysis of social media assumed in this work. We
consider a system which could easily and practically be de-
ployed today as a sentiment analysis system.
2.1 Overview

algorithm performing the classifying task.

The design of a machine learning system involves making
several design choices taking into account the nature of the
data to be classiﬁed and the classiﬁcation task:
• Select the training data used to train the machine learning
• Select the data cleaning mechanisms which remove useless
• Select a labeling technique for the training data to indicate
• Select the feature vector that transforms each document
in the training data into a numerical vector for the ma-
chine learning algorithm.
• Select a classiﬁcation algorithm that learns a function

what is the true class for each document.

characteristics for classiﬁcation.

from the processed numerical vectors.

Training and testing data: Learning algorithms are
sensitive to the size of the training set. A training set (1)
must be suﬃciently large to represent a majority of the hy-
pothesis space, and (2) must have suﬃcient datapoints from
each class as an imbalanced training set can pose diﬃcul-
ties in learning. In addition, the training and testing data
should possess the property of data stationarity i.e., they
must be drawn from the same distribution to ensure that
correlations found in the former are also true in the latter.
Preprocessing: Raw data often contains elements that
are diﬃcult to process such as special characters, web links,
or text in diﬀerent languages. Many algorithms are sensitive
to such elements. Thus, data used for training needs to be
preprocessed to ensure the best representative datapoints
are used. For example, textual data is ﬁltered by language
to simplify learning language-speciﬁc textual patterns.

Labeling: The accuracy of labeling the training set has
a direct impact on the overall accuracy of the learned hy-
pothesis function f . To label a suﬃciently large training set,
noisy-labeling techniques can be employed for automatic la-
beling as long as the noise in the labeling is low.

Feature extraction: The core of a learning model is the
extraction of features [8] from a preprocessed data instance
that is representative for the data. Each data instance is
mapped to a vector of numerical values which is a datapoint.
The goal is to capture the important features of a data in-
stance as a vector of numbers such that a learning algorithm
can correctly associate datapoints with their corresponding
class. For example, textual data can be transformed via a
bag-of-words approach which results in a vector of 0’s and 1’s
indicating which words are absent or present respectively.

Learning algorithm: Several learning approaches exist,
each algorithm determines the space of possible discriminant
functions h that can be learned. Learning algorithms diﬀer
in how they correlate example features with class labels, so
the best algorithm may depend on the dataset.

Below we describe the design for the sentiment analy-
sis system considered in this work. The design is largely
based on the one used in [6], and is using simple techniques

that were shown to be eﬀective for document-level sentiment
analysis.
2.2 Data Cleaning

While, there are numerous data cleaning steps, we selected
the ones we describe below as they have been shown previ-
ously to work well in previous work [22].

Data cleaning removes features of the data that are unim-
portant for classiﬁcation allowing higher accuracy classiﬁers
to be created with less training data. As an example, con-
sider the word “happy” in the training data. Such a word
is typically a great indication of positive sentiment, and it
is great for any machine learning algorithm to ﬁnd such a
correlation and use this correlation in a classiﬁer. However,
this word may show up in the training data as “Happy”,
“HAPPY”, or “happy”. Without data cleaning, the training
data must have enough occurrences of each of these forms to
learn such a correlation to understand each form of “happy”
in a testing set can indicate positive sentiment. With data
cleaning, such a correlation is learned much easier as each
form of “happy” is mapped to the same feature to help build
evidence of this correlation. Similar logic can be applied to
the other types of data cleaning which a practical learning
system for sentiment analysis may employ.

URL removal. Each URL is replaced with “url”. A spe-
ciﬁc URL may indicate a negative or positive sentiment due
to the time in which the training data was created, but such
sentiment of a URL may not always be true for futuristic
testing sets.

Username removal. Each username, (e.g., “@mrsmith”)
is replaced with “username”. Usernames are removed for the
same reasoning that URLs are removed.

Repeated letter removal. E.g., “happpppppy” is re-
placed with “happy”. Limiting the number of repeated let-
ters will help map the misspellings (with repeated letters)
to the actual English.

Case normalization. Case normalization is performed
for similar reasoning as repeated letter removal since it en-
sures that two diﬀerent types of case usage are mapped to
the same word. E.g., “Happy ToWn” is replaced with “happy
town”.

English text. The testing sets we use are in English
text, so we did not want to consider other languages. We
employed standard language detection tools to remove non-
English text from our training data.
2.3 Labeling

The labeling process assigns a class to each training dat-
apoint. We consider binary sentiment classiﬁcation, i.e. the
class for each datapoint is either positive or negative (senti-
ment). As manually labeling suﬃcient datapoints by a hu-
man experts is tedious and error-prone, we use automated
labeling. Automated labeling uses the metadata or speciﬁc
features of the data to provide a label.

Twitter dataset: For our Twitter datasets, similarly
to [6] we leverage datapoints that have emoticons and use
the sentiment of the emoticon to label, i.e., presence of a
happy smiley indicates a positive tweet and vice versa. We
strip all emoticons from the input tweets because otherwise,
their presence may negatively impact the accuracies of our
classiﬁers. This step causes the classiﬁer to learn from the
other features present in the tweet. Therefore, even if the
test data contains an emoticon, it does not inﬂuence the

85classiﬁer because emoticon, as a feature was removed from
the training data.

Android dataset: For our Android dataset, we leverage
the rating supplied by the user to indicate the sentiment of
the comment. We take comments with a rating score higher
than 3 as having a positive sentiment and less than 3 as
having a negative sentiment.
2.4 Feature Extraction

Feature extraction transforms a document into a numer-
ical vector that is convenient for a learning system. We
assume our system uses a common approach denoted bag-
of-words wherein a mapping from every word in the train-
ing data to an element of a numerical vector is constructed.
Note that the number of elements in a numerical vector can
get quite large (≈50K in our learning system). For each dat-
apoint, a numerical vector is created where an element has a
value of 1 if the corresponding word exists in the datapoint
while it has a value of 0 otherwise. Thus, the numerical
vectors have element values of 0s and 1s while the vector is
typically sparse (few 1s and many 0s).
2.5 Model Training

We consider model training for binary classiﬁcation, i.e.,
a classiﬁer h is learned which maps datapoints into two
classes. Speciﬁcally, given a training set of datapoints of
the form {(x1, y1), ..., (xn, yn)}, the learning algorithm out-
puts a function y = h(x). Each vector xi = (cid:3)x1, x2, ..., xn(cid:4)
consists of values called features 1 and each value yi is a dis-
crete value 1, ..., M called a class label. We focus on the case
where M = 2 which represents a binary classiﬁer.

The goal of a learning system is to achieve a high accu-
racy. For a learned classiﬁer h we deﬁne accuracy as the
proportion of datapoints such that h correctly maps a dat-
apoint to its correct class. Speciﬁcally, given a testing data
{(x1, y1), ..., (xt, yt)}, accuracy is the proportion of these
points where h(xi) = yi. Below we describe the classiﬁers
we consider in this work.
Naives Bayes: The general Bayes classiﬁer h(x), uses the
class posterior probabilities given a datapoint, i.e., f∗
i (x) =
P (C = i|X = x). Applying Bayes rule gives P (C = i|X =
, where P (X = x) is identical for
x) =
all classes, and therefore can be ignored. This gives us the
following Bayes classiﬁer:

P (X=x|C=i)P (C=i)

P (X=x)

h(x) = argmaxiP (X = x|C = i)P (C = i)

(1)

ﬁnds the maximum a posteriori probability hypothesis given
a datapoint x. Because, direct estimation of P (X = x|C =
i) from a training set is diﬃcult when the feature space is
high-dimensional, approximations are commonly used. When
features are assumed to be independent, we refer to the re-
sulting classiﬁer as the Naive Bayes classiﬁer which substi-
tutes into Equation 1 the following:

P (X = x|C = i) =

P (Xj = xj|C = i)

(2)

j=1

Support Vector Machines: We describe a binary Support
Vector Machine (SVM). The goal of an SVM is to search

1As a comment on our notation throughout this work, xi
refers to the ith datapoint while xi refers the to the ith
feature of a datapoint.

n(cid:2)

a hyperplane in the reproducing kernel hilbert space [23]
that maximizes the margin between the two classes of data-
points with the smallest training error [24]. This problem is
typically formulated as the following quadratic optimization
problem:

minimize

subject to

w + C

T

1
w
2
1 − yi(w

i=1

xi + b) ≤ ξi, ξi ≥ 0

T

(3)

n(cid:3)

ξi

the variables are a weight vector w, a hyperplane oﬀset b,
and slack variables ξi. The separating hyperplane is orthog-
onal to w and b oﬀset from the origin. There is a single
slack variable ξi for each training datapoint (xi, yi). A slack
variable is 0 if the training datapoint is outside the mar-
gin on the correct side of the hyperplane. Alternatively, a
slack variable’s value increases based on the degree to which
the point is misclassiﬁed, that is, the further the point is
away from the hyperplane. Thus, loss is captured by a sum-
mation over these slack variables, C
i=1 ξi, where C is a
constant that controls loss versus generality of the model.
The regularizer term to ensure generality of the model is
to maximize the margin of the hyperplane. The margin is
wT w , so minimizing the inverse in the objective is equivalent
to maximizing the margin.

(cid:4)n

2

The classiﬁer for SVM is the following:

(cid:5)

h(x) =

+1 wT x + b > 0
−1
else

(4)

where w and b are learned from the optimization problem
above over the training data.
Ensemble: For NB, SVM, or other model training algo-
rithms, additional ensemble techniques can be used to im-
prove accuracy. An ensemble aggregates the results of mul-
tiple classiﬁers from diﬀerent model trainings to increase
accuracy. The ensemble of multiple classiﬁers is a classiﬁer
itself. In our work, we study an ensemble technique called
bagging which improves the generality of the ﬁnal, learned
classiﬁer. Given a set of training data S, we create k random
subsets of this data s1, s2, ..., sk, s.t., ∀i, si ⊂ S. Then, we
can train k classiﬁers h1(x), h2(x), ..., hk(x), and the overall
ensemble classiﬁer is a majority vote of these k classiﬁers.
Such a technique is used in practice to reduce the eﬀect of
outliers in training data.
3.

INTEGRITY ATTACKS

In this section, we describe the attack model we assume,
the general attacks against machine learning, and concrete
ways to generate such attacks against real-world machine
learning systems for sentiment analysis.
3.1 Attacker Model

The popularity and importance of sentiment analysis pro-
vides incentives for attackers to attempt to inﬂuence it. An
attacker can try to compromise the conﬁdentiality or in-
tegrity of a victim’s learning system. Conﬁdentiality attacks
enable an attacker to learn the classiﬁcation model by query-
ing the classiﬁer with crafted datapoints. Integrity attacks
allow an attacker to decrease the accuracy of the classiﬁ-
cation model by modifying or inserting data at some step
prior to model training. We focus on integrity attacks as
they provide more incentives for the attackers.

For a realistic scenario, we assume an attacker never mod-
iﬁes training data already collected by a learning system.

86Such attacks can be addressed by more traditional security
techniques such as ﬁrewalls, secure operating systems, or
cryptographic signatures.
Instead, we assume an attacker
inﬂuences training data by inserting data into a data set
prior to its collection by a learning system. Such inﬂuence is
possible as data sets are typically generated based on content
generated from online user proﬁles. An attacker can create
numerous proﬁles and generate content from each proﬁle. As
shown in previous work [25] attackers can compromise Twit-
ter accounts and use them for malicious purposes. For exam-
ple, previous work [25] has studied spam based on Twitter
accounts.
In our work, we assume that the compromised
Twitter accounts are used to generate content to inﬂuence
the learning system of a sentiment analysis classiﬁer.

We assume that the attacker transforms the training dataset
by injecting a certain number of maliciously selected or crafted
datapoints. Every data point added for training aﬀects the
model learned, and the maliciously created data points will
strive to degrade the performance of the classiﬁer. Such an
attack subverts data stationarity as the training dataset be-
comes dissimilar to a testing set which the victim aims to
have high accuracy on. Unlike previous work that assumes
that such injection is performed on the pre-processed data
or synthetic datasets, we assume that the injection happens
before data is pre-processed and feature vectors are com-
puted. This model captures realistic scenarios of sentiment
analysis application deployment where the attacker does not
have direct control over the feature vectors. Furthermore,
we also assume scenarios where an attacker may have partial
knowledge about certain preprocessing steps of a victim’s
classiﬁer. Considering such a scenario brings insight into
the value for a victim to keep information of their system
secret from possible attackers.

Note that integrity attacks are diﬀerent from evasion at-
tacks. An evasion attack knows a learned model and aims
to create datapoints which are classiﬁed incorrectly by that
model. In contrast.
integrity attacks insert malicious dat-
apoints into the training data inﬂuencing the classiﬁer to
have poor accuracy.

We assume that the adversary has knowledge of various
components of a victim’s classiﬁer and inserts datapoints
into the training set based on this knowledge. Components
that can be known by an adversary include: training data
set, data cleaning, labeling, feature extraction, and learn-
ing algorithm. We deﬁne the following types of adversaries,
ordered from the weakest to the strongest based on their
capabilities.

Adversary-0. This is an adversary that can insert a lim-
ited amount of datapoints in the training set. Such data-
points are subject to domain-restrictions (e.g., respect the
character limit). He also knows the labels associated with
the training data used by the victim but cannot modify
them.

Adversary-I. This is an adversary that has all the capa-
bilities of Adversary-0 and in addition controls the labeling
of datapoints inserted into the training set. For example,
the attacker knows the noisy-labeling technique.

Adversary-II. This is an adversary that has all the capa-
bilities of Adversary-I and in addition knows the algorithms
of the learning system, knows all training data, and can alter
the contents of datapoints inserted into the training set.

Adversary-IIP. This is an adversary that has all the ca-
pabilities of Adversary-II except he only has a partial knowl-

edge of the data cleaning (for instance, has knowledge of user
name removal but not repeated letter removal) performed by
the victim’s learning system.

Adversary-III. This is an adversary that has all the ca-
pabilities of Adversary-II and in addition knows a limited
testing set which the adversary aims to inﬂuence classiﬁca-
tion on. These assumptions are the greatest of any adver-
sary.

Adversary-IIIP. This is an adversary that has all the
capabilities of Adversary-III except he only has a partial
knowledge of the data cleaning performed by the victim’s
learning system.

3.2 Attack Description

All the attacks have the ultimate goal to inject datapoints
in order to inﬂuence the accuracy of the classiﬁer. The dif-
ference between the attacks is how datapoints are selected or
crafted (a summary is shown in Table 1). For a full attack,
these steps would be repeated several times to increase the
inﬂuence the attack has on the victim’s model.

We describe four attacks conducted by attackers with in-
creased strength: REPEAT, LABEL-FLIP, POISON-1, and
POISON-2. The ﬁrst two attacks consist of inserting an
existing or modiﬁed datapoint, while the next two attacks
consist of inserted newly crafted datapoints.
REPEAT: This is the simplest attack created by an at-
tacker in class Adversary-0. The attack works as follows:
the attacker selects a random datapoint of a preselected
class from the training data, creates a copy of the data-
point, and then inserts the copy of datapoint into the train-
ing set. The attacker exploits a model’s sensitivity to an
imbalanced training set. The attacker only needs access to
a set of datapoints from a given class to replicate and insert
many datapoints into the training set.
LABEL-FLIP: This attack is conducted by an adversary
of type Adversary-I. The attack selects a random datapoint,
creates a copy of the datapoint, changes the label of the
copied datapoint, and then inserts the copy with its changed
label into the training set. By switching the label, the model
uses mislabeled points for training and with enough misla-
beled points starts to incorrectly associate certain features
with the classes they should actually represent.
POISON-1: This attack requires signiﬁcantly more infor-
mation about the victim’s learning system and can be con-
ducted by an adversary of type Adversary-II. A datapoint is
crafted by selecting a set of features and class label which are
the least ﬁtting to the victim’s learned model. The creation
requires the learned model which is a result of knowing the
training dataset and learning system of the victim which
is why an Adversary-II is required. Additionally, the cre-
ation requires the solving of an optimization problem which
depends on the feature extraction and machine learning al-
gorithm which we detail in Section 3.3. When a datapoint
is crafted, it is added back into the training set to deter-
mine which features to use in the next crafted datapoint.
This attack can also be performed by an adversary of type
Adversary-IIP, the only diﬀerence is that the attack may
have less impact due to partial knowledge of the victim’s
learning system.
POISON-2: This attack is similar to POISON-1 while re-
quiring additional knowledge of a testing set and is con-
ducted by an attacker from class Adversary-III. The data-
points are crafted like in POISON-1, but the features are

87Table 1: Summary of various integrity attacks on learning systems

Name
REPEAT
LABEL-FLIP
POISON-1
POISON-2

Exploit
Imbalance
Label noise
Outliers
Outliers

Data insertion technique (Target: Victim’s training data)
Replicate datapoint
Change label class
Craft datapoint that least ﬁts the victim’s learning model
Craft datapoint that least ﬁts the victim’s learning model based on victim’s
testing data

Adversary
Adversary-0
Adversary-I
Adversary-II/IIP
Adversary-III/IIIP

ATTACKER'S 

LEARNING SYSTEM

Training Data

@John: iPhone is 
so cooooool!

Data Cleaning

USER iPhone is 
so cool

Labeling
<Text: USER iPhone is 
so cool, Label: POSITIVE>
Feature Extraction
<Features: 10001101100, 
Label: POSITIVE>

Model Training

Victim's Learning System

Attack Points
Insert as 
training data

POISON-1

Classiﬁcation Model

Figure 2: Poisoning attack scenario against learning
systems.

weighted by their frequency in the testing set. The weight-
ing ensures features are preferred if they occur more often
in a testing set. Focusing on those features in the testing set
allows higher accuracy degradation with fewer crafted dat-
apoints. The crafted datapoints are still outliers like with
POISON-1, but their features are focused on the testing set
features. This attack can also be performed by an adver-
sary of type Adversary-IIIP, the only diﬀerence is that the
attack may have less impact due to partial knowledge of the
victim’s learning system.
3.3 Poisoning Real Learning Systems

The POISON-1 and POISON-2 attacks craft points which
oppose the model learned by the victim’s learning system
(see Figure 2). We deﬁne optimization formulations which
capture how to craft points against the SVM and NB learn-
ing systems. Our formulations consider the bag-of-words
feature extraction which is typical for sentiment analysis and
other machine learning techniques using textual data.

Previous research eﬀorts [10, 14, 26] assume that the fea-
tures present in the samples are determined randomly or
ignore their distributions altogether. However, this is rarely
the case in real-world datasets as there are several practical
application-speciﬁc constraints such as bag-of-words feature
extraction and character limit that prevent an attacker from
launching attacks with arbitrary vectors. For instance, con-
sider the case of sentiment analysis using Twitter — an ad-
versary must use 0,1 element values and obey the constraint
of 140 characters per datapoint when attempting to poison
the data stream. This scenario constrains an attacker to
craft only sparse binary vectors for their attack. Below, we
propose techniques with these constraints to attack an SVM
and an NB classiﬁer. These same techniques are used for
scenarios of partial knowledge.

SVM poisoning with sparse binary vectors. We de-
scribe techniques for POISON-1 and POISON-2 against an
SVM classiﬁer on textual sentiment analysis data. Equa-
tion 3 describes the optimization problem that minimizes
loss while maximizing the margin. When a training data-
point (xi, yi) is ﬁt well by the model, then the loss for that
datapoint is captured by the variable ξi, which is equivalent
to 1 − yi(wT x + b). In contrary, the goal of the attacker is
to create a training datapoint (x, y) which maximizes loss of
the SVM while obeying the constraints for a sparse binary
vector:

1 − y(w

T

x + b)
maximize
subject to xi ∈ {0, 1}, ∀i

x − 1 ≤ L, y ∈ {−1, +1}

T
c

(5)

c is a vector containing the word length (including a space
separating character) of each feature, and L is the maxi-
mum number of characters in the document. In the Twitter
dataset, the minimal size to append an emoticon is 3 char-
acters (2 characters for “:)” or “:(” along with a space), and
the maximal Tweet size is 140 characters, so L = 137. In
the Android comments dataset, the maximal size for a com-
ment is 1200 characters, so L = 1200. Solving Equation 5
involves solving two knapsack problems: one sets y = −1
and the other sets y = +1. The largest value from these two
knapsacks is chosen as the ﬁnal result. Although optimally
solving the general knapsack is NP-Hard there is a (1+)-
approximation algorithm [27] which we utilize that runs in
time polynomial in 1
 .

POISON-2 improves its impact over POISON-1 as we
leverage a known test set. An additional step is included
which adjusts the likelihood to select each feature based on
the rate of occurrence of that feature in the test set. Let t
be a vector with an element ti for each feature in the testing
set. The ti value is the sum of class label values of those
testing set points that contain the ith feature. A value ti
has a large positive value if it indicates strong sentiment in
the testing set and vice versa for negative values. Let the
operator (cid:10) be a pairwise multiplication of vector elements
such that w(cid:10)t = (cid:3)w1∗t1, w2∗t2, ...(cid:4). The POISON-2 attack
is then performed as follows:

1 − y((w (cid:10) t)
maximize
subject to xi ∈ {0, 1}, ∀i

T

x + b)

x − 1 ≤ L, y ∈ {−1, +1}

T
c

(6)

NB poisoning with sparse binary vectors. We now
show a POISON-12 attack on NB. Equation 1 shows the
classiﬁcation for NB which selects the class label i which
maximizes posteriori probability P (X = x|C = i). We can
ﬁnd a datapoint which opposes the model by ﬁnding a data-
point which maximizes the ratio of the correct and incorrect
posteriori probabilities:
2There is a POISON-2 for NB that is similar to the version
for SVM, but we omit those details in this work.

88(cid:6)

P (C = −1|X = x)
P (C = +1|X = x)

maximize
subject to xi ∈ {0, 1}, ∀i

(cid:7)y

x − 1 ≤ L, y ∈ {−1, +1}

T

c

(7)

Training dataset

Training dataset size
Machine learning al-
gorithm
Ensemble

Table 2: Experimental Setup

Default
value

Egyptian

30K
SVM

Experimented val-
ues

Election,
(see

Android

Egyptian,
and
Section 4)
15K, 30K, and 45K
SVM and NB

No ensemble

No ensemble and bag-
ging ensemble

is labeled by their comment ratings which range from 1-5,
and we let ratings of 1 and 2 represent negative sentiment
while 4 and 5 represent positive sentiment (we do not use
comments with ratings of 3).

Selecting testing datasets. For the testing sets, we
manually label a set of Tweets and Android comments. We
use the Tweets testing set to measure accuracy of a learn-
ing system trained with Tweets, and the same for Android
comments. Each testing set consists of 200 positive and 200
negative datapoints.

Selecting size of the training datasets. To under-
stand how many datapoints were suﬃcient to train an ac-
curate model we varied the size of the training dataset and
observed accuracy. For Twitter, we found that a training
dataset of 12K datapoints resulted in 83% accuracy while
growing slightly to 86% with 30K datapoints, and the ac-
curacy did not increase by adding more datapoints. We
observed similar accuracy from the Android dataset as well.
So, we use 30K datapoints (15K positive and 15K negative)
as our default training dataset size. For the Election dataset,
due to a limited number of tweets with emoticons, we use
just 12K datapoints (6K positive and 6K negative). For
the Android dataset, we chose to remain consistent with the
Egyptian dataset by randomly selecting 30K positive and
30K negative datapoints as the training dataset.
5. EVALUATION

We study the eﬀectiveness of the attacks described in Sec-
tion 3 by evaluating sensitivity to diﬀerent classiﬁers, train-
ing datasets, and training dataset sizes. We plot the accu-
racy of the learning system as a function of the number of
attack datapoints inserted into the victim’s training dataset.
Table 2 shows the default values and the diﬀerent parame-
ters we vary during the experiments.

We experiment with an ensemble technique. For this, the
training dataset is randomly sampled 10 times to create 10
training datasets. These new datasets are half the size of the
original training dataset, and they are not disjoint from each
other. A model is learned from each dataset. For classiﬁca-
tion, a datapoint is classiﬁed by each model, and the ﬁnal
classiﬁcation is a majority vote from each of the 10 models.
5.1 Attack comparison

To evaluate the impact of the attacks on the accuracy
of the classiﬁer, we insert a varying number of attack dat-
apoints (depicting the attacker’s actions) into the victim’s
training set. Figure 3(a) shows the accuracy for each attack
using the default dataset (Egyptian) and the default learning
system (SVM). The order of increasing eﬀectiveness is as ex-
pected since each attack requires more information than the
previous attack: REPEAT, LABEL-FLIP, POISON-1, and
POISON-2. To better illustrate the impact of POISON-2,

In this form it is unintuitive how to solve, but we will
show this problem is a knapsack problem as well. We ﬁrst
transform the objective of the optimization problem by let-
i and P (Xi = 1|C = −1) = p−
ting P (Xi = 1|C = +1) =p +
i .
Then, we substitute those terms that do not rely on the
variable x, the substituted constant a is computed once for
a model and remains the same for any value of the variable
x:

(cid:7)y

(cid:6)
P (C = −1|X = x)
P (C = +1|X = x)
⎛
⎜⎝ P (C = −1)
⎛
⎝a

(cid:11)n
i=1(1 − p−
(cid:11)n
i )
i=1(1 − p+
⎞
i )
⎠y

P (C = +1)

i − p−
p−
i − p−
p+

i p+
i p+

(cid:2)
∀i:xi=1

i

i

=

=

(cid:11)
∀i:xi=1
(cid:11)

∀i:xi=1

i

p−
1−p−
i
p+
i
1−p+

i

y

⎞
⎟⎠

(8)

We then take the logarithm of the objective as maximiz-
ing the logarithm also maximizes the original objective. By
maximizing the logarithm of the objective from Equation 7
we are left with a knapsack problem similar to the SVM
case:

(cid:6)

(cid:3)
∀i:xi=1

log

(cid:7)

i − p−
p−
i − p−
p+

i p+
i p+

i

i

(9)

ylog(a) + y

maximize
subject to xi ∈ {0, 1}, ∀i

x − 1 ≤ L, y ∈ {−1, +1}

T
c

4. METHODOLOGY

We use three diﬀerent datasets for our experiments, two

Twitter datasets and an Android dataset.

Collecting the datasets. The ﬁrst two datasets are
tweets from Twitter users spanning two diﬀerent time pe-
riods: “Egyptian”, consisting of 16 million tweets collected
between January 23rd and February 8th, 2011 capturing the
time period of the Egyption revolution, and “Elections”, con-
sisting of 11 million tweets collected between November 5
and November 12, 2012 capturing the time period of the
US presidential election. The ﬁrst dataset was collected
and made available by NIST [28]. We collected the second
dataset using Twitter’s streaming API (with IRB approval).
The third dataset, “Android”, pertains to comments made
on a popular smartphone market, Google Play, a market-
place for Android applications. We have developed an in-
house crawler that we used to take a snapshot of the entire
Google Play store on Nov 17, 2012. Our snapshot consists
of 150K applications that have 625K user comments accom-
panying them.

Processing the datasets. To obtain English documents
from our data we perform standard language ﬁltering tech-
niques [29] trained by an existing language corpus provided
by the Natural Language Toolkit [30]. On the resulting En-
glish documents, we apply the standard data cleaning steps
described in Section 2.2. The Twitter datasets are labeled
by using only Tweets with emoticons. The Android dataset

89y
c
a
r
u
c
c
A

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0

LABEL-FLIP
REPEAT
POISON-1
POISON-2

y
c
a
r
u
c
c
A

 5

 10

 15

 20

 25

 30

 1

 0.9

 0.8

 0.7

 0.6

 0.5

LABEL-FLIP
REPEAT
POISON-1
POISON-2

 0

 0.2

 0.4

 0.6

 0.8

 1

 1.2

 1.4

y
c
a
r
u
c
c
A

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0

y
c
a
r
u
c
c
A

SVM
NB

 5

 10

 15

 20

 25

 30

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0

SVM
NB

 5

 10

 15

 20

 25

 30

Number of added data ponts (thousands)

Number of added data ponts (thousands)

Number of added data ponts (thousands)

Number of added data ponts (thousands)

(a)

(b)

(c)

(d)

Figure 3: [a,b] Comparison of REPEAT, LABEL-FLIP, POISON-1, and POISON-2 attacks (a) 0-30K attack datapoints and
(b) 0-1.5K attack datapoints. [c,d] Comparison of attack eﬀectiveness against diﬀerent machine learning algorithms SVM and
NB: (c) LABEL-FLIP and (d) POISON-1. All attacks are performed against training datasets with 30K datapoints.

we provide Figure 3(b) which has the same results just drawn
at a ﬁner scale. Overall, we note that POISON-2, POISON-
1, and LABEL-FLIP degrade accuracy to 50% with roughly
500, 15K, and 30K attacker datapoints respectively. De-
grading to 50% is signiﬁcant as the integrity of the classiﬁer
is fully broken at this point since the classiﬁer has been re-
duced to a trivial random guessing classiﬁer.

Creating mislabeled points is more eﬀective than simply
swapping labels as POISON-1 degrades performance twice
as fast as LABEL-FLIP. When an adversary aims to degrade
accuracy on a known testing set, the POISON-2 attack in-
creases impact by an order of magnitude when comparing
with the other attacks due to focusing the attack on features
occurring most often in the testing set. Note that with only
500 tweets a POISON-2 attack can degrade to 50% the ac-
curacy of a classiﬁer. An attacker can easily generate these
number of tweets, particularly if he has under his control
several compromised accounts.

5.2 Sensitivity to learning systems

Learning algorithm. Figure 3(c) compares the eﬀec-
tiveness of the LABEL-FLIP attack for the SVM and NB
classiﬁers. For LABEL-FLIP the NB classiﬁer consistently
performs worse than the SVM classiﬁer by roughly 2% accu-
racy which is consistent as the attack occurs. The POISON-
1 attack shown in Figure 3(d) has less impact on NB than
on SVM as the SVM classiﬁer is more sensitive to outliers
which forces the model to adjust sharply to these outliers.
The result shows that NB accuracy is only degraded by 5%
at the point when the POISON-1 attack against the SVM
classiﬁer degrades accuracy to 50%.

Ensemble. Figures 4(a) and 4(b) show the accuracy
achieved by the ensemble classiﬁer as a function of the added
datapoints. The ensemble reduces the eﬀectiveness of the at-
tacks which can be seen as a LABEL-FLIP attack requires
17K points against a victim with ensemble as opposed to
14K points against a victim without ensemble to degrade
accuracy to the same point of 75%. A bagging ensemble
could provide a valid mitigation strategy, but we do note
that it takes more computational eﬀort to train multiple
classiﬁers, and the mitigation is not substantial enough to
completely protect the integrity of a learning system.

Training dataset. We show the eﬀectiveness of LABEL-

FLIP and POISON-1 against diﬀerent datasets in Figures 4(c)
and 4(d). Note that Egyptian and Android are trained with
a training set of size 30K while Election is trained with a
training set of size 12K. In the LABEL-FLIP experiment, we

note that the Election dataset is more susceptible due to its
smaller training set size, while for the Egyptian and Android
datasets which are quite diverse, the LABEL-FLIP dimin-
ishes the accuracy to nearly 50% when the number of attack
points is equal to the training set size. For the POISON-1
attack, the Android dataset allows datapoints up to 1200
characters long allowing POISON-1 to craft much stronger
attacks points diminishing accuracy to 50% after 6K added
datapoints instead of 15K added datapoints in the Egyptian
dataset.

Training dataset size. We study the eﬀectiveness of
the LABEL-FLIP and POISON-1 attacks when the training
dataset size changes, where all the training datapoints are
selected from the Egyptian dataset. Figures 5(a) and 5(b).
show that the attacks have less impact when the model is
trained with larger training sets.

Sensitivity summary. From our attack sensitivity ex-
periments we ﬁnd the following. The POISON-1 attack had
signiﬁcant diﬀerence between the two classiﬁers. The classi-
ﬁer for SVM is more complex which allows SVM to exploit
more structure in datapoints of the training set, but this
complexity is the reason SVM suﬀers from the POISON-1 at-
tack. NB considers frequencies of each feature separately, so
POISON-1 attacking combinations of incorrect features does
not signiﬁcantly harm the model. A victim with a larger
training set will force an attacker to use more eﬀort in de-
grading accuracy. Additionally, a victim in a scenario where
a datapoint is less constrained, such as Android comments
compared with Twitter, is more vulnerable to POISON-1
attacks. A POISON-1 attacker is able to be more malicious
with each datapoint due to the fewer constraints.

6. POISONING WITH PARTIAL KNOWL-

EDGE

So far in our evaluation we assume the attacker has full
knowledge of the victim’s processing steps in the learning
system, and now we show cases where the attacker only
has partial knowledge of these steps. We focus on partial
knowledge for three typical data cleaning steps detailed in
Section 2.2, Repeated Letter Removal (happppppy − >
happy), Case Normalization (HappY − > happy), and
Username Removal (@John − > username).
In a real-
world scenario, it may be possible that the attacker learns
other aspects of the learning system, but he/she may be
unable to learn all of these data cleaning steps. We evalu-
ate scenarios with partial knowledge by having the attacker

90y
c
a
r
u
c
c
A

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0

 1

 0.9

 0.8

 0.7

 0.6

 0.5

y
c
a
r
u
c
c
A

Single SVM
Ensemble SVM

 5

 10

 15

 20

 25

 30

Single SVM
Ensemble SVM

 0

 2

 4

 6

 8

 10

 12

 14

y
c
a
r
u
c
c
A

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0

y
c
a
r
u
c
c
A

Android dataset
Egyptian dataset
Election dataset

 5

 10

 15

 20

 25

 30

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0

Number of added data ponts (thousands)

Number of added data ponts (thousands)

Number of added data points (thousands)

Android dataset
Egyptian dataset
Election dataset

 2

 4

 8

 14
Number of added data points (thousands)

 10

 12

 6

(a)

(b)

(c)

(d)

Figure 4: [a,b] Comparison of attack eﬀectiveness against no ensemble and bagging ensemble: (a) LABEL-FLIP, (b) POISON-
1.
[c,d] Comparison of attack eﬀectiveness against diﬀerent datasets Egyptian, Election, and Android: (c) LABEL-FLIP and
(d) POISON-1. All attacks are performed against training datasets with 30K datapoints.

Table 3: Most popular features used in attacks with
partial knowledge

Full

knowledge

user-
name
you
not
but
love
thanks
good
i’m
miss
was

1530

670
660
600
440
400
400
460
400
340

Missing cleaning step

Repeated Let-
ter Removal

Case
malization

Nor-

Username
Removal

user-
name
you
not
but
love
thanks
i’m
good
miss
can’t

1530

680
670
610
450
410
440
400
390
320

user-
name
you
not
but
I’m
miss
love
have
was
don’t

1540

you

580
570
540
410
370
370
330
300
300

not
but
i’m
love
thanks
miss
good
was
can’t

680

680
630
460
450
410
400
380
350
330

create POISON-1 attack points when not performing one of
those data cleaning steps. First, we provide some statistics
which characterize how the attack points diﬀer, and then we
show the accuracy degradation by inserting these created at-
tack points into a victim’s training dataset where the victim
performs all data cleaning steps.

Table 3 shows the frequency of the top 10 features used
in 15K attack points. We gain two important insights from
Table 3. First, the attack without the Username Removal
cleaning step does not use username in its attacks. This is
due to the attacker not mapping all usernames to the same
feature, and the inclusion of a username in a tweet happens
to be a strong indication of positive sentiment according
to our Election dataset. Since, the username feature in-
dicates positive sentiment, the poisoning attacks select the
username feature to be included in attack datapoints that
are labeled with negative sentiment. Second, despite this
discrepancy with usernames, the top 10 most frequent fea-
tures are fairly consistent across the diﬀerent attacks with
partial knowledge. Such similarities indicate that despite
diﬀerences in the models learned by each attack with par-
tial knowledge, the models learn the same features which
have high indication of sentiment.

Table 4 shows statistics for the three scenarios of par-
tial knowledge. The total number of features in the trained
model for each scenario is shown, and for reference, the num-
ber of features in the model with all data cleaning steps is

Table 4: Statistics of attacks with partial knowledge

Feature
Count

Model
Attack
New
Lost
Same

Repeated
Letter
moval

52034
5344
1746
1583
3598

Missing cleaning step

Case Normal-
ization

Re-

59820
4596
1274
1859
3322

Username
Removal

69438
5818
2455
1818
3363

≈51K features. Data cleaning combines numerous features
into more meaningful features so missing one of the steps
(i.e., having only partial knowledge) increases the number of
features. The “attack features” count captures the number of
unique features used in the generated 15K attack datapoints
for a given scenario of partial knowledge. We additionally
show the number of “new”, “lost”, and “same” features used
in the partial knowledge scenarios compared with the sce-
nario where the attacker has full knowledge of the cleaning
steps. We observed a large number of new features for the
Username Removal case that were not from the additional
new model features of distinct usernames. The increase in
new features for this attack is actually due to the username
feature being used less in the attack as observed in Table
3 which has both advantages and disadvantages in terms of
attack eﬀectiveness in this scenario.

Figure 5(c) shows the attack eﬀectiveness for each sce-
nario of partial knowledge. The scenario with full knowledge
is shown as a baseline. Lacking the knowledge of the Re-
peated Letter Removal step has little diﬀerence from the
scenario of full knowledge. Lacking the knowledge of Case
Normalization step does harm the eﬀectiveness of the at-
tack as accuracy is degraded to 68% (as opposed to 50%) af-
ter 14K attack points. Interestingly, lacking the knowledge
of Username Removal step results in a stronger attack
when fewer attack datapoints are used, but the eﬀectiveness
plateaus after many attack datapoints are added.

We investigated the unexpected behavior when Username
Removal step is not known to the attacker, and we ﬁrst
discuss the steep accuracy decrease with few attack data-
points and then the plateau behavior when numerous at-
tack datapoints are added. The steep accuracy decrease is
due to more non-username features being used in the attack
since the lack of username cleaning puts less emphasis in the
model on the username features. Thus, testing datapoints
without the username attributes are inaccurately classiﬁed
quicker. The plateau observed in the curve is due to not

91y
c
a
r
u
c
c
A

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0

Model training set size 15k
Model training set size 30k
Model training set size 45k

y
c
a
r
u
c
c
A

 5

 10

 15

 20

 25

 30

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0

Model training set size 15k
Model training set size 30k
Model training set size 45k

y
c
a
r
u
c
c
A

 5

 10

 15

 20

 25

 30

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0

Number of added data ponts (thousands)

Number of added data ponts (thousands)

Full knowledge
No Username Removal
No Case Normalization
No Repeated Letter Removal

 2

 4

 12
Number of added tweets (thousands)

 10

 6

 8

 1

 0.9

 0.8

 0.7

 0.6

y
c
a
r
u
c
c
A

 14

 0.5

 0

Without test set usernames
With test set usernames

 2

 4

 12
Number of added tweets (thousands)

 10

 6

 8

 14

(a)

(b)

(c)

(d)

Figure 5: [a,b] Comparison of attack eﬀectiveness against varying training set sizes 15K, 30K, and 45K: (a) LABEL-FLIP and
(b) POISON-1. [c,d] Comparison of attack eﬀectiveness for (c) all scenarios of partial knowledge and our standard testing set
and (d) the username scenario of partial knowledge and a testing set with usernames removed. Attacks in [c,d] are performed
against training datasets with 30K datapoints while those in [a,b] are performed on datasets of varying sizes.

having enough eﬀect on those testing set datapoints which
contain usernames. We veriﬁed that the usernames in the
testing set were causing this plateau by including a com-
parison where we perform the same experiment with partial
knowledge but remove all username features from the testing
set in Figure 5(d). The case without usernames in the test-
ing set has accuracy degradation to 50% without a plateau
eﬀect at 60% accuracy.

7. RELATED WORK
Poisoning SVMs: The closest existing work to ours is a
poisoning attack against SVM classiﬁer accuracy in the work
of Biggio et al. [19]. That work formulates a non-convex op-
timization problem which maximizes loss of a testing set
given a model learned from a training set with a poisoned
data point. This optimization problem is based on complete
information about the learner’s data, and there are no con-
straints over their data points which allows them to use the
technique of gradient ascent to ﬁnd a good solution to this
hard non-convex problem. We distinguish ourselves from
that work as the attacker sometimes has partial knowledge
about the learner, and we consider the domain-speciﬁc con-
straints on the search space.
In our search space we are
constrained to using only sparse binary vectors, and thus
our search space itself is non-convex and causes problems.
We choose to use a simple linear objective function instead
that exploits SVMs sensitivity to outliers, and we employ
heuristics to solve over the non-convex search space.
Adversarial machine learning: Learning in the presence
of an adversary is an active area of research. The study of
learning from examples with malicious errors was initiated
by Valiant [11] and Littlestone et al. [31] which aimed to
understand accuracy of a learning algorithm given a ﬁxed
probability of error. Barreno et al. [32], Newsome et al. [33],
and Zhou et al. [18] studied the eﬃcacy of intrusion detection
systems based on machine learning when an attacker aims to
subvert such a system. Biggio et al. [17] demonstrate how
adversarial label ﬂipping performs against an SVM which
performs training to be robust to random label noise, and
Xiao et al. [34] extend this work by improving the adversarial
label changing while additionally considering the eﬀects on
SVMs with non-linear kernel functions. Our work considers
a learning system system with all the practical preprocessing
steps that warp the space of acceptable datapoints which
can be altered. Finding attack datapoints in such a space is
far more diﬃcult due to the induced non-convexity, and we

provide experimental results as opposed to analytical results
shown in prior works.
Mimicry attacks: Existing work aims to understand how
to craft datapoints that can bypass a classiﬁer. Note that
this is diﬀerent from the attacks we consider, as the goal
of the attack is not to inﬂuence the learning system, but
to bypass detection. For example, the work in [26, 35, 36]
studies spam, focusing on datapoint creation for getting past
the spam ﬁlter. Work on malware detection in PDFs [20, 21]
similarly shows how to craft datapoints (PDFs) which are
classiﬁed as negative when they are actually malicious. This
work considers similar practical constraints on the search
space that we consider when degrading a classiﬁer.
Opinion mining and sentiment analysis: Previous work
studied opinion mining and sentiment analysis. Pang et
al. [37] describe techniques and approaches for an opinion-
oriented information retrieval. Yang et al. [38] use web-blogs
to construct a corpora for sentiment analysis and use emoti-
cons assigned to blog posts as indicators of users’ mood.
Read et al. [22] used emoticons to form a training set from
Usenet newsgroup documents similar to how we have con-
structed training sets in our work. Lam et al. [39] provide
preliminary analysis on evaluating classiﬁers with test data
having noisy labels and analyze the bounds of the error rate
for the classiﬁcation. Pal et al. [40] build a probabilistic
model of the classiﬁer in the absence of a true label using a
latent variable model.

8. CONCLUSION

We demonstrate how to subvert a real-world learning sys-
tem for sentiment analysis, a common task that is critical
in numerous data analytics applications. The attacks we
present consider practical aspects of a learning system that
were not considered in prior work. The practical concerns
consist of the varied capabilities of an attacker, knowledge
of an attacker, and constraints on feasible datapoints. We
present attacks matching several classes of attackers with
varied capabilities and knowledge. We formulate poisoning
attacks, crafting malicious datapoints, to ensure created at-
tack points meet constraints imposed by the set of feasible
datapoints. We evaluate our attacks with three real-world
datasets and two typical machine learning algorithms. As
a further practical concern in our evaluation, we consider
these poisoning attacks when the full learning system of the
victim is unknown and the attacker must work with partial
knowledge.

929. REFERENCES

[1] S. Rodgers and E. Thorson, “The interactive

advertising model: How users perceive and process
online ads,” JOIA, 2000.

[2] Y. Koren, R. Bell, and C. Volinsky, “Matrix

factorization techniques for recommender systems,”
Computer, vol. 42, no. 8, pp. 30–37, 2009.

[3] J. Bollen, A. Pepe, and H. Mao, “Modeling public

mood and emotion: Twitter sentiment and
socio-economic phenomena,” in CWSM, 2011.
[4] M. D. Conover, J. Ratkiewicz, M. Francisco,

B. Gon¸calves, A. Flammini, and F. Menczer, “Political
polarization on twitter,” in CWSM, 2011.

[5] A. Tumasjan, T. O. Sprenger, P. G. Sandner, and

I. M. Welpe, “Predicting elections with twitter: What
140 characters reveal about political sentiment,” in
CWSM, 2010.

[6] A. Pak and P. Paroubek, “Twitter as a corpus for

sentiment analysis and opinion mining,” in Proceedings
of LREC, vol. 2010, 2010.

[7] R. Feldman, “Techniques and applications for

sentiment analysis,” Communications of the ACM,
vol. 56, no. 4, pp. 82–89, 2013.

[8] I. Guyon, Feature extraction: foundations and

applications. Springer, 2006, vol. 207.

[9] B. Nelson, B. I. Rubinstein, L. Huang, A. D. Joseph,
S. J. Lee, S. Rao, and J. Tygar, “Query strategies for
evading convex-inducing classiﬁers,” JMLR, 2012.

[10] G. Ateniese, G. Felici, L. V. Mancini, A. Spognardi,
A. Villani, and D. Vitali, “Hacking smart machines
with smarter ones: How to extract meaningful data
from machine learning classiﬁers,” arXiv, 2013.

[11] L. G. Valiant, “Learning disjunctions of conjunctions,”

in AI, 1985.

[12] M. Kearns and M. Li, “Learning in the presence of

malicious errors,” SIAM Journal on Computing,
vol. 22, no. 4, pp. 807–837, 1993.

[13] A. R. Klivans, P. M. Long, and R. A. Servedio,

“Learning halfspaces with malicious noise,” JMLR,
2009.

[14] B. Biggio, G. Fumera, and F. Roli, “Security

evaluation of pattern classiﬁers under attack,”
Transactions on Knowledge and Data Engineering,
2013.

[15] Y. Chen, C. Caramanis, and S. Mannor, “Robust
sparse regression under adversarial corruption,” in
Proceedings of ICML, 2013.

[16] M. Großhans, C. Sawade, M. Br¨uckner, and

T. Scheﬀer, “Bayesian games for adversarial regression
problems,” ICML, 2013.

[17] B. Biggio, B. Nelson, and P. Laskov, “Support vector
machines under adversarial label noise,” ACML, 2011.

[18] Y. Zhou, M. Kantarcioglu, B. Thuraisingham, and

B. Xi, “Adversarial support vector machine learning,”
in ACM SIGKDD, 2012.

[19] B. Biggio, B. Nelson, and P. Laskov, “Poisoning
attacks against support vector machines,” arXiv
preprint arXiv:1206.6389, 2012.

[20] N. ˇSrndic and P. Laskov, “Practical evasion of a

learning-based classiﬁer: A case study,” in Proceedings
of S&P, 2014.

[21] ——, “Detection of malicious pdf ﬁles based on

hierarchical document structure,” in Proceedings of
NDSS. Citeseer, 2013.

[22] J. Read, “Using emoticons to reduce dependency in

machine learning techniques for sentiment
classiﬁcation,” in ACL SRW, 2005.

[23] N. Aronszajn, “Theory of reproducing kernels,” Trans.

Amer. Math. Soc, vol. 68, no. 3, pp. 337–404, 1950.

[24] V. Vapnik, “The nature of 6tatistical learning theory,”

Data mining and knowledge discovery, pp. 1–47, 6.

[25] K. Thomas, C. Grier, D. Song, and V. Paxson,

“Suspended accounts in retrospect: an analysis of
twitter spam,” in Proceedings of the 2011 ACM
SIGCOMM conference on Internet measurement
conference. ACM, 2011, pp. 243–258.

[26] G. L. Wittel and S. F. Wu, “On attacking statistical

spam ﬁlters,” in Proceedings of the ﬁrst conference on
email and anti-spam (CEAS). California, USA, 2004.

[27] D. P. Williamson and D. B. Shmoys, The Design of
Approximation Algorithms, 1st ed. New York, NY,
USA: Cambridge University Press, 2011.
Available: {http://trec.nist.gov/data/tweets/}

[28] “NIST Twitter Dataset for TREC 2011.” [Online].

[29] J. C. Schmitt, “Trigram-based method of language
identiﬁcation,” Oct. 29 1991, uS Patent 5,062,143.
[30] E. Loper and S. Bird, “Nltk: The natural language

toolkit,” in ACL-02 NLPCL, 2002.

[31] N. Littlestone, “Learning quickly when irrelevant

attributes abound: A new linear-threshold algorithm,”
ML, 1988.

[32] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and

J. Tygar, “Can machine learning be secure?” in ACM
ICCS, 2006.

[33] J. Newsome, B. Karp, and D. Song, “Paragraph:

Thwarting signature learning by training maliciously,”
in RAID, 2006.

[34] H. Xiao, H. Xiao, and C. Eckert, “Adversarial label

ﬂips attack on support vector machines,” 2012.

[35] J. Graham-Cumming, “How to beat an adaptive spam

ﬁlter,” in Presentation at the MIT Spam Conference,
2004.

[36] D. Lowd and C. Meek, “Good word attacks on

statistical spam ﬁlters,” in CEAS, 2005.

[37] B. Pang and L. Lee, “Opinion mining and sentiment

analysis,” FTIR, 2008.

[38] C. Yang, K. H.-Y. Lin, and H.-H. Chen, “Emotion

classiﬁcation using web blog corpora,” in IEEE Web
Intelligence, 2007.

[39] C. P. Lam and D. G. Stork, “Evaluating classiﬁers by

means of test data with noisy labels,” in AI, 2003.

[40] C. Pal, G. Mann, and R. Minerich, “Putting semantic

information extraction on the map: Noisy label
models for fact extraction,” in AAAI WIIW, 2007.

93