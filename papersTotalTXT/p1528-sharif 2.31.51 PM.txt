Accessorize to a Crime: Real and Stealthy Attacks on

State-of-the-Art Face Recognition

Mahmood Sharif

Carnegie Mellon University

Pittsburgh, PA, USA

mahmoods@cmu.edu

Lujo Bauer

Carnegie Mellon University

Pittsburgh, PA, USA
lbauer@cmu.edu

Sruti Bhagavatula

Carnegie Mellon University

Pittsburgh, PA, USA
srutib@cmu.edu
Michael K. Reiter

University of North Carolina

Chapel Hill, NC, USA
reiter@cs.unc.edu

ABSTRACT
Machine learning is enabling a myriad innovations, including
new algorithms for cancer diagnosis and self-driving cars.
The broad use of machine learning makes it important to
understand the extent to which machine-learning algorithms
are subject to attack, particularly when used in applications
where physical security or safety is at risk.

In this paper, we focus on facial biometric systems, which
are widely used in surveillance and access control. We de-
ﬁne and investigate a novel class of attacks: attacks that
are physically realizable and inconspicuous, and allow an at-
tacker to evade recognition or impersonate another individ-
ual. We develop a systematic method to automatically gen-
erate such attacks, which are realized through printing a pair
of eyeglass frames. When worn by the attacker whose image
is supplied to a state-of-the-art face-recognition algorithm,
the eyeglasses allow her to evade being recognized or to im-
personate another individual. Our investigation focuses on
white-box face-recognition systems, but we also demonstrate
how similar techniques can be used in black-box scenarios,
as well as to avoid face detection.

1.

INTRODUCTION

Machine learning (ML) is a technology that is profoundly
changing the world. Some of the transformative innovations
that it enables, such as customized search results and au-
tomated translations, might seem minor. Other innovations
are clearly revolutionary, enabling new applications or sig-
niﬁcantly increasing our quality of life—examples include
algorithms for cancer diagnosis and self-driving vehicles.

The broad use of ML has also caused a rising interest to
understand the extent to which ML algorithms and applica-
tions are vulnerable to attacks. These attacks are especially
worrisome due to humans’ increasing reliance on technol-

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS’16 October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4139-4/16/10.
DOI: http://dx.doi.org/10.1145/2976749.2978392

ogy and ML (sometime trusting them even with their own
lives [36]). The domains that received the majority of the at-
tention have a common characteristic: the adversary is able
to precisely control the digital representation of the input to
the ML tools. For instance, in spam detection, the attacker
can control the contents of the email and the address of the
sender. Hence, unsurprisingly, it has been shown that at-
tacks in many of these domains can be eﬀective at evading
ML classiﬁers designed to detect them [23, 35, 43].

In this paper, we explore the extent to which facial bio-
metric systems are vulnerable to attacks. These systems are
widely used for various sensitive purposes, including surveil-
lance and access control [26, 27, 28]. Thus, attackers who
mislead them can cause severe ramiﬁcations.

In contrast to domains previously explored, attackers who
aim to mislead facial biometric systems often do not have
precise control over the systems’ input. Rather, attackers
may be able to control only their (physical) appearance. The
process of converting the physical scene into a digital input is
not under the attackers’ control, and is additionally aﬀected
by factors such as lighting conditions, pose, and distance. As
a result, it is more diﬃcult for attackers to manipulate or
craft inputs that would cause misclassiﬁcation than it would
be, for example, in the domain of spam detection.

Another diﬃculty that attackers face in the case of facial
biometric systems is that manipulating inputs to evade the
ML classiﬁers might be easily observed from outside the sys-
tems. For example, attackers can wear an excessive amount
of makeup in order to evade a surveillance system deployed
at banks [14]. However, these attackers may draw an in-
creased attention from bystanders, and can be deterred by
traditional means, e.g., the police.

In the light of these two challenges, we deﬁne and study
a new class of attacks: attacks that are physically realizable
and at the same time are inconspicuous. In such attacks,
the attacker manipulates the physical state that an ML al-
gorithm is analyzing rather than the digitized representation
of this state. At the same time, the manipulations of phys-
ical state needed by the attacks are suﬃciently subtle that
they are either imperceptible to humans or, if perceptible,
seem natural and not representative of an attack.

Inconspicuousness We focus, unlike most related work
(e.g., [7]), on attacks that are inconspicuous, i.e., a person
who is physically present at a scene, or a person who looks at

1528the input gathered by a sensor (e.g., by watching a camera
feed), should not notice that an attack is taking place.

We believe that this focus on attacks that are not read-
ily apparent to humans is important for two reasons: First,
such attacks can be particularly pernicious, since they will
be resistant to at least cursory investigation. Hence, they are
a particularly important type of attacks to investigate and
learn how to defend against. Second, such attacks help the
perpetrators (whether malicious or benign) achieve plausible
deniability; e.g., a person seeking merely to protect her pri-
vacy against aggressive use of face recognition by merchants
will plausibly be able to claim that any failure of an ML
algorithm to recognize her is due to error or chance rather
than deliberate subterfuge.

Physical Realizability In this work, we are interested in
attacks that can be physically implemented for the purpose
of fooling facial biometric systems.
In addition, we focus
on attacks on state-of-the-art algorithms. Previous work
on misleading ML algorithms often targets algorithms that
are not as sophisticated or as well trained as the ones used
in practice (see Sec. 2), leaving the real-world implications
of possible attacks unclear. In contrast, we focus on those
attacks that pose a realistic and practical threat to systems
that already are or can easily be deployed.

We divide the attacks we study into two categories, which
diﬀer both in the speciﬁc motivations of potential attackers
and in the technical approaches for implementing the at-
tacks. The two categories of attacks we are interested in are
dodging and impersonation. Both dodging and imperson-
ation target face recognition systems (FRSs) that perform
multi-class classiﬁcation—in particular, they attempt to ﬁnd
the person to whom a given face image belongs.

Impersonation In an impersonation attack, the adversary
seeks to have a face recognized as a speciﬁc other face. For
example, an adversary may try to (inconspicuously) disguise
her face to be recognized as an authorized user of a lap-
top or phone that authenticates users via face recognition.
Or, an attacker could attempt to confuse law enforcement
by simultaneously tricking multiple geographically distant
surveillance systems into “detecting” her presence in diﬀer-
ent locations.

Dodging In a dodging attack, the attacker seeks to have
her face misidentiﬁed as any other arbitrary face. From a
technical standpoint, this category of attack is interesting
because causing an ML system to arbitrarily misidentify a
face should be easier to accomplish with minimal modiﬁca-
tions to the face, in comparison to misclassifying a face as
a particular impersonation target. In addition to malicious
purposes, dodging attacks could be used by benign individ-
uals to protect their privacy against excessive surveillance.

In this paper we demonstrate inconspicuous and physi-
cally realizable dodging and impersonation attacks against
FRSs. We detail the design of eyeglass frames that, when
printed and worn, permitted three subjects (speciﬁcally, the
ﬁrst three authors) to succeed at least 80% of the time when
attempting dodging against state-of-the-art FRS models.
Other versions of eyeglass frames allowed subjects to imper-
sonate randomly chosen targets. For example, they allowed
one subject, a white male, to impersonate Milla Jovovich,
a white female, 87.87% of the time; a South-Asian female
to impersonate a Middle-Eastern male 88% of the time; and

a Middle-Eastern male to impersonate Clive Owen, a white
male, 16.13% of the time. We also show various other results
as extensions to our core methods, in accomplishing imper-
sonation with less information (so called black-box models,
see Sec. 3) and in evading face detection.

Our contributions in this paper are threefold:

I. We show how an attacker that knows the internals of
a state-of-the-art FRS can physically realize imperson-
ation and dodging attacks. We further show how these
attacks can be constrained to increase their inconspic-
uousness (Sec. 4–5).

II. Using a commercial FRS [25], we demonstrate how an
attacker that is unaware of the system’s internals is
able to achieve inconspicuous impersonation (Sec. 6).
III. We show how an attacker can be invisible to facial
biometric systems by avoiding detection through the
Viola-Jones face detector, the most popular face-detec-
tion algorithm [42] (Sec. 7).

Sec. 2–3 survey related work and provide background. In
Sec. 4 we describe our method for generating physically real-
izable white-box attacks, and detail experiments that show
its eﬀectiveness in Sec. 5. We demonstrate that similar at-
tacks can be carried out against black-box FRSs in Sec. 6,
and that they can additionally be used to evade face detec-
tion in Sec. 7. We discuss the implications and limitations
of our approach in Sec. 8 and conclude with Sec. 9.
2. RELATED WORK

Face Detection Systems (FDSs) and FRSs are tradition-
ally evaluated with the implicit assumption of zero-eﬀort
attackers (i.e., the attackers do not actively attempt to fool
the system) [33, 42]. Nevertheless, there is extensive liter-
ature on attacks targeting FDSs and FRSs. In this section
we review the most noteworthy attempts. Broadly, how-
ever, existing work is diﬀerent than ours in at least one of
the following ways:
it does not attempt to achieve incon-
spicuousness and physical realizability; it tries to achieve
diﬀerent goals than ours (e.g., compromising the privacy of
the enrolled users); or the attacks target algorithms that are
not as advanced as the state of the art.

Face Detection Due to its high accuracy and speed, the
object-detection algorithm proposed by Viola and Jones [42]
(VJ) has become widely used for general object detection,
and is by far the dominant face detector used in practice [19,
26, 30]. Motivated by increasing privacy through invisibil-
ity to FRSs, Harvey attempted to design makeup and hair
styles that can be used to evade detection by the VJ face de-
tector [14]. His approach was based on manually observing
how the detector operates, and did not yield inconspicuous
results. With similar goals, Yamada et al. leveraged the fact
that camera sensors are sensitive to near infra-red light to
design light-emitting glasses that can help evade face detec-
tion by adding signiﬁcant noise to the captured image [44].
While eﬀective for evading detection, this technique is nei-
ther inconspicuous nor deniable.

Face Recognition Previous work found that subjects can
be eﬀectively impersonated to FRSs using 3d-printed masks
or face images downloaded from online social networks [7,
22]. These attacks are not inconspicuous, and can largely
be thwarted by anti-spooﬁng mechanisms, such as liveness
detection [7, 22].

1529Other work showed how to compromise the privacy of
users enrolled in FRSs [11, 12]. For example, Fredrikson
et al. presented model inversion attacks, in which face im-
ages of enrolled users are reconstructed from neural networks
trained to perform face recognition [11]. The goal of these
attacks is diﬀerent than ours. While compromised images
can be used to impersonate users, these impersonations will
not be inconspicuous, and can be detected via anti-spooﬁng
mechanisms.

Feng and Prabhakaran proposed a system to help artists
come up with makeup and hair designs that can be used to
prevent FRSs (including Eigenfaces [40] and Fisherfaces [1])
from recognizing subjects [10]. In their approach, they split
the face image into seven components, and determined which
components deviated most from the general population’s av-
erage. They hypothesized that these discriminative compo-
nents are the most signiﬁcant for distinguishing one per-
son from others, and showed that occluding, painting, or
transforming the discriminative components could prevent
automated recognition. Their work is diﬀerent than what is
being proposed here in three main ways: First, inconspicu-
ousness and deniability of the attack were not part of their
goals. Second, their attack does not enable the attacker to
intentionally masquerade as a speciﬁc subject in the system.
Finally, the algorithms on which they test their attack are
not considered the state-of-the-art in face recognition.

Speech Recognition Carlini et al. recently demonstrated
physically realizable attacks on speech-recognition systems [5].
They showed how to craft sounds that are diﬃcult or im-
possible for humans to understand, but are interpreted as
speciﬁc commands by speech-recognition systems. In con-
trast to their work, we focus on systems that receive visual
inputs through cameras.

3. BACKGROUND
3.1 Threat Model

We assume an attacker who gains access to the FRS to
mount a dodging or impersonation attack after the system
has been trained. That is, the adversary cannot “poison”
the FRS by altering training data, injecting mislabeled data,
etc. Rather, our adversary can alter only the composition of
inputs to be classiﬁed based on her knowledge of the classi-
ﬁcation model, potentially derived by examining the entire
model or by observing its outputs for some inputs.

As Papernot et al. discuss, adversaries diﬀer in the amount
of knowledge they have about the system they are attack-
ing [31]. We typically assume that an adversary knows the
algorithm used for classiﬁcation. On the other hand, dif-
ferent assumptions can plausibly be made about the adver-
sary’s knowledge of the feature space and the trained clas-
siﬁcation model. In this paper, we focus on adversaries who
know the feature space, since features used for image clas-
siﬁcation are usually considered to be publicly known [11].
We also assume a white-box scenario: the attacker knows
the internals (architecture and parameters) of the system
being attacked [31]. We do, however, discuss extensions to
the black-box scenario in Sec. 6.
3.2 Deceiving Neural Networks

In this paper we focus our exploration on FRSs based
on neural networks (NNs) and particularly deep neural net-

works (DNNs) [16]. DNNs can generalize extremely well in
the presence of large training sets and can achieve state-
of-the-art results on many challenging ML problems. For
example, DNNs were able to outperform humans in the face-
veriﬁcation task—determining whether pairs of face images
belong to the same person [17].

Given their high generalization capability, Szegedy et al.
surprised the research community by ﬁnding that DNNs can
be misled by mildly perturbing inputs [39]. More speciﬁcally,
their work showed that adversarial examples that appear
visually indistinguishable from non-adversarial examples can
be systemically found under the white-box scenario.

For an input x that is classiﬁed as f (x) by the DNN,
Szegedy et al.’s goal was to ﬁnd a perturbation r of minimal
norm (i.e., as imperceptible as possible) such that x + r
would be classiﬁed into a desired class ct. The objective of
ﬁnding the perturbation is modeled by:

(cid:16)|f (x + r) − ht| + κ|r|(cid:17)

argmin

r

constrained on x + r ∈ [0, 1], where f produces a probabil-
ity distribution (an element of [0, 1]N ) over the N possible
classes, κ is a constant that can be tuned to balance the
misclassiﬁcation and imperceptibility objectives, ht is the
one-hot vector of class ct, and | · | is a norm function. Min-
imizing |f (x + r) − ht| results in misclassiﬁcation, whereas
minimizing |r| increases the imperceptibility of the perturba-
tion. When f (·) and |·| are diﬀerentiable, this optimization
problem can be solved via ﬁrst- or second-order optimization
algorithms, such as Gradient Descent [4] or Limited-memory
BFGS [29]. For DNNs, f (·) is diﬀerentiable, as this is nec-
essary for training the network via back-propagation. The
norm function usually used is the Euclidean norm, which is
also diﬀerentiable. Subsequent work proposed conceptually
similar algorithms for generating imperceptible adversarial
examples [13, 31].

Initially, the existence of adversarial examples was at-
tributed to the complexity of learning the high-dimensional
manifold necessary to separate the classes:
it was conjec-
tured that adversarial examples are improbable to encounter
in training and hence remain as inevitable “blind spots”
when learning from ﬁnite, although large, training sets [39].
This conjecture was supported by showing that injecting
correctly labeled adversarial examples in the training phase
slightly increases the norm of the adversarial perturbations
needed to mislead a trained classiﬁer [39]. Later work at-
tributes adversarial examples to the non-ﬂexibility of the
classiﬁcation models [9, 13]. According to this work, since
each DNN unit (i.e., neuron) is activated on a linear combi-
nation of its inputs, slight changes to each input accumulate
to large changes in the unit’s output, which results in mis-
classiﬁcation.

Previous work highlights an important diﬀerence between
human and machine vision: while humans are unaﬀected by
small changes to images, machines can be greatly aﬀected.
Our research exploits this insight to fool FRSs with adapta-
tions to attacks that are both inconspicuous and physically
realizable.

4. TECHNICAL APPROACH

In this section we describe our approach to attacking white-
box FRSs. First, we provide the details of the state-of-the-
art FRSs we attack (Sec. 4.1). Then, we present how we

1530build on previous work [13, 31, 39] to formalize how an at-
tacker can achieve impersonation, and how to generalize the
approach for dodging (Sec. 4.2). Finally, we discuss how the
attacker can conceptually and formally tweak her objective
to enable the physical realizability of the attack (Sec. 4.3).
4.1 White-box DNNs for Face Recognition

Using ML models and particularly DNNs, researchers have
developed FRSs that can outperform humans in their ability
to recognize faces [17]. Parkhi et al. recently developed a 39-
layer DNN for face recognition and veriﬁcation that achieves
state-of-the-art performance (and outperforms humans) [33]
on the Labeled Faces in the Wild (LFW) [17] challenge, a
benchmark for testing FRSs’ ability to classify images taken
under unconstrained conditions.

To demonstrate dodging and impersonation we use three
DNNs: First, we use the DNN developed by Parkhi et al.1,
which we refer to as DNNA. Second, we trained two DNNs
(termed DNNB and DNNC ) based on the structure of Parkhi
et al.’s to recognize celebrities as well as people who were
available to us in person for testing real attacks. The pur-
pose of using DNNB and DNNC is to test the physically re-
alized versions of our attacks, which require people to wear
glasses designed by our algorithm. Testing dodging requires
the subjects to be known to the classiﬁer (otherwise they
would always be misclassiﬁed). Similarly, testing imperson-
ation is more realistic (and more diﬃcult for the attacker) if
the DNN is able to recognize the impersonator. The DNNs
we use in this work can be conceptualized as diﬀerentiable
functions that map input images to probability distributions
over classes. An image is counted as belonging to the class
that receives the highest probability, optionally only if that
probability exceeds a predeﬁned threshold.

DNNADNNADNNA
Parkhi et al. trained DNNA to recognize 2622
celebrities. They used roughly 1000 images per celebrity for
training, for a total of about 2.6M images. Even though they
used much less data for training than previous work (e.g.,
Google used 200M [38]), their DNN still achieves 98.95%
accuracy, comparable to other state-of-the-art DNNs [17].

DNNBDNNBDNNB and DNNCDNNCDNNC Using DNNA for physical realizability
experiments is not ideal, as it was not trained to recognize
people available to us for testing physically realized attacks.
Therefore, we trained two additional DNNs.

DNNB was trained to recognize ten subjects: ﬁve peo-
ple from our lab (the ﬁrst three authors and two additional
researchers who volunteered images for training), and ﬁve
celebrities for whom we picked images from the PubFig im-
age dataset [21].
In total, the training set contained ﬁve
females and ﬁve males of ages 20 to 53 years. The celebri-
ties we used for training were: Aaron Eckhart, Brad Pitt,
Clive Owen, Drew Barrymore, and Milla Jovovich.

DNNC was trained to recognize a larger set of people,
arguably posing a tougher challenge for attackers attempt-
ing impersonation attacks. In total, DNNC was trained to
recognize 143 subjects: 140 celebrities from PubFig’s [21]
evaluation set, and the ﬁrst three authors.

We trained DNNB and DNNC via transfer learning, a tra-
ditional procedure to train DNNs from other, pre-trained,
DNNs [45]. Transfer learning reduces the need for large
amounts of data for training a DNN by repurposing a pre-
trained DNN for a diﬀerent, but related, classiﬁcation task.

1http://www.robots.ox.ac.uk/˜vgg/software/vgg face/

This is performed by copying an initial set of layers from the
existing DNN, appending new layers to them, and training
the parameters of the new layers for the new task. Conse-
quently, the layers copied from the old network serve for fea-
ture extraction and the extracted features are fed to the new
layers for classiﬁcation. Previous work has shown that trans-
fer learning is eﬀective in training high-performance DNNs
from ones that have already been trained when they perform
closely related tasks [45].

We followed the suggestion of Yosinski et al. [45] to train
DNNB and DNNC . More speciﬁcally, we used the ﬁrst 37
layers of DNNA for feature extraction. Then, we appended
a fully connected layer of neurons with a sigmoid activa-
tion function followed by a softmax layer, and updated the
weights in the fully connected layer via the back-propagation
algorithm. The neurons in the fully-connected layers serve
as linear classiﬁers that classify the features extracted by
DNNA into identities, and the softmax layer transforms their
output into a probability distribution. Training more lay-
ers (i.e., more parameters to tune) was prohibitive, as the
amount of data we could collect from people available for
testing physically realizable attacks was limited. We used
about 40 images per subject for training—an amount of im-
ages that was small enough to collect, but high enough to
train highly performing DNNs. On images held aside for
testing, DNNB achieved classiﬁcation accuracy of 97.43%,
and DNNC achieved accuracy of 96.75%.

Similarly to Parkhi et al., we used 2d aﬃne alignment to
align face images to a canonical pose at the input of the
DNNs [33]. Additionally, we resized input images to 224 ×
224 (the input dimensionality of the DNNs we use). We used
MatConvNet, an NN toolbox for MATLAB, to train DNNB
and DNNC , run the DNNs, and test our attacks [41].
4.2 Attacking White-box FRSs

Following Parkhi et al. we adopt the softmaxloss score
to measure the correctness of classiﬁcations [33]. Formally,
given an input x of class cx that is classiﬁed as f (x) (a vector
of probabilities), softmaxloss is deﬁned as follows:
e(cid:104)hcx ,f (x)(cid:105)
c=1 e(cid:104)hc,f (x)(cid:105)

softmaxloss(f (x), cx) = − log

(cid:17)

(cid:16)

(cid:80)N

where (cid:104)·,·(cid:105) denotes inner product between two vectors, N is
the number of classes, and (as discussed in Section 3.2) hc is
the one-hot vector of class c. It follows that softmaxloss is
lower when the DNN classiﬁes x correctly, and higher when
the classiﬁcation is incorrect. We use softmaxloss to deﬁne
the impersonation and dodging objectives of attackers.

Impersonation An attacker who wishes to impersonate
a target t needs to ﬁnd how to perturb the input x via an
addition r to maximize the probability of class ct. Szegedy et
al. deﬁned this as minimizing the distance between f (x + r)
and ht (see Sec. 3.2). Similarly, we deﬁne the optimization
problem to be solved for impersonation as:

argmin

r

softmaxloss(f (x + r), ct)

In other words, we seek to ﬁnd a modiﬁcation r of image x
that will minimize the distance of the modiﬁed image to the
target class ct.

Dodging In contrast to an attacker who wishes to imper-
sonate a particular target, an attacker who wishes to dodge

1531recognition is only interested in not being recognized as her-
self. To accomplish this goal, the attacker needs to ﬁnd how
to perturb the input x to minimize the probability of the
class cx. Such a perturbation r would maximize the value
of softmaxloss(f (x + r), cx). To this end, we deﬁne the op-
timization problem for dodging as:

(cid:0) − softmaxloss(f (x + r), cx)(cid:1)

argmin

r

To solve these optimizations, we use the Gradient Descent
(GD) algorithm [4]. GD is guaranteed to ﬁnd a global min-
imum only when the objective is convex, but in practice
often ﬁnds useful solutions regardless. In a nutshell, GD is
an iterative algorithm that takes an initial solution to r and
iteratively reﬁnes it to minimize the objective. GD itera-
tively evaluates the gradient, g, and updates the solution
by r = r − αg, for a positive value α (i.e., r is updated
by “stepping” in the direction of the steepest descent). GD
stops after convergence (i.e., changes in the objective func-
tion become negligible) or after a ﬁxed number of iterations.
In our work, we ﬁx the number of iterations.
4.3 Facilitating Physical Realizability

Accomplishing impersonation or dodging in a digital en-
vironment does not guarantee that the attack will be physi-
cally realizable, as our experiments in Sec. 5.1 show. There-
fore, we take steps to enable physical realizability. The ﬁrst
step involves implementing the attacks purely with facial ac-
cessories (speciﬁcally, eyeglass frames), which are physically
realizable via 3d- or even 2d-printing technologies. The sec-
ond step involves tweaking the mathematical formulation of
the attacker’s objective to focus on adversarial perturbations
that are a) robust to small changes in viewing condition; b)
smooth (as expected from natural images); and c) realiz-
able by aﬀordable printing technologies.
In what follows,
we describe the details of each step.

4.3.1 Utilizing Facial Accessories
While an attacker who perturbs arbitrary pixels that over-
lay her face can accomplish impersonation and dodging at-
tacks, the perturbations may be impossible to implement
successfully in practice (see Sec. 5.1). To address this, we
utilize perturbed facial accessories (in particular, eyeglass
frames) to implement the attacks. One advantage of facial
accessories is that they can be easily implemented. In partic-
ular, we use a commodity inkjet printer (Epson XP-830) to
print the front plane of the eyeglass frames on glossy paper,
which we then aﬃx to actual eyeglass frames when phys-
ically realizing attacks. Moreover, facial accessories, such
as eyeglasses, help make attacks plausibly deniable, as it is
natural for people to wear them.

Unless otherwise mentioned, in our experiments we use
the eyeglass frames depicted in Fig. 1. These have a similar
design to frames called “geek” frames in the eyewear indus-
try.2 We select them because of the easy availability of a
digital model. After alignment, the frames occupy about
6.5% of the pixels of the 224 × 224 face images, similarly to
real frames we have tested. This implies that the attacks
perturb at most 6.5% of the pixels in the image.

To ﬁnd the color of the frames necessary to achieve imper-
sonation or dodging we ﬁrst initialize their color to a solid
color (e.g., yellow). Subsequently, we render the frames onto

2E.g., see: http://goo.gl/Nsd20I

Figure 1: The eyeglass frames used to fool FRSs (before tex-
ture perturbation). By Clker-Free-Vector-Images; source:
https://goo.gl/3RHKZA.

the image of the subject attempting the attack and itera-
tively update their color through the GD process. In each
iteration, we randomize the position of the frames around
a reference position by moving them by up to three pixels
horizontally or vertically, and by rotating them up to four
degrees. The rationale behind doing so is to craft adversar-
ial perturbations that are tolerant to slight movements that
are natural when physically wearing the frames.
4.3.2 Enhancing Perturbations’ Robustness
Due to varying imaging conditions, such as changes in
expression and pose, two images of the same face are unlikely
to be exactly the same. As a result, to successfully realize
the attacks, attackers need to ﬁnd perturbations that are
independent of the exact imaging conditions. In other words,
an attacker would need to ﬁnd perturbations that generalize
beyond a single image, and can be used to deceive the FRSs
into misclassifying many images of the attacker’s face.

Thus far, the techniques to ﬁnd perturbations were spe-
ciﬁc to a given input. To enhance the generality of the per-
turbations, we look for perturbations that can cause any
image in a set of inputs to be misclassiﬁed. To this end,
an attacker collects a set of images, X, and ﬁnds a single
perturbation that optimizes her objective for every image
x ∈ X. For impersonation, we formalize this as the follow-
ing optimization problem (dodging is analogous):

(cid:88)

x∈X

argmin

r

softmaxloss(f (x + r), l)

4.3.3 Enhancing Perturbations’ Smoothness
Natural images (i.e., those captured in reality) are known
to comprise smooth and consistent patches, where colors
change only gradually within patches [24]. Therefore, to
enhance plausible deniability, it is desirable to ﬁnd pertur-
bations that are smooth and consistent. In addition, due to
sampling noise, extreme diﬀerences between adjacent pixels
in the perturbation are unlikely to be accurately captured by
cameras. Consequently, perturbations that are non-smooth
may not be physically realizable.

To maintain the smoothness of perturbations, we update
the optimization to account for minimizing total variation
(TV ) [24]. For a perturbation r, TV (r) is deﬁned as:

(ri,j − ri+1,j)2 + (ri,j − ri,j+1)2(cid:17) 1

TV (r) =

(cid:88)

(cid:16)

2

i,j

where ri,j are is a pixel in r at coordinates (i, j). TV (r)
is low when the values of adjacent pixels are close to each
other (i.e., the perturbation is smooth), and high otherwise.
Hence, by minimizing TV (r) we improve the smoothness of
the perturbed image and improve physical realizability.
4.3.4 Enhancing Perturbations’ Printability
The range of colors that devices such as printers and
screens can reproduce (the color gamut) is only a subset

1532of the [0, 1]3 RGB color space. Thus, to be able to success-
fully use a printer to realize adversarial perturbations, it is
desirable to craft perturbations that are comprised mostly
of colors reproducible by the printer. To ﬁnd such perturba-
tions, we deﬁne the non-printability score (NPS ) of images
to be high for images that contain unreproducible colors,
and low otherwise. We then include minimizing the non-
printability score as part of our optimization.
Let P ⊂ [0, 1]3 be the set of printable RGB triplets. We

deﬁne the NPS of a pixel ˆp as:

NPS (ˆp) =

|ˆp − p|

(cid:89)

p∈P

If ˆp belongs to P , or if it is close to some p ∈ P , then NPS (p)
will be low. Otherwise, NPS (p) will be high. We intuitively
generalize the deﬁnition of NPS of a perturbation as the
sum of NPS s of all the pixels in the perturbation.

In practice, to approximate the color gamut of a printer,
we print a color palette that comprises a ﬁfth of the RGB
color space (with uniform spacing). Subsequently, we cap-
ture an image of the palette with a camera to acquire a set of
RGB triplets that can be printed. As the number of unique
triplets is large (≥ 10K), the computation of NPS becomes
computationally expensive. To this end, we quantize the set
of colors to a set of 30 RGB triplets that have a minimal
variance in distances from the complete set, and use only
those in the deﬁnition of NPS ; this is an optimization that
we have found eﬀective in practice.

In addition to having limited gamut, printers do not re-
produce colors faithfully, i.e., the RGB values of a pixel re-
quested to be printed do not correspond exactly to the RGB
values of the printed pixel. To allow us to realize perturba-
tions with high ﬁdelity, we create a map m that maps colors
(i.e., RGB triplets) requested to be printed to colors that
are actually printed. We then utilize this map to realize
perturbations with high ﬁdelity.
In particular, to print a
perturbation as faithfully as possible, we replace the value
p of each pixel in the perturbation with ˜p, such that the
printing error, |p− m(˜p)|, is minimized. This process can be
thought of as manual color management [20].

5. EVALUATION

We separately evaluate attacks that take place purely in
the digital domain (Sec. 5.1) and physically realized attacks
(Sec. 5.2).
5.1 Digital-Environment Experiments

We ﬁrst discuss experiments that assess the diﬃculty of
deceiving FRSs in a setting where the attacker can manipu-
late the digital input to the system, i.e., modify the images
to be classiﬁed on a per-pixel level. Intuitively, an attacker
who cannot fool FRSs successfully in such a setting will also
struggle to do so physically in practice.

Experiment Description.

We ran eight dodging and impersonation attacks (see Sec.
4.2) on the white-box DNNs presented in Sec. 4.1. Be-
tween experiments, we varied (1) the attacker’s goal (dodg-
ing or impersonation), (2) the area the attacker can per-
turb (the whole face or just eyeglass frames that the subject
wears), and (3) the DNN that is attacked (DNNA, DNNB,
or DNNC ). The frames we use are depicted in Fig. 1.

Figure 2: A dodging attack by perturbing an entire face.
Left: an original image of actress Julia Jones (by Wenner
Media; source: http://goo.gl/OFOIGB). Middle: A per-
turbed image for dodging. Right: The applied perturbation,
after multiplying the absolute value of pixels’ channels ×20.

We measured the attacker’s success (success rate) as the
fraction of attempts in which she was able to achieve her
goal. For dodging, the goal is merely to be misclassiﬁed,
i.e., the DNN, when computing the probability of the image
belonging to each target class, should ﬁnd that the most
probable class is one that does not identify the attacker. For
impersonation, the goal is to be classiﬁed as a speciﬁc target,
i.e., the DNN should compute that the most probable class
for the input image is the class that identiﬁes the target.
In impersonation attempts, we picked the targets randomly
from the set of people the DNN recognizes.

To simulate attacks on DNNA and DNNC , we chose at
random 20 subjects for each experiment from the 2622 and
143 subjects, respectively, that the DNNs were trained on.
We simulated attacks on DNNB with all ten subjects it was
trained to recognize. To compute statistics that generalize
beyond individual images, we performed each attack (e.g.,
each attempt for subject X to impersonate target Y) on
three images of the subject and report the mean success
rate across those images. We ran the gradient descent pro-
cess for at most 300 iterations, as going beyond that limit
has diminishing returns in practice. A summary of the ex-
periments and their results is presented in Table 1.

Experiment Results.

In experiments 1 and 2 we simulated dodging and imper-
sonation attacks in which the attacker is allowed to perturb
any pixel on her face. The attacker successfully achieved
her goal in all attempts. Moreover, the adversarial exam-
ples found under these settings are likely imperceptible to
humans (similar to adversarial examples found by Szegedy
et al. [39]): the mean perturbation of a pixel that overlaid
the face was 1.7, with standard deviation 0.93. An example
is shown in Fig. 2.

We believe that the types of attacks examined in exper-
iments 1 and 2 are far from practical. Because the pertur-
bations are too subtle and lack structure, they may be too
complicated to physically realize, e.g., it is likely to be im-
possible to modify a human face in exactly the way required
by the attack. Moreover, the amount by which pixels are
perturbed is often much smaller than the error that occurs
when printing a perturbation and then capturing it with a
scanner or a camera. Our experiments show that the average
error per pixel in a perturbation when printed and scanned
is 22.9, and its standard deviation is 38.22. Therefore, even
if the attacker can successfully realize the perturbation, she
is still unlikely to be able to deceive the system.

To this end, in experiments 3–8 we simulated dodging
and impersonation attacks in which we perturbed only eye-
glass frames of eyeglasses worn by each subject. With the

1533Experiment # Area perturbed

Goal

Model # Attackers

Success rate

1
2

3
4
5
6
7
8

Entire face
Entire face

Eyeglass frames
Eyeglass frames
Eyeglass frames
Eyeglass frames
Eyeglass frames
Eyeglass frames

Dodging

Dodging
Dodging
Dodging

DNNA
Impersonation DNNA
DNNA
DNNB
DNNC
Impersonation DNNA
Impersonation DNNB
Impersonation DNNC

20
20

20
10
20
20
10
20

100.00%
100.00%

100.00%
100.00%
100.00%
91.67%
100.00%
100.00%

Table 1: A summary of the digital-environment experiments attacking DNNA, DNNB, and DNNC under the white-box
scenario. In each attack we used three images of the subject that we sought to misclassify; the reported success rate is the
mean success rate across those images.

Figure 3: An impersonation using frames. Left: Actress
Kaylee Defer (by Fanpop; source: http://goo.gl/nTqqtF).
Image classiﬁed correctly with probability 1. Middle:
Perturbing frames to impersonate (actress) Nancy Travis.
Right: The target (by Mingle Media TV; source: https:
//goo.gl/2BEx9a).

exception of experiment 6, the attacker was able to dodge
recognition or impersonate targets in all attempts. In exper-
iment 6 impersonators succeeded in about 91.67% of their
attempts to fool DNNA using perturbed eyeglasses. We hy-
pothesize that due to the large number of classes DNNA
recognizes, the image space between some attacker-target
pairs is occupied by other classes. Therefore, impersonating
these targets requires evading several classiﬁcation bound-
aries, making impersonation attacks more diﬃcult.

Fig. 3 shows an example of a successful impersonation at-

tempt using eyeglass frames.

5.2 Physical-Realizability Experiments

As discussed in Sec. 4.3, to physically realize an attack,
we utilize a set of perturbed eyeglass frames, ensure that
the perturbation is smooth and eﬀective for misclassifying
more than one image, and enhance the reproducibility of
the perturbation’s colors by the printing device. To achieve
these goals and impersonate a target t, an attacker ﬁnds a
perturbation by solving the following optimization problem:

argmin

r

(cid:16)(cid:0)(cid:88)

softmaxloss(x + r , ct )(cid:1)+
(cid:17)

x∈X
κ1 · TV (r) + κ2 · NPS (r)

where κ1 and κ2 are constants for balancing the objectives
and X is a set of images of the attacker. The formulation
for dodging is analogous.

In this section we report on experiments for evaluating the
eﬃcacy of this approach in fooling DNNB and DNNC under
semi-controlled imaging conditions.

Experiment Description.

The ﬁrst three authors, whom DNNB and DNNC were
trained to recognize, participated in the experiments;3 we
refer to them as subjects SC , SB, and SA, respectively. For
each subject we attempted two dodging attacks and two
impersonation attacks—one of each type of attack on each
of DNNB and DNNC . The targets in impersonation attacks
were randomly selected (see Table 2).

We collected images of the subjects using a Canon T4i
camera. To prevent extreme lighting variations, we collected
images in a room without exterior windows. Subjects stood
a ﬁxed distance from the camera and were told to main-
tain a neutral expression and to make slight pose changes
throughout the collection. While these collection conditions
are only a subset of what would be encountered in practice,
we believe they are realistic for some scenarios where FRS
technology is used, e.g., within a building for access control.
For each subject, we collected 30–50 images in each of ﬁve
sessions. In the ﬁrst session, we collected a set of images that
was used for generating the attacks (referred to as the set
X in the mathematical representation). In this session, the
subjects did not wear the eyeglass frames.
In the second
and third sessions, the subjects wore eyeglass frames to at-
tempt dodging against DNNB and DNNC , respectively. In
the fourth and the ﬁfth sessions, the subjects wore frames
to attempt impersonation against DNNB and DNNC .

We physically realized attacks by printing the eyeglass
frames with an Epson XP-830 printer on Epson Glossy photo
paper. Realizing these attacks is cheap and aﬀordable; the
approximate cost for printing an eyeglass frame is $0.22.
Once printed, we cut out the frames and aﬃxed them to the
frames of an actual pair of eyeglasses. Examples of realiza-
tions are shown in Fig. 4.

To ﬁnd the perturbation, the parameters κ1 and κ2 in the
optimization were set to 0.15 and 0.25, respectively. The
computational overhead of mounting attacks prohibited a
broad exploration of the parameter space, but we found
these values eﬀective in practice.
In addition, we limited
the number of iterations of the GD process to 300.

Experiment Results.

To evaluate DNNB and DNNC in a non-adversarial set-
ting, we classiﬁed the non-adversarial images collected in the
ﬁrst session. All the face images of the three subjects were

3The fourth author was excluded from the experiments due
to logistical challenges posed by physical distance.

1534DNN

DNNB

DNNC

Subject (attacker) info
Subject

Identity

Dodging results

Impersonation results

SR

E(p(correct-class))

Target

SR

SRT

E(p(target))

SA
SB
SC
SA
SB
SC

3rd author
2nd author
1st author
3rd author
2nd author
1st author

100.00%
97.22%
80.00%
100.00%
100.00%
100.00%

0.01
0.03
0.35
0.03
<0.01
<0.01

Milla Jovovich

SC

Clive Owen

John Malkovich

Colin Powell
Carson Daly

87.87% 48.48%
88.00% 75.00%
16.13%
0.00%
100.00% 100.00%
16.22%
100.00% 100.00%

0.00%

0.78
0.75
0.33
0.99
0.08
0.90

Table 2: A summary of the physical realizability experiments. To the left, we report the DNN attacked and the identity of
the subjects (the attackers in the simulation). When not attempting to fool both DNNs, the subjects were originally classiﬁed
correctly with mean probability >0.85. SR is the success rate. SRT is the success rate when using a threshold. E(p(class)) is
the mean (expected) probability of the class when classifying all images. Results for SC when attacking DNNB were achieved
with glasses that occupy 10% of the area of the image being classiﬁed; results for the other experiments were achieved with
glasses that occupy 6% of the image.

classiﬁed correctly. The mean probability of the correct class
across the classiﬁcation attempts was above 0.85, which im-
plies that naive attempts at impersonation or dodging are
highly unlikely to succeed.
In contrast, our experiments
showed that an attacker who intentionally attempts to de-
ceive the system will usually succeed. Table 2 summarizes
these results. Fig. 4 presents examples of successful dodging
attacks and impersonations.

Dodging All three subjects successfully dodged face recog-
nition by wearing perturbed eyeglass frames. When wearing
frames for dodging, as shown in Fig. 4a, all of SA’s images
collected in the second and the third sessions were misclassi-
ﬁed by DNNB and DNNC , respectively. When dodging, the
mean probability DNNB assigned to the SA’s class dropped
remarkably from 1 to 0.01. Similarly, the mean probability
assigned to cSA by DNNC dropped from 0.85 to 0.03. Thus,
the dodging attempts made it highly unlikely that the DNNs
would output cSA , the correct classiﬁcation result. SB was
able to dodge recognition in 97.22% of the attempts against
DNNB and in 100% of the attempts against DNNC . When
classifying SB’s images, the mean probability assigned by
the DNNs to cSB became ≤ 0.03 as a result of the attack,
also making it unlikely to be the class assigned by the DNN.
SC ’s attempts in dodging recognition against DNNC were
also successful: all his images collected in the third session
were misclassiﬁed, and the probability assigned to cSC was
low (<0.01). However, when wearing a perturbed version of
the frames shown in Fig. 1; none of SC ’s images collected
as part of the second session misled DNNB. We conjecture
that because SC was the only subject who wore eyeglasses
among the subjects used for training DNNB, cSC became
a likely class when classifying images showing people wear-
ing eyeglasses. Therefore, it became particularly hard for
SC to fool DNNB by perturbing only a small area of the
image. Nevertheless, by increasing the size of the frames
such that they occupied 10% of the area of the aligned im-
age (frames shown in Fig. 5) it became possible for SC to
achieve physically realizable dodging at the cost of decreased
inconspicuousness. Using the larger frames, SC was able to
dodge recognition in 80% of the images (mean probability
of cSC dropped to 0.35).
Impersonation To simulate impersonation attempts, each
subject was assigned two random targets: one for fooling
DNNB and one for fooling DNNC . SA, a 41-year-old white
male, was assigned to impersonate Milla Jovovich, a 40-year-

old white female, and John Malkovich, a 62-year-old white
male; SB, a 24-year-old South Asian female, was assigned
to impersonate SC , a 24-year-old Middle Eastern male, and
Colin Powell, a 79-year-old white male; and SC was assigned
to impersonate Clive Owen, a 51-year-old male, and Carson
Daly, a 43-year-old male.

Both of SA’s impersonation attempts were successful: 87.87%

of his images collected in the fourth session were misclassi-
ﬁed by DNNB as Milla Jovovich (the mean probability of the
target was 0.78), and all the images collected in the ﬁfth ses-
sion were misclassiﬁed as John Malkovich (mean probability
of 0.99). In contrast, SB and SC had mixed success. On the
one hand, SB misled DNNB by successfully impersonating
SC in 88% of her attempts (the mean probability of the tar-
get was 0.75), and SC misled DNNC into misclassifying him
as Carson Daly in all of his attempts (mean probability of
0.99). On the other hand, they were able to successfully im-
personate Colin Powell and Clive Owen4 (respectively) only
in about one of every six attempts. This success rate may
be suﬃcient against, e.g., access-control systems that do not
limit the number of recognition attempts, but may not be
suﬃcient against systems that limit the number of attempts
or in surveillance applications. We hypothesize that some
targets are particularly diﬃcult for some subjects to imper-
sonate. We also believe, however, that even in those cases
further reﬁnement of the attacks can lead to greater success
than we have so far measured.

In practice, to tune the security of the FRS, system opera-
tors may set a minimum threshold that the maximum prob-
ability in the DNN’s output will have to exceed for a classiﬁ-
cation result to be accepted. Such a threshold balances secu-
rity and usability: If the threshold is high, misclassiﬁcation
occurs less often, but correct classiﬁcations may be rejected,
thus harming usability. On the other hand, for low thresh-
olds, correct classiﬁcations will be accepted most of the time,
but misclassiﬁcations are more likely to occur, which would
harm security. Therefore, system operators usually try to
ﬁnd a threshold that balances the usability and the security
of the deployed FRS. Using the test data, we found that
by setting the threshold to 0.85, the false acceptance rate
of DNNB became 0, while true acceptance became 92.31%.
This threshold strikes a good balance between usability and
security [18]; by using it, false acceptance (of zero-eﬀort im-

4Similarly to dodging, SC required larger frames to imper-
sonate the target.

1535(a)

(b)

(c)

(d)

Figure 4: Examples of successful impersonation and dodging attacks. Fig. (a) shows SA (top) and SB (bottom) dodging
against DNNB. Fig. (b)–(d) show impersonations.
Impersonators carrying out the attack are shown in the top row and
corresponding impersonation targets in the bottom row. Fig. (b) shows SA impersonating Milla Jovovich (by Georges Biard;
source: https://goo.gl/GlsWlC); (c) SB impersonating SC ; and (d) SC impersonating Carson Daly (by Anthony Quintano;
source: https://goo.gl/VfnDct).

Figure 5: The eyeglass frames used by SC for dodging recog-
nition against DNNB.

postors) never occurs, while true acceptance remains high.
Following a similar procedure, we found that a threshold of
0.90 achieved a reasonable tradeoﬀ between security and us-
ability for DNNC ; the true acceptance rate became 92.01%
and the false acceptance rate became 4e−3. Attempting
to decrease the false acceptance rate to 0 reduced the true
acceptance rate to 41.42%, making the FRS unusable.

Using thresholds changes the deﬁnition of successful im-
personation: to successfully impersonate the target t, the
probability assigned to ct must exceed the threshold. Eval-
uating the previous impersonation attempts under this def-
inition, we found that success rates generally decreased but
remained high enough for the impersonations to be consid-
ered a real threat (see Table 2). For example, SB’s success
rate when attempting to fool DNNB and impersonate SC
decreased from 88.00% without threshold to 75.00% when
using a threshold.

Time Complexity The DNNs we use in this work are
large, e.g., the number of connections in DNNB, the small-
est DNN, is about 3.86e8. Thus, the main overhead when
solving the optimization problem via GD is computing the
derivatives of the DNNs with respect to the input images.
For NI images used in the optimizations and NC connec-
tions in the DNN, the time complexity of each GD iteration
is O(NI ∗NC ). In practice, when using about 30 images, one
iteration of GD on a MacBook Pro (equipped with 16GB of
memory and a 2.2GHz Intel i7 CPU) takes about 52.72 sec-
onds. Hence, running the optimization up to 300 iterations
may take about 4.39 hours.

6. EXTENSION TO BLACK-BOX MODELS
So far we have examined attacks where the adversary has
access to the model she is trying to deceive.
In general,
previous work on fooling ML systems has assumed knowl-
edge of the architecture of the system (see Sec. 2). In this
section we demonstrate how similar attacks can be applied
in a black-box scenario. In such a scenario, the adversary
would typically have access only to an oracle O which out-
puts a result for a given input and allows a limited number of
queries. The threat model we consider here is one in which
the adversary has access only to the oracle.

We next brieﬂy describe a commercial FRS that we use in
our experiments (Sec. 6.1), and then describe and evaluate
preliminary attempts to carry out impersonation attacks in
a black-box setting (Sec. 6.2–6.3).
6.1 Face++: A Commercial FRS

Face++ is a cross-platform commercial state-of-the-art
FRS that is widely used by applications for facial recog-
nition, detection, tracking, and analysis [46].
It has been
shown to achieve accuracy over 97.3% on LFW [8]. Face++
allows users to upload training images and labels and trains
an FRS that can be queried by applications. Given an im-
age, the output from Face++ is the top three most proba-
ble classes of the image along with their conﬁdence scores.
Face++ is marketed as “face recognition in the cloud.” Users
have no access to the internals of the training process and
the model used, nor even to a precise explanation of the
meaning of the conﬁdence scores. Face++ is rate-limited to
50,000 free queries per month per user.

To train the Face++ model, we used the same training

data used for DNNB in Sec. 4.1 to create a 10-class FRS.
6.2 Impersonation Attacks on Face++

The goal of our black-box attack is for an adversary to
alter an image to which she has access so that it is mis-
classiﬁed. We attempted dodging attacks with randomly
colored glasses and found that it worked immediately for
several images. Therefore, in this section we focus on the
problem of impersonation from a given source to a target.

1536We treat Face++ as an example black-box FRS, with its
query function modeled as the oracle O(x). The oracle re-
turns candidates, an ordered list of three classes numbered
from 1 to 3 in decreasing order of conﬁdence.

Our algorithm for attacking Face++ uses particle swarm
optimization (PSO) [6] as a subroutine. We begin by sum-
marizing this technique.

Particle Swarm Optimization (PSO) Particle swarm
optimization is a heuristic and stochastic algorithm for ﬁnd-
ing solutions to optimization problems by mimicking the
behavior of a swarm of birds [6].
It iterates on a set of
candidate solutions, called particles, and collectively called
a seed, that it updates based on the evaluation of an ob-
jective function. Each particle is a candidate solution and
occupies a position in the solution space. The value of a
particle is the result of evaluating the objective function on
that particle’s position in the solution space. In each iter-
ation, each particle is updated by adding a velocity to its
position. The velocity is a weighted and randomized linear
combination of the distance between (1) the current posi-
tion of the particle and its best position thus far (Pbest ) and
(2) the current position of the particle and the best position
taken by any particle thus far (Gbest ), where “best” indicates
that the objective function evaluates to the smallest value.
Once a termination criterion is met, Gbest should hold the
solution for a local minimum.

We choose this over other black-box optimization meth-
ods such as surrogate models [3]—which require that the
adversary has the training data and enough computational
resources to train an eﬀective surrogate—and genetic algo-
rithms [2]—which though similar to PSO are much less com-
putationally eﬃcient [15, 34].

Since Face++ only returns the top three classes, PSO can
make progress if target is in the top three results reported
for any particle in the ﬁrst iteration. However, when target
is not in the top three for any set of initial solutions, the
algorithm does not get any feedback on appropriate direc-
tions. To address this, we implement an algorithm we call
recursive impersonation (Alg. 1). The goal of the algorithm
to is reach the ﬁnal target by attempting multiple interme-
diate impersonations on varying targets in successive runs
of PSO. This is done in the hope that attempting an in-
termediate impersonation will move the swarm away from
the previous solution space and toward candidate solutions
that may result in the target being returned in the top three
classes. If the target does not initially appear in the can-
didate list that results from querying the oracle with the
original image, we select the class with the second highest
conﬁdence to be the intermediate target. On subsequent
PSO runs, the most commonly occurring class labels that
have not yet been used as targets become the new interme-
diate targets.

In our implementation, we modify the PSO subroutine
to globally keep track of all the particles used in the last
iteration of PSO as well as all particles throughout all PSO
iterations for which invoking the oracle resulted in target
appearing in the candidate list. The invoking algorithm has
access to these saved particles and uses them in order to
select a new intermediate impersonation target or to provide
a seed for the next impersonation attempt.

On each run, PSO aims to minimize an objective function
deﬁned by f (x + r), where r is the perturbation applied to
the image x. f (·) is computed based on the output from the

oracle O. The value of this objective at every particle is then
used to move the swarm in a new direction during the next
iteration of PSO. We experimented with several deﬁnitions
of f (·). In practice, we found the following to be the most
eﬀective:

f (x) =

rank ·
scoretop
scoretarget
maxObjective

if target ∈ candidates
if target /∈ candidates

(cid:40)

The function uses the input x to query O for the candidates.
The variable score top denotes the conﬁdence score of the top-
ranked item in candidates.
If the target is in candidates
is its conﬁdence score and rank its rank
then score target
in candidates. When target is successfully impersonated,
f (·) receives its minimal value of 1.
If target is not in
the top three candidates, the function should evaluate to
maxObjective, which we set to be a suﬃciently large value
to indicate a result far less optimal than Pbest or Gbest .

Algorithm 1: Recursive Impersonation
1 Initialize epoch = 0, numParticles, epochs max and seed.
2 Set candidates = O(imageoriginal ).
3 if target ∈ candidates then target current = target;
4 else target current = 2nd most probable class of candidates;
5 while epoch ≤ epoch max do

solution was found. exit.

Run PSO subroutine with target current and seed.
if any particle impersonated target during PSO then
else if target ∈ candidates of any query during PSO then
target current = target. Clear seed.
seed ⊇ particles that produced this candidate from the
current PSO run.

else

if new candidate emerges from current PSO run then
target current = new candidate. Clear seed.
seed ⊇ particles that produced this candidate from
the current PSO run.

else

no solution was found. exit.

end

end
epoch = epoch + 1

6
7
8
9
10
11

12
13
14
15

16
17
18
19
20
21 end

6.3 Results
Experiment Description To evaluate our methodology,
we picked four {source, target} pairs from among the sub-
jects on which Face++ was trained. Two of the pairs were
chosen at random. The other two pairs were chosen as chal-
lenging impersonations by making sure that the target was
not one of the top three classes reported by Face++ for the
source image. To make the attack realistic in terms of possi-
bility of physical realization, we restricted the perturbations
to a pair of glasses. We did not attempt to physically realize
the attack, however.

We ran our experiments with 25 particles. The particles
were initialized by randomly generating 25 distinct pairs of
glasses with smooth color variations. We ran the algorithm
for a maximum of 15 epochs or attempts, and set the it-
eration limit of PSO to 50 and maxObjective to 50. We
also assigned weights in computing the velocity such that a
higher weight was given to Gbest than Pbest .

Experiment Results The results from the experiments
are shown in Table 3. All attempted impersonations suc-

1537ceeded. Noteworthy is the low number of queries needed for
the attacks, which shows that rate-limiting access to services
will not always stop an online attack.

7. EXTENSION TO FACE DETECTION

In this section, we show how to generalize the basic ap-
proach presented in Sec. 4.2 to achieve invisibility to facial
biometric systems.
In an invisibility attack, an adversary
seeks to trick an ML system not into misclassifying one per-
son as another, but into simply failing to detect the presence
of a person.

We examine this category of attacks for two reasons. First,
most ML systems for identifying faces have two phases: de-
tecting the presence of a face and then identifying the de-
tected face. The detection phase is typically less closely tied
to training data than the recognition phase. Hence, tech-
niques to circumvent detection have the potential to apply
more broadly across multiple systems.

Second, avoiding detection corresponds naturally to one
type of motivation—the desire to achieve privacy. In seeking
to achieve privacy, a person may speciﬁcally want to avoid
causing culpability to be placed on another person. Simi-
larly, a mislabeling of a face might be more likely to arouse
suspicion or alert authorities than would the failure to notice
the presence of a face at all, as might occur at an airport
security checkpoint where faces detected by FDSs are con-
ﬁrmed against face images of passengers expected to travel,
or faces of people wanted by the authorities.

In this work, we show how to perform invisibility attacks
while attempting to maintain plausible deniability through
the use of facial accessories. We defer the examination of
the physical realizability of these attacks to future work.
7.1 The Viola-Jones Face Detector

As mentioned in Sec. 2, the Viola-Jones (VJ) face detector
was designed with eﬃciency and accuracy in mind. The key
idea to achieve both goals is to use a cascade of classiﬁers
that have an ascending order of complexity. Each classiﬁer
is trained to detect the majority of the positive instances
(presence of a face) and reject a large number of the negative
instances. To detect an object in an image, several sub-
windows are taken from the image and are evaluated by the
detector. To be detected as a positive example, the sub-
window needs to be classiﬁed as a positive example by all
the classiﬁers in the cascade. On the other hand, being
rejected by one classiﬁer in the cascade results in classifying
a sub-window as a negative example. Sub-windows that are
rejected by simple classiﬁers are not further evaluated by
the more sophisticated classiﬁers.

A classiﬁer in the cascade is composed of a combination
of weak classiﬁers. A weak classiﬁer i is a simple classiﬁer
that outputs one of two possible values, ˜ai or ˆai, based on
one feature value, fi(·), and a threshold bi. Given a classiﬁer
that is composed of C weak classiﬁers, its decision function
is deﬁned as:

Classify(x) =

( ˜ai − ˆai)(fi(x) > bi) + ˆai

> T

i=1

where T is the passing threshold of the classiﬁer, x is the
sub-window, and fi(x) > bi evaluates to 1 if true and 0
otherwise.

As explained above, the VJ detector rejects a sub-window

(cid:18) C(cid:88)

(cid:16)

(cid:17)(cid:19)

Figure 6: An example of an invisibility attack. Left: original
image of actor Kiefer Sutherland. Middle: Invisibility by
perturbing pixels that overlay the face. Right: Invisibility
with the use of accessories.

in case one of its classiﬁers rejects it. Thus, to evade de-
tection it is suﬃcient to fool one cascade stage. Since the
trained VJ is an open source (i.e., white-box) classiﬁer [19],
to ﬁnd a minimal perturbation that can be used for eva-
sion, we could potentially adapt and utilize the solution
proposed by Szegedy et al. [39]. However, to solve the op-
timization problem, we need the classiﬁcation function to
be diﬀerentiable—as previously explained in Section 3.2—
which Classify(x) is not. Therefore, we utilize the sigmoid
function, sig (as is often done in ML [37]), and formulate
the optimization problem as:

(cid:32)(cid:18) C(cid:88)

(cid:16)

i=1

argmin

r

( ˜ai− ˆai)·sig(k·(fi(x+r)−bi))+ ˆai

+c|r|

(cid:19)

(cid:17)−T

(cid:33)

(1)
where k is a positive real number that can be tuned to
control the precision of the approximation. With this ap-
proximation, we can perform gradient descent to solve the
optimization problem.
7.2 Experiment Results

By generating a perturbation to evade a speciﬁc stage of
the detector via the above technique, we are able to learn
how to tweak pixel intensities in speciﬁc regions to success-
fully evade the whole cascade. To test this approach, we
randomly selected 20 frontal images from the PubFig [21]
face dataset, and tested whether each could be permuted to
evade detection by fooling the ﬁrst classiﬁer in the cascade.
We generated perturbed images as follows: we limited the
perturbation to the area of the face, set c to 0.015 (as we
found this to yield smaller perturbations in practice), and
then performed line search on k to ﬁnd the minimal per-
turbation necessary to evade the classiﬁer, using a Limited
BFGS [29] solver to solve the optimization (Eqn. 1).

For 19 out of the 20 images it was possible to evade de-
tection. For the images that achieved evasion, the mean
perturbation—the aggregate change in the value of the R,
G, and B channels—of a pixel that overlays the face was
16.06 (standard deviation 6.35), which is relatively high and
noticeable. As Fig. 6 shows, in some cases even the minimal
perturbation necessary to evade detection required making
changes to faces that could draw increased attention.

In another version of the attack, in which we sought to
increase both the success rate and plausible deniability, we
ﬁrst added facial accessories speciﬁcally selected for their
colors and contrast to the image; we then perturbed the im-
age as in the previous attack. The accessories we used were:
eyeglasses, a blond wig, bright eye contacts, eye blacks, and
a winter hat. With this approach, it was possible to evade
detection for all 20 face images. In addition, the amount by
which each pixel needed to be perturbed dropped remark-

1538Source

Clive Owen

Drew Barrymore

SD
SD

Target

Success rate Avg. # queries

SA
SB
SC

Clive Owen

100%
100%
100%
100%

109
134
25
59

Table 3: Results of four attempted impersonation attacks, each run three times. SA–SC are the same subjects from Sec. 5.2.
SD is a 33-year-old Asian female. Each attempt had a diﬀerent (randomized) initial seed and velocities. Number of queries
is the total number of queries made of the oracle in the PSO iterations.

ably, thus contributing to plausible deniability. The mean
perturbation of a pixel to avoid detection was 3.01 (stan-
dard deviation 2.12). An illustration of evasion attacks on
VJ that utilize accessories is shown in Fig. 6.

8. DISCUSSION AND LIMITATIONS
Impact of Attacks As our reliance on technology in-
creases, we sometimes forget that it can fail. In some cases,
failures may be devastating and risk lives [36]. Our work and
previous work show that the introduction of ML to systems,
while bringing beneﬁts, increases the attack surface of these
systems (e.g., [23, 35]). Therefore, we should advocate for
the integration of only those ML algorithms that are robust
against evasion.

In this work we show that FRSs are vulnerable to a new
type of attack:
inconspicuous and physically realizable at-
tacks that lead to dodging or impersonation. Such attacks
can be especially pernicious as they can resist cursory inves-
tigation (at the least) and can provide attackers with plau-
sible deniability. While we demonstrate the eﬃcacy of these
attacks on fooling one kind of DNN architecture, previous
work has shown that adversarial examples can generalize
beyond one DNN [39, 13]. Therefore, we believe our results
imply that DNNs used for purposes beyond what we study
may also be at risk.

Possible Defenses In future work, we plan to explore de-
fenses to ameliorate risks caused by these attacks. Defenses
explored in previous work either were ineﬀective in pre-
venting adversarial examples (although they slightly harmed
their imperceptibility) [13, 39], or were tested only on sim-
ple tasks (hand-written digit recognition) or on DNNs that
are not state of the art [32]. We propose to defend against
attacks diﬀerently. Adversarial attacks on vision systems
exploit the fact that systems are sensitive to small changes
in images to which humans are not. To address this, we
propose to develop algorithms that reason about images
more similarly to humans.
In particular, we believe that
approaches that classify images based on their attributes
rather than on the intensities of their pixels may be eﬀec-
tive (e.g., [21]).

Limitations The variations in imaging conditions that we
investigate in this work are narrower than can be encoun-
tered in practice. For instance, we controlled lighting by tak-
ing images in a room that does not have external windows.
These conditions are applicable to some practical cases (e.g.,
an FRS deployed within a building). However, other practi-
cal scenarios are more challenging, and eﬀective attacks may
have to be tolerant to a larger range of imaging conditions.
For example, an attacker may not be able to control the
lighting or her distance from the camera when an FRS is

deployed in the street for surveillance purposes. We plan to
investigate such challenging scenarios in future work.

In addition, the notion of inconspicuousness is subjective,
and the only way to measure it adequately would include
performing human-subject studies. We believe that the at-
tacks demonstrated in this paper strongly suggest that it is
possible to generate attacks that will remain unnoticeable by
humans or will arouse no suspicion. We defer to future work
achieving and evaluating attacks that meet this high stan-
dard for inconspicuousness. Furthermore, we will explore
new directions to improve the color consistency, contrast,
and texture-shape of the attacks we create, which is likely
to enhance inconspicuousness.
9. CONCLUSION

In this paper, we demonstrated techniques for generat-
ing accessories in the form of eyeglass frames that, when
printed and worn, can eﬀectively fool state-of-the-art face-
recognition systems. Our research builds on recent research
in fooling machine-learning classiﬁers by perturbing inputs
in an adversarial way, but does so with attention to two
novel goals: the perturbations must be physically realizable
and inconspicuous. We showed that our eyeglass frames en-
abled subjects to both dodge recognition and to impersonate
others. We believe that our demonstration of techniques to
realize these goals through printed eyeglass frames is both
novel and important, and should inform future deliberations
on the extent to which ML can be trusted in adversarial set-
tings. Finally, we extended our work in two additional direc-
tions, ﬁrst, to so-called black-box FRSs that can be queried
but for which the internals are not known, and, second, to
defeat state-of-the-art face detection systems.
10. ACKNOWLEDGEMENTS

We would like to thank Ariel Rao and Aya Fukami for
volunteering their images and Matthew Fredrikson for dis-
cussions about adversarial machine learning.
11. REFERENCES
[1] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman.

Eigenfaces vs. ﬁsherfaces: Recognition using class
speciﬁc linear projection. IEEE Trans. Pattern
Analysis and Machine Intelligence, 19(7), 1997.
[2] A. D. Bethke. Genetic Algorithms As Function

Optimizers. PhD thesis, University of Michigan, 1980.
[3] A. J. Booker, J. Dennis Jr, P. D. Frank, D. B. Seraﬁni,
V. Torczon, and M. W. Trosset. A rigorous framework
for optimization of expensive functions by surrogates.
Structural optimization, 17(1):1–13, 1999.

[4] L. Bottou. Large-scale machine learning with

stochastic gradient descent. In Proc. COMPSTAT,
2010.

1539[5] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr,

C. Shields, D. Wagner, and W. Zhou. Hidden voice
commands. In Proc. USENIX Security, 2016.

[6] R. Eberhart and J. Kennedy. A new optimizer using

particle swarm theory. In Proc. MHS, 1995.

Framework. https://github.com/mobilesec/
authentication-framework-module-face.

[27] NEC. Face recognition.

http://www.nec.com/en/global/solutions/biometrics/
technologies/face recognition.html.

[7] N. Erdogmus and S. Marcel. Spooﬁng in 2d face

[28] NEURO Technology. SentiVeillance SDK.

recognition with 3d masks and anti-spooﬁng with
kinect. In Proc. IEEE BTAS, 2013.

[8] H. Fan, Z. Cao, Y. Jiang, Q. Yin, and C. Doudou.
Learning deep face representation. arXiv preprint
arXiv:1403.2802, 2014.

http://www.neurotechnology.com/sentiveillance.html.

[29] J. Nocedal. Updating quasi-newton matrices with

limited storage. Mathematics of computation,
35(151):773–782, 1980.

[30] OpenALPR. OpenALPR - Automatic License Plate

[9] A. Fawzi, O. Fawzi, and P. Frossard. Fundamental

Recognition. http://www.openalpr.com/.

limits on adversarial robustness. In Proc. ICML,
Workshop on Deep Learning, 2015.

[10] R. Feng and B. Prabhakaran. Facilitating fashion

camouﬂage art. In ACM MM, 2013.

[31] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson,

Z. B. Celik, and A. Swami. The limitations of deep
learning in adversarial settings. In Proc. IEEE Euro
S&P, 2015.

[11] M. Fredrikson, S. Jha, and T. Ristenpart. Model

[32] N. Papernot, P. McDaniel, X. Wu, S. Jha, and

inversion attacks that exploit conﬁdence information
and basic countermeasures. In Proc. ACM CCS, 2015.

[12] J. Galbally, C. McCool, J. Fierrez, S. Marcel, and

J. Ortega-Garcia. On the vulnerability of face
veriﬁcation systems to hill-climbing attacks. Pattern
Recognition, 43(3):1027–1038, 2010.

[13] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining

and harnessing adversarial examples. In ICLR, 2015.

[14] A. Harvey. CV Dazzle: Camouﬂage from face

detection. Master’s thesis, New York University, 2010.
Available at: http://cvdazzle.com.

[15] R. Hassan, B. Cohanim, O. De Weck, and G. Venter.
A comparison of particle swarm optimization and the
genetic algorithm. In Proc. MDO, 2005.

[16] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast

learning algorithm for deep belief nets. Neural
Computation, 18(7):1527–1554, 2006.

[17] G. B. Huang, M. Ramesh, T. Berg, and

E. Learned-Miller. Labeled faces in the wild: A
database for studying face recognition in
unconstrained environments. Technical Report 07-49,
University of Massachusetts, Amherst, October 2007.

[18] L. Introna and H. Nissenbaum. Facial recognition

technology: A survey of policy and implementation
issues. 2010. https://goo.gl/eIrldb.

[19] Itseez. OpenCV: Open Source Computer Vision.

http://opencv.org/.

[20] N. Koren. Color management and color science. http:

//www.normankoren.com/color management.html.
[21] N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K.

Nayar. Attribute and simile classiﬁers for face
veriﬁcation. In Proc. ICCV, 2009.

A. Swami. Distillation as a defense to adversarial
perturbations against deep neural networks. In Proc.
IEEE S&P, 2016.

[33] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep

face recognition. In Proc. BMVC, 2015.

[34] L. M. Rios and N. V. Sahinidis. Derivative-free

optimization: a review of algorithms and comparison
of software implementations. Journal of Global
Optimization, 56(3):1247–1293, 2013.

[35] N. Rndic and P. Laskov. Practical evasion of a

learning-based classiﬁer: A case study. In Proc. IEEE
S&P, 2014.

[36] P. Robinette, W. Li, R. Allen, A. M. Howard, and

A. R. Wagner. Overtrust of robots in emergency
evacuation scenarios. In Proc. HRI, 2016.

[37] D. E. Rumelhart, G. E. Hinton, and R. J. Williams.

Learning internal representations by error
propagation. Technical report, DTIC Document, 1985.

[38] F. Schroﬀ, D. Kalenichenko, and J. Philbin. Facenet:

A uniﬁed embedding for face recognition and
clustering. In Proc. CVPR, 2015.

[39] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna,

D. Erhan, I. J. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In Proc. ICLR, 2014.

[40] M. Turk and A. Pentland. Eigenfaces for recognition.

Journal of cognitive neuroscience, 3(1):71–86, 1991.

[41] A. Vedaldi and K. Lenc. MatConvNet – Convolutional

neural networks for MATLAB. In Proc. ACM MM,
2015.

[42] P. Viola and M. Jones. Rapid object detection using a

boosted cascade of simple features. In Proc. CVPR,
2001.

[22] Y. Li, K. Xu, Q. Yan, Y. Li, and R. H. Deng.

[43] G. L. Wittel and S. F. Wu. On attacking statistical

Understanding OSN-based facial disclosure against
face authentication systems. In Proc. AsiaCCS, 2014.

[23] B. Liang, M. Su, W. You, W. Shi, and G. Yang.

Cracking classiﬁers for evasion: A case study on the
Google’s phishing pages ﬁlter. In Proc. WWW, 2016.

[24] A. Mahendran and A. Vedaldi. Understanding deep

image representations by inverting them. In Proc.
CVPR, 2015.

[25] Megvii Inc. Face++. http://www.faceplusplus.com/.
[26] MobileSec. Mobilesec Android Authentication

spam ﬁlters. In Proc. CEAS, 2004.

[44] T. Yamada, S. Gohshi, and I. Echizen. Privacy visor:

Method based on light absorbing and reﬂecting
properties for preventing face image detection. In
Proc. SMC, 2013.

[45] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How
transferable are features in deep neural networks? In
Proc. NIPS, 2014.

[46] E. Zhou, Z. Cao, and Q. Yin. Naive-deep face

recognition: Touching the limit of LFW benchmark or
not? arXiv preprint arXiv:1501.04690, 2015.

1540