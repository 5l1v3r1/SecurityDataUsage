Precise Enforcement of Progress-Sensitive Security

Scott Moore

Harvard University

Aslan Askarov
Harvard University

Stephen Chong
Harvard University

ABSTRACT
Program progress (or termination) is a covert channel that may leak
sensitive information. To control information leakage on this chan-
nel, semantic deﬁnitions of security should be progress sensitive
and enforcement mechanisms should restrict the channel’s capac-
ity. However, most state-of-the-art language-based information-
ﬂow mechanisms are progress insensitive—allowing arbitrary in-
formation leakage through this channel—and current progress-sen-
sitive enforcement techniques are overly restrictive.

We propose a type system and instrumented semantics that to-
gether enforce progress-sensitive security more precisely than ex-
isting approaches. Our system is permissive in that it is able to
accept programs in which the termination behavior depends only
on low-security (e.g., public or trusted) information. Our system
is parameterized on a termination oracle, and controls the progress
channel precisely, modulo the ability of the oracle to determine the
termination behavior of a program based on low-security informa-
tion. We have instantiated the oracle for a simple imperative lan-
guage with a logical abstract interpretation that uses an SMT solver
to synthesize linear rank functions.

In addition, we extend the system to permit controlled leakage
through the progress channel, with the leakage bound by an ex-
plicit budget. We empirically analyze progress channels in existing
Jif code. Our evaluation suggests that security-critical programs
appear to satisfy progress-sensitive security.

Categories and Subject Descriptors
D.4.6 [Security and protection]: Information Flow Controls

General Terms
Security, Languages

Keywords
Termination channels, progress channels, information ﬂow

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$15.00.

1.

INTRODUCTION

The security of a system can depend upon the termination be-
havior of a program. For conﬁdentiality, program termination is a
covert channel: if conﬁdential information can inﬂuence whether a
program terminates or diverges, then an adversary observing pro-
gram execution may learn this conﬁdential information [4]. For
integrity, if untrusted input inﬂuences the termination behavior of
a program, then an attacker may be able to make a system unavail-
able, by causing a server loop to exit (e.g., “inputs of death” [12])
or by causing a program to diverge (e.g., “inputs of coma” [13]).

Termination channels have traditionally been considered benign
since they were assumed to leak only one bit of information. How-
ever, the situation is worse for interactive systems, where an adver-
sarial observer can observe intermediate output [4]. In such sys-
tems, it is the progress of the computation that leaks secret infor-
mation. Progress channels may leak an arbitrary amount of infor-
mation, as illustrated in the program in Listing 1, in which secret
is a variable that contains a conﬁdential positive integer. The pro-
gram prints out to channel L a number of zeroes equal to the value
of secret, and then diverges. The program thus leaks the value of
secret to an observer of channel L.

i < MAXINT ;

( i = 0;
i++)
while ( i == secret ) do skip ;
outputL(0) ;

{

for

}

Listing 1: Brute force termination channel

As shown by Askarov et al. [4], a program such as in Listing 1
leaks a 32-bit integer in under 6 seconds. In the presence of concur-
rency, progress channels have higher bandwidth, because leakage
is linear in the number of processes.

Progress-sensitive security [4] is a noninterference-based seman-
tic security condition that prevents information leakage via progress
channels. By contrast, progress-insensitive security conditions ig-
nore progress channels.

Most strong information security conditions are progress insen-
sitive (e.g., [56, 58, 54, 28, 27, 42]). This is not because progress
channels are believed to be benign, but because traditional enforce-
ment mechanisms (such as security-type systems [56, 43] for in-
formation-ﬂow control) are not able to precisely control progress
channels.

Security-type systems for progress-sensitive security (e.g., [55,
51, 37]) are overly restrictive, disallowing any loops where the loop
guard may depend on conﬁdential information. This rules out many
useful and secure programs.
Indeed, state-of-the-art information
ﬂow tools (such as Jif [35], SparkADA [6], and FlowCaml [47])

881outputL(0) ;
while ( secret > 0) do

:= secret − stride ;

secret

outputL(1) ;

Listing 2: Progress-sensitive secure if stride > 0

are progress insensitive in order to accept useful programs, at the
cost of also accepting programs that leak information via progress
channels.

We propose a type system and a runtime mechanism that together
precisely enforce progress-sensitive security in a simple interactive
imperative language. Our system is parameterized on an oracle
that reasons about the termination behavior of loops. If the ora-
cle determines that the termination or divergence of a loop depends
only on public information, then the termination behavior of the
loop does not reveal any conﬁdential information, and execution
of the loop may proceed. Otherwise, program execution is termi-
nated, thus preventing leakage of conﬁdential information through
the loop’s termination behavior. Our system controls the progress
channel precisely, modulo the ability of the oracle to determine the
termination behavior of loops. As analyses for program termination
improve, so will the precision of our enforcement.

The oracle reasons at run time about termination behavior. This
allows the oracle to be more precise, as it may use public infor-
mation speciﬁc to a particular execution of the program. It would
of course be possible for the oracle to reason statically, providing
run-time performance beneﬁts, perhaps at the cost of precision.

Our system soundly enforces progress-sensitive security, reject-
ing programs that leak information via progress channels. (Such
programs would be accepted by existing progress-insensitive type
systems.) We have implemented a prototype of the oracle using
logical abstract interpretation for termination analysis. This oracle
is sufﬁciently precise to allow us to accept as secure some programs
that progress-sensitive type systems reject.

Example. The program in Listing 2 contains a loop with a guard
that depends on conﬁdential information: the contents of variable
secret. An observer of channel L will see a zero output, and, de-
pending on whether the loop terminates, an output of one. Does this
program reveal conﬁdential information to the observer of chan-
nel L? Provided the public variable stride is positive, the loop is
guaranteed to terminate, as the value in secret will eventually be
negative. If stride is non-positive, then the termination behavior
depends on conﬁdential information:
the initial value of secret.
Since the value of stride cannot be determined statically, a purely
static enforcement technique would not be able to accept this pro-
gram as secure. Provided the oracle is sufﬁciently sophisticated, in
an execution where stride is positive, our system would be able to
accept the execution as secure, and allow execution to continue.1

outputL(0) ;
while ( secret > 0) do

:= secret + 1;

secret
outputL(1) ;

Listing 3: Progress-sensitive insecure

1Our system actually requires a cast annotation on the loop, which
indicates that the oracle must examine the loop at run time. The
cast annotation is described in Section 2.

Example. By contrast, the program in Listing 3 always reveals
conﬁdential information: a value of one is output on channel L if
and only if the initial value of secret is non-positive. Most existing
information-ﬂow security type systems would accept this program
as secure, despite the information leak through the progress chan-
nel. Our system rejects this program, since the oracle is unable to
prove that the termination behavior of this program depends only
on public information.
Budgeted semantics For some programs, it may be acceptable
to have some information leaked through progress channels. We
extend our system with an explicit budget for information leakage
through the progress channel. The amount of information leaked
via progress channels is tracked at runtime: for each loop that is
encountered where the termination behavior may leak high-security
information, the budget is reduced. Once the budget limit is reached,
the program is terminated, preventing additional conﬁdential infor-
mation from being leaked. The budget allows us to establish an
information theoretic bound on the information leaked via progress
channels, and provides a continuum of security between progress
sensitive and progress insensitive security conditions.

The rest of the paper is structured as follows. In Section 2 we
present a simple interactive imperative language with an annota-
tion that indicates that the termination oracle should be consulted
at run time. We present a type system in Section 3. The type system
and runtime semantics enforce a progress sensitive security condi-
tion, which we discuss in Section 4. Section 5 extends our system
and our security guarantees with a budget for leaking information
through the progress channel. We extend our results from a simple
two-point lattice of security levels to arbitrary security lattices in
Section 6.

We have implemented the type system and runtime system, in-
cluding a termination oracle that uses logical abstract interpretation
to reason about program termination and an SMT solver to synthe-
size linear rank functions to help prove termination. Section 7 de-
scribes our implementation. In addition, we have modiﬁed the Jif
compiler [35] to track progress channels, and applied the modiﬁed
compiler to a large Jif program, which we also describe in Sec-
tion 7. By manual inspection we determined that all loops detected
by the modiﬁed compiler are guaranteed to terminate, and more-
over, it is straightforward to reason that they terminate. We thus
believe that it is practical and feasible to enforce progress sensitive
security.

Section 8 discusses applications and extensions of our system
together with a survey of related work. We conclude in Section 9.
2. LANGUAGE AND SEMANTICS

We present a simple imperative language in which to explore
enforcement of progress-sensitive security guarantees. We assume
a lattice (L,(cid:118)), such that L is a set of security levels, and (cid:118) is a
relation over L that indicates permitted information ﬂow between
security levels: for l1, l2 ∈ L, if l1 (cid:118) l2, then information at
security level l1 may ﬂow to security level l2. We write l1 (cid:116) l2
for the join of l1 and l2. For clarity, we initially assume that L =
{L, H}, and L (cid:118) H but H (cid:54)(cid:118) L. Security levels can describe the
conﬁdentiality or integrity of information. Intuitively, L represents
low security (public or trusted) information, and H represents high
security (secret or untrusted) information. In this paper, we focus
on reasoning about the conﬁdentiality of information, although our
results also apply to integrity. In Section 6 we generalize our results
to arbitrary lattices.

Language syntax is presented in Figure 1. Commands are mostly
standard, with the exception of an explicit output command, and a

882Values
Expressions
Commands

v
e
c

::= n
::=
::=

v | x | e1 ⊕ e2
skip | stop | x := e | c1; c2 |
outputl(e) | if e then c1 else c2 |
while e do c | castp[c]

Figure 1: Language syntax

“cast” command, described below. Expressions consist of values v,
program variables x, and total binary operations over expressions.
For simplicity, we restrict values to integers. Output command
outputl(e) evaluates expression e to a value, and outputs the value
on channel l. Without loss of generality, we assume that there is
one channel for each security level.

We also assume a security environment Γ that maps variables to
security levels. Intuitively, if Γ(x) = l, then only information at se-
curity level l or below will ever be stored in variable x. The security
environment is used both in the runtime semantics of the language
(speciﬁcally, by the cast command), and in the type system.

Cast command castp[c] dynamically checks whether the termi-
nation behavior of c is determined by the current values of low-
security variables (i.e., variables x such that Γ(x) = L). Every
command castp[c] has a unique label p, which is used to identify
the program point of the command. Whenever this label is clear
from the context or is unimportant (as in most of the examples) we
omit it for clarity.

Intuitively, we are concerned with protecting the initial values
of high-security variables (i.e., variables x such that Γ(x) = H).
We assume that there is an attacker that knows the initial values
of low-security variables and observes outputs on the channel for
security level L. An execution of a program is regarded as secure
if the attacker is unable to learn anything about the initial values of
high-security variables. This will be deﬁned formally in Section 4.
Semantics A memory is a function from variables to values. We
say that two memories m1 and m2 are low equivalent, written
m1 ≈L m2, when they agree on the values of low-security vari-
ables: m1 ≈L m2 (cid:44) ∀x. Γ(x) (cid:118) L. m1(x) = m2(x).
An output trace is a ﬁnite list of the form (v1, l1) :: (v2, l2) ::
··· :: (vn, ln), where each (vi, li) corresponds to an output of value
vi on channel li, and (vn, ln) is the most recent output. An empty
output trace is denoted by . The projection of output trace o, de-
noted o(cid:22)L, contains all and only values of o that were output to
channel L:

(cid:40)

(cid:22)L = 
o :: (v, l)(cid:22)L =

(o(cid:22)L) :: v
o(cid:22)L

if l (cid:118) L
if l (cid:54)(cid:118) L

We say that output traces o1 and o2 are low equivalent, written
o1 ≈L o2, if and only if o1 (cid:22)L = o2 (cid:22)L.
Program conﬁgurations have the form (cid:104)c, m, o(cid:105) where c is the
command to be evaluated, m is the current memory, and o is the
output trace produced so far. Semantic transitions have the form
(cid:104)c, m, o(cid:105) −→ (cid:104)c(cid:48), m(cid:48), o(cid:48)(cid:105). The transition relation −→ is mostly
standard, and is presented in Figure 2. We write m(e) = v to
indicate that expression e evaluates to value v when variables x
occurring in e are replaced with their values m(x).

The rule for castp[c] requires that the termination or divergence
of command c is determined by low-security information. If that
is not the case, then the program is stuck. Rule S-CAST uses a
termination oracle O(p, m, o) to determine whether the termina-
tion behavior of c depends only on low-security information. The
oracle is given a program point p that identiﬁes a cast castp[c],

S-SKIP
(cid:104)skip, m, o(cid:105) −→ (cid:104)stop, m, o(cid:105)

S-ASSIGN
(cid:104)x := e, m, o(cid:105) −→ (cid:104)stop, m[x (cid:55)→ v], o(cid:105)

m(e) = v

S-SEQ-1
(cid:48)(cid:105)
(cid:104)c1, m, o(cid:105) −→ (cid:104)stop, m
(cid:48)
, o
(cid:104)c1; c2, m, o(cid:105) −→ (cid:104)c2, m
(cid:48)(cid:105)
(cid:48)
, o

S-SEQ-2
(cid:104)c1, m, o(cid:105) −→ (cid:104)c

(cid:48)(cid:105)
c
, o
(cid:104)c1; c2, m, o(cid:105) −→ (cid:104)c
(cid:48)
1; c2, m

(cid:48)
1, m

(cid:48)

1 (cid:54)= stop
(cid:48)
(cid:48)
, o

(cid:48)(cid:105)

S-IF
m(e) (cid:54)= 0 =⇒ i = 1

m(e) = 0 =⇒ i = 2

(cid:104)if e then c1 else c2, m, o(cid:105) −→ (cid:104)ci, m, o(cid:105)

S-WHILE-TRUE
(cid:104)while e do c, m, o(cid:105) −→ (cid:104)c; while e do c, m, o(cid:105)

m(e) (cid:54)= 0

S-WHILE-FALSE
(cid:104)while e do c, m, o(cid:105) −→ (cid:104)stop, m, o(cid:105)

m(e) = 0

m(e) = v

S-OUTPUT
(cid:104)outputl(e), m, o(cid:105) −→ (cid:104)stop, m, o :: (v, l)(cid:105)
S-CAST
O(p, m, o) ∈ {TERMINATE, DIVERGE}

(cid:104)castp[c], m, o(cid:105) −→ (cid:104)c, m, o(cid:105)

Figure 2: Semantics

the current memory m, and the current output trace o (and, im-
plicitly, the original program c0 and the initial memory m0), and
responds with one of TERMINATE, DIVERGE, or UNKNOWN. In-
tuitively, if the oracle responds TERMINATE then low-security in-
formation is sufﬁcient to determine that c will terminate. That is,
command c is guaranteed to terminate for any execution of pro-
gram c0 that starts with an initial memory that is low equivalent
to m0 and reaches castp[c], after producing an output trace o(cid:48) that
is low equivalent to o, in memory m(cid:48) that is low-equivalent to m.
Formally, if O(p, m, o) = TERMINATE, then
(cid:48) ≈L m ∧ o
0, (cid:105) −→∗ (cid:104)castp[c]; c
(cid:48)
(cid:48)

(cid:48)
, o
if ((cid:104)c0, m

0 ≈L m0 ∧ m
(cid:48)
. m

∀m
(cid:48)
0, m

(cid:48)

(cid:48)

(cid:48) ≈L o.
(cid:48)(cid:105))
, o
, m
∗ such that
(cid:48)(cid:105) −→∗ (cid:104)stop, m

∗ and o
, o

(cid:104)c, m

(cid:48)

∗

∗(cid:105).

, o

then there exist m

Similarly, if O(p, m, o) = DIVERGE then any low-equivalent exe-
cution of c0 that reaches castp[c] is guaranteed to diverge:

∀m

(cid:48)
0, m

(cid:48)

0 ≈L m0 ∧ m
(cid:48)
. m

(cid:48)
, o
if ((cid:104)c0, m
then there does not exist m
(cid:104)c, m
(cid:48)

(cid:48) ≈L o.
(cid:48) ≈L m ∧ o
(cid:48)(cid:105))
0, (cid:105) −→∗ (cid:104)castp[c]; c
(cid:48)
(cid:48)
, o
, m
∗ and o
∗ such that
(cid:48)(cid:105) −→∗ (cid:104)stop, m
, o

(cid:48)

∗

∗(cid:105).

, o

883Γ, pc (cid:96) skip : L
(cid:48)

pc (cid:116) l

Γ, pc (cid:96) x := e : L

(cid:48) (cid:118) Γ(x)

Γ (cid:96) e : l

Γ, pc (cid:96) c1 : l1

Γ, pc (cid:116) l1 (cid:96) c2 : l2

Γ, pc (cid:96) c1; c2 : l1 (cid:116) l2

Γ (cid:96) e : l

Γ, pc (cid:116) l (cid:96) ci : li

Γ, pc (cid:96) if e then c1 else c2 : l1 (cid:116) l2
(cid:48) (cid:96) c : l
Γ (cid:96) e : l
(cid:48)

Γ, pc (cid:116) l (cid:116) l

Γ, pc (cid:96) while e do c : l

(cid:48)

Γ, H (cid:96) c : l

Γ, L (cid:96) castp[c] : L
pc (cid:116) l

Γ (cid:96) e : l
(cid:48) (cid:118) l
Γ, pc (cid:96) outputl(e) : L

(cid:48)

Figure 3: Type system: commands

If the oracle responds with UNKNOWN then the oracle is unable
to determine whether the termination behavior of castp[c] depends
only on low-security information.

While the problem of proving program termination is clearly un-
decidable, there are many approaches to implementing sound and
useful (but incomplete) termination oracles. Simple, albeit im-
precise program analysis could identify common patterns (for ex-
ample, identifying for loops such that the loop counter is a low-
security variable, and the stride is a constant). For more complex
programs, existing tools (e.g., [19, 31, 52]) for proving program ter-
mination can be employed. More sophisticated approaches, such as
the work of Cook et al. [20], are able to automatically synthesize
sufﬁcient conditions to prove loop termination. We describe our
prototype implementation in Section 7.

3. TYPE SYSTEM
This section presents the typing rules for our language. The rules
for expressions are standard and have form Γ (cid:96) e : l, meaning that
in environment Γ, level l is an upper bound on the information that
may be learned by evaluating expression e. Typing rules for com-
mands have form Γ, pc (cid:96) c : l, where pc is the program counter
level, and l is the termination level. Figure 3 presents typing rules
for commands in our language. Termination level l of command c
is an upper bound on how much information may be learned by
observing c’s termination. For simple commands, such as skip,
assignment, and output, the termination level is always L—these
commands always terminate and thus the termination of the com-
mand reveals no information. The rule for sequential composition
c1; c2 propagates the termination level of c1 into the pc-level of c2,
since c2 executes only if c1 terminates. Therefore, if the termina-
tion level of c1 is H, no low assignments or low outputs are allowed
in c2. The termination level of c1; c2 is the join of the termination
levels of the individual commands. The termination level of con-
ditional if e then c1 else c2 is the join of the termination levels of
branches c1 and c2.

For command while e do c, the termination of the loop may de-
pend on both the guard expression e and the termination of loop
body c. Thus, the termination level for a while loop contains the

join of the level of the guard expression l and the termination level
of c. In addition, if the while loop diverges then program’s nonter-
mination may reveal that the while loop was executed. The pc-level
is an upper bound on the information that may be learned by know-
ing that the while loop executed, and so the pc-level is folded into
the termination level of the loop.
Example. Let Γ(h) = H and Γ(low ) = L. Consider program

while h > 0 do h := h − low ;
outputL(1)

The type system rejects this program, because the termination level
of the while loop is H, and so the pc-level of the output command
is also H, which does not type check.

The typing rule for cast[c] is noteworthy. It ignores the termina-
tion level of subcommand c, and the termination level of the casted
command is L. The intuition is that the termination behavior of c
is assumed to depend only on low-security information, and thus
the termination or non-termination of c will not leak secret infor-
mation. This assumption will be validated at runtime by the termi-
nation oracle. This is secure because the oracle’s decision is based
only on low-security information, and thus the success or failure
of the cast at runtime does not reveal any secret information. Sub-
command c must type check with a pc-level of H, which ensures
that c does not contain any assignments to low-security variables
or low-security outputs. This is a technical simpliﬁcation with no
loss of expressiveness—any casted command c that is well-typed
under low pc-level can be transformed into a form with (possibly
many) casts around subcommands of c that are well-typed under
high-pc. The latter is possible because a command of the form
Γ, L (cid:96) while h do c is well-typed if and only if Γ, H (cid:96) while h do c
is well-typed; the same holds for the conditionals. If original com-
mand contains no high loops or conditionals, then the cast is redun-
dant and can be omitted.
Example. Let Γ(h) = H and Γ(low ) = L. Consider program

cast[while h > 0 do h := h − low ];
outputL(1)

This program is accepted by the type system, because the termina-
tion level of the cast is L, and so the pc level of the output command
is also L, and thus the output command type checks. The termina-
tion of the while loop here depends on the values of low and h. In
particular, when low > 0, the while loop will terminate regardless
of the value of h; when low ≤ 0, termination depends on h. At
runtime, according to S-CAST, the termination oracle needs to ex-
amine variable low, and execution continues only when low > 0.

On placement of casts Note that our language allows casts to be
placed around any code block, not just loops. In fact, limiting casts
to just loops would be insufﬁcient, as illustrated by the following
program.

if h > 0

then while h(cid:48) > 0 do h(cid:48) := h(cid:48) − low ;
else skip;

outputL(1)

Placing the cast around the loop would not close the termination
channel of this program: when h ≤ 0, the program takes the
else branch, omitting the request to the termination oracle; yet, if
low ≤ 0, the subsequent output of value 1 reveals h ≤ 0, be-
cause otherwise the execution would have been stopped. For this
reason, our type system rules out casts in high contexts. The cor-
rect placement of cast for this example is around the if command,
which would allow the program to type check.

8844. SECURITY

We deﬁne security in terms of an attacker that is able to observe
both the initial values of low-security variables and the low-security
output of a program execution. We assume the attacker knows the
program text.
We say that conﬁguration (cid:104)c, m, (cid:105) emits incomplete trace τ,
written (cid:104)c, m, (cid:105) ↓ τ, if there exists command c(cid:48), memory m(cid:48), and
output trace o such that (cid:104)c, m, (cid:105) −→∗ (cid:104)c(cid:48), m(cid:48), o(cid:105) and o(cid:22)L = τ.
Intuitively, if a conﬁguration emits trace τ then the attacker ob-
serves the outputs τ during the execution of the program.

We strengthen the observational model to allow the attacker to
determine when an execution will no longer produce additional
output on the attacker’s channel (because, for example, the program
terminates, diverges, or gets stuck). Trace τ is maximal, written τ•,
when there are no more public outputs possible in the computation
that emitted τ•. We say that conﬁguration (cid:104)c, m, (cid:105) emits maximal
trace τ•, written (cid:104)c, m, (cid:105) ↓ τ•, if there exists c(cid:48), m(cid:48), and o such
that (cid:104)c, m, (cid:105) −→∗ (cid:104)c(cid:48), m(cid:48), o(cid:105) and there is no more public out-
put possible from (cid:104)c(cid:48), m(cid:48), o(cid:105). For example, programs outputL(1)
and outputL(1); while 1 do skip both emit the same maximal trace
(1, L)•.
maximal traces τ•, and use metavariable t to range over traces.

We use the term trace to refer to both incomplete traces τ and

Given that an attacker has observed some trace t, the attacker’s
knowledge [3] is the set of initial memories that could have pro-
duced trace t and have the same initial values for all low-security
variables. Formally, we have

Deﬁnition 1 (Attacker knowledge). Given a program c, initial mem-
ory m, and a trace of public outputs t, deﬁne attacker knowledge

k(c, m, t) (cid:44) {m

(cid:48) | m ≈L m

(cid:48) ∧ (cid:104)c, m

(cid:48)

, (cid:105) ↓ t}

Attacker knowledge is monotonic in t: the more public outputs
are produced in the trace, the fewer memories are consistent with
the output. Note that in this deﬁnition, a smaller knowledge set
corresponds to more precise information.

Our baseline condition for security is progress-sensitive nonin-

terference [4].2

A program satisﬁes progress-sensitive noninterference if attacker

knowledge remains constant regardless of the observed outputs.

Deﬁnition 2 (Progress-sensitive noninterference). Program c satis-
ﬁes progress-sensitive noninterference (PSNI) if for all initial mem-
ories m and traces t such that (cid:104)c, m, (cid:105) ↓ t, attacker knowledge
does not change, i.e.,

k(c, m, t) = {m

(cid:48) | m ≈L m

(cid:48)}

Example. Program outputL(1) satisﬁes Deﬁnition 2. The only
output produced by this program reveals no secret information to
the attacker, and the attacker knowledge is the set of all low-equiva-
lent memories.

However, program while h > 0 do skip; outputL(1) does not

satisfy Deﬁnition 2. By Deﬁnition 1, we have

k(c, m[h (cid:55)→ 0], (1, L)•) = {m

(cid:48)

(cid:48)

| m

(h) ≤ 0 ∧ m ≈L m

(cid:48)}.

Here {m(cid:48) | m(cid:48)(h) ≤ 0 ∧ m ≈L m(cid:48)} is the attacker knowledge
after seeing output (1, L). Because the attacker knowledge is a
strict subset of the set of all memories that are low-equivalent to
the initial memory m, the program is insecure.

2Askarov et al. [4] call this condition termination-sensitive nonin-
terference.

The type system, together with the runtime mechanism, soundly
enforces progress-sensitive noninterference. The following theo-
rem is the main result of this section.
Theorem 1 (Soundness of enforcement). Given program c, if for
some security level l we have Γ, L (cid:96) c : l then c satisﬁes progress-
sensitive noninterference.

The proof of Theorem 1 follows as a special case of Theorem 2,

which is presented in the following section.

5. TERMINATION LEAKAGE BUDGET

The operational semantics of Section 2 ensures that if the ter-
mination behavior of command cast[c] cannot be shown to de-
pend purely on low-security information, then the execution will
get stuck, preventing any leakage of high-security information. If
the execution continued, then the next low-security output pro-
duced by the program would allow the attacker to learn that the
command terminated, which may reveal high-security information.
Similarly, if the command diverges, then the failure to produce
another low-security output may allow the attacker to learn high-
security information. Indeed, the termination behavior of the pro-
gram in Listing 1 reveals everything about the initial value of the
high-security variable secret.

However, continued execution does not necessarily reveal every-
thing about the high-security inputs to the program: the actual in-
formation leaked may be less than expected. In this section, we
extend the semantics of Section 2 to allow a limited amount of in-
formation to be released via the termination behavior of the pro-
gram.

h(cid:48) := h ;
cast [

Consider the following program, in which variables h, h(cid:48), and
hstep are secret (Γ(h) = Γ(h(cid:48)) = Γ(hstep) = H), and variable
low is public (Γ(low ) = L).
1 while low > 0 do {
2
3
4
5
6
7
8
9

]
outputL(low ) ;
low := low − 1 ;

h(cid:48) := h(cid:48) − hstep

while h(cid:48) > 0 do

}

Consider the ﬁrst iteration of the outer loop. The termination or
divergence of the inner loop on Lines 4–5 cannot be established
using only low-security information. Execution of the low output
on Line 7 reveals secret information to the attacker; speciﬁcally, it
reveals that either hstep is positive or h ≤ 0.

Now consider the second iteration of the outer loop. Again, the
termination or divergence of the inner loop reveals to the attacker
the same condition—either hstep is positive or h ≤ 0. Critically, it
does not reveal any more information than was revealed in the ﬁrst
iteration.
Indeed, a sufﬁciently powerful oracle could show that
if the program has produced any low output, then any subsequent
execution of the inner loop will terminate.

To track and control the amount of information leaked through
the termination channel, we introduce a termination leakage bud-
get B. The budget bounds the number of outputs that may reveal
information about the program’s termination behavior.
We extend program conﬁgurations to ﬁve-tuples (cid:104)c, m, o, r, s(cid:105),
where r is a release counter and s is a pending release bit that
is either 0 or 1. The extended operational semantics will ensure
that r counts the number of output events that may allow an attacker

885S-CAST-BUDGET
(cid:104)castp[c], m, o, r, s(cid:105) −→ (cid:104)c, m, o, r, 1(cid:105)

O(p, m, o) = UNKNOWN

m(e) = v

r + s ≤ B

S-OUTPUT-L
(cid:104)outputL(e), m, o, r, s(cid:105) −→ (cid:104)stop, m, o :: (v, L), r + s, 0(cid:105)
S-OUTPUT-H
(cid:104)outputH (e), m, o, r, s(cid:105) −→ (cid:104)stop, m, o :: (v, H), r, s(cid:105)

m(e) = v

Figure 4: Selected rules for budgeted semantics

to learn secret information. Moreover, s will equal 1 only when
the production of the next low-security output might reveal secret
information to the attacker; when s = 0, the next low-security
output leaks no information.
5.1 Budgeted semantics

We deﬁne a new transition relation for the extended program
conﬁgurations. We lift every rule in Figure 2 (except for rule S-
OUTPUT) to a rule for the new conﬁgurations so that r and s remain
unchanged. In addition, we add rules in Figure 4.

Rule S-CAST-BUDGET sets s to 1 when the oracle fails to de-
termine whether a cast command will terminate or diverge. Rule
S-OUTPUT-L applies when an output to the low-security channel
is performed. It increments the release counter by s and clears the
pending bit—subsequent public outputs are guaranteed to reveal
no information until another cast is reached. Additionally, the rule
enforces the termination leakage budget B, requiring that release
counter r does not exceed B. Rule S-OUTPUT-H is a lifted rule for
outputs on high-security channels. It allows arbitrary output to the
high-security channel, and preserves the values of r and s.

The type system for the language does not change.

5.2 Security

Well-typed programs executed using the budgeted semantics may
not satisfy progress-sensitive noninterference, but they do satisfy a
weaker semantic security condition. To state this condition, we ﬁrst
introduce the notions of release events and progress release events.
Release events
For clarity and easier reference, in the follow-
ing deﬁnitions, we use boxes to highlight relevant semantic transi-
tions. A release event is a transition that produces an output that al-
lows the attacker’s knowledge to improve: the event releases high-
security information to the attacker. As per Theorem 1, well-typed
programs executed with the semantics of Section 2 contain no re-
lease events.

Deﬁnition 3 (Release event). Given a program c, memory m, and
an output trace o(cid:48) such that
(cid:104)c, m, , 0, 0(cid:105) −→∗ (cid:104)c(cid:48), m(cid:48), o(cid:48), r(cid:48), s(cid:48)(cid:105) −→ (cid:104)c(cid:48)(cid:48), m(cid:48)(cid:48), o(cid:48)(cid:48), r(cid:48)(cid:48), s(cid:48)(cid:48)(cid:105)
then the boxed transition is a release event if
(cid:48) (cid:22)L) ⊃ k(c, m, o

k(c, m, o

(cid:48)(cid:48) (cid:22)L)

the fact that the output occurred that reveals information. We cap-
ture this intuition by deﬁning progress release events to be release
events that reveal only as much information as knowing that some
output was produced.
Deﬁnition 4 (Progress release event). Given a program c, mem-
ory m, and an output trace o(cid:48) such that
(cid:104)c, m, , 0, 0(cid:105) −→∗ (cid:104)c(cid:48), m(cid:48), o(cid:48), r(cid:48), s(cid:48)(cid:105) −→ (cid:104)c(cid:48)(cid:48), m(cid:48)(cid:48), o(cid:48)(cid:48), r(cid:48)(cid:48), s(cid:48)(cid:48)(cid:105)
then the boxed transition is a progress release event, if it is a release
event and it holds that

(cid:48)(cid:48) (cid:22)L) =

k(c, m, o

(cid:48) (cid:22)L :: (v, L)).

(cid:91)

v∈Z

v∈Z k(c, m, o(cid:48) (cid:22)L :: (v, L)) is called progress

k(c, m, o

Here, the term(cid:83)

knowledge [4].
Example. In program

while h > 0 do skip;
outputL(1)

the low output is a progress release event. We have
(cid:48) ≈L m ∧ m

k(c, m[h (cid:55)→ 0], (1, L)•) = {m

(cid:48)|m

(cid:48)

(h) ≤ 0}.

(cid:91)

For progress knowledge, we observe that the only possible low out-
put here is exactly (1, L); we have

k(c, m[h (cid:55)→ v], o

(cid:48) (cid:22)L :: (v, L)) = k(c, m[h (cid:55)→ 0], (1, L)).

v∈Z
Clearly, by Deﬁnition 4 any progress release event is also a re-
lease event, but not the other way around. For example, in program
low := h; outputL(low ), the low output is not a progress release
event.

With the deﬁnition of progress release events at hand, we can

formulate our theorem for budgeted semantics.
Theorem 2 (Budgeted progress release). Given a program c such
that Γ, L (cid:96) c : l for some security level l then execution of c
with budget B contains at most B release events, all of which are
progress release events.

A proof of this theorem is available in the companion technical

report [34].

Note that Theorem 1 is a special case of Theorem 2 for budget
B = 0. Moreover, when B is inﬁnite, Theorem 2 implies that well-
typed programs satisfy progress-insensitive noninterference [4], a
semantic security condition that allows an attacker to learn infor-
mation through progress release events.
Interpreting the termination leakage budget The budget en-
forced on a program has a number of interpretations. The sim-
plest interpretation is that B is a bound on the number of times
the progress channel may be exploited. An information-theoretic
interpretation of this is that the amount of information that may be
conveyed via the progress channel is at most log2(B +1) bits. This
is a pessimistic bound. Because there are at most B + 1 possible
observations, the expression log2(B + 1) bounds both Shannon
entropy and min-entropy [49] notions of leakage.

Example. In program low := h; outputL(low ), the low output is
the release event: attacker knowledge changes with the output.

A particular class of release events that are interesting to us are
progress release events, which are release events that leak informa-
tion via the progress channel. That is, it is not the value output, but

6. MULTI-LEVEL SECURITY

So far we have only considered a simple two-point lattice of se-
curity levels. However, many real systems for which information
security is a concern require richer lattices to express their security.
In this section we extend the language semantics and type system

8866.1 Budgets for multiple levels

We generalize the budgeted semantics by providing a budget for
each security level. For example, if each security level represents
the information of a security principal, then each principal may set
their budget independently. We write B(l) for the leakage bud-
get of security level l. Intuitively, budget B(l) bounds information
leakage via progress channels from level l to any other level l(cid:48) such
that l (cid:54)(cid:118) l(cid:48).
Extending the release counter and pending release bits We
track the number of progress release events for each security level,
essentially maintaining a release counter for each security level.
Let R be a function from security levels to release counters, such
that R(l) is the number of progress release events that have oc-
curred that may have leaked information at level l to some level l(cid:48)
such that l (cid:54)(cid:118) l(cid:48). When we encounter an output that can potentially
leak information at level l, we conservatively increment the release
counters for all levels below l, that is, all l(cid:48) such that l(cid:48) (cid:118) l.

We generalize the pending release bit in a similar way, by track-
ing a separate pending bit for each security level. Since a function
from security levels to a single bit is isomorphic to a set of secu-
rity levels, we generalize the pending release bit to a set of security
levels S. If l ∈ S then the pending bit for level l is set, meaning
that the next output on a channel l(cid:48), such that l(cid:48) (cid:54)(cid:118) l, may reveal
information from level l. Thus, if l ∈ S and an output occurs to
channel l(cid:48) such that l (cid:54)(cid:118) l(cid:48), then the output release counter for level l
is incremented, and l is removed from S. We refer to S as the set
of pending levels.
Budget update To formally specify how release counters and the
set of pending levels are updated when an output occurs to chan-
nel l, we deﬁne function update(R, S, l). This function takes three
arguments: R is a release counter, S is a set of pending levels,
and l is the security level of the channel on which the output oc-
curred. The function returns a pair (R(cid:48), S(cid:48)) of the updated release
counter R(cid:48) and the updated set of pending levels S(cid:48).
three sets S = S1 (cid:93) S2 (cid:93) S3 such that

For a ﬁxed security level l, let us rewrite S as a disjoint union of

S1 ={l
S2 ={l
S3 ={l

(cid:48) | l
(cid:48) | l
(cid:48) | l

(cid:48) ∈ S ∧ l
(cid:48) ∈ S ∧ l
(cid:48) ∈ S ∧ l

(cid:48) (cid:118) l}
(cid:48) (cid:54)(cid:118) l ∧ l (cid:118) l
(cid:48) (cid:54)(cid:118) l ∧ l (cid:54)(cid:118) l

(cid:48)}
(cid:48)}

Set S1 contains levels that ﬂow to l, including l itself. Set S2
contains all levels that are strictly higher than l, and set S3 is the
set of levels that are incomparable with l. Figure 5b visualizes this
partitioning for an arbitrary lattice when S contains all levels.
Example. Consider S = {N, H} and l = M. Then, according to
the deﬁnition above, S1 = ∅, S2 = {H}, and S3 = {N}.

Recall that the set S is the set of levels such that an output may
reveal information at those levels. After an output on channel l, we
need to consume budgets for levels in S2 and S3, but not S1. The
budget for levels in S1 need not be consumed because information
at any level l(cid:48) ∈ S1 is allowed to ﬂow to level l. Output on channel l
may, however, reveal information at levels in S2 and S3. We deﬁne
the updated release counter R(cid:48) as a function of l(cid:48) for which it holds
that

(cid:40)

(cid:48)

(cid:48)

(l

R

) =

R(l(cid:48)) + 1 if l(cid:48) (cid:54)(cid:118) l and ∃l(cid:48)(cid:48) ∈ S2 ∪ S3. l(cid:48) (cid:118) l(cid:48)(cid:48)
R(l(cid:48))

otherwise

This deﬁnition increments release counter for all levels l(cid:48) that do
not ﬂow to l, and for which there is a bound l(cid:48)(cid:48) in S2 ∪ S3. The
condition on the ﬁrst line of the above deﬁnition ensures that we do

(a) 4-element Hasse diagram (b) Disjoint sets in update function

Figure 5: Example security lattice and disjoint sets in update

to arbitrary security lattices. The extension is non-trivial, and is
particularly interesting for budgeted semantics.
Assume an arbitrary lattice of security levels L with bottom and
top elements ⊥ and (cid:62) respectively. We extend the syntax for cast
to the form castl,l(cid:48)
p [c] where l and l(cid:48) are security levels. Level l is an
upper bound on the information that the oracle is permitted to use to
reason about the termination behavior of command c. In Section 2,
this level is implicitly assumed to be L. Level l(cid:48) is an upper bound
on what information is allowed to be leaked by (non)termination of
program c. This level is only relevant for budgeted semantics, as
explained below, and in Section 5 it is implicitly assumed to be H.
We do not place any restrictions on the relationship between lev-
els l and l(cid:48). However, if l(cid:48) (cid:118) l then the termination behavior of
the loop is permitted to reveal no more information than the oracle
uses to reason about termination behavior, and so the budgeted se-
mantics give no additional beneﬁt over the standard semantics. We
thus expect (but do not require) that l(cid:48) (cid:54)(cid:118) l.
Example. To clarify our exposition throughout this section we use
an example four-element security lattice, illustrated by the Hasse
diagram in Figure 5a. This lattice contains four security levels
L, M, N, H, such that L (cid:118) M (cid:118) H and L (cid:118) N (cid:118) H, but
M (cid:54)(cid:118) N and N (cid:54)(cid:118) M (and also H (cid:54)(cid:118) M (cid:54)(cid:118) L and H (cid:54)(cid:118) N (cid:54)(cid:118) L).
Consider the following example program, where variables m, n,

and h have security levels M, N, and H respectively.

h := h + m + n;
castL,H [while h > 0 do skip; ]
outputM (1);
outputL(1);

This program contains a loop that introduces a progress channel:
termination of the loop depends on information at levels H, M,
and N. The two outputs to levels M and L expose this progress
channel. Let us look carefully at what information is revealed by
each of these outputs.

The output on M reveals information about H and N to level M.
Similarly, output on level L reveals information about M, N, and
H to level L. Note that both outputs are potentially dangerous,
leaking information to adversaries observing on different channels.
If the order of the two output commands is swapped, as in pro-

gram

h := h + m + n;
castL,H [while h > 0 do skip];
outputL(1);
outputM (1);

then we regard only the ﬁrst output as leaking information. This
is because an observer of channel M is also permitted to observe
channel L (since L (cid:118) M). Thus, the ﬁrst output reveals to level
M (and L and N) that the loop terminated, and the second output
to level M does not provide any additional information.

887R

Command
initial state
castL,H [while h do skip]
outputM (1)
outputL(1)
Figure 6: Example of budget update in multi-level setting

L M N H
0
0
0
0
1
0
0
1

S
∅
{H}
{M}
∅

0
0
0
1

0
0
1
1

S-CAST
O(p, m, o) ∈ {TERMINATE, DIVERGE}
(cid:104)castl,l(cid:48)
p [c], m, o, R, S(cid:105) −→ (cid:104)c, m, o, R, S(cid:105)

= S ∪ {l

S-CAST-BUDGET
(cid:48)}
(cid:48)
S
(cid:104)castl,l(cid:48)

O(p, m, o) = UNKNOWN
p [c], m, o, R, S(cid:105) −→ (cid:104)c, m, o, R, S
(cid:48)(cid:105)

S-OUTPUT

m(e) = v

(cid:48)
(R
(cid:48)
. R

(cid:48)
) = update(R, S, l)
) ≤ B(l

)

(cid:48)

, S
(cid:48)
(l

(cid:48)

∀l

(cid:104)outputl(e), m, o, R, S(cid:105) −→ (cid:104)stop, m, o :: (v, l), R

(cid:48)

(cid:48)(cid:105)

, S

Figure 7: Budgeted semantics for multi-level setting: selected rules

not unnecessary consume budgets for levels that are not bounded
by a level in S2 ∪ S3. For example, when S2 ∪ S3 is an empty set,
R does not change.
For the updated pending release bits S(cid:48), which security levels
should be in it? Clearly all of set S1, as if l(cid:48) ∈ S1, a future output
at a level l(cid:48)(cid:48) such that l(cid:48) (cid:54)(cid:118) l(cid:48)(cid:48) will reveal information about l(cid:48) via
a progress channel. Sets S2 and S3 do not need to be in S(cid:48), as
we have already accounted for information leaked via the progress
channel for these levels. We may, however, need to add level l to S(cid:48).
If S2 is non-empty, then there is a level l(cid:48) ∈ S2 such that l (cid:118) l(cid:48).
Thus, it is possible that information at level l ﬂowed to level l(cid:48),
where it inﬂuenced the termination behavior of the program. Thus,
a future output may reveal via a progress channel information at
level l that has not yet been accounted for in the budget. We thus
deﬁne the updated pending release bits S(cid:48) as follows.

(cid:48) (cid:44) S1 ∪ {l | S2 (cid:54)= ∅}

S

Example. Figure 6 presents an example program together with the
set of pending levels and the values of release counters for every
level, during this program execution when h = 0.

6.2 Semantics

Figure 7 presents budgeted semantics for the extended language.
The semantics for the multi-level setting resembles the budgeted
semantics of Section 5.1, with the difference that it uses release
counter R and the set of pending levels S. Program conﬁgura-
tions have the form (cid:104)c, m, o, R, S(cid:105). As before, the semantics is
parametrized over the termination oracle. Generalization of the ter-
mination oracle to multi-level setting is straightforward, and we
omit it here. The only notable aspect is that when the oracle is
given cast label p for cast castl,l(cid:48)
p [c], the oracle is permitted to use
only information up to level l to reason about the termination be-
havior of command c.

As before, rule S-CAST does not modify the release counter R
or pending levels S. Rule S-CAST-BUDGET applies when the or-
acle returns UNKNOWN. Recall that level l(cid:48) is an upper bound on
the termination level of c. This means that (non)termination of c
reveals information up to l(cid:48). Subsequently, any output on level l(cid:48)(cid:48)

such that l(cid:48)(cid:48) (cid:54)(cid:118) l(cid:48) must consume some part of the termination bud-
get for l(cid:48). Therefore, this rule adds l(cid:48) to the set of pending levels.
Finally, rule S-OUTPUT updates the release counter and the pend-
ing levels before an output on channel l. Given updated R(cid:48) and S(cid:48),
the execution is allowed when the budget constraints are satisﬁed.
6.3 Typing rules

Most of the typing rules from Figure 3 can be extended to the
multi-level case in a straightforward manner by replacing any oc-
currence of level L with level ⊥. The rule for castl,l(cid:48)
p [c] is more
interesting and we show it below.
(cid:48)(cid:48) (cid:118) l
Γ, pc (cid:116) l (cid:96) c : l
(cid:48)(cid:48)
Γ, pc (cid:96) castl,l(cid:48)
p [c] : pc (cid:116) l

l

(cid:48)

The rule requires that c is well-typed with some termination
level l(cid:48)(cid:48). The only requirement on l(cid:48)(cid:48) is that it needs to be bounded
by level l(cid:48). Because information up to l may be used by the oracle,
we require that c is well-typed under context pc (cid:116) l. This prevents
laundering information through the termination oracle itself. For a
similar reason, the termination level of this command is set to pc(cid:116)l.

To illustrate this rule, let us consider a few examples.

Example. Program castL,M [while h > 0 do skip] is not well-
typed because the termination level of the while loop is H, and
M (cid:54)(cid:118) H.

On the other hand, the program castL,H [while h > 0 do skip] is

well-typed.

Example. Program

castM,H [

outputL(1)
while h > 0 do h := h − m

]

is rightfully rejected by the type system. The release event in this
program is subtle. The oracle is allowed to use information up to
level M. This means that if m > 0, and the oracle can deduce
that the while loop will terminate, then the low output preceding
the loop will occur. On the other hand, if m ≤ 0, the oracle must
return UNKNOWN. The presence of the low output right before the
loop will therefore depend on level M, which violates progress-
sensitive noninterference.

On nested casts Unlike the simple type system of Section 3,
nested casts are allowed in the presence of multiple security levels.
The following examples illustrates how such scenario may appear
in the presence of budgeted semantics.

Example. Consider the program below which has two nested casts.

castL,M [ while m > 0 do {

. . .

castL,H [while h > 0 do h−−] . . .

} ]

outputL(1) ;

Assume that the termination oracle is unable to prove termination
of the outer cast statement, but can prove termination of the inner
cast statement. In this case, by the time the execution reaches the
output statement, one unit of M leakage budget is consumed, while
no leakage budget of H is consumed.

Soundness To formulate soundness for multiple levels we gener-
alize our deﬁnitions from Section 4.

First, observe that our deﬁnition of projection from Section 4
easily extends to multiple levels. This allows us to generalize the

888cast [ while x < 10 do {

y := 0
while y < 10 do y := y + 1
x := x + 1 } ]

Listing 4: Example program with nested loops where x, y are high
variables

deﬁnition of ↓ to multiple levels. Similarly, we generalize deﬁni-
tions of attacker knowledge at level l, release event from level l,
and progress release event from level l. Using these deﬁnitions, we
can formulate soundness for multiple security levels.

Theorem 3 (Budgeted progress release at l). Given a program c
such that Γ,⊥ (cid:96) c : l(cid:48) for some security level l(cid:48) then execution of c
with budget B contains at most B(l) release events from level l, all
of which are progress release events.

A proof of this theorem is available in the companion technical

report [34].

7. EVALUATION

We have evaluated the feasibility of our approach in two parts.
First, we have implemented the (non-budgeted) language seman-
tics and type system for a simple interactive imperative language,
including an implementation of a suitable termination oracle. This
establishes that techniques for reasoning about program termina-
tion can be adapted to reasoning about progress channels. Second,
we extended the Jif compiler [35] to track information ﬂow via
progress channels, and analyzed a Jif application. We ﬁnd it is fea-
sible to enforce progress-sensitive security conditions for security-
critical applications. We report the details of our evaluation below.
7.1 Prototype implementation

Our prototype termination oracle is based on work by Chawd-
hary et al. [14], which is a form of logical abstract interpretation
over a specialized abstract domain for termination. A particular
advantage of this analysis is its performance, compared to analy-
ses that are based on binary reachability (e.g., Terminator [19]).
The analysis is parameterized over an algorithm for discovering ter-
mination arguments. Following the instantiation given by Chawd-
hary et al., we use the linear rank synthesis algorithm of Podelski
and Rybalchenko [38]. Our termination analysis uses the z3 SMT
solver [1] for linear rank synthesis and for eliminating spurious pro-
gram paths.

Our prototype is furthermore extended with a simple constant
propagation analysis that is applied to low variables when casts are
encountered at run time. This allows us to ﬁnd termination argu-
ments that rely on the current run-time values of low variables.

We use our implementation to validate the security of the all ex-
amples in this paper. Analyzing a program like the one in Listing 4
results in 31 calls to z3, with an overall time of under 0.8 seconds
on a machine with a 2.4 GHz CPU. For more complex programs,
this overhead will certainly be larger. Of course, for subprograms
that always terminate, like Listing 4, the analysis can be done stat-
ically ahead of time. We discuss related work that could improve
performance in Section 8.

Because we currently do not take into account the output his-
tory of the program, we cannot achieve the tight bound on budget
consumption for the example in the beginning of Section 5. An
implementation of more precise oracle that would take the output
history into account is deferred to future work.

# Loops

# Casts

Jif std-lib
Civitas

75
310

66
89

Termination

arith
28
88

heap
35
1

errors

3
-

Figure 8: Audit of casts required to rule out intra-procedural
progress channels in the Jif standard library and the Civitas secure
voting system. Casts are categorized by the termination argument
required to prove them safe: linear arithmetic or heap shape. Loops
that are intended to always terminate but may not are reported as
errors.

We use our experience with this prototype as a guideline for eval-
uating the feasibility of enforcing progress-sensitive guarantees in
real-world applications, which we discuss next.
7.2 Audit of progress channels in Civitas

Civitas [18] is a remote voting system that provides veriﬁable re-
sults while protecting voter conﬁdentiality. The security of Civitas
relies on two factors: strong properties of the underlying crypto-
graphic protocols for voting and information ﬂow guarantees of the
implementation. To address the latter, Civitas is implemented in
Jif [35], a security-typed language which is believed to enforce a
progress-insensitive security condition.

Our premise for this evaluation is that, despite Jif providing only
progress-insensitive guarantees, Civitas (and most other security
typed programs) satisfy a stronger, progress-sensitive security con-
dition. To evaluate this claim, we extended Jif with our multi-level
security type system to track progress channels within methods.
We focus only on intra-procedural progress channels, and ignore
any inter-procedural progress channels. We identiﬁed 66 loops in
the Jif standard library and 89 loops in Civitas that require casts
to secure possible progress channels. The loops that did not re-
quire casts were either dependent on public information or had no
low side-effects following them within the containing method. We
manually categorized each cast by the termination analysis neces-
sary to demonstrate that it is secure. Figure 8 reports our ﬁndings.
Termination analysis Notably, we discovered three simple termi-
nation bugs in the Jif standard library. The containsAll method of
the AbstractCollection class uses a loop to iterate over elements of
the given collection but the loop is missing an increment statement.
As a result, the method will terminate when called with an empty
collection as an argument, but diverge otherwise. In both linked list
implementations provided by the library, the hash code of the list is
intended to be computed by iterating over this list’s elements and
combining the hash codes of each, but the current element is not
advanced between iterations. Similar to the ﬁrst bug, these imple-
mentations will terminate for empty lists but diverge otherwise. Jif
programs using these methods may inadvertently leak information.
All of the remaining loops always terminate, regardless of input.
This matches our initial intuition: most programs are intended to
terminate and are thus likely to satisfy a stronger, progress-sensitive
security condition. Encouragingly, in practice the analysis neces-
sary to prove the absence of progress channels is minimal. We
found that all but one of the loops we needed to secure in Civitas
were simple loops where the loop counter was a low-security vari-
able, the loop bounds were not changed in the loop body, and the
stride was constant. Such loops are easily proven to terminate with
existing termination tools, for example by the analysis we use in
our prototype implementation. The remaining loop uses a collec-
tion iterator from the standard library. Its termination could either
be proved directly with a more powerful tool for heap-based termi-
nation analysis or by appeal to a model of the standard library.

889In the standard library, we found that a heap-based termination
analysis, e.g., [8], would be necessary for 35 of the 66 casts. While
this type of analysis is more complex, it can be applied once for the
library, and subsequent uses of the library can rely on models that
express the termination-relevant properties of collections as arith-
metic operations [19].

We conclude from this audit that strengthening the guarantees
provided by security typed languages is feasible; non-malicious
programs are likely to require minimal modiﬁcation. Thus, it is not
unreasonable to require progress-sensitive guarantees from real-
world security-critical applications.

8. DISCUSSION AND RELATED WORK
Declassiﬁcation The budgeted semantics and type system allow
the attacker to learn a limited amount of secret information, a form
of declassiﬁcation. Much recent work on language-based infor-
mation ﬂow has considered weakening noninterference using de-
classiﬁcation policies to specify what information may be released,
when, where and by whom (see Sabelfeld and Sands [46] for a sur-
vey). Casts for which the oracle is unable to determine whether the
command terminates or diverges can be considered a form of what
information release: the system reveals the termination behavior
of the cast. (The information theoretic bound on this information
provides a form of quantitative information release, another kind of
what information release.)

There are several existing security-type systems that enforce de-
classiﬁcation policies (e.g., [45, 16]), but the semantic security con-
dition enforced is progress insensitive. We note that even if these
type systems were strengthened to enforce a progress-sensitive se-
curity condition by rejecting high-security loops (as in, e.g., [32])
it is not clear how declassiﬁcation annotations may enforce a re-
quirement that secret information is leaked only via the progress
channel.

Assume our commands are extended with declassiﬁcation of ex-
pressions, as in e.g., [3], and consider the following program in
which the loop guard is explicitly declassiﬁed at each iteration.

guard := declassify(h > 0)
while guard do {
h := h − 1;
guard := declassify(h > 0);

}

This program is accepted by the type system of [3]. However, be-
cause after the declassiﬁcation, the loop guard is low, the type sys-
tem also accepts a program that contains a low output in the body
of the loop:

guard := declassify(h > 0)

1
2 while guard do {
h := h − 1;
3
guard := declassify(h > 0);
4
outputL(1);
5
6

}

This program reveals more information than just the fact that the
loop terminates:
it reveals the initial value of h if h is positive,
similar to the example in Listing 1.

Type systems that are designed to prevent information launder-
ing [44, 2, 7, 33] reject the program above, because of the update to
variable h on Line 3, and thus these type systems appear unsuitable
for straightforward adaptation to progress sensitivity.
Progress (in)sensitivity Much recent work on language-based
information ﬂow relies on progress-insensitive noninterference [4]
as the underlying target security condition. Demange and Sands

observe [24] that security guarantees of progress-insensitive non-
interference may be too weak for small secrets. They distinguish
between small and big secrets in programs, and propose a coarse-
grained type system that guarantees progress-sensitive security for
the small secrets and progress-insensitive security for the big se-
crets. This approach can easily beneﬁt from the results of our work.
Secure multi-execution [25, 29] addresses the problem of termi-
nation channels by enforcing strict isolation between outputs on
different security levels. The price is high performance overhead,
and non-trivial modiﬁcation of the semantics of the program.
It
is moreover unclear whether secure multi-execution may be ap-
plied to policies beyond noninterference. Compared to that, our
approach is minimally-invasive and does not change the intended
semantics of the program. This enables straightforward composi-
tion with other work on language-based information ﬂow.

Progress-sensitive enforcement appears in literature on concur-
rent information ﬂow. The enforcement mechanism of Boudol [10],
is, similarly to our approach, parametrized over a class of terminat-
ing programs, but unlike our work, it does not take runtime infor-
mation into account; moreover, nonterminating programs are ruled
out. Type systems of [48, 11, 41] enforce progress-sensitivity by
permitting high loops but disallowing public side effects after that;
this is similar to what one achieves in our language without cast
command.

Recent work by Stefan et al. addresses the problem of termi-
nation channels by spawning background threads for high compu-
tations [53]. A thread may wait upon a spawned computation to
inspect its result; doing so reveals whether the computation ter-
minated, and raises the security level of the waiting thread. This
technique is largely complimentary to ours, and relies on light-
weight concurrency for efﬁciency. It may be an adequate alterna-
tive to halting program execution when the termination oracle fails
or when the progress leakage budget is exhausted.
Integrity While the technical development of this paper focuses
on conﬁdentiality, our results apply to integrity as well. Clarkson
and Schneider [17] introduce two characterizations of integrity:
contamination and suppression. Contamination occurs when un-
trusted input propagates to trusted output; suppression occurs when
the program’s output omits correct output. We believe progress in-
tegrity attacks can be viewed as a form of suppression.
Termination analysis This work is inspired by recent progress
on static analyses for proving termination of realistic imperative
programs [19, 31, 52].

As outlined in Section 7, our current prototype implementation
uses the logical abstract interpretation for termination analysis of
Chawdhary et al. [14]; we currently support programs with linear
termination arguments. Because our language semantics are pa-
rameterized on an oracle for termination analysis, improvements
in automated termination analysis will increase the precision of
our enforcement mechanism. In particular, results on proving ter-
mination for recursive programs [21] and programs with polyno-
mial [23], bit-vector [22], and heap-based [8] termination argu-
ments offer possibilities for further improving precision of the ter-
mination oracle.

Recent work on conditional termination [20] statically computes
preconditions under which a program terminates.
Incorporating
these results may lead to more efﬁcient ways to incorporate low-
security information at runtime.
Quantitative bounds Our budgeted semantics enforces a simple
quantitative bound on the amount of information that may be leaked
via a progress channel. Here, our information-theoretic bound of
log2(B + 1) bits of progress leakage is similar in spirit to the

890bounds presented by Zhang, Askarov, and Myers [5, 57]. A log-
arithmic bound is also given by Rafnsson and Sabelfeld [40]; they
buffer outputs and give the bound in the number of the buffered
output batches.

Much recent work on quantitative information ﬂow focuses on
what the attacker may learn about the secrets based on a single
observation [49, 30]. Incorporating these results provides an inter-
esting avenue for future work.

Smith and Alpizar study non-termination of probabilistic pro-
grams [50]. They demonstrate that when probability of nontermi-
nation in well-typed programs is small, nontermination does not
skew the probability distribution of low outputs. A key technical
element of their proof machinery is a program transformation that
eliminates all high computations in a program. These results ap-
pear particularly relevant for understanding computational security
guarantees of programs that use cryptographic primitives (which
would otherwise be formulated “modulo termination”).
Timing channels Timing channels are known to be a dangerous
covert channel in computer systems. Exploiting timing channels
requires a strong adversary who has access to an external clock in
order to measure timing of the individual outputs. Compared to
that attacker model, we assume a weaker but more widespread at-
tacker who is limited to counting the number of low outputs. This
attacker model is fairly common in both traditional systems as well
as cloud-based batch services, e.g., map/reduce. Because our at-
tacker model considers only a speciﬁc aspect of low observations,
a more precise characterization of security is possible. Indeed, the
information-theoretic bound on the progress channel that we ob-
tain in this work is tighter than the one used in general mitigation
of timing channels [5, 57].
Auditing for information ﬂow Work on auditing systems for
information ﬂow violations pivots around explicit ﬂow violations
(e.g., [36]) or audit of authority decisions for declassiﬁcation [39,
9, 15]. Our work provides means for auditing progress channel vio-
lations. The semantics for low outputs can be augmented to record
failed casts without stopping program execution; these records can
be subjected to a security audit at a later point in time.
Hybrid type checking Our cast operation is related to Flana-
gan’s hybrid type checking [26], where calls to the oracle in our
semantics correspond to dynamic type casts. Unlike hybrid type
checking, however, the budgeted semantics permits continuation of
computation even if the cast fails, at the cost of consuming a unit
of budget.

9. CONCLUSION

We have presented a type system and a runtime mechanism that
together precisely enforce progress sensitive information security,
controlling information leakage via progress channels (also known
as termination channels). The system is parameterized on an oracle
that reasons about the termination behavior of loops. We have im-
plemented such an oracle using logical abstract interpretation [14].
To the best of our knowledge, this is the ﬁrst time cutting-edge
static analysis for program termination has been applied to enforce
strong information security properties.

We have extended our system to track at run time information
leakage via progress channels, and restrict such leakage according
to a budget. This provides a continuum of security guarantees be-
tween progress-sensitive and progress-insensitive security: a zero
budget on leakage enforces progress-sensitive security, and an inﬁ-
nite budget enforces progress-insensitive security.

The paper is primarily concerned with providing security guar-
antees for conﬁdentiality: we ensure that progress channels do not

leak conﬁdential information. However, integrity is a well-known
dual of conﬁdentiality, and our results can also provide integrity
guarantees, preventing untrusted input from inﬂuencing the termi-
nation behavior of a program.

Progress-sensitive security provides stronger guarantees than pro-
gress-insensitive security. The additional effort required to provide
these stronger guarantees appears reasonable. We extended the
Jif compiler [35] to track information ﬂow via progress channels,
and analyzed Civitas [18], a remote voting system implemented
in Jif. The termination behavior of all loops detected by the ex-
tended compiler depends only on low-security information and are
amenable to existing termination analyses. We conclude that Civ-
itas (and likely other security-typed programs) appears to satisfy
a stronger security condition than that implied by the standard Jif
type system, and a suitable termination oracle would be able to
show this with little additional programmer effort.

Acknowledgements
We thanks the anonymous reviewers for their helpful comments.
Andrei Sabelfeld provided helpful comments. This research is sup-
ported by the National Science Foundation under Grant No. 1054172
and by the Air Force Research Laboratory.

References
[1] The Z3 Theorem Prover., 2008. Software release,

http://research.microsoft.com/projects/Z3.

[2] A. Askarov and A. Sabelfeld. Localized delimited release:
Combining the what and where dimensions of information
release. In Proc. ACM Workshop on Programming
Languages and Analysis for Security (PLAS), pages 53–60,
June 2007.

[3] A. Askarov and A. Sabelfeld. Gradual release: Unifying

declassiﬁcation, encryption and key release policies. In Proc.
IEEE Symp. on Security and Privacy, pages 207–221, May
2007.

[4] A. Askarov, S. Hunt, A. Sabelfeld, and D. Sands.

Termination-insensitive noninterference leaks more than just
a bit. In ESORICS, pages 333–348, Oct. 2008.

[5] A. Askarov, D. Zhang, and A. C. Myers. Predictive

black-box mitigation of timing channels. In ACM
Conference on Computer and Communications Security,
pages 297–307, 2010.

[6] J. Barnes. High Integrity Software: The SPARK Approach to

Safety and Security. Addison Wesley, Apr. 2003. ISBN
0321136160.

[7] G. Barthe, S. Cavadini, and T. Rezk. Tractable enforcement

of declassiﬁcation policies. In Proc. IEEE Computer
Security Foundations Symposium, June 2008.

[8] J. Berdine, B. Cook, D. Distefano, and P. W. O’Hearn.

Automatic termination proofs for programs with
shape-shifting heaps. In Proceedings of the 18th
international conference on Computer Aided Veriﬁcation,
CAV’06, pages 386–400, Berlin, Heidelberg, 2006.
Springer-Verlag.

[9] A. Blankstein. Analyzing audit trails in the Aeolus security
platform. Master’s thesis, MIT, Cambridge, MA, USA, June
2011.

891[10] G. Boudol. On typing information ﬂow. In Proceedings of
the Second international conference on Theoretical Aspects
of Computing, ICTAC’05, pages 366–380, Berlin,
Heidelberg, 2005. Springer-Verlag.

[11] G. Boudol and I. Castellani. Non-interference for concurrent

programs and thread systems. Theoretical Computer
Science, 281(1):109–130, June 2002.

[12] C. Cadar, V. Ganesh, P. M. Pawlowski, D. L. Dill, and D. R.
Engler. EXE: Automatically generating inputs of death. In
Proceedings of the 13th ACM Conference on Computer and
Communications Security, 2006.

[13] R. Chang, G. Jiang, F. Ivancic, S. Sankaranarayanan, and

V. Shmatikov. Inputs of coma: Static detection of
denial-of-service vulnerabilities. In Proceedings of the IEEE
Computer Security Foundations Symposium, pages 186–199,
Washington, DC, USA, 2009. IEEE Computer Society.

[14] A. Chawdhary, B. Cook, S. Gulwani, M. Sagiv, and H. Yang.

Ranking abstractions. In Proceedings of the Theory and
practice of software, 17th European conference on
Programming languages and systems, ESOP’08/ETAPS’08,
pages 148–162, 2008.

[15] W. Cheng, D. R. K. Ports, D. Schultz, J. Cowling, V. Popic,

A. Blankstein, D. Curtis, L. Shrira, and B. Liskov.
Abstractions for usable information ﬂow control in Aeolus.
In Proceedings of the 2012 USENIX Annual Technical
Conference, Boston, MA, USA, June 2012. USENIX.

[16] S. Chong and A. C. Myers. End-to-end enforcement of

erasure and declassiﬁcation. In Proceedings of the 21st IEEE
Computer Security Foundations Symposium, pages 98–111,
Piscataway, NJ, USA, June 2008. IEEE Press.

[17] M. R. Clarkson and F. B. Schneider. Quantiﬁcation of

integrity. In Proceedings of the 2010 23rd IEEE Computer
Security Foundations Symposium, CSF ’10, pages 28–43,
Washington, DC, USA, 2010. IEEE Computer Society.

[18] M. R. Clarkson, S. Chong, and A. C. Myers. Civitas: Toward

a secure voting system. In Proceedings of the 2008 IEEE
Symposium on Security and Privacy, SP ’08, pages 354–368,
Washington, DC, USA, 2008. IEEE Computer Society.

[19] B. Cook, A. Podelski, and A. Rybalchenko. Termination
proofs for systems code. In Proceedings of the 2006 ACM
SIGPLAN conference on Programming language design and
implementation, PLDI ’06, pages 415–426, New York, NY,
USA, 2006. ACM.

[20] B. Cook, S. Gulwani, T. Lev-Ami, A. Rybalchenko, and

M. Sagiv. Proving conditional termination. In Proceedings
of the 20th international conference on Computer Aided
Veriﬁcation, CAV ’08, pages 328–340, 2008.

[21] B. Cook, A. Podelski, and A. Rybalchenko. Summarization
for termination: no return! Form. Methods Syst. Des., 35(3):
369–387, Dec. 2009.

[22] B. Cook, D. Kroening, P. Rümmer, and C. M. Wintersteiger.

Ranking function synthesis for bit-vector relations. In
Proceedings of the 16th international conference on Tools
and Algorithms for the Construction and Analysis of
Systems, TACAS’10, pages 236–250, 2010.

[23] P. Cousot. Proving program invariance and termination by

parametric abstraction, lagrangian relaxation and
semideﬁnite programming. In R. Cousot, editor, Veriﬁcation,
Model Checking, and Abstract Interpretation, volume 3385
of Lecture Notes in Computer Science, pages 1–24. Springer
Berlin / Heidelberg, 2005.

[24] D. Demange and D. Sands. All Secrets Great and Small. In

Programming Languages and Systems. 18th European
Symposium on Programming, ESOP 2009, number 5502 in
LNCS, pages 207–221. Springer Verlag, 2009.

[25] D. Devriese and F. Piessens. Noninterference through secure

multi-execution. In Security and Privacy (SP), 2010 IEEE
Symposium on, pages 109 –124, may 2010.

[26] C. Flanagan. Hybrid type checking. In Conference record of
the 33rd ACM SIGPLAN-SIGACT symposium on Principles
of programming languages, POPL ’06, pages 245–256, New
York, NY, USA, 2006. ACM.

[27] R. Grabowski and L. Beringer. Noninterference with
dynamic security domains and policies. In 13th Asian
Computing Science Conference, Focusing on Information
Security and Privacy, 2009.

[28] S. Hunt and D. Sands. On ﬂow-sensitive security types. In

Proc. 33rd ACM Symp. on Principles of Programming
Languages (POPL), pages 79–90, Charleston, South
Carolina, USA, Jan. 2006.

[29] V. Kashyap, B. Wiedermann, and B. Hardekopf. Timing- and

termination-sensitive secure information ﬂow: Exploring a
new approach. In Security and Privacy (SP), 2011 IEEE
Symposium on, pages 413 –428, may 2011.

[30] B. Köpf and G. Smith. Vulnerability bounds and leakage

resilience of blinded cryptography under timing attacks. In
2010 IEEE Computer Security Foundations, July 2010.

[31] D. Kroening, N. Sharygina, A. Tsitovich, and C. M.

Wintersteiger. Termination analysis with compositional
transition invariants. In Proceedings of the 22nd
international conference on Computer Aided Veriﬁcation,
CAV’10, pages 89–103, Berlin, Heidelberg, 2010.
Springer-Verlag.

[32] A. Lux and H. Mantel. Declassiﬁcation with explicit

reference points. In 14th European Symposium on Research
in Computer Security, volume 5789 of LNCS, pages 69–85.
Springer, 2009.

[33] J. Magazinius, A. Askarov, and A. Sabelfeld. Decentralized

delimited release. In APLAS, pages 220–237, 2011.

[34] S. Moore, A. Askarov, and S. Chong. Precise enforcement of

progress-sensitive security. Technical Report TR-04-12,
Harvard School of Engineering and Applied Sciences, 2012.

[35] A. C. Myers, L. Zheng, S. Zdancewic, S. Chong, and

N. Nystrom. Jif 3.0: Java information ﬂow. Software release,
http://www.cs.cornell.edu/jif, July 2006.

[36] J. Newsome and D. Song. Dynamic taint analysis for

automatic detection, analysis, and signature generation of
exploits on commodity software. In Proceedings of the
Network and Distributed System Security Symposium, 2005.

892[37] K. R. O’Neill, M. R. Clarkson, and S. Chong.

Information-ﬂow security for interactive programs. In
Proceedings of the 19th IEEE Computer Security
Foundations Workshop, pages 190–201. IEEE Computer
Society, June 2006.

[38] A. Podelski and A. Rybalchenko. A complete method for the

synthesis of linear ranking functions. In B. Steffen and
G. Levi, editors, Veriﬁcation, Model Checking, and Abstract
Interpretation, volume 2937 of Lecture Notes in Computer
Science, pages 465–486. Springer Berlin / Heidelberg, 2004.

[39] V. Popic. Audit trails in the Aeolus distributed security

platform. Master’s thesis, MIT, Cambridge, MA, USA, Sept.
2010. Also as Technical Report MIT-CSAIL-TR-2010-048.

[40] W. Rafnsson and A. Sabelfeld. Limiting information leakage

in event-based communication. In Proceedings of the ACM
SIGPLAN Sixith Workshop on Programming Languages and
Analysis for Security, June 2011.

[41] W. Rafnsson, D. Hedin, and A. Sabelfeld. Securing

interactive programs. In Proceedings of the 2012 25th IEEE
Computer Security Foundations Symposium, CSF ’12, 2012.

[42] A. Russo and A. Sabelfeld. Dynamic vs. static ﬂow-sensitive

security analysis. In Proceedings of the IEEE Computer
Security Foundations Symposium, 2010.

[43] A. Sabelfeld and A. C. Myers. Language-based

information-ﬂow security. IEEE Journal on Selected Areas
in Communications, 21(1):5–19, Jan. 2003.

[44] A. Sabelfeld and A. C. Myers. A model for delimited
information release. In Proc. International Symp. on
Software Security (ISSS’03), volume 3233 of LNCS, pages
174–191. Springer-Verlag, Oct. 2004.

[45] A. Sabelfeld and A. C. Myers. A model for delimited

release. In Proc. 2003 International Symposium on Software
Security, number 3233 in Lecture Notes in Computer
Science, pages 174–191. Springer-Verlag, 2004.

[46] A. Sabelfeld and D. Sands. Declassiﬁcation: Dimensions

and principles. J. Computer Security, 2009.

[47] V. Simonet. The Flow Caml System: documentation and

user’s manual. Technical Report 0282, Institut National de
Recherche en Informatique et en Automatique (INRIA), July
2003.

[48] G. Smith. A new type system for secure information ﬂow. In
Proc. IEEE Computer Security Foundations Workshop, pages
115–125, June 2001.

[49] G. Smith. Quantifying information ﬂow using min-entropy.

In QEST, pages 159–167, 2011.

[50] G. Smith and R. Alpízar. Nontermination and secure

information ﬂow. Mathematical Structures in Computer
Science (Special Issue on Programming Language
Interference and Dependence), 21:1183–1205, Dec. 2011.

[51] G. Smith and D. Volpano. Secure information ﬂow in a

multi-threaded imperative language. In Proc. ACM Symp. on
Principles of Programming Languages, pages 355–364, Jan.
1998.

[52] F. Spoto, F. Mesnard, and E. Payet. A termination analyzer

for java bytecode based on path-length. ACM Trans.
Program. Lang. Syst., 32(3):8:1–8:70, Mar. 2010.

[53] D. Stefan, A. Russo, P. Buiras, A. Levy, J. C. Mitchell, and

D. Mazières. Addressing covert termination and timing
channels in concurrent information ﬂow systems. In
Proceedings of the 17th ACM SIGPLAN International
Conference on Functional Programming, New York, NY,
USA, June 2012. ACM Press.

[54] N. Swamy, M. Hicks, S. Tse, and S. Zdancewic. Managing
policy updates in security-typed languages. In Proceedings
of the 19th IEEE Computer Security Foundations Workshop,
pages 202–216. IEEE Computer Society, 2006.

[55] D. Volpano and G. Smith. Eliminating covert ﬂows with
minimum typings. In Proc. 10th IEEE Computer Security
Foundations Workshop, pages 156–168, 1997.

[56] D. Volpano, G. Smith, and C. Irvine. A sound type system

for secure ﬂow analysis. Journal of Computer Security, 4(3):
167–187, 1996.

[57] D. Zhang, A. Askarov, and A. C. Myers. Predictive

mitigation of timing channels in interactive systems. In ACM
Conference on Computer and Communications Security,
pages 563–574, 2011.

[58] L. Zheng and A. C. Myers. Dynamic security labels and

noninterference. In Proc. 2nd Workshop on Formal Aspects
in Security and Trust, IFIP TC1 WG1.7. Springer, Aug. 2004.

893