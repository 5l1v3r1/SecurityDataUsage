An Extremely Simple Oblivious RAM Protocol

Path ORAM:

Emil Stefanov†, Marten van Dijk‡, Elaine Shi∗, Christopher Fletcher◦,

Ling Ren◦, Xiangyao Yu◦, Srinivas Devadas◦

† UC Berkeley

‡ UConn

∗ UMD

◦ MIT CSAIL

ABSTRACT
We present Path ORAM, an extremely simple Oblivious
RAM protocol with a small amount of client storage. Partly
due to its simplicity, Path ORAM is the most practical
ORAM scheme for small client storage known to date. We
formally prove that Path ORAM requires O(log2 N/ log χ)
bandwidth overhead for block size B = χ log N . For block
sizes bigger than ω(log2 N ), Path ORAM is asymptotically
better than the best known ORAM scheme with small client
storage. Due to its practicality, Path ORAM has been
adopted in the design of secure processors since its proposal.
Categories and Subject Descriptors
K.6.5 [Management of Computing and Information
Systems]: Security and Protection
General Terms
Algorithms, Security
Keywords
Oblivious RAM; ORAM; Path ORAM; access pattern

1.

INTRODUCTION

It is well-known that data encryption alone is often not
enough to protect users’ privacy in outsourced storage appli-
cations. The sequence of storage locations accessed by the
client (i.e., access pattern) can leak a signiﬁcant amount of
sensitive information about the unencrypted data through
statistical inference. For example, Islam et. al. demon-
strated that by observing accesses to an encrypted email
repository, an adversary can infer as much as 80% of the
search queries [21].

Oblivious RAM (ORAM) algorithms, ﬁrst proposed by Gol-
dreich and Ostrovsky [13], allow a client to conceal its access
pattern to the remote storage by continuously shuﬄing and
re-encrypting data as they are accessed. An adversary can ob-
serve the physical storage locations accessed, but the ORAM

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516660.

algorithm ensures that the adversary has negligible probabil-
ity of learning anything about the true (logical) access pat-
tern. Since its proposal, the research community has strived
to ﬁnd an ORAM scheme that is not only theoretically inter-
esting, also practical [4, 7, 12, 14–17, 22, 23, 25–27, 30, 33–38].
In this paper, we propose a novel ORAM algorithm called
Path ORAM 1. This is to date the most practical ORAM
construction under small client storage. We prove theoret-
ical bounds on its performance and also present matching
experimental results.

Path ORAM makes the following contributions:

Simplicity and practical eﬃciency. In comparison to
other ORAM algorithms, our construction is arguably much
simpler. Although we have no formal way of measuring its
simplicity, the core of the Path ORAM algorithm can be
described in just 16 lines of pseudocode (see Figure 1) and
our construction does not require performing sophisticated
deamortized oblivious sorting and oblivious cuckoo hash
table construction like many existing ORAM algorithms [4,
7, 12–17, 22, 25–27, 35–37]. Instead, each ORAM access can
be expressed as simply fetching and storing a single path
in a tree stored remotely on the server. Path ORAM’s
simplicity makes it more practical than any existing ORAM
construction with small (i.e., constant or poly-logarithmic)
local storage.

Asymptotic eﬃciency. We prove that for a reasonably
large block size B = χ·log N bits where N is the total number
of blocks, Path ORAM with recursion (where recursion is
proposed in Shi et al. [30]; see Section 3.7) achieves an
asymptotic bandwidth cost of O(log2 N/ log χ) blocks, and
consumes O(log2 N/ log χ)ω(1) block client-side storage2. In
other words, to access a single block, the client needs to
in reality access O(log2 N/ log χ) physical blocks to hide its
access patterns from the storage server. The above result
achieves a failure probability of N−ω(1), negligible in N .

As pointed out later in Section 1.1, our result outperforms
the best known ORAM for small client storage [22], both in
terms of asymptotics and practicality, for reasonably large
block sizes, i.e., block sizes typically encountered in practical
applications.

Practical and theoretic impact of Path ORAM. Since
we ﬁrst proposed Path ORAM [32] in February 2012, it

1Our construction is called Path ORAM because data on
the server is always accessed in the form of tree paths.
2Throughout this paper, when we write the notation g(n) =
O(f (n)) · ω(1), we mean that for any function h(n) = ω(1),
it holds that g(n) = O(f (n)h(n)).

299ORAM Scheme

Kushilevitz et al. [22]

Gentry et al. [11] (B = χ log N )

Client Storage

(# blocks)

O(1)

Read & Write Bandwidth

(# blocks)

O(log2 N/ log log N )

O(log2 N ) · ω(1)

O(log3 N/(log log N log χ)) · ω(1)

Recursive Path ORAM (B = χ · log N ) O(log2 N/ log χ) · ω(1)

O(log2 N/ log χ)

(store stash on client)

Recursive Path ORAM (B = χ · log N )

(store stash on server)

O(log N ) · ω(1)

O(log2 N/ log χ) · ω(1)

Table 1: Comparison to other ORAM schemes: Asymptotic bandwidth cost. B is the block size (in terms of #
bits), N is the total number of blocks. The Path ORAM Stash is deﬁned in Section 3. The failure probability is set to N−ω(1)
in this table, i.e., negligible in N .

has made both a practical and a theoretic impact in the
community.

On the practical side, Path ORAM is the most suitable
known algorithm for hardware ORAM implementations due
to its conceptual simplicity, small client storage, and practical
eﬃciency. Ren et al. built a simulator for an ORAM-enabled
secure processor based on the Path ORAM algorithm [29] and
the Ascend processor architecture [9, 10] uses Path ORAM
as a primitive. Maas et al. [24] implemented Path ORAM on
a secure processor using FPGAs and the Convey platform.
On the theoretic side, subsequent to the proposal of Path
ORAM, several theoretic works adopted the same idea of
path eviction in their ORAM constructions — notably the
works by Gentry et al. [11] and Chung et al. [6]. These two
works also try to improve ORAM bounds based on the binary
tree construction by Shi et al. [30]; however, as pointed out
in Section 1.1 our bound is asymptotically better than those
by Gentry et al. [11] and Chung et al. [6]. Gentry’s Path
ORAM variant construction has also been applied to secure
multiparty computation [11].

Novel proof techniques. Although our construction is
simple, the proof for upper bounding the client storage is
quite intricate and interesting. Our proof relies on the cre-
ation of a second ORAM construction and a reduction from
Path ORAM to the second ORAM construction. We provide
concrete bounds showing that for M load/store operations
on N data blocks, recursive Path ORAM with client storage
≤ R log N/ log χ blocks, server storage 28N blocks and band-
width 14(log N )2/ log χ blocks per load/store operation, fails
during one of the M load/store operations with probability
≤ 14 · 0.625−RM log N/ log χ. Our empirical results in Sec-
tion 5 indicate that the constants in practice are even lower
than our theoretic bounds.

1.1 Related Work

Oblivious RAM was ﬁrst investigated by Goldreich and
Ostrovsky [12, 13, 25] in the context of protecting software
from piracy, and eﬃcient simulation of programs on oblivious
RAMs. Since then, there has been much subsequent work
[4,6,7,11–15,17,22,25–27,35,37] devoted to improving ORAM
constructions. Path ORAM is based upon the binary-tree
ORAM framework proposed by Shi et al. [30].

Optimality of Path ORAM. Under small (i.e., constant or
poly-logarithmic) client storage, the best known ORAM was
proposed by Kushilevitz et al., and has O(log2 N/ log log N )
blocks bandwidth cost [22].

Our Path ORAM algorithm with recursion as in Shi et
al. [30] is competitive with Kushilevitz et al. [22] in terms of
bandwidth cost, when the block size is at least Ω(log2 N ) bits;
and can asymptotically outperform Kushilevitz et al. [22]
for larger block sizes. For example, for block size B =
Ω(log2 N ) bits, our bandwidth cost is O(log2 N/ log log N )
blocks, matching the best known bound of Kushilevitz et
al. [22], under small client storage.

Of particular interest is the case when block size is at least
Ω(λ) bits, where λ is the security parameter (e.g., λ = 128
or λ = 256) and N = poly(λ) — since this is what one
typically encounters in practical applications. In this case,
recursive Path ORAM’s bandwidth cost is only O(log N )
blocks; moreover it has O(1) round-trips since the depth
of recursion would be constant. Goldreich and Ostrovsky
show that under O(1) client storage, any ORAM algorithm
must have bandwidth overhead Ω(log N ) (regardless of the
block size). Since then, a long-standing open question is
whether it is possible to have an ORAM construction that has
O(1) or poly log(N ) client-side storage and O(log N ) blocks
bandwidth cost [13, 14, 22]. Our bound partially addresses
this open question for reasonably large block sizes.

Comparison with Gentry et al. and Chung et al..
Gentry et al. [11] improve on the binary tree ORAM scheme
proposed by Shi et al. [30]. To achieve 2−λ failure probabil-
ity, their scheme achieves O(λ(log N )2/(log λ log χ)) blocks
bandwidth cost, for block size B = χ · log N bits. Assuming
that N = poly(λ), their bandwidth cost is O(λ log N/ log χ)
blocks.
In comparison, recursive Path ORAM achieves
O(log2 N/ log χ) blocks bandwidth cost. Note that typi-
cally λ (cid:29) log N since N = poly(λ). Therefore, recursive
Path ORAM is much more eﬃcient than the scheme by
Gentry et al.. Table 1 presents this comparison, assuming
a failure probability of N−ω(1), i.e., negligible in N . Since
N = poly(λ), the failure probability can also equivalently be
written as λ−ω(1). We choose to use N−ω(1) to simplify the
notation in the asymptotic bounds.

Chung and Pass [6] proved a similar (in fact slightly worse)
bound as Gentry et al. [11]. As mentioned earlier, our bound
is asymptotically better than Gentry et al. [11] or Chung
and Pass [6].

Very recently, Chung et al. proposed another statisti-
cally secure binary-tree ORAM algorithm [5] based on Path
ORAM. Their theoretical bandwidth bound is log log n factor
worse than ours. Their simulation results suggest an empiri-
cal bucket size of 4 [1] — which means that their practical
bandwidth cost is a constant factor worse than Path ORAM,

300since they require operating on 3 paths in expectation for
each data access, while Path ORAM requires reading and
writing only 1 path.

Statistical security. We note that Path ORAM is also sta-
tistically secure (not counting the encryption). Statistically
secure ORAMs have been studied in several prior works [2,8].
All known binary-tree based ORAM schemes and variants
are also statistically secure [6, 11, 30] (assuming each bucket
is a trivial ORAM).

2. PROBLEM DEFINITION

We consider a client that wishes to store data at a remote
untrusted server while preserving its privacy. While tradi-
tional encryption schemes can provide conﬁdentiality, they
do not hide the data access pattern which can reveal very
sensitive information to the untrusted server. In other words,
the blocks accessed on the server and the order in which
they were accessed is revealed. We assume that the server
is untrusted, and the client is trusted, including the client’s
processor, memory, and disk.

The goal of ORAM is to completely hide the data access
pattern (which blocks were read/written) from the server.
From the server’s perspective, read/write operations are
indistinguishable from random requests.

Notations. We assume that the client fetches/stores data
on the server in atomic units, referred to as blocks, of size
B bits each. For example, a typical value for B for cloud
storage is 64 − 256 KB while for secure processors smaller
blocks (128 B to 4 KB) are preferable. Throughout the paper,
let N be the working set, i.e., the number of distinct data
blocks that are stored in ORAM.

Like all other related work, our ORAM constructions do
not consider information leakage through the timing channel,
such as when or how frequently the client makes data requests.
Achieving integrity against a potentially malicious server is
discussed in Section 3.8. We do not focus on integrity in our
main presentation.

3. THE PATH ORAM PROTOCOL
We ﬁrst describe the Path ORAM protocol with N/χ +
O(log N ) · ω(1) blocks of client storage, and then later in
Section 3.7 we explain how the client storage can be reduced
to O(log2 N/ log χ) · ω(1) blocks via recursion.
3.1 Overview

We now give an informal overview of the Path ORAM
protocol. The client stores a small amount of local data in
a stash. The server-side storage is treated as a binary tree
where each node is a bucket that can hold up to a ﬁxed
number of blocks.

Main invariant. We maintain the invariant that at any
time, each block is mapped to a uniformly random leaf bucket
in the tree, and unstashed blocks are always placed in some
bucket along the path to the mapped leaf.

Whenever a block is read from the server, the entire path
to the mapped leaf is read into the stash, the requested block
is remapped to another leaf, and then the path that was just
read is written back to the server. When the path is written
back to the server, additional blocks in the stash may be
evicted into the path as long as the invariant is preserved
and there is remaining space in the buckets.
3.2 Server Storage

Simplicity. We aim to provide an extremely simple ORAM
construction in contrast with previous work. Our scheme
consists of only 16 lines of pseudo-code as shown in Figure 1.

Data on the server is stored in a tree consisting of buckets
as nodes. The tree does not have to necessarily be a binary
tree, but we use a binary tree in our description for simplicity.

Security deﬁnitions. We adopt the standard security def-
inition for ORAMs from [34]. Intuitively, the security deﬁni-
tion requires that the server learns nothing about the access
pattern. In other words, no information should be leaked
about: 1) which data is being accessed; 2) how old it is
(when it was last accessed); 3) whether the same data is
being accessed (linkability); 4) access pattern (sequential,
random, etc); or 5) whether the access is a read or a write.

Definition 1

(Security definition). Let

(cid:126)y := ((opM , aM , dataM ), . . . , (op1, a1, data1))

denote a data request sequence of length M , where each opi
denotes a read(ai) or a write(ai, data) operation. Speciﬁcally,
ai denotes the identiﬁer of the block being read or written,
and datai denotes the data being written. In our notation,
index 1 corresponds to the most recent load/store and index
M corresponds to the oldest load/store operation.

Let A((cid:126)y) denote the (possibly randomized) sequence of
accesses to the remote storage given the sequence of data
requests (cid:126)y. An ORAM construction is said to be secure if (1)
for any two data request sequences (cid:126)y and (cid:126)z of the same length,
their access patterns A((cid:126)y) and A((cid:126)z) are computationally
indistinguishable by anyone but the client, and (2) the ORAM
construction is correct in that it returns on input (cid:126)y data that
is consistent with (cid:126)y with probability ≥ 1 − negl(|(cid:126)y|), i.e., the
ORAM may fail with probability negl(|(cid:126)y|).

Binary tree. The server stores a binary tree data structure
of height L = (cid:100)log2(N )(cid:101)− 1 and 2L leafs. The tree can easily
be laid out as a ﬂat array when stored on disk. The levels of
the tree are numbered 0 to L where level 0 denotes the root
of the tree and level L denotes the leafs.

Bucket. Each node in the tree is called a bucket. Each
bucket can contain up to Z real blocks. If a bucket has less
than Z real blocks, it is padded with dummy blocks to always
be of size Z. It suﬃces to choose the bucket size Z to be a
small constant such as Z = 4 (see Section 5.1).
Path. Let x ∈ {0, 1, . . . , 2L − 1} denote the x-th leaf node in
the tree. Any leaf node x deﬁnes a unique path from leaf x
to the root of the tree. We use P(x) to denote set of buckets
along the path from leaf x to the root. Additionally, P(x, (cid:96))
denotes the bucket in P(x) at level (cid:96) in the tree.
Server storage size. Since there are about N buckets in
the tree, the total server storage used is about Z · N blocks.
3.3 Client Storage and Bandwidth

The storage on the client consists of 2 data structures, a

stash and a position map:

Stash. During the course of the algorithm, the client locally
stores a small number of blocks in a local data structure
S called the stash. In Section 6, we prove that the stash
has a worst-case size of O(log N ) · ω(1) blocks with high

301N

S

position

Total # blocks outsourced to server

B
Z
P(x)
P(x, (cid:96))

L = (cid:100)log2 N(cid:101) − 1 Height of binary tree
Block size (in bits)
Capacity of each bucket (in blocks)
path from leaf node x to the root
the bucket at level (cid:96) along the path
P(x)
client’s local stash
client’s local position map
block a is currently associated with leaf
node x, i.e., block a resides somewhere
along P(x) or in the stash.

x := position[a]

Table 2: Notations.

probability. In fact, in Section 5.2, we show that the stash
is usually empty after each ORAM read/write operation
completes.

Position map. The client stores a position map, such that
x := position[a] means that block a is currently mapped to
the x-th leaf node — this means that block a resides in some
bucket in path P(x), or in the stash. The position map
changes over time as blocks are accessed and remapped.

Bandwidth. For each load or store operation, the client
reads a path of Z log N blocks from the server and then writes
them back, resulting in a total of 2Z log N blocks bandwidth
used per access. Since Z is a constant, the bandwidth usage
is O(log(N )) blocks.

Client storage size. Note that the position map is of
size N L = N log N bits, or equivalently, N/χ blocks with
block size B = χ · log N . In Section 3.7, we use this prop-
erty to recursively store the position map in log N/ log χ
separate Path ORAMs. This reduces the client storage to
O(log2 N/ log χ)·ω(1) at the cost of increasing the bandwidth
to O(log2 N/ log χ).

If the client stores its storage at the server, then at every
load/store operation the client needs to retrieve this storage
from the server. Since the client accesses the log N/ log χ
separate ORAMs one after another, the client only needs
suﬃcient storage for reading in a single path and stash of
each ORAM separately: This leads to a client storage of
O(log N ) · ω(1) and bandwidth O(log2 N/ log χ) · ω(1).
3.4 Path ORAM Initialization

The client stash S is initially empty. The server buckets are
intialized to contain random encryptions of the dummy block
(i.e., initially no block is stored on the server). The client’s
position map is ﬁlled with independent random numbers
between 0 and 2L − 1. The position map initially contains
null for the position of every block. The position null is a
special value indicating that the corresponding block has
never been accessed and the client should assume it has a
default value of zero.
3.5 Path ORAM Reads and Writes

In our construction, reading and writing a block to ORAM
is done via a single protocol called Access described in Fig-
ure 1. Speciﬁcally, to read block a, the client performs
data ← Access(read, a, None) and to write data∗ to block a,
the client performs Access(write, a, data∗). The Access proto-
col can be summarized in 4 simple steps:

S ← S ∪ ReadBucket(P(x, (cid:96)))

Access(op, a, data∗):
1: x ← position[a]
2: position[a] ← UniformRandom(0 . . . 2L − 1)
3: for (cid:96) ∈ {0, 1, . . . , L} do
4:
5: end for
6: data ← Read block a from S
7: if op = write then
8:
9: end if
10: for (cid:96) ∈ {L, L − 1, . . . , 0} do
11:
12:
13:
14:
15: end for

S ← (S − {(a, data)}) ∪ {(a, data∗)}

S(cid:48) ← {(a(cid:48), data(cid:48)) ∈ S : P(x, (cid:96)) = P(position[a(cid:48)], (cid:96))}
S(cid:48) ← Select min(|S(cid:48)|, Z) blocks from S(cid:48).
S ← S − S(cid:48)
WriteBucket(P(x, (cid:96)), S(cid:48))

16: return data

Figure 1: Protocol for data access. Read or write a
data block identiﬁed by a. If op = read, the input parameter
data∗ = None, and the Access operation reads block a from
the ORAM. If op = write, the Access operation writes the
speciﬁed data∗ to the block identiﬁed by a and returns the
block’s old data.

1. Remap block (Lines 1 to 2): Randomly remap the
position of block a to a new random position. Let x
denote the block’s old position.
2. Read path (Lines 3 to 5): Read the path P(x) con-

taining block a.

3. Update block (Lines 6 to 9): If the access is a write,

update the data stored for block a.

4. Write path (Lines 10 to 15): Write the path back
and possibly include some additional blocks from the
stash if they can be placed into the path. Buckets are
greedily ﬁlled with blocks in the stash in the order of
leaf to root, ensuring that blocks get pushed as deep
down into the tree as possible. A block a(cid:48) can be placed
in the bucket at level (cid:96) only if the path P(position[a(cid:48)])
to the leaf of block a(cid:48) intersects the path accessed P(x)
at level (cid:96). In other words, if P(x, (cid:96)) = P(position[a(cid:48)], (cid:96)).

Subroutines. We now explain the ReadBucket and the
WriteBucket subroutine. For ReadBucket(bucket), the client
reads all Z blocks (including any dummy blocks) from the
bucket stored on the server. Blocks are decrypted as they
are read.

For WriteBucket(bucket, blocks), the client writes the blocks
blocks into the speciﬁed bucket on the server. When writing,
the client pads blocks with dummy blocks to make it of size
Z — note that this is important for security. All blocks (in-
cluding dummy blocks) are re-encrypted, using a randomized
encryption scheme, as they are written.
Computation. Client’s computation is O(log N ) · ω(1) per
data access. In practice, the majority of this time is spent
decrypting and encrypting O(log N ) blocks per data access.
We treat the server as a network storage device, so it only
needs to do the computation necessary to retrieve and store
O(log N ) blocks per data access.

3023.6 Security Analysis

To prove the security of Path-ORAM, let (cid:126)y be a data
request sequence of size M . By the deﬁnition of Path-ORAM,
the server sees A((cid:126)y) which is a sequence

p = (positionM [aM ], positionM−1[aM−1], . . . , position1[a1]),

where positionj[aj] is the position of address aj indicated
by the position map for the j-th load/store operation, to-
gether with a sequence of encrypted paths P(positionj(aj)),
1 ≤ j ≤ M , each encrypted using randomized encryption.
The sequence of encrypted paths is computationally indis-
tinguishable from a random sequence of bit strings by the
deﬁnition of randomized encryption (note that ciphertexts
that correspond to the same plaintext use diﬀerent random-
ness and are therefore indistinguishable from one another).
The order of accesses from M to 1 follows the notation from
Deﬁnition 1.

Notice that once positioni(ai) is revealed to the server,
it is remapped to a completely new random label, hence,
positioni(ai) is statistically independent of positionj(aj) for
j < i with aj = ai. Since the positions of diﬀerent addresses
do not aﬀect one another in Path ORAM, positioni(ai) is
statistically independent of positionj(aj) for j < i with aj (cid:54)=
ai. This shows that positioni(ai) is statistically independent
of positionj(aj) for j < i, therefore, (by using Bayes rule)
j=1 P rob(positionj(aj)) = (2l)−M . This proves
that A((cid:126)y) is computationally indistinguishable from a random
sequence of bit strings.
Now the security follows from Theorem 1 in Section 6:
For a stash size O(log N ) · ω(1) Path ORAM fails (in that it
exceeds the stash size) with at most negligible probability.

P rob(p) =(cid:81)M

3.7 Recursion to Reduce Client Storage
For χ ≥ 2, we now explain how to reduce the client storage
from N/χ blocks to O((log N )2/ log χ) · ω(1) blocks using
an approach similar to the one introduced in [34]. The
reduction of client storage comes at the cost of increasing the
bandwidth from O(log N ) to O(log2 N/ log χ). Notice that
assuming that the block size B = χ · log(N ) bits with χ ≥ 2
is a reasonable assumption that has been made by Stefanov
et al. [33, 34] and Shi et al. [30]. For example, a standard
4KB block consists of 32768 bits and this assumption holds
for all N ≤ 216382.
The main idea is to recursively store the position map in
a smaller Path ORAM with N(cid:48) = N/χ blocks of size B bits.
After log N/ log χ recursions, this leads to a constant sized
position map of the ﬁnal Path ORAM. The position map
stored on the client is only the one O(1) sized position map for
the last level of recursion. The client still needs to store the
an O(log N )· ω(1) stash for each of the O(log N/ log χ) levels
of recursion, resulting in a total of O((log N )2/ log χ) · ω(1)
blocks for the total client storage.

Path ORAM access with recursion. Consider a recur-
sive Path ORAM made up of a series of ORAMs called
ORam0, ORam1, ORam2, . . . , ORamX where ORam0 contains
the data blocks, the position map of ORami is stored in
ORami+1, and the client stores the position map for ORamX .
To access a block in ORam0, the client looks up its position in
ORam1, which triggers a recursive call to look up the position
of the position in ORam2, and so on until ﬁnally a position
of ORamX is looked up in the client storage. Essentially,

we can replace lines 1–2 in Figure 1 with a recursive call to
Access.
3.8 Integrity

Our protocol can be easily extended to provide integrity
(with freshness) for every access to the untrusted server
storage. Because data from untrusted storage is always
fetched and stored in the form of a tree paths, we can achieve
integrity by simply treating the Path ORAM tree as a Merkle
tree where data is stored in all nodes of the tree (not just
the leaf nodes). In other words, each node (bucket) of the
Path ORAM tree is tagged with a hash of the following form

H(b1 (cid:107) b2 (cid:107) . . . (cid:107) bZ (cid:107) h1 (cid:107) h2)

where bi for i ∈ {1, 2, . . . , Z} are the blocks in the bucket
(some of which could be dummy blocks) and h1 and h2 are the
hashes of the left and right child. For leaf nodes, h1 = h2 = 0.
Hence only two hashes (for the node and its sibling) needs
to be read or written for each ReadBucket or WriteBucket
operation.

In [28], Ren et al. further optimize the integrity veriﬁcation

overhead for the recursive Path ORAM construction.

4. APPLICATIONS
4.1 Oblivious Binary Search Tree

Based on a class of recursive, binary tree based ORAM
constructions, Gentry et al. propose a novel method for
performing an entire binary search using a single ORAM
lookup [11]. Their method is immediately applicable to
Path ORAM. As a result, Path ORAM can be used to
perform search on an oblivious binary search tree, using
O(log2 N/ log χ) bandwidth. Note that since a binary search
requires navigating a path of O(log N ) nodes, using existing
generic ORAM techniques would lead to bandwidth cost of
O((log N )3/ log log N ).
4.2 Stateless ORAM

Oblivious RAM is often considered in a single-client model,
but it is sometimes useful to have multiple clients accessing
the same ORAM. In that case, in order to avoid complicated
(and possibly expensive) oblivious state synchronization be-
tween the clients, Goodrich et al. introduce the concept of
stateless ORAM [18] where the client state is small enough so
that any client accessing the ORAM can download it before
each data access and upload it afterwards. Then, the only
thing clients need to store is the private key for the ORAM
(which does not change as the data in the ORAM changes).
In our recursive Path ORAM construction, we can down-
load and upload the client state before and after each ac-
cess. Since the client state is only O(log2 N/ log χ) · ω(1)
and the bandwidth is O(log2 N/ log χ), we can reduce the
permanent client state to O(1) and achieve a bandwidth
of O(log2 N/ log χ) · ω(1). Note that during an access the
client still needs about O(log N ) · ω(1) transient memory to
perform the Access operation, but after the Access operation
completes, the client only needs to store the private key.
4.3 Secure Processors

In a secure processor setting, private computation is done
inside a tamper-resistant processor (or board) and main mem-
ory (e.g., DRAM) accesses are vulnerable to eavesdropping

303and tampering. As mentioned earlier, Path ORAM is partic-
ularly amenable to hardware design because of its simplicity
and low on-chip storage requirements.

Fletcher et al. [9, 10] and Ren et al. [29] built a simu-
lator for a secure processor based on Path ORAM. They
optimize the bandwidth cost and stash size of the recursive
construction by using a smaller Z = 3 in combination with a
background eviction process that does not break the Path
ORAM invariant.

Maas et al. [24] built a hardware implementation of a Path
ORAM based secure processor using FPGAs and the Convey
platform.

Ren et al. [29] and Maas et al. [24] report about 1.2X to 5X
performance overhead for many benchmarks such as SPEC
traces and SQLite queries. To achieve this high performance,
these hardware Path ORAM designs rely on on-chip caches
while making Path ORAM requests only when last-level
cache misses occur.

5. EVALUATION
5.1 Stash Occupancy Distribution

Stash occupancy. In both the experimental results and the
theoretical analysis, we deﬁne the stash occupancy to be the
number of overﬂowing blocks (i.e., the number of blocks that
remain in the stash) after the write-back phase of each ORAM
access. This represents the persistent local storage required
on the client-side. In addition, the client also requires under
Z log2 N transient storage for temporarily caching a path
fetched from the server during each ORAM access.

Our main theorem in Section 6 shows the probability of
stash overﬂow decreases exponentially with the stash size,
given that the bucket size Z is large enough. This theorem
is veriﬁed by experimental results as shown in Figure 3 and
Figure 2. In each experiment, the ORAM is initially empty.
We ﬁrst load N blocks into ORAM and then access each
block in a round-robin pattern. I.e., the access sequence is
{1, 2, . . . , N, 1, 2, . . . , N, 1, 2, . . .}. Section 6.3 shows this is
a worst-case access pattern in terms of stash occupancy for
Path ORAM. We simulate our Path ORAM for a single run
for about 250 billion accesses after doing 1 billion accesses for
warming-up the ORAM. It is well-known that if a stochastic
process is regenerative (empirically veriﬁed to be the case for
Path ORAM), the time average over a single run is equivalent
to the ensemble average over multiple runs (see Chapter 5
of [19]).
Figure 2 shows the minimum stash size to get a failure
probability less than 2−λ with λ being the security parameter
on the x-axis. In Table 3, we extrapolate those results for
realistic values of λ. The experiments show that the required
stash size grows linearly with the security parameter, which
is in accordance with the Main Theorem in Section 6 that
the failure probability decreases exponentially with the stash
size. Figure 3 shows the required stash size for a low failure
probability (2−λ) does not depend on N . This shows Path
ORAM has good scalability.
Though we can only prove the theorem for Z ≥ 6, in
practice, Z = 4 and Z = 5 also work well. Z = 3 behaves
relatively worse in terms of stash occupancy, and it is unclear
whether Z = 3 works.

We only provide experimental results for small security
parameters to show that the required stash size is O(λ) and

Figure 2: Empirical estimation of the required stash
size to achieve failure probability less than 2−λ where
λ is the security parameter. Measured for N = 216,
but as Figure 3 shows, the stash size does not de-
pend on N (at least for Z = 4). The measurements
represent a worst-case (in terms of stash size) ac-
cess pattern. The stash size does not include the
temporarily fetched path during Access.

Security Parameter (λ)

80
128
256

5

6

Bucket Size (Z)
4
Max Stash Size
89
147
303

63
105
218

53
89
186

Table 3: Required max stash size for large secu-
rity parameters. Shows the maximum stash size re-
quired such that the probability of exceeding the
stash size is less than 2−λ for a worst-case (in terms
of stash size) access pattern. Extrapolated based on
empirical results for λ ≤ 26. The stash size does not
include the temporarily fetched path during Access.

does not depend on N . Note that it is by deﬁnition infeasible
to simulate for practically adopted security parameters (e.g.,
λ = 128), since if we can simulate a failure in any reason-
able amount of time with such values, they would not be
considered secure.

A similar empirical analysis of the stash size (but with the

path included in the stash) was done by Maas et al. [24].

5.2 Bucket Load
Figure 4 gives the bucket load per level for Z ∈ {3, 4, 5}.
We prove in Section 6.3 that for Z ≥ 6, the expected usage
of a subtree T is close to the number of buckets in it. And
Figure 4 shows this also holds for 4 ≤ Z ≤ 5. For the levels
close to the root, the expected bucket load is indeed 1 block
(about 25% for Z = 4 and 20% for Z = 5). The fact that the
root bucket is seldom full indicates the stash is empty after
a path write-back most of the time. Leaves have slightly
heavier loads as blocks accumulate at the leaves of the tree.
Z = 3, however, exhibits a diﬀerent distribution of bucket
load (as mentioned in Section 5.1 and shown in Figure 2,
Z = 3 produces much larger stash sizes in practice).

304(a) Z=3

(b) Z=4

(c) Z=5

Figure 4: Average bucket load of each level for diﬀerent bucket sizes. The error bars represent the 1/4 and
3/4 quartiles. Measured for a worst-case (in terms of stash size) access pattern.

an empty ORAM; s completely deﬁnes all the randomness
needed to determine, for each block address a, its leaf label
and which bucket/stash stores the block that corresponds
to a. In particular the number of real blocks stored in the
buckets/stash can be reconstructed.
We assume an inﬁnite stash and in our analysis we investi-
gate the usage u(TL(Z)[s]) of the stash deﬁned as the number
of real blocks that are stored in the stash after a sequence
s of load/store operations. In practice the stash is limited
to some size R and Path-ORAM fails after a sequence s of
load/store operations if the stash needs more space: this
happens if and only if the usage of the inﬁnite stash is at
least u(TL(Z)[s]) > R.

Theorem 1

(Main). Let a be any sequence of block
addresses with a working set of size at most N . For a bucket
size Z = 7, tree depth L ≥ (cid:100)log N(cid:101) + 1 and stash size R,
the probability of a Path ORAM failure after a sequence of
load/store operations corresponding to a, is at most

P rob(u(TL(Z)[s]) > R|a(s) = a) ≤ 14 · 0.625R,

where the probability is over the coin ﬂips that determine x
and y in s = (a, x, y).
For Z = 6 and L ≥ (cid:100)log N(cid:101) + 3 we obtain the bound
370 · 0.667R.

As a corollary, for s load/store operations on N data blocks,
Path ORAM with client storage ≤ R blocks, server storage
28N blocks and bandwidth 14 log N blocks per load/store
operation, fails during one of the s load/store operations with
probability ≤ s · 14 · 0.625R. So, if we assume the number of
load/stores is equal to s = poly(N ), then, for a stash of size
O(log N )ω(1), the probability of Path ORAM failure during
one of the load/store operations is negligible in N .

Proof outline. The proof of the main theorem consists of
several steps: First, we introduce a second ORAM, called
∞-ORAM, together with an algorithm that post-processes
the stash and buckets of ∞-ORAM in such a way that if ∞-
ORAM gets accessed by a sequence s of load/store operations,
then post-processing leads to a distribution of real blocks
over buckets that is exactly the same as the distribution as
in Path ORAM after being accessed by s.
Second, we characterize the distributions of real blocks
over buckets in (a not post-processed) ∞-ORAM for which

Figure 3: The stash size to achieve failure proba-
bility less than 2−λ does not depend on N (Z = 4).
Measured for a worst-case (in terms of stash size)
access pattern. The stash size does not include the
temporarily fetched path during Access.

6. THEORETIC BOUNDS

In this section we will bound the probability that, after a
sequence of load/store operations, non-recursive Path-ORAM
exceeds its stash size: We will show that this probability
decreases exponentially in the size of the stash for a constant
bucket size.
By TL(Z) we denote a non-recursive Path-ORAM with
L+1 levels in which each bucket stores Z real/dummy blocks;
the root is at level 0 and the leafs are at level L.

We deﬁne a sequence of load/store operations s as a triple
(a, x, y) that contains (1) the sequence a = (aj)s
j=1 of block
addresses of blocks that are loaded/stored, (2) the sequence of
labels x = (Xj)s
j=1 as seen by the server, and (3) the sequence
y = (Yj)s
j=1 of remapped leaf labels. (a1, Y1, X1) corresponds
to the most recent load/store operation, (a2, Y2, X2) corre-
sponds to the next most recent load/store operation, etc.
The number of load/store operations is denoted by s = |s|.
The working set corresponding to a is deﬁned as the number
of distinct block addresses aj in a. We write a(s) = a.
By TL(Z)[s] we denote the distribution of real blocks in
TL(z) after a sequence s of load/store operations starting with

305post-processing leads to a stash usage > R. We show that the
stash usage after post-processing is > R if and only if there
exists a subtree T for which its ”usage” in ∞-ORAM is more
than its ”capacity”. This means that we can use the union
bound to upper bound P rob(TL(Z)[u(s)] > R|a(s) = a ) as
a sum of probabilities over subtrees.

Third, we analyze the usage of subtrees T . We show
how a mixture of a binomial and a geometric probability
distribution expresses the probability of the number of real
blocks that do not get evicted from T after a sequence s
of load/store operations. By using the Chernoﬀ bounding
technique we prove the main theorem.
6.1 ∞-ORAM

We deﬁne ∞-ORAM, denoted by RL(Z), as an ORAM
that exhibits the same tree structure as Path-ORAM with
L + 1 levels but where each bucket has an inﬁnite size. The
interface of RL(Z) operates as the interface of TL(∞) with
the following distinguishing exception: Each bucket in RL(Z)
is partially stored in server storage and partially stored in
the ORAM stash at the client; Z indicates the maximum
number of blocks in a bucket that can be stored in server
storage. If a bucket has more than Z real blocks, then the
additional blocks are stored in the ∞-ORAM stash.
We deﬁne post-processing3 G(RL(Z)[s]) as a Greedy al-
gorithm G that takes as input the state of RL(Z) after a
sequence s of load/store operations and repeats the following
strategy:

1. Select a block in the stash that was not selected before.
Suppose it has leaf label L and is stored in the bucket
at level h on the path from the root to leaf L.
2. Find the highest level i ≤ h such that the bucket at
level i on the path to leaf L stores < Z blocks. If such
a bucket exists, then use it to store the block. If it does
not exist, then leave the block in the stash.

Lemma 1. The stash usage in a post-processed ∞-ORAM

is exactly the same as the stash usage in Path-ORAM:

u(G(RL(Z)[s])) = u(TL(Z)[s]).

Proof: We ﬁrst notice that the order in which the Greedy
strategy G processes blocks from the stash does not aﬀect
the number of real blocks in each bucket that are stored in
server storage after post-processing: Suppose the Greedy
strategy ﬁrst processes a stash block b1 with label X1 stored
in a bucket at level h1 and suppose it ﬁnds empty space in
server storage at level i1; the Greedy strategy up to block b1
has used up all the empty space in server storage allocated
to the buckets along the path to the leaf with label X1 at
levels i1 < i ≤ h1. Suppose next a stash block b2 with label
X2 stored in a bucket at level h2 ﬁnds empty space in server
storage at level i2; the Greedy strategy up to block b2, which
includes the post-processing of b1, has used up all the empty
space in server storage allocated to the buckets along the
path to the leaf with label X2 at levels i2 < i ≤ h2. If we
swap the post-processing of b1 and b2, then b2 is able to ﬁnd
empty space in the bucket at level i2, but may ﬁnd empty
space at a higher level since b1 has not yet been processed.
In this case, i2 < i1 and paths X1 and X2 intersect at least
up to and including level i1. This means that b2 is stored
3Notice that the post-processing allows us to reason about
Path-ORAM’s stash size. It does not hide memory access
patterns: ∞-ORAM + post-processing is only a tool for
analysis, it is not an ORAM algorithm in itself.

at level i1 and b1 will use the empty space in server storage
at level i2. This means that the number of blocks that are
stored in the diﬀerent buckets after post-processing b1 and
b2 is the same if b1 is processed before or after b2.
Now, we are able to show, by using induction in |s|, that for
each bucket b in a post-processed ∞-ORAM after a sequence
s of load/store operations, the number of real blocks stored
in b that are in server storage is equal to the number of
blocks stored in the equivalent bucket in Path-ORAM after
applying the same sequence s of load/store operations: The
statement clearly holds for |s| = 0 when both ORAMs are
empty. Suppose it holds for |s| ≥ 0. Consider the next
load/store operation to some leaf L(cid:48). After reading the path
to leaf L(cid:48) into the cache, Path-ORAM moves blocks from its
cache/stash into the path from root to leaf L(cid:48) according to
algorithm G(.). Therefore, postprocessing after the ﬁrst |s|
operations followed by the Greedy approach of Path ORAM
that processes the (|s| + 1)-th operation is equivalent to post-
processing after |s| + 1 operations where some blocks may
be post-processed twice. Since the order in which blocks are
post-processed does not matter, we may group together the
multiple times a block b is being post-processed and this is
equivalent to post-processing b exactly once as in G(.). The
number of real blocks in the stash is the same and this proves
the lemma.

6.2 Usage/Capacity Bounds
To investigate which distributions of real blocks over buck-
ets in a not post-processed ∞-ORAM RL(Z) lead to a stash
usage of > R after post-processing, we start by analyzing
distributions of blocks over subtrees: When we talk about
a subtree T , we always implicitly assume that it is rooted
at the root of the ORAM tree, i.e., T contains the root of
the ORAM tree and all its nodes are connected to this root.
We deﬁne n(T ) to be the total number of nodes in T . For
∞-ORAM we deﬁne the usage u(RL(Z)[s]; T ) of T after a
sequence s of load/store operations as the actual number of
real blocks that are stored in the buckets of T (in ∞-ORAM
blocks in a bucket are either in server storage or in the stash).

The following lemma characterizes the stash usage:

Lemma 2. The stash usage u(G(RL(Z)[s])) in post-processed

∞-ORAM is > R if and only if there exists a subtree T in
RL(Z) such that u(RL(Z)[s]; T ) > n(T )Z + R.

Proof: For a stash of size R, since post-processing can at
most store n(T )Z real blocks in T in server storage, we may
interpret n(T )Z +R as the capacity of T . Clearly, if the usage
of T is more than the capacity of T , then post-processing
leads to a stash usage of > R.
Suppose that u(G(RL(Z)[s])) > R. Deﬁne T as the union
of all paths from the root to a bucket b for which the buckets
along the path in G(RL(Z)[s]) each have Z real blocks stored
in server storage. Then, u(G(RL(Z)[s]); T ) > n(T )Z + R.
Also, if a real block in T or in the stash originated from
a bucket b, then the Greedy approach of post-processing
implies that the buckets on the path from where the block is
stored in T to bucket b each store Z real blocks, therefore,
b ∈ T . This shows that u(RL(Z)[s]; T ) ≥ u(G(RL(Z)[s]); T ),
which ﬁnishes the proof of the lemma.

306Let a be a sequence of block addresses. As a corollary to

Lemmas 1 and 2, we obtain

P rob(u(TL(Z)[s]) > R|a(s) = a)

= P rob(u(G(RL(Z)[s])) > R|a(s) = a)
= P rob(∃T u(RL(Z)[s]; T ) > n(T )Z + R|a(s) = a)
P rob(u(RL(Z)[s]; T ) > n(T )Z + R|a(s) = a),

≤ (cid:88)

T

where the inequality follows from the union bound. The
probabilities are over the coin ﬂips that determines s given
a(s) = a.
to the Catalan number Cn, which is ≤ 4n,

Since the number of ordered binary trees of size n is equal

P rob(u(TL(Z)[s]) > R|a(s) = a) ≤

(1)
P rob(u(RL(Z)[s]; T ) > nZ + R|a(s) = a).

4n max

T :n(T )=n

(cid:88)

n≥1

6.3 Chernoff Bound

We will upper bound the probability in the sum of the right-
hand side of (1) for T with n(T ) = n. To this purpose let
a = (aj)s
j=1 be a sequence of block addresses with working
set size N , i.e., a has N diﬀerent block addresses. Let
s = (a, y = (Yj)s
j=1) be a sequence of load/store
operations. In this section we only provide a proof sketch,
precise arguments are in Section 6.4.

j=1, x = (Xj)s

For each block Yj we wish to determine the probability
that Yj gets evicted from T . We ﬁrst observe that Yj never
gets evicted if Yj ∈ V , the set of leafs in T that are also leafs
in the ORAM tree. Therefore, the expectation of the usage
of T is at most

E[u(RL(Z)[s]; T )|a(s) = a] ≤
E[#Yj ∈ V ] + E[u(RL(Z)[s]; T )|a(s) = a,∀jYj (cid:54)∈ V ].

Since the probability that Yj ∈ V is equal to |V |/2L,

E[#Yj ∈ V ] = N|V |/2L.

(2)

j=1 and (Yj)N

Next we observe that Yj can only get evicted over paths
to leafs with labels Xj, Xj−1, . . . , X1 (which correspond to
the more recent load/store operations). Let uj, as a function
of the label values for Xj, Xj−1, . . . , X1, be the probability
that Yj is evicted from T given Yj (cid:54)∈ V .
In ∞-ORAM
the worst-case address pattern a for a ﬁxed working set
size N does not duplicate block addresses, i.e., s = N and
as a consequence the labels in (Xj)N
j=1 are all
statistically independent of one another: Assuming non-
duplicated block addresses, u1 ≤ u2 ≤ u3 ≤ . . ., hence,
there exist segments U1 = {1, 2, . . . , q1} (cid:54)= ∅, U2 = {q1 +
1, . . . , q1 + q2} (cid:54)= ∅, . . ., such that within each segment Uk
the probabilities uj, j ∈ Uk, are all equal to one another. I.e,
there exists a probability hk such that hk = uj, j ∈ Uk. We
notice that the number of segments Uk with hk < 1 equals
n− 2|V |, the number of buckets that are one edge away from
T , minus 1. By the deﬁnition of hk,
E[#Yj, j ∈ Uk, not evicted from T | |Uk|] = (1 − hk)|Uk|.
(3)
We notice that uj+1 (cid:54)= uj if and only if Xj+1 matters in
the calculation of uj+1, i.e., there exists a block that can
get evicted from T over the path to the leaf with label Xj+1
while it cannot get evicted over a path to a leaf with label
Xi, 1 ≤ i ≤ j. This means that Xj+1 (cid:54)∈ V and, for each

Figure 5: Subtree T for proof in Section 6.4.

1 ≤ i ≤ j, the intersection of the path to Xj+1 and the path
to Xi lies inside T , i.e., a block with label Xj+1 cannot get
evicted from T over a path to leaf Xi. This proves

P rob(Xj+1 such that uj+1 = uj)

= P rob(Xj+1 ∈ V or

Xj+1 can be evicted from T over (Xi)j

i=1)

= P rob(Xj+1 evicted from T|Xj+1 (cid:54)∈ V )P rob(Xj+1 (cid:54)∈ V )

+P rob(Xj+1 ∈ V )

= uj(1 − |V |/2L) + |V |/2L
= 1 − (1 − uj)(1 − |V |/2L).

This shows that |Uk| has a geometric distribution with ex-
pectation E[|Uk|] equal to

(1 − (1 − hk)(1 − |V |/2L))q−1(1 − hk)(1 − |V |/2L)q

(cid:88)

q≥1

=

1

(1 − hk)(1 − |V |/2L)

.

Combining (3) and (4) yields

(4)

E[u(RL(Z)[s]; T )|a(s) = a,∀jYj (cid:54)∈ V ]

≤ n−2|V |(cid:88)

k=1

1 − hk

(1 − hk)(1 − |V |/2L)

n − 2|V |
1 − |V |/2L .

=

Together with (2), the expectation of the usage of T is at
most N|V |/2L + (n − 2|V |)/(1 − |V |/2L).
In (1) we are only interested in a non-zero probability that
T ’s usage is > nZ +R. Therefore, since |V | ≤ n and T ’s usage
is at most N , we may constrain |V | ≤ N/Z. For N/2L ≤ 2
and Z ≥ 4, the expectation is at most n/(1−|V |/2L) ≤ 2n. In
the sum of the right-hand side of (1) we bound the probability
that T ’s usage is > nZ + R, which is a multiple larger than
the expectation of the usage in T . This means that we are
able to apply a Chernoﬀ bounding technique to prove the
main theorem: For bucket size Z large enough, we are able
to compensate the 4n term in (1).
6.4 Details of Chernoff Bound
Before getting into proof details, we start by giving some
intuition on how blocks get evicted from a subtree T in ∞-
ORAM. For simplicity we assume T has no leafs with the
ORAM tree in common. Figure 5 gives an example of a
subtree T ; the ﬁgure also shows the boundary nodes that
are at one edge distance from T . When blocks get evicted
from T , they will pass through one of these boundary nodes
to a higher level outside T . X1 is the most recent label of a
block that was read from ORAM and written back with a
new random label Y1. X2 is the next older label of a block

307that was read from ORAM and written back with a new
random label Y2, etc.

We deﬁne Sj as the set of leafs of the ORAM tree that
can be reached through a path that has a boundary node
in common with one of the paths to X1, X2, . . ., or Xj. In
the example: |S1| < |S2|, since X1 and X2 have no boundary
point in common; S2 = S3, since X3 and X1 have a boundary
point in common; S3 = S4, since X4 and X2 have a boundary
point in common; |S4| < |S5|, since X5 has no boundary
point in common with S4.

Notice that the block with label Yj only gets evicted if it
is in Sj, i.e., one of the more recent load/stores of a path
toXi, 1 ≤ i ≤ j, has a boundary node in common with
the path to Yj over which the corresponding block will get
evicted from T . We introduce the following notation: w1 =
|S1| < w2 = |S2| = |S3| = |S4| < w3 = |S5| . . . and q1 = 1
denoting the number of sets Sj with size equal to w1, q2 = 3
denoting the number of sets Sj with size equal to w2 etc. Let
m1 = I(Y1 (cid:54)∈ S1), m2 = I(Y2 (cid:54)∈ S2)+I(Y3 (cid:54)∈ S3)+I(Y4 (cid:54)∈ S4),
where I(.) is the indicator function outputting 1 if the input
is true and 0 if the input is false. We are interested in the
j mj, which is the number of blocks not evicted

sum M =(cid:80)

from T , in other words it equals the usage of T .

The probability distribution of qi is a geometric distribu-
tion with its parameter a function of wi. The probability
distribution of mi is a binomial distribution with its parame-
ters characterized by qi and wi. The probability distribution
of wi is not correlated with qi and mi. These dependencies
allow us to use Bayes rule to compute the probability distri-
bution on M . The proof now proceeds by only considering
the worst case sequence (wi)i≥1 and use probability generat-
ing functions to upper bound the tail distribution of M by
using Chernoﬀ bounding techniques.

j=1 and (Yj)N

We will now prove in detail an upper bound on the prob-
ability in the sum of the right-hand side of (1) for general
T with n(T ) = n. To this purpose let a = (aj)s
j=1 be a
sequence of block addresses with working set size N , i.e., a
has N diﬀerent block addresses.
Worst-case address pattern. In ∞-ORAM a worst-case
address pattern a for a ﬁxed working set size N does not
duplicate block addresses, i.e., s = N and as a consequence
the labels in (Xj)N
j=1 (that deﬁne s given a(s) =
a) are all statistically independent of one another:
Suppose that there exists an address in a that has been
loaded/stored twice in ∞-ORAM. Then, there exist indices i
and j, i < j, with ai = aj. Without the j-th load/store, the
working set remains the same and it is more likely for older
blocks corresponding to ak, k > j to not have been evicted
from T (since there is one less load/store that could have
evicted an older block to a higher level outside T ; also notice
that buckets in ∞-ORAM are inﬁnitely sized, so, removing
the j-th load/store does not generate extra space that can
be used for storage of older blocks that otherwise would not
have found space). So, to maximize the probability of the
right-hand side of (1), we may assume that a = (aj)s
j=1 is a
sequence of block addresses without duplicates.

Blocks that remain in T with probability 1. Let V be
the set of leafs in T that are also leafs in the ORAM tree.
Notice that if a block corresponds to a Yj ∈ V , then it never
gets evicted from T since the whole path to the leaf with
label Yj is part of T . The probability that m0 out of the N

labels Yj are in V is equal to the binomial

(cid:32)

(cid:33)

p0 = P rob(m0) =

2L )m0 (1 − |V |
|V |

2L )N−m0 .

(

(5)

N
m0

T ’s usage in (1) is upper bounded by the number m0 of Yj’s
that are in V added to T ’s usage after an inﬁnite sequence
of load/store operations, where (1) all Yj (cid:54)∈ V (those Yj that
were in V are replaced by labels not in V and the sequence
is extended with labels not in V ) and (2) labels Xj and Yj
are uniformly distributed and statistically independent.

In (1) we are only interested in a non-zero probability that
T ’s usage is > nZ + R. Therefore, without loss of generality,
we may constrain |V | to lead to such a non-zero probability.
Since |V | ≤ n and T ’s usage is at most the number N of
diﬀerent blocks, we may assume |V |Z ≤ nZ + R ≤ N , hence,
(6)

|V | ≤ N/Z.

Block eviction from T . Consider the inﬁnite sequence
of load/store operations. For each leaf label X we deﬁne
X(T ) as the set of leafs that can be reached from a bucket
b (cid:54)∈ T that is on the path from the root to X. Notice that
X(T ) = ∅ if and only if X ∈ V . Furthermore, the number
of diﬀerent non-empty sets X(T ) is equal to the number of
nodes in the ORAM tree that are one edge away from T :

(7)

i=1Xi(T ).

# distinct non-empty sets X(T ) = 1 + n − 2|V |.
The non-empty sets X(T ) do not intersect with V .
By the deﬁnition of Xi(T ), if ∞-ORAM reads/writes a
path to Xi and Yj ∈ Xi(T ) for j ≥ i, then Yj gets written to
a bucket outside T : The block corresponding to Yj is counted
in T ’s usage if and only if Yj (cid:54)∈ Sj = ∪j
Eviction probability as a function of the sizes |Sj|.
The sizes of sets Sj are uniquely determined by the sequence
w1 = |S1| = . . . = |Sq1|, w2 = |Sq1+1| = . . . = |Sq1+q2|, etc.
By (7), the number of wj is at most 1 + n − 2|V | and if
w1+n−2|V | exists, then its is equal to 2L − |V |.
(Y((cid:80)k−1
) that are (cid:54)∈ S(cid:80)k−1
Let mk be equal to the number of blocks in the subsequence
S(cid:80)k
t=1 qt+1 = . . . =
, in other words, they do not get evicted from T .
Since leaf labels Yj are (cid:54)∈ V , the probability that Yj (cid:54)∈ Sj is
equal to 1 − |Sj|/(2L − |V |), hence,

t=1 qt)+1, . . . , Y(cid:80)k

t=1 qt

t=1 qt

P rob(mk|(wt, qt, mt)k−1

t=1 , wk, qk) = P rob(mk|wk, qk)

(cid:32)

(cid:33)

=

qk
mk

(1 −

wk

2L − |V | )mk (

wk

2L − |V | )qk−mk .

(8)

Probability (8) is zero for k ≥ 1 + n− 2|V |, so we may restrict
k to ≤ n − 2|V |.
Since leaf labels Xj are not constrained, the probability
that Sj has size |Sj| = |Sj−1| is equal to (|Sj−1| + |V |)/2L,
the probability that Xj ∈ Sj−1 ∪ V . Hence,

P rob(qk|(wt, qt, mt)k−1

t=1 , wk) = P rob(qk|wk)

wk + |V |

)qk−1(1 − wk + |V |

= (

).

(9)

2L

2L

The probability that wk takes on a certain value only de-
pends on the previous values wt, t < k, since they determine
the sizes of previously added Xj(T )s and therefore determine
the sizes of sets Xj(T ) that can still be added to Sk−1:

P rob(wk|(wt, qt, mt)k−1

t=1 ) = P rob(wk|(wt)k−1
t=1 ).

(10)

308We combine (8), (9) and (10) by using Bayes rule twice:

and the probability generating function corresponding to pk

(cid:89)

k≥1

P rob((wt, qt, mt)t≥1)

=

P rob(mk|wk, qk)P rob(qk|wk)P rob(wk|(wt)k−1
t=1 )

= P rob((wt)t≥1)

P rob(mk|wk, qk)P rob(qk|wk).

where

(cid:89)

k≥1

is equal to (cid:88)

q≥1

pk where

1 +

= 1 +

If we sum over all possible sequences (wt, qt)t≥1 we obtain

P rob((wt)t≥1) ·

P rob(mk|wk, qk)P rob(qk|wk)

P rob((wt)t≥1) ·

P rob((mt)t≥1)

=

=

(wt,qt)t≥1

(cid:88)
·(cid:89)
(cid:88)
·(cid:89)

k≥1

(wt)t≥1

(cid:88)

k≥1

q≥1

P rob(mk|wk, qk = q)P rob(qk = q|wk).

Substituting (8) and (9) yields

(cid:89)

(cid:88)
)q−1(1 − wk + |V |

P rob((wt)t≥1)

(wt)t≥1

k≥1
) ·

2L

P rob((mt)t≥1) =

(cid:88)
(cid:33)

q≥1

(

pk =

(cid:32)

wk + |V |

2L

q
mk

(1 −

wk

2L − |V | )mk (

wk

2L − |V | )q−mk .

(11)

Expected number of blocks in T . Notice that the ex-
pectation E[mk|(wt)t≥1] is equal to

(cid:88)

q≥1

=

wk + |V |

2L

(

)q−1(1 − wk + |V |

2L

)q(1 −

wk

2L − |V | )

1 − wk/(2L − |V |)
1 − (wk + |V |)/2L = 1/(1 − |V |/2l).

This is proved in a diﬀerent way in Section 6.3, where we use
(6) to show that the expectation of the usage of T is at most
2n for utilization N/2L ≤ 2 and for bucket size Z ≥ 4. For
this reason we expect to be able to use a Chernoﬀ bound to
prove that with negligible probability T ’s usage is more than
T ’s capacity:

Probability generating functions. From (5) and (11):

n−2|V |(cid:88)

t=0

P rob(

p =

mt > nZ + R) ≤ max
(wt)t≥1

(cid:88)

(mt)

n−2|V |
t=0

with (cid:80)n−2|V |

t=0

mt>nZ+R

p, where

p0 ·(cid:89)

k≥1

The probability generating function corresponding to p0 is

(cid:32)

(cid:33)

N(cid:88)

m=0

|V |
2L )m(1 − |V |

2L )N−mX m

(

N
m

= (1 + (|V |/2L)(X − 1))N ≤ exp((|V |/2L)(X − 1)N )

αq−1(1 − α)[1 + β(X − 1)]q.

(12)

α =

wk + |V |

2L

≤ 1 and β = 1 −

wk

2L − |V | .

By the binomial theorem, (12) is equal to

(β(X − 1))m

αq−1(1 − α)(β(X − 1))m

(cid:32)

(cid:33)

q
m

q(cid:88)
(cid:33)

m=0

αq−1(1 − α)

(cid:32)

(cid:88)

(cid:88)

q≥1

= 1 +

= 1 +

(cid:88)
(cid:88)

m≥1

m≥1

q≥m

q
m
αm−1
(1 − α)m (β(X − 1))m.
(cid:21)m

(X − 1)

(cid:88)

(cid:20)

1 − N/(Z2L)

m≥1

Notice that α ≤ 1 and β/(1 − α) = 1/(1 − |V |/2L) ≤
1/(1 − N/(Z2L)) by (6).4 This proves that (12) is at most

(X − 1)

(X − 1)

(1 − N/(Z2L)) − (X − 1)

≤ exp(

(1 − N/(Z2L)) − (X − 1)

)

for 1 ≤ X < 2 − N/(Z2L).

of the total number of blocks M =(cid:80)

The probability generating function (corresponding to p)
k mk in T (T ’s usage)
given (wt)t≥1 is equal to the product of the probability
generating functions of pj, 0 ≤ j ≤ n − 2|V |, which is at
most

(X − 1)

(1 − N/(Z2L)) − (X − 1)

(n − 2|V |) + (X − 1)|V | N

2L ).

exp(

Let

L ≥ (cid:100)log N(cid:101) − 1.

(13)
Then N/2L ≤ 2, hence, the contribution to (X − 1)|V | in
the exponential is negative and we obtain the upper bound

(X − 1)

(1 − N/(Z2L)) − (X − 1)

n).

exp(

Let M [X] denote this function.
Markov’s inequality. By Markov’s inequality, ∀z with

1 < ez < 2 − N/(Z2L),

(14)

pk.

the tail distribution of the total number of blocks M in T
given (wt)t≥1 is upper bounded by

P rob(M ≥ nZ + R) = P rob(ezM ≥ ez(nZ+R))

≤ E[ezM ]

ez(nZ+R) =

M [ez]
ez(nZ+R) .

4If we do not use the bound α ≤ 1 but explicitly maximize
over all possible sequences (wt)t≥1 that may correspond to a
subtree T of size n, then we may be able to prove the main
theorem for even smaller bucket sizes Z = 4 or Z = 5.

309Since the upper bound is independent of (wt)t≥1, it holds
for the total number of blocks in T without a condition on
(wt)t≥1, i.e.,

P rob(u(RL(Z)[s]; T ) > nZ + R|a(s) = a) ≤ M [ez]
ez(nZ+R)

= exp(n

ez − 1

2 − N/(Z2L) − ez − zZ

− zR).

(cid:20)

(cid:21)

Main theorem. The ﬁrst part of the main theorem follows
by substituting Z = 7, ez = 1.6 and bounding L ≥ (cid:100)log N(cid:101)+1
(notice that both (13) and (14) hold), which gives 0.625R/4.3n
as an upper bound on the tail distribution. Plugging this
into the union bound (1) yields

P rob(u(TL(Z)[s]) > R|a(s) = a)

4n0.625R/4.3n ≤ 14 · 0.625R.

(15)

≤(cid:88)

n≥1

By substituting Z = 6, ez = 1.5 and bounding L ≥ (cid:100)log N(cid:101) +
3 we obtain the bound 370 · 0.667R, the second part of the
main theorem.
Acknowledgments
This work is partially supported5 by the NSF Graduate Re-
search Fellowship grants DGE-0946797 and DGE-1122374,
the DoD NFSEG Fellowship, NSF grant CNS-1314857, DARPA
CRASH program N66001-10-2-4089, and a grant from the
Amazon Web Services in Education program. We would
like to thank Kai-Min Chung and Jonathan Katz for helpful
discussions, Hubert Chan for his generous and unconditional
help, and Kai-Min Chung for pointing out that our algorithm
is statistically secure.
7. REFERENCES
[1] Personal communication with Kai-Min Chung, 2013.
[2] M. Ajtai. Oblivious rams without cryptogrpahic

assumptions. In Proceedings of the 42nd ACM symposium
on Theory of computing, STOC ’10, pages 181–190, 2010.

[3] D. Asonov and J.-C. Freytag. Almost optimal private

information retrieval. In PET, 2003.

[4] D. Boneh, D. Mazieres, and R. A. Popa. Remote oblivious

storage: Making oblivious RAM practical. Manuscript,
http://dspace.mit.edu/bitstream/handle/1721.1/62006/
MIT-CSAIL-TR-2011-018.pdf, 2011.

[5] K.-M. Chung, Z. Liu, and R. Pass. Statistically-secure oram
with ˜O(log2 n) overhead. http://arxiv.org/abs/1307.3699,
2013.

[6] K.-M. Chung and R. Pass. A simple oram.

https://eprint.iacr.org/2013/243.pdf, 2013.

[7] I. Damg˚ard, S. Meldgaard, and J. B. Nielsen. Perfectly

secure oblivious RAM without random oracles. In TCC,
2011.

[8] I. Damg˚ard, S. Meldgaard, and J. B. Nielsen. Perfectly

secure oblivious ram without random oracles. In TCC, pages
144–163, 2011.

[9] C. Fletcher, M. van Dijk, and S. Devadas. Secure Processor

Architecture for Encrypted Computation on Untrusted
Programs. In Proceedings of the 7th ACM CCS Workshop
on Scalable Trusted Computing, pages 3–8, Oct. 2012.

[10] C. W. Fletcher. Ascend: An architecture for performing

secure computation on encrypted data. In MIT CSAIL CSG
Technical Memo 508, April 2013.

[11] C. Gentry, K. Goldman, S. Halevi, C. Julta, M. Raykova,

and D. Wichs. Optimizing oram and using it eﬃciently for
secure computation. In PETS, 2013.

[12] O. Goldreich. Towards a theory of software protection and

simulation by oblivious rams. In STOC, 1987.

5Any opinions, ﬁndings, and conclusions or recommendations
expressed in this material are those of the author(s) and do
not necessarily reﬂect the views of the funding agencies.

[13] O. Goldreich and R. Ostrovsky. Software protection and

simulation on oblivious rams. J. ACM, 1996.

[14] M. T. Goodrich and M. Mitzenmacher. Privacy-preserving

access of outsourced data via oblivious RAM simulation. In
ICALP, 2011.

[15] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and

R. Tamassia. Oblivious ram simulation with eﬃcient
worst-case access overhead. In Proceedings of the 3rd ACM
workshop on Cloud computing security workshop, CCSW
’11, pages 95–100, New York, NY, USA, 2011. ACM.

[16] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and
R. Tamassia. Practical oblivious storage. In CODASPY,
2012.

[17] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and
R. Tamassia. Privacy-preserving group data access via
stateless oblivious RAM simulation. In SODA, 2012.

[18] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and
R. Tamassia. Privacy-preserving group data access via
stateless oblivious ram simulation. In Proceedings of the
Twenty-Third Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA ’12, pages 157–167. SIAM, 2012.

[19] M. Harchol-Balter. Performance Modeling and Design of

Computer Systems: Queueing Theory in Action.
Performance Modeling and Design of Computer Systems:
Queueing Theory in Action. Cambridge University Press,
2013.

[20] A. Iliev and S. W. Smith. Protecting client privacy with

trusted computing at the server. IEEE Security and Privacy,
3(2):20–28, Mar. 2005.

[21] M. Islam, M. Kuzu, and M. Kantarcioglu. Access pattern
disclosure on searchable encryption: Ramiﬁcation, attack
and mitigation. In Network and Distributed System Security
Symposium (NDSS), 2012.

[22] E. Kushilevitz, S. Lu, and R. Ostrovsky. On the (in)security
of hash-based oblivious RAM and a new balancing scheme.
In SODA, 2012.

[23] J. R. Lorch, B. Parno, J. W. Mickens, M. Raykova, and

J. Schiﬀman. Shroud: Ensuring private access to large-scale
data in the data center. FAST, 2013:199–213, 2013.

[24] M. Maas, E. Love, E. Stefanov, M. Tiwari, E. Shi,

K. Asanovic, J. Kubiatowicz, and D. Song. Phantom:
Practical oblivious computation in a secure processor. ACM
CCS, 2013.

[25] R. Ostrovsky. Eﬃcient computation on oblivious rams. In

STOC, 1990.

[26] R. Ostrovsky and V. Shoup. Private information storage

(extended abstract). In STOC, pages 294–303, 1997.

[27] B. Pinkas and T. Reinman. Oblivious RAM revisited. In

CRYPTO, 2010.

[28] L. Ren, C. Fletcher, X. Yu, M. van Dijk, and S. Devadas.

Integrity veriﬁcation for path oblivious-ram. In Proceedings
of the 17th IEEE High Performance Extreme Computing
Conference, September 2013.

[29] L. Ren, X. Yu, C. Fletcher, M. van Dijk, and S. Devadas.

Design space exploration and optimization of path oblivious
ram in secure processors. In Proceedings of the Int’l
Symposium on Computer Architecture, pages 571–582, June
2013. Available at Cryptology ePrint Archive, Report
2012/76.

[30] E. Shi, T.-H. H. Chan, E. Stefanov, and M. Li. Oblivious
RAM with O((log N )3) worst-case cost. In ASIACRYPT,
pages 197–214, 2011.

[31] S. W. Smith and D. Saﬀord. Practical server privacy with

secure coprocessors. IBM Syst. J., 40(3):683–695, Mar. 2001.

[32] E. Stefanov and E. Shi. Path o-ram: An extremely simple

oblivious ram protocol. CoRR, abs/1202.5150, 2012.

[33] E. Stefanov and E. Shi. ObliviStore: High performance

oblivious cloud storage. In IEEE Symposium on Security
and Privacy, 2013.

[34] E. Stefanov, E. Shi, and D. Song. Towards practical

oblivious RAM. In NDSS, 2012.

[35] P. Williams and R. Sion. Usable PIR. In NDSS, 2008.
[36] P. Williams and R. Sion. Round-optimal access privacy on

outsourced storage. In CCS, 2012.

[37] P. Williams, R. Sion, and B. Carbunar. Building castles out
of mud: practical access pattern privacy and correctness on
untrusted storage. In CCS, 2008.

[38] P. Williams, R. Sion, and A. Tomescu. Privatefs: A parallel

oblivious ﬁle system. In CCS, 2012.

310