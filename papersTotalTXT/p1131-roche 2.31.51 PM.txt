POPE: Partial Order Preserving Encoding

Daniel S. Roche*, Daniel Apon†, Seung Geol Choi*, Arkady Yerukhimovich‡

*United States Naval Academy, Annapolis, Maryland, USA
†University of Maryland, College Park, Maryland, USA
‡MIT Lincoln Laboratory, Lexington, Massachusetts, USA

{roche,choi}@usna.edu, dapon@cs.umd.edu, arkady@ll.mit.edu

ABSTRACT
Recently there has been much interest in performing search queries
over encrypted data to enable functionality while protecting sensi-
tive data. One particularly efﬁcient mechanism for executing such
queries is order-preserving encryption/encoding (OPE) which re-
sults in ciphertexts that preserve the relative order of the underlying
plaintexts thus allowing range and comparison queries to be per-
formed directly on ciphertexts. Recently, Popa et al. (S&P 2013)
gave the ﬁrst construction of an ideally-secure OPE scheme and
Kerschbaum (CCS 2015) showed how to achieve the even stronger
notion of frequency-hiding OPE. However, as Naveed et al. (CCS
2015) have recently demonstrated, these constructions remain vul-
nerable to several attacks. Additionally, all previous ideal OPE
schemes (with or without frequency-hiding) either require a large
round complexity of O(log n) rounds for each insertion, or a large
persistent client storage of size O(n), where n is the number of
items in the database. It is thus desirable to achieve a range query
scheme addressing both issues gracefully.

In this paper, we propose an alternative approach to range queries
over encrypted data that is optimized to support insert-heavy work-
loads as are common in “big data” applications while still maintain-
ing search functionality and achieving stronger security. Speciﬁ-
cally, we propose a new primitive called partial order preserving
encoding (POPE) that achieves ideal OPE security with frequency
hiding and also leaves a sizable fraction of the data pairwise in-
comparable. Using only O(1) persistent and O(n) non-persistent
client storage for 0 <  < 1, our POPE scheme provides extremely
fast batch insertion consisting of a single round, and efﬁcient search
with O(1) amortized cost for up to O(n1−) search queries. This
improved security and performance makes our scheme better suited
for today’s insert-heavy databases.

INTRODUCTION

1.
Range queries over big data. A common workﬂow in “Big Data”
applications is to collect and store a large volume of information,
then later perform some analysis (i.e., queries) over the stored data.
In many popular NoSQL key-value stores such as Google BigTable
[14] and its descendants, e.g. [17, 41, 42, 43], the most important

Publication rights licensed to ACM. ACM acknowledges that this contribution was
authored or co-authored by an employee, contractor or afﬁliate of the United States
government. As such, the Government retains a nonexclusive, royalty-free right to
publish or reproduce this article, or to allow others to do so, for Government purposes
only.
CCS’16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978345

query operation is a range query, which selects rows in a contigu-
ous block sorted according to any label such as an index, times-
tamp, or row id.

In order to support high availability, low cost, and massive scal-
ability, these databases are increasingly stored on remote and po-
tentially untrusted servers, driving the need to secure the stored
data. While traditional encryption protects the conﬁdentiality of
stored data, it also destroys ordering information that is necessary
for efﬁcient server-side processing, notably for range queries. An
important and practical goal is therefore to provide data security for
the client while allowing efﬁcient query handling by the database
server.

In many big data scenarios, a moderate number of range queries
over a huge amount of data are performed. For example, a typi-
cal application might be the collection of data from low-powered
sensor networks as in [45], where insertions are numerous and hap-
pen in real-time, whereas queries are processed later and on more
capable hardware. In this work, we target this type of scenario.
Range queries with order-preserving encoding (OPE). A simple
and efﬁcient solution for performing range queries over encrypted
data was recently proposed by Popa et al. [35] who showed how to
build an order-preserving encoding (OPE) scheme 1, which guaran-
tees that enc(x) < enc(y) iff x < y, allowing range queries to be
performed directly over encoded values. Additionally, this scheme
achieves the ideal security goal for OPE of IND-OCPA (indistin-
guishability under ordered chosen-plaintext attack) [7] in which ci-
phertexts reveal no additional information beyond the order of the
plaintexts. This scheme differs from traditional encryption in two
ways. First, the encoding procedure is interactive requiring multi-
ple rounds of communication between the data owner (client) and
the database (server). Second, the ciphertexts produced are mutable
so previously encoded ciphertexts may have to be updated when a
new value is encoded. This approach requires O(log n) rounds of
communication and O(1) client storage, where n is the number of
items in the database.

A different trade-off between client storage and communication
is given by Kerschbaum and Schröpfer [30] achieving just O(1)
communication to encode elements (from a uniform random distri-
bution), but requiring O(n) persistent client storage to maintain a
directory providing the mapping between each OPE ciphertext to
the corresponding plaintext — proportional to the storage require-
ments on the remote database itself.

When used for range searches over encrypted data, these two
schemes either require signiﬁcant communication, or signiﬁcant
client storage. Moreover, in the second of these schemes the direc-
tory in the persistent client storage depends on the full dataset. Thus

1We abuse notation and use OPE to refer to both order-preserving
encryption and order-preserving encoding.

1131Here
Popa et al. [35]
Kerschbaum & Schröpfer [30]

Comm. Rounds
Insert

Query
O(1)

1

O(log n) O(log n)

1

1

Amortized

Client Storage

Communication Working set

Persistent

O(1)

O(log n)

O(1)

O(n)
O(1)
O(n)

O(1)
O(1)
O(n)

(cid:17)

(cid:16) n2−
Incomparable
Elements
m − n
0
0

Ω

Figure 1: Comparison of OPE-based range search schemes. n is the total number of inserts, and m is the total number of search queries. The communication
complexity is given in number of encrypted elements. For our scheme we require at most O(n1−) total number of queries. In [35] and ours, the O(1)
persistent storage is used for storing the encryption key. The incomparable elements refers to the number of element pairs (out of Θ(n2) total) that cannot be
compared even after m queries are performed.

it is not easily amenable to a setting with multiple inserting clients,
a common deployment scenario in big data applications (e.g., mul-
tiple weak sensors encrypting and inserting data for analysis in the
cloud), as the persistent storage has to be synchronized across all
the clients.

Hence, we ask the following question:

In the scenario of a large number of insertions and a moder-
ate number of range queries, can we design a secure range-
query scheme with both small, non-persistent client-side stor-
age and much lower communication cost?

Toward stronger security: frequency-hiding and more. As
recently pointed out by Naveed et al. [33], security provided by
OPE may be insufﬁcient for many applications. Speciﬁcally, they
showed attacks that use frequency analysis and sorting of cipher-
texts to decrypt OPE encrypted values using some auxiliary infor-
mation. To counter the ﬁrst of these attacks, Kerschbaum [29] pro-
posed a stronger notion of security (IND-FAOCPA) that also hides
the frequency of OPE-encoded elements (i.e. hides equality). How-
ever, even this does not address all known attacks on OPE. Hence,
this paper asks the following question:

Can we design an efﬁcient range query scheme with security
better than frequency-hiding?

1.1 Our Work
Our contribution.
In this paper we give a positive answer to
both of the above questions, proposing an alternative range query
scheme that we call partial order preserving encoding or POPE.
Speciﬁcally, our POPE construction satisﬁes the following proper-
ties when storing n items using O(1) persistent and O(n) working
storage on the client and performing at most O(n1−) range queries
for any constant 0 <  < 1:

• Trivial insert operations consisting of 1 message from the
client to the server and no computation for the server. Fur-
thermore, a large number of data insertions can be performed
only with a single round in a batch.

• O(1)-round (amortized) communication per range query.
• No persistent client storage between operations except the

encryption key.

• Greater security than IND-FAOCPA. Our scheme leaks noth-
ing beyond the order of (some of the) inserted elements while
also hiding equality. Moreover, a fraction of plaintext pairs
remain incomparable even after the queries.

See Figure 1 for how this compares to existing schemes.
We have implemented our construction and tested it on a variety
of workloads, comparing to other schemes and also measuring its

network performance. We ﬁnd that our scheme is especially suit-
able for typical big data applications where there are many more
inserts than queries. As an example data point, with about one mil-
lion insertions and one thousand range queries, our POPE scheme
is 20X faster than the scheme by Popa et al.

We also experimentally validate our claim of improved security
by observing how many data items remain unsorted (i.e., the server
cannot learn their relative order) after some number of queries are
performed over real-world data. Speciﬁcally, we ran an experiment
where we inserted over 2 million public employee salary ﬁgures
from [1] and then performed 1000 random range queries. Figure 2
shows the server’s view of the salary data after various numbers of
queries. The black lines indicate elements whose position in the
global order the server knows (the shading of the lines indicates
the fraction of comparable points in each value range with lighter
shading indicating a lower fraction), while the contiguous white re-
gions represent data points whose relative order is unknown. Note
that for a typical OPE scheme, this image would be fully black (all
order revealed).

See Section 5 for more details on our implementation and further

experimental data.
POPE tree: no sorting when inserting data. Our main tech-
nique to make this possible is lazy indexing. Speciﬁcally, unlike
OPE, we do not sort the encoded values on insert, instead only
partially sorting values when necessary during query execution. If
we regard the actual location in the search tree data structure as an
implicit encoding of an encrypted value, our scheme gives a par-
tially ordered encoding, and hence the name of our construction,
POPE (partial order preserving encoding).

In particular, our scheme works by building a novel tree data
structure (inspired by buffer trees [5]), which we call a POPE tree,
where every node contains an unsorted buffer and a sorted list of el-
ements. The invariant that we maintain is that the sorted elements
of a node impose a partial order on both sorted and unsorted ele-
ments of its child nodes. That is, all sorted and unsorted values at
child i will lie between values i − 1 and i in the parent’s sorted
list. We stress that there is no required relation between unsorted
elements of a node and the elements of its child nodes. In partic-
ular, unsorted elements of the root node do not need to satisfy any
condition. That said, one can simply insert a value by putting it in
the unsorted buffer of the root node.

Having the server incrementally reﬁne this POPE tree on range
queries allows us to achieve both better efﬁciency and stronger se-
curity. In particular,

• Insertion is extremely simple by putting the encrypted label
in the unsorted buffer of the root of the POPE tree. More-
over, a large number of items can be inserted in a batch, and
the entire task takes only a single round. We note that the
interactive OPE scheme in [35] cannot support a batch in-
sertion, since each insertion is involved with traversing and
changing the encoding tree structure, and it’s quite difﬁcult

1132Figure 2: Server’s view of salary data whose order is incrementally revealed after inserting more than 2 million salary entries and then performing 1000
random range queries. The black lines indicate entries whose order is known by the server, while the white regions indicate entries that remain pairwise
incomparable after some number of queries.

to parallelize this procedure maintaining the consistency of
the tree structure.

• The cost of sorting encrypted elements can be amortized over
the queries performed. In particular, on each query we only
need to sort the part of the data that is accessed during the
search, leaving much of the data untouched. This allows us
to support range queries with much better efﬁciency and si-
multaneously achieve stronger security by having some frac-
tion of pairs of elements remain incomparable.

• Since encodings are sorted during searches, the cost of per-
forming a batch of search queries is often much cheaper than
performing these queries individually, as later queries do not
need to sort any elements already sorted in earlier queries.

We now describe the key properties of our data structure in more
detail. Intuitively, thanks to the required condition between sorted
elements of a node and the elements of its child nodes, the sorted
values at each node can serve as an array of simultaneous pivot
elements for the elements in the child nodes, in the sense of Quick-
sort [28]. To maintain this property we make use of client working-
set storage to partition a set of unsorted elements according to the
values at the parent. Speciﬁcally, we require the client to be able
to read in a list of O(n) encrypted values and then to partition
a stream of other encrypted values according to these split points.
Using this amount of client working-set storage we can ensure that
the depth of the buffer tree remains O(1), allowing for low amor-
tized latency per client query. Note that any elements stored in the
same unsorted buffer at the end of the procedure remain incompa-
rable.

2. PRELIMINARIES
2.1 Security with No Search Queries

The security deﬁnitions of OPE variants consider how much in-
formation is revealed by the ciphertexts that are created when data
is inserted. This measure is important since OPE ciphertexts must
inherently reveal ordering information. The ideal security achiev-
able even without any search queries is revealing ordering infor-
mation of the underlying plaintexts but nothing more. Our POPE
scheme, however, gives much stronger guarantee of revealing no
information about the underlying plaintexts during insertion. In-
stead, ordering information is gradually leaked as more and more
search queries are performed. In this section, we brieﬂy discuss

the security guarantees that OPE variants and our scheme provide,
before any search queries are performed.
Security of OPE. The security notion for OPE schemes is IND-
OCPA (indistinguishability under ordered chosen-plaintext attack)
[7, 35]: Ciphertexts reveal no additional information beyond the
order of the plaintexts. However, Naveed et al. [33] demonstrated
this level of security is sometimes insufﬁcient, by showing how
the revealed order can be used to statistically recover a signiﬁcant
amount of plaintext data in an OPE-protected medical database.
Security of frequency-hiding OPE. To address the above issue,
Kerschbaum [29] proposed a stronger security notion, called indis-
tinguishability under frequency-analyzing ordered chosen plaintext
attack (IND-FAOCPA). Informally, the deﬁnition requires that ci-
phertexts reveal no additional information beyond a randomized
order of the plaintexts. A randomized order Y (some permutation
of [n] for n-element sequences) of some sequence X of possibly
non-distinct elements is an ordering you can obtain from the se-
quence by breaking ties randomly. For example, the randomized
order of X1 = (1, 4, 2, 9) or X2 = (2, 8, 5, 20) could only be
Y1 = (1, 3, 2, 4) (meaning “ﬁrst in sorted order was inserted ﬁrst,
third in sorted order was inserted next,” and so on) because X1, X2
began totally ordered. However, the sequence X3 = (1, 2, 2, 3)
has two possible randomized orders, namely Y2 = (1, 2, 3, 4) and
Y3 = (1, 3, 2, 4).

Note that for any randomized order, e.g. Y1 = (1, 3, 2, 4), there
are many sequences that could map onto it (depending only on the
domain of the sequence and the constraints imposed by known par-
tial order information on the sequence). This property of a ran-
domized order is useful for hiding frequency. The motivating ex-
ample for frequency-hiding security is a database that stores a large
number n of encodings for which the underlying label space L =
{(cid:96)1, ..., (cid:96)T} is small, i.e., T (cid:28) n. For example, [29] considered a
setting where each label is either (cid:96)1 = “female” (F ) or (cid:96)2 = “male”
(M ), with the sequence (F, F, M, M ) ideally encoded as, say,
(2, 1, 3, 4). Examining only (2, 1, 3, 4) does not reveal if the under-
lying sequence was originally (F, F, F, F ), (F, F, F, M ), (F, F,
M, M ), (M, F, M, M ), or (M, M, M, M ).

To turn an OPE scheme into a frequency-hiding OPE scheme,
consider adding a small, random fractional component to the OPE-
ordered ﬁeld during encoding, e.g. X1 = (1, 1, 2, 2) becomes e.g.
X(cid:48)
1 = (1.12, 1.36, 2.41, 2.30), which randomly maps X1 to the
ordering Y = (1, 2, 4, 3), and then X(cid:48)
1 is encoded under the OPE
scheme. In [29], this type of scheme is shown IND-FAOCPA secure
in the programmable random oracle model.

 0 200 400 600 800 10000500000100000015000002000000number of queriessorted elements1133We use this approach to add frequency hiding in POPE. How-
ever, even this stronger deﬁnition fails to protect against all known
attacks as it still reveals the order between all distinct plaintexts in
the database, allowing for sorting-based attacks.
Security of POPE. POPE, on the other hand, fully hides all in-
serted plaintexts until search queries are performed. Looking ahead,
our POPE scheme encrypts an item using semantically secure en-
cryption. This implies in the POPE construction, ciphertexts reveal
no information about the underlying plaintexts. Of course, it is not
sufﬁcient to just discuss security on insert without considering what
happens on queries. Thus, we give a security deﬁnition below cap-
turing what happens both on insertion and during search queries.
2.2 Security with Search Queries

We propose a simulation-based deﬁnition that captures both ideal
OPE security and frequency-hiding even when considering what
happens during the search procedure. Speciﬁcally, we require the
existence of a simulator simulating the view of the protocol execu-
tion given only a randomized order of (some of) the plaintexts. We
model this by a random order oracle rord which just takes the in-
dices of two data items and returns which item is larger according
to some ﬁxed randomized order. Since the simulated view is con-
structed using only this oracle, the only information leaked in the
real protocol corresponds to the oracle queries made to the rord
oracle, i.e., the randomized order on the queried plaintexts.

To formalize the simulation tasks, for a sequence seq of inser-
tion and search operations, we deﬁne the proﬁle proﬁle(seq) of
sequence seq to be a sequence where each value in seq is replaced
with a unique index (simply incrementing starting from 1) to iden-
tify the operation. An example sequence and its proﬁle can be:

seq : (insert 10, insert 100, range [8, 20], insert 41).
proﬁle(seq) : (insert 1, insert 2, range [3, 4], insert 5).

DEFINITION 1. A range query protocol Π is called frequency-
hiding order-preserving, for any honest-but-curious server S, if
there is a simulator Sim such that for any sequence seq of inser-
tions and searches, the following two distributions are computa-
tionally indistinguishable:

VIEWΠ,S(seq) ≈c Simrordseq(·,·)

Π

(proﬁle(seq)),

where the left-hand side denotes the real view of S when executing
the protocol Π with seq as the client’s input, and the right-hand
side is the output of the simulator Sim taking as input proﬁle(seq)
and referring to oracle rord. The oracle rordseq(·,·) works as
follows:

rordseq(i, j): It is initialized with a randomized order π of the
labels in seq by breaking ties randomly. Then, for each query
(i, j), return whether the ith label has a higher ranking than
the jth, according to π.

Since the simulator refers to only the proﬁle and the oracle, we
can say that for any protocol satisfying the above deﬁnition, the
protocol transcript leaks to the server only the proﬁle and the ran-
domized order of the queried plaintexts. One beneﬁt of this def-
inition is it covers both non-interactive FH-OPE schemes and our
interactive POPE scheme.
Leaking only a partial order. Recall that our POPE scheme grad-
ually leaks the ordering information as more comparisons are made
in order to execute the queries. To formally treat the amount of in-
formation that remains hidden after some number of range queries,
we introduce a deﬁnition that captures the number of points that
remain incomparable even after some queries are performed.

(cid:1) = 6 initially

of four labels labels = ((cid:96)1, (cid:96)2, (cid:96)3, (cid:96)4). There are(cid:0)4

First, we explain what we mean by the number of incomparable
element pairs with transitivity. For example, consider a sequence
unordered pairs: {(cid:96)1, (cid:96)2}, {(cid:96)1, (cid:96)3}, {(cid:96)1, (cid:96)4}, {(cid:96)2, (cid:96)3}, {(cid:96)2, (cid:96)4},
{(cid:96)3, (cid:96)4}. During query execution the order of some of these pairs
may become known to the server, i.e., if it queries the rord oracle
on the indices of some such pair or if the order can be inferred from
its previous queries. For example, given info = ((cid:96)1 > (cid:96)2, (cid:96)2 > (cid:96)4),
then due to transitivity, the server can infer (cid:96)1 > (cid:96)4. However, the
following pairs still remain incomparable:

2

{(cid:96)1, (cid:96)3},{(cid:96)2, (cid:96)3},{(cid:96)3, (cid:96)4}

Armed with this notion of incomparable pairs with transitivity, we
give the following deﬁnition:

DEFINITION 2. Let n, m denote the number of insertions and
range searches respectively. A range query protocol Π is frequency-
hiding partial order preserving with u incomparable element pairs
with transitivity, if for any operation sequence seq with n inserts
and m range queries, the simulator successfully creates a simu-
lated view required by Deﬁnition 1 while leaving at least u pairs
of elements that are incomparable with transitivity based on the
queries made by the simulator to rord.

In this paper, whenever we consider incomparable pairs, we con-
sider it with transitivity, and from now on, we will omit the phrase
“with transitivity”. Note that both the OPE scheme by Popa et
al. [35] and the FH-OPE scheme by Kerschbaum [29] have 0 in-
comparable element pairs for any n inserts, even with 0 searches.
However, our POPE scheme shows a more gradual information
leakage. We discuss this in more detail in Section 4.

3. MAIN CONSTRUCTION
3.1 Overview

Our scheme consists of a client and a server, which we denote by
Cl and Ser respectively. Cl holds an encryption key and performs
insertions and range query operations through interactive protocols
with Ser. (In fact, only the range query operation is interactive,
which is a key beneﬁt of our construction!)

As Cl is stateless and needs to remember nothing (other than the
secret key), all data is stored encrypted by Ser. To organize this
data and facilitate fast lookups, Ser maintains a POPE tree to hold
the ciphertexts. The high-level structure of this tree is similar to a
B-tree, where each node has a bounded number of children and all
leaf nodes are at the same depth. In fact, the number of children of
any POPE tree internal node is between L/2 + 1 and L + 1, where
L is the local temporary storage capacity of Cl.

Where the POPE tree differs from a standard B-tree is that ev-
ery node contains an unsorted buffer of ciphertexts with unbounded
size. The beneﬁts of our construction, both in terms of efﬁciency
and security, stem from the use of these unsorted buffers. For ef-
ﬁciency, they allow to delay expensive sorting and data movement
operations until necessary to execute a range query. Security ben-
eﬁts stem from the fact that the relative order of elements in the
same unsorted buffer is not revealed to an attacker.

The insertion protocol is trivial: Cl encrypts the plaintext value
to be inserted and sends it to Ser, who simply appends the new ci-
phertext to the root node’s unsorted buffer. Because semantically
secure encryption is used, the ciphertexts do not reveal anything
about their true values or order, not even whether two inserted val-
ues are the same or different. All of the actual sorting and ordering
is delayed until queries are performed.

1134Before completing a range query, Ser interacts with Cl to split
the tree according to each of the two query endpoints. This sub-
routine — the most sophisticated in our entire construction — has
three stages. First, for all the internal POPE tree nodes along the
search path for the query endpoint, the unsorted buffers are cleared
out. This clearing of the buffers proceeds from root to leaf, and
involves streaming all buffer ciphertexts back from Ser to Cl, who
responds for each one with the index of which child node that ci-
phertext should ﬂow down to. Recall that we maintain each internal
node having at most L + 1 children; this allows the operation to be
performed efﬁciently by Cl without overﬂowing the client’s size-L
local storage.

This initial stage of the split ends at a leaf node. The second
stage involves reducing the size of that leaf node’s buffer to at most
L, the size of Cl’s local storage. This leaf node buffer reduction
proceeds by selecting L random ciphertexts from the leaf node’s
buffer, and using Cl to split the single leaf into L + 1 new sibling
leaf nodes, according to these randomly-selected elements. These
L randomly sampled ciphertexts are inserted into the parent node
as partition elements between the new leaf nodes. This leaf node
splitting procedure is repeated until the resulting leaf node has a
buffer of size at most L.

However, we may have inserted too many new children into the
parent node, causing it to have more than the limit of L+1 children.
So a rebalance operation must ﬁnally be completed, from the leaf
back up to the root node, creating new internal nodes as necessary
until they all have at most L + 1 children as required. Note that this
stage does not require any further ordering or consultation with Cl.
After performing this split operation for both endpoints, the ac-
tual range query can now be completed by Ser returning to Cl
all the ciphertexts in all buffers of nodes between the two query
endpoints. Again, this does not require any further ordering infor-
mation from Cl. Of particular importance for security is that there
may be large unsorted buffers even after the range query completes,
because all contents of those buffers lie entirely within or outside
of the desired range. The server either returns all of none of the
ciphertexts in these buffers, but still does not (and does not need to)
learn their order.
Parameters. Recall that the parameter n represents the total num-
ber of items inserted into the database, and the parameter m rep-
resents the total number of range query operations performed. The
client can temporarily store L+O(1) labels in its local memory for
the duration of a given query. Let L = n for constant 0 <  < 1.
Notation. To support realistic application scenarios, we distin-
guish between two types of data that Ser holds: (i) labels (cid:96) and
(ii) blocks that are composed of a POPE-encoded label (cid:96) and an
arbitrary, encrypted payload v(cid:96). This models the case when range
searches over POPE-encoded labels are used to retrieve the pay-
loads. No searching directly over the payloads is supported.

We remark that, in principle, for every distinct label (cid:96), there
could be many distinct blocks ((cid:96), v(cid:96)1 ), ((cid:96), v(cid:96)2 ), ... stored by Ser.
However, we will restrict to the special case when for each label (cid:96)
there is at most one block ((cid:96), v(cid:96)) in order to convey the main ideas
more clearly. (Note this distinctness property holds w.h.p. if we use
the tie-breaking procedure described in Section 2.1.)
3.2 Encryption of Labels
In our system, whenever Cl communicates a label (cid:96) to Ser we
have Cl always send a ciphertext ¯(cid:96) to Ser, where ¯(cid:96)← Enck((cid:96)).
Besides an encryption of the label itself, this ciphertext must also
encrypt (a) the tie-breaking random value necessary for frequency-
hiding POPE and (b) an indication of the label’s origin (left or right
query endpoint, or insertion).

Tie-breaking randomness. Consider for example that the labels
(1, 2, 2, 3) have been inserted, followed by a range query for all
values between 2 and 3, requiring a total of six encryptions. From
Section 2.1, tie-breaking randomness can be thought of as adding
a random fractional part to each plaintext before encrypting, so
for example we encrypt the labels (1.89, 2.15, 2.35, 3.93) and the
range query endpoints 2.23 and 3.38.
Origin bits. This hides the repeated label 2, but creates a new
problem: the labels 2.15 and 3.93 which should be included in a
range search between 2 and 3, would be excluded because of the
tie-breaking. So we also include two bits π for the origin of the
plaintext: πl = 00 and πr = 11 for left and right query endpoints
respectively, and πm = 01 for an insertion. These bits are inserted
between the actual label and the tie-breaking values, so (contin-
uing the previous example), we would insert the encryptions of
(1.01.89, 2.01.15, 2.01.35, 3.01.93) and query endpoints 2.00.23
and 3.11.38. This forces the range search to return the three correct
values.
Two-block ciphertexts. Even treating the two origin bits as part of
the label, each plaintext becomes two blocks long, so that a straight-
forward application of CTR or CBC mode encryption results in
ciphertexts of three blocks. One can achieve better efﬁciency by
not including the tie-breaking randomness but still enabling the re-
ceiver to compute it. In particular, let f be a PRP, and let:

• enck(m(cid:107)π): Choose a random string r. Return the pair

(r, fk(r + 1)⊕(m(cid:107)π)).

• deck(c1, c2): Compute m(cid:107)π ← fk(c1 + 1)⊕c2 and the tie

breaking randomness u← fk(c1 + 2). Return (m, π, u),

Note it’s just the CTR mode of encryption. Even though the
ciphertext doesn’t explicitly contain the tie-breaking randomness,
the reconstructed u serves for this purpose.
3.3 Server Memory Layout

Ser statefully maintains the POPE tree T , which is a balanced

L-ary tree with root r.

• Each non-leaf node u ∈ T stores a buﬀer and a list.
• Each leaf node u ∈ T stores a buﬀer only.

A buﬀer stores an unbounded, unsorted set of (encryptions of)
blocks {((cid:96)1, v(cid:96)1 ), ((cid:96)2, v(cid:96)2 ), ..}, and a list stores at most L sorted
(encryptions of) labels ((cid:96)1, ..., (cid:96)L).
Main invariant of the POPE tree T . We will enforce the follow-
ing, main order-invariant on Ser’s tree T :

Let (cid:96)j−1 and (cid:96)j be the (j − 1)th and jth sorted labels at some
(non-leaf) node u in T . Then, for all labels (cid:96) in the sub-tree Tuj

rooted at the jth child uj of u, we have (cid:96)j−1 < (cid:96) ≤ (cid:96)j.

Intuitively, this guarantee of global partial ordering enables the L
sorted labels (cid:96)1, ..., (cid:96)L at each node u to serve as an array of si-
multaneous pivot elements, in the sense of Quicksort [28], for the
L + 1 sub-trees rooted at u’s (at most) L + 1 children u1, ..., uL+1.
Looking ahead, we use this simple, parallel pivot idea in conjunc-
tion with the parameter setting L = n, implying T has depth
(cid:100)1/(cid:101) = O(1), to enable Ser to traverse and maintain the tree T
with low amortized latency over repeated batches of Cl queries.
3.4 The POPE Protocol

We now present more formally our protocol POPE consisting of
three operations: Setup, Insert, and Search. The Search protocol
results in additional calls to helper protocols Split and Rebalance,
described afterward.

1135Implementing Setup. At Setup, Cl and Ser do:

Setup:

– Cl generates private keys for label/block encryption.
– Ser initializes T as a root r with empty buffer and list.

Implementing Insert. To Insert a block ((cid:96), v), Cl and Ser do:

Insert ((cid:96), v):

– Cl sends (encrypted) block ((cid:96), v) to Ser.
– Ser appends block ((cid:96), v) to the end of the current root node’s buffer.

After Setup and possibly many Insert operations (but no Search
operations), the POPE tree T held by Ser appears as in Figure 3.

Figure 3: The state of Ser’s tree T prior to any Search queries.

Implementing Search. For Cl to Search for the range of blocks
held by Ser in T between two labels (cid:96)left and (cid:96)right, Cl and Ser do:
Search ((cid:96)left, (cid:96)right):

– Cl and Ser engage in an interactive protocol Split twice,

– After each Split, Cl identiﬁes for Ser the leaf node

once for (cid:96)left and once for (cid:96)right.
uleft (or uright) in T that matches the label (cid:96)left (or (cid:96)right).

– Ser sends the blocks in [uleft, uright] to Cl.
How to Split the POPE Tree. For Cl to Split Ser’s tree T at label
(cid:96) ∈ {(cid:96)left, (cid:96)right}, Cl and Ser engage in an interactive protocol. This
operation will return the leaf node whose buffer contains the given
label with the guarantee that all nodes along the path from the root
to that leaf have empty buffers.
Individual Split calls always begin at the current root r ∈ T .
After any (non-leaf) node u ∈ T is split, Ser learns (from Cl)
the index i ∈ [L + 1] of the next child ui of u to be Split. The
Split protocol proceeds recursively down some path of T , splitting
subsequent children ui, ui,j, ... until terminating at a leaf node u.
(For readability in what follows, we assume that Ser always returns
whole nodes to Cl for each Search response.)

We break our description of Split into two broad cases: (i) the
Splits of internal, i.e., non-leaf nodes, and (ii) the Splits of leaf
nodes.
Case (i) — Splits at internal nodes: For splits at internal nodes u
with children denoted ui, Cl and Ser do:

Split ((cid:96)) — for internal nodes u:
– Ser sends L = u.list to Cl.
– Ser streams ((cid:96)(cid:48), v(cid:48)) ∈ u.buﬀer to Cl.
– Cl sends the sorted index i ∈ [L + 1]
of each ((cid:96)(cid:48), v(cid:48)) in L to Ser.
– Ser appends block ((cid:96)(cid:48), v(cid:48)) to ui.buﬀer

During this operation, Cl either (a) sees the searched-for label
(cid:96) ∈ {(cid:96)left, (cid:96)right} (and discovers node ui to proceed to), or (b) dis-

covers the node ui that may contain label (cid:96) based on its boundary
values.

The block movement in splits at internal nodes is illustrated in

Figure 4. (The outcomes of three “splits” are shown.)

Figure 4: The ﬂow of blocks in recursive Split’s of Ser’s tree T .

Case (ii) — Splits at the leaves: For splits at leaf node u with
parent node u∗, Cl and Ser do:

Split ((cid:96)) — for leaf nodes u:
– If |u.buﬀer| ≤ L, return.
– Ser samples L labels L = {(cid:96)1, ..., (cid:96)L} from u.buﬀer.
– Ser creates new root u∗ if u is the root node, or sets u∗ to
– Ser sends L to Cl.
– Cl sorts L and returns it to Ser.
– Ser inserts L new sibling leaf nodes ui into parent u∗

u’s parent otherwise.

as well as new labels L into u∗.list at the position previously
occupied by u (node u is deleted after it is split).
– Ser streams ((cid:96)(cid:48), v(cid:48)) ∈ u.buﬀer to Cl
– Cl sends the sorted index i ∈ [L + 1]
of each ((cid:96)(cid:48), v(cid:48)) in u.buﬀer to Ser.
– Ser inserts block ((cid:96)(cid:48), v(cid:48)) into sibling node ui

– Cl indicates to Ser the index i of new leaf node matching l

Note that if the size of the buffer is smaller than the local storage
capacity L of Cl, then this operation does nothing, and the split is
complete. Otherwise, as in Case (i) of Split, Cl will learn which of
the sibling leaf nodes ui to recursively Split in order to ﬁnd label
(cid:96). In this way, a single Split operation may recursively result in
multiple leaf node Split’s, with smaller and smaller buffers.
As an example, the new state of Ser’s tree T immediately after
Cl’s ﬁrst Split call (which splits the starting leaf node of T , i.e. the
root) is as depicted in Figure 5.
Clean-up Step: Rebalancing a Split POPE Tree. After complet-
ing the Split protocol above, the resulting leaf node at which Split
terminates will have size at most L, but some internal node’s sorted
list may be larger than L because of the insertions from their chil-
dren — see case (ii) of Split. This would be problematic in future
Split operations on those internal nodes, as they would send u.list
to Cl, who only has room for L items.

11364.2 Security Analysis

THEOREM 2. The POPE protocol is a frequency-hiding order-

preserving range query protocol.

PROOF. We show that our POPE scheme satisﬁes Deﬁnition 1
by showing a simulator. The simulator is very simple. For each
insert, the simulator sends enck(0); due to semantic security of the
underlying encryption, the simulation is indistinguishable. To sim-
ulate search queries, the simulator runs the adversarial server’s al-
gorithm, and during the simulation, when the server needs to com-
pare two encrypted labels, the simulator simply queries the rord
oracle to get the answer. It’s obvious that the simulated view is
(cid:4)
indistinguishable to the real view of the server.

order on n items always has(cid:0)n

Security with queries. Range query schemes leak some informa-
tion of underlying plaintexts from adaptive search queries. In this
case, one important security measure can be the number of pairs of
incomparable elements. In any range query scheme, search queries
reveal some partial order on the underlying plaintexts. Recall that
a partial order ≺ on a set of elements S is isomorphic to a directed
acyclic graph, closed under transitive closure, whose nodes are el-
ements of S and whose edges encode the binary relation. A total
elements x, y ∈ S are said to be incomparable iff neither x ≺ y
nor y ≺ x. In a total order (such as the randomized order of [29]),
no pair of elements is incomparable. In our POPE scheme, each
search query gradually leaks the ordering information of the un-
derlying plaintexts. In particular, with a small number of search
queries, there will be many pairs of incomparable elements.

(cid:1) edges. In any partial order, two

2

(cid:17)

(cid:16)

THEOREM 3. After n insertions and m query operations with
local storage of size L, where mL ∈ o(n), our POPE scheme is
mL logL n − n
frequency-hiding partial-order-preserving with Ω
incomparable pairs of elements.

n2

PROOF. Note the simulator in the above proof uses oracle rord
whenever the server algorithm needs to compare the elements. So,
we can prove the theorem by using a counting argument on the
number of labels that the server compares. We model the server’s
view of the ciphertext ordering as some k ciphertexts whose order
is completely known, and where the remaining n − k ciphertexts
are partitioned into one of k + 1 buckets according to the k ordered
ciphertexts. Essentially, this is a worst-case scenario where all in-
ternal node buffers in the POPE tree are empty, the total size of all
internal node sorted lists is k, and the remaining n − k ciphertexts
reside in leaf node buffers.

We focus on the round complexity for range queries (insertion
gives no change in the number of comparable elements). From
Theorem 1, the total rounds of communication for range queries,
after n insertions and m range queries, is O(m logL n). From the
construction, each round of communication can add at most L new
ciphertexts to those whose sorted order is completely known.

Therefore, in the worst case, the server has k = O(mL logL n)
ciphertexts in its sorted order, creating k + 1 buckets in which
the other values are placed. Thus, the worst-case split that mini-
mizes the total number of incomparable elements is for the remain-
ing values to be partitioned equally among these buckets. Thus,
we have b = (cid:98)(n − k)/(k + 1)(cid:99) ciphertexts in each unsorted

(cid:1) incomparable items, for a total

bucket. Each bucket contains(cid:0)b
of (k + 1) ·(cid:0)b

(cid:1) = Ω( n2

k − n) incomparable pairs.

(cid:4)

2

2

Privacy against a malicious server. Note the above theorem
considers the worst case. This implies we can easily achieve pri-
vacy against a malicious server with tiny additional costs, that is,

Figure 5: The state of Ser’s tree T after the very ﬁrst Split ends: the new
root r := u∗ (empty buffer, full list), plus L + 1 leaves.

To ﬁx this, after completing the Split protocol, Ser calls the fol-
lowing operation on the parent of the resulting leaf node in order to
rebalance the labels in the lists of the internal nodes. We empha-
size that Rebalance is purely a local data structure manipulation,
and does not require interaction from Cl, since the unsorted buffer
of the rebalanced nodes is empty due to prior Split, having only
sorted labels in the list.

More concretely, to Rebalance at node u (initially the parent of

the leaf where Split ended), Ser does:

Rebalance(u):
– If |u.list| ≤ L, return.
– If u has no parent u∗, create a fresh root node r for T
and set u∗ := r.
– Partition u.list into sorted sublists of size at most L each by selecting
L = [every (L + 1)’th element in u.list].
– Create |L| new sibling nodes and insert them as well as the new labels
L into parent node u∗.
– Call Rebalance(u∗).

This completes the description of our main POPE protocol.

4. ANALYSIS
4.1 Cost Analysis

We analyze amortized costs on the round complexity and band-

width per operation.

THEOREM 1. After n insertions and m query operations with

local storage of size L, our scheme has the following costs:

1. Insert always requires a single round, and Search requires

O(logL n) rounds in expectation.

2. The total expected bandwidth over all (n + m) operations
(excluding the bandwidth necessary for sending the search
results) is

O(cid:0)mL logL n + n logL m + n logL(lg n)(cid:1).

The proof is found in Appendix 8.
Remark. With L = n, 0 <  < 1, Theorem 1 implies that
Search takes O(1) rounds in expectation. Moreover, when L = n
and m = O(n1−) as well, the amortized bandwidth per opera-
tion becomes O(1). This is exactly our target scenario of many
insertions and relatively few searches.

1137by making sure that (1) all the ciphertexts that the server asks the
client to compare are legitimate, that is, created by the client (to en-
sure this, the labels should now be encrypted with IND-CCA2 en-
cryption), and (2) the number of the server’s comparison requests
should be within the bounds of Theorem 1.

Unfortunately, this augmented system doesn’t achieve full ma-
licious security; in particular, a malicious server may omit some
values from the query answers, although it cannot inject a fake re-
sult due to IND-CCA2 security of the underlying encryption. Ef-
ﬁciently achieving full malicious security is left as an interesting
open problem.

5. EVALUATION
5.1 Experimental setup

We have made a proof-of-concept implementation of our POPE
scheme in order to test the practical utility of our new approach.
The code is written in Python3 and our tests were performed using a
single core on a machine with an Intel Xeon E5-2440 2.4 GHz CPU
and 72GB available RAM. Our implementation follows the details
presented in Section 3. The symmetric cipher used is 128-bit AES,
as provided by the PyCrypto library. The full source code of our
implementation is available at https://github.com/dsroche/pope.
Database size. While we performed experiments on a wide range
of database sizes and number of range queries, our “typical” start-
ing point is one million insertions and one thousand range queries.
This is the same scale as recent work in the databases community
for supporting range queries on outsourced data [31], and would
therefore seem to be a good comparison point for practical pur-
poses.
Parameters. In our experiments, we varied the total database size
between one thousand and 100 million entries, each time perform-
ing roughly m = n1/2 range queries and with L = n1/4 local
client storage. That is,  = 0.25 in these experiments. The size of
each range being queried was randomly selected from a geometric
distribution with mean 100; that is, each range query returned on
average 100 results.
Network. Our main experiments were performed in a local setup,
but with careful measurement of communication and under the as-
sumption that network bandwidth (i.e., amortized communication
size) and latency (i.e., round complexity) would be the bottlenecks
of a remote cloud implementation. In particular, in our network
experiments, we used the tc “trafﬁc control” utility to add speciﬁc
latency durations as well as bandwidth limitations. This allowed
us to test the behavior under controlled but realistic network set-
tings, when we throttled the network slower than 5ms of latency
and 20Mbps bandwidth.
Comparison with Popa et al. We compared our construction ex-
perimentally to that of Popa et al. [35], who had a setting most
similar to ours. Further comparison benchmarks, such as to [30] or
even to ORAMs, might provide further insight, and we leave this
as future work.

For a fair comparison to prior work, we also implemented the
mOPE scheme of [35] in Python3 along with our implementation
of POPE. We followed the description in their work, using a B-tree
with at most 4 items per node to store the encryptions. To get a
fair comparison, we used the same framework as our POPE experi-
ments, with the client that receives sorting and partitioning requests
from the server. In the case of mOPE, each round of communica-
tion consisted of sending a single B-tree node’s worth of cipher-
texts, along with one additional ciphertext to be encoded, and re-
ceiving the index of the result within that sorted order. We acknowl-

edge that our implementation is likely less tuned for efﬁciency than
that of the original authors, but it gives a fair comparison to our
own implementation of POPE. It is also important to note that our
communication cost measurements depend only on the algorithm
and not on the efﬁciency of the implementation.
Measuring communication and running time. When our tests
measured communication (in terms of rounds and total ciphertexts
transferred) and running time, we did not include the cost of the
server sending the search results; this is inherent in the operation
being performed and would be the same for any alternative imple-
mentation.
5.2 Experimental workloads
Local setting: huge data, various search patterns.
In our main
experiments, where we wanted to scale the number of database en-
tries from one thousand up to 100 million entries, we used synthetic
data consisting of random pairs of English words for both label and
payload values. For these experiments we also did not actually
transfer the data over a network, but merely measured the theo-
retical communication cost. This allowed us to test a much wider
range of experimental sizes, as we found a roughly 10x slowdown
in performance when running over a network, even with no throt-
tling applied.

We were able to run experiments with POPE up to 100 million
entries, limited only by the storage space available on our test ma-
chine. We observed no signiﬁcant change in per-operation perfor-
mance after one million entries, indicating our construction should
scale well to even larger datasets.
5.3 Local Setting
Experimental communication costs. Figures 6 and 7 show the
communication costs, the total number of rounds of communica-
tion, and the average number of ciphertexts transferred per opera-
tion. The number of insertions n is shown in the plots, and for each
n searches allowing L = n1/4
experiment we performed m =
entries stored in temporary memory on the client.

√

The actual size of each range being searched was, on average,
100 database entries. While the distribution of searches does not
affect the running time of mOPE, for POPE we varied among three
distributions of the random range queries: (i) uniformly distributed
queries, (ii) search queries all “bunched” at the end after all in-
sertions, (iii) a single, repeated query, performed at random inter-
vals among the insertions. According to our theoretical analysis,
the “bunched” distribution should be the worst-case scenario and
the repeated query should be the best-case. In practice we did not
see much difference in performance between bunched or random
queries, though as expected, we observed improved performance
for the repeated query case.
Networked setting: real salary data. To test performance over a
realistic network with latency and bandwidth restrictions, we used
the California public employee payroll data from 2014, available
from [1], as a real dataset on which to perform additional experi-
ments. This dataset lists payroll information for roughly 2.3 mil-
lion public employees. We used the total salary ﬁeld as our “label”
value (on which range queries are performed), and the names as the
payload values.

We were not able to complete any test runs of the mOPE using
actual network communication over the salary dataset; based on
partial progress in our experiment we estimate it would take several
days to complete just one test run of this experiment using mOPE
and actual network communication with our Python implementa-
tion.

1138Figure 6: Total rounds of communication for POPE and mOPE, plotted in
log/log scale according to total number of insertions n. Lower is better. The
number of range queries in all cases was

√

n.

Figure 7: Amortized communication costs for POPE and mOPE, according
to total number of insertions n. Lower is better. The number of range
queries in all cases was

√

n.

As these ﬁgures demonstrate, the round complexity for POPE,
which is constant per range query, is several orders of magnitude
less than that of mOPE. Furthermore, when averaged over all op-
erations, the number of ciphertexts transferred per operation for
POPE is roughly 7 in the worst case, whereas for mOPE this in-
creases logarithmically with the database size.
√
Experimental running time. The per-second operations counts,
n range queries,
for our main experiments with n insertions, m =
and L = n1/4 client-side storage, are presented in Figure 8. For
POPE, the performance increases until roughly 1 million entries,
after which the per-operation performance holds steadily between
50,000 operations per second with random, distinct queries, and
110,000 operations per second with a single, repeated query.

For one million entries and using our Python implementation
without parallelization, we achieved over 55,000 operations per
second with POPE vs.
less than 2,000 operations per second for
mOPE, without even accounting for the network communication.
Our POPE construction is well-suited particularly for problems
with many more insertions than range queries; indeed, the O(1)
theoretical performance guarantees hold only when mL < n. Fig-
ure 9 shows the effects of varying numbers of range queries on
POPE performance. Although the performance of POPE clearly
degrades with increasing numbers of queries performed, this ex-
periment shows competitive performance compared to mOPE even
when m = n.

Figure 8: Operations performed per second for POPE and mOPE. Higher
is better. The number of range queries in all cases was

n.

√

Figure 9: Degradation in POPE performance with increasing number of
queries, measured in operations per second. Higher is better. In all experi-
storage at L = 32. For these choices, 210 ≈ √
ments, the number of insertions n was ﬁxed at 1 million, and the client-side
n queries is as shown in
prior ﬁgures, and our O(1)-cost analysis holds up to m ≈ 215.

5.4 Experimental Network Effects

We tested the effects of varying network latency and bandwidth
using the California public employees payroll data as described
above. Our workload consisted of all 2,351,103 insertions as well
as 1,000 random range queries at random points throughout the in-
sertions. Each range query result size was ﬁxed at 100 entries.

Figure 10 shows the effects of latency on the POPE implemen-
tation. With less than 5ms of latency, the cost is dominated by that
of the POPE computation and other overhead. Beyond this level,
the total runtime scales linearly with the latency. Note that 10ms
to 30ms represents typical server response times within the same
continent over the Internet.

Figure 11 shows the effects of bandwidth limitations on our con-
struction. Without any latency restriction, we limited the band-
width between 1 and 20 megabits per second (Mbps), which is the
typical range for 4G (on the low end) and home broadband Inter-
net (on the high end) connections. We can see that, past roughly 10
Mbps, the other overhead of the implementation begins to dominate
and there is no more signiﬁcant gain in speed.

6. RELATED WORK
Order-Preserving and Order-Revealing Encryption. Order-
preserving encryption (OPE) [3, 7, 8] guarantees that enc(x) <
enc(y) iff x < y. Thus, range queries can be performed directly
over the ciphertexts in the same way that such a query would be

101102103104105106107108109103104105106107108total rounds of communicationnumber of entriesmOPEPOPE, random queriesPOPE, bunched queriesPOPE, repeated queries 0 10 20 30 40 50 60103104105106107108ciphertexts transfered per opnumber of entriesmOPEPOPE, random queriesPOPE, bunched queriesPOPE, repeated queries 0 20 40 60 80 100 120 140 160103104105106107108thousands of ops per secondnumber of entriesPOPE, repeated queriesPOPE, bunched queriesPOPE, random queriesmOPE 0 10 20 30 40 50 60210212214216218220thousands of ops per secondnumber of queriesPOPEmOPE1139Symmetric searchable encryption (SSE) was ﬁrst proposed by
Song, Wagner, and Perrig [39] who showed how to search over
encrypted data for keyword matches in sub-linear time. The ﬁrst
formal security deﬁnition for SSE was given by Goh [23], Curt-
mola et al. [16] showed the ﬁrst SSE scheme with sublinear search
time and compact space, while Cash et al. [13] showed the ﬁrst SSE
scheme with sublinear search time for conjunctive queries. Recent
works [34, 13, 19] achieve performance within a couple orders of
magnitude of unencrypted databases for rich classes of queries in-
cluding boolean formulas over keyword, and range queries. Of par-
ticular interest is the work of Hahn and Kerschbaum [27] who show
how to use lazy techniques to build SSE with quick updates. We
refer interested readers to the survey by Bösch et al. [12] for an
excellent overview of this area.

Oblivious RAM [24, 38, 40, 44] and oblivious storage schemes
[26, 4, 18, 32] can be used for the same applications as OPE and
POPE, but achieve a stronger security deﬁnition that additionally
hides the access pattern, and therefore incur a larger performance
cost than our approach.

Finally, we note that techniques such as fully-homomorphic en-
cryption [22], public-key searchable encryption [9, 11, 37], and
secure multi-party computation [46, 6, 25] can enable searching
over encrypted data while achieving the strongest possible security.
However, these approaches would require performing expensive
cryptographic operations over the entire database on each query
and are thus prohibitively expensive. Very recently cryptographic
primitives such as order-revealing encryption [10], as well as gar-
bled random-access memory [21], have offered the potential to
achieve this level of security for sub-linear time search. However,
all constructions of these primitive either rely on very non-standard
assumptions or are prohibitively slow.
Lazy data structures and I/O complexity. Our POPE tree is is
similar in concept to the Buffer Tree of [5]. Their data structure de-
lays insertions and searches in a buffer stored at each node, which
are cleared (thus executing the actual operations) when they be-
come sufﬁciently full. The main difference here is that our buffers
contain only insertions, and they are cleared only when a search
operation passes through that node.

We also point out an interesting connection to I/O complexity
regarding the size of local storage. In our construction, as in [36],
the client is treated as an oracle to perform comparisons of cipher-
texts. If we think of the client’s memory as a “working space” of
size L, and the server’s memory as external disk, then from [2] it
can be seen that performing m range queries on a database of size
n ≥ m requires a total transfer bandwidth of at least Ω(m logL m)
ciphertexts. (This is due to the lower bound on the I/O complexity
of sorting, and the fact that m range queries can reveal the order
of a size-m subset.) In particular, this means that the mOPE con-
struction from [36] cannot be improved without either limiting the
number of queries, or increasing the client-side storage, both of
which we do for POPE.
Acknowledgments We thank Jonathan Katz and David Cash for
recommending the importance of the improved security of POPE.
We also thank the anonymous reviewers for their useful comments.
Daniel S. Roche’s work is supported in part by Ofﬁce of Naval
Research (ONR) award N0001416WX01489 and National Science
Foundation (NSF) awards #1319994 and #1618269. Daniel Apon’s
work is supported in part by NSF awards #1111599, #1223623, and
#1514261. Seung Geol Choi’s work is supported in part by ONR
awards N0001416WX01489 and N0001416WX01645, and NSF
award #1618269. Arkady Yerukhimovich’s work is sponsored by
the Assistant Secretary of Defense for Research and Engineering
under Air Force Contract No. FA8721-05-C-0002 and/or FA8702-

Figure 10: Total running time for 2.3 million insertions and 1000 random
range queries, running over a network with varying artiﬁcially-induced la-
tency times.

Figure 11: Total running time for 2.3 million insertions and 1000 random
range queries, running over a network with varying bandwidth limitations.

performed over the plaintext data. However, OPE comes with a
security cost. None of the original schemes [3, 7] achieve the
ideal security goal for OPE of IND-OCPA (indistinguishability un-
der ordered chosen-plaintext attack) [7] in which ciphertexts reveal
no additional information beyond the order of the plaintexts.
In
fact Boldyreva et al. [7] prove that achieving a stateless encryp-
tion scheme with this security goal is impossible under reasonable
assumptions. The existing schemes, instead, either lack formal
analysis or strive for weaker notions of security which have been
shown to reveal signiﬁcant amount of information about the plain-
text [8]. The ﬁrst scheme to achieve IND-OCPA security was the
order-preserving encoding scheme of Popa et al. [35].

A related primitive to OPE is order-revealing encryption (ORE)
[7], which provides a public mechanism for comparing two en-
crypted values and thus also enables range searches over encrypted
data. (Note, OPE is the special case where this mechanism is lex-
icographic comparison,) The ﬁrst construction of ORE satisfying
ideal security [10] was based on multi-linear maps [20] and is thus
unlikely to be practical in the near future. An alternative scheme
based only on pseudorandom functions [15], however, has addi-
tional leakage that weakens the achieved security.
OPE alternatives. In addition to OPE there are several other lines
of work that enable searching over encrypted data. Typically, these
works provide stronger security than provided by OPE; in particu-
lar they do not reveal the full order of the underlying data as hap-
pens with OPE. However, the additional security guarantees come
at a signiﬁcant performance cost with even the latest schemes being
one to two orders of magnitude slower than the latest OPE-based
implementations [34, 19].

 400 600 800 1000 1200 1400 1600 1800 2000 0 5 10 15 20 25 30 35 40 45 50total time (s)induced network latency (ms)POPE, random queries 0 500 1000 1500 2000 2500 3000 3500 4000 4500 0 2 4 6 8 10 12 14 16 18 20total time (s)induced bandwidth limit (Mbps)POPE, random queries114015-D- 0001. Any opinions, ﬁndings, conclusions or recommenda-
tions expressed in this material are those of the author(s) and do not
necessarily reﬂect the views of the Assistant Secretary of Defense
for Research and Engineering.

7. REFERENCES
[1] California public employee payroll data, 2014. Source: Transparent

California, http://transparentcalifornia.com/downloads/.

[2] Alok Aggarwal and Jeffrey Scott Vitter. The I/O complexity of

sorting and related problems. In ICALP 1987, volume 267 of LNCS,
pages 467–478. Springer Berlin Heidelberg, 1987.

[3] Rakesh Agrawal, Jerry Kiernan, Ramakrishnan Srikant, and Yirong

Xu. Order-preserving encryption for numeric data. In ACM SIGMOD
2014, pages 563–574, 2004.

[4] Daniel Apon, Jonathan Katz, Elaine Shi, and Aishwarya

Thiruvengadam. Veriﬁable oblivious storage. In PKC 2014, volume
8383 of LNCS, pages 131–148. Springer, Heidelberg, March 2014.

[5] Lars Arge. The buffer tree: A technique for designing batched

external data structures. Algorithmica, 37(1):1–24, 2003.
[6] Michael Ben-Or, Shaﬁ Goldwasser, and Avi Wigderson.

Completeness theorems for non-cryptographic fault-tolerant
distributed computation (extended abstract). In 20th ACM STOC,
pages 1–10, 1988.

[7] Alexandra Boldyreva, Nathan Chenette, Younho Lee, and Adam

O’Neill. Order-preserving symmetric encryption. In
EUROCRYPT 2009, pages 224–241, 2009.

[8] Alexandra Boldyreva, Nathan Chenette, and Adam O’Neill.

Order-preserving encryption revisited: Improved security analysis
and alternative solutions. In CRYPTO 2011, pages 578–595, 2011.

[9] Dan Boneh, Giovanni Di Crescenzo, Rafail Ostrovsky, and Giuseppe

Persiano. Public key encryption with keyword search. In
EUROCRYPT 2004, volume 3027 of LNCS, pages 506–522.
Springer, Heidelberg, May 2004.

[10] Dan Boneh, Kevin Lewi, Mariana Raykova, Amit Sahai, Mark

Zhandry, and Joe Zimmerman. Semantically secure order-revealing
encryption: Multi-input functional encryption without obfuscation.
In EUROCRYPT 2015, Part II, volume 9057 of LNCS, pages
563–594. Springer, Heidelberg, April 2015.

[11] Dan Boneh and Brent Waters. Conjunctive, subset, and range queries

on encrypted data. In TCC 2007, pages 535–554, 2007.

[12] Christoph Bösch, Pieter H. Hartel, Willem Jonker, and Andreas Peter.

A survey of provably secure searchable encryption. ACM Comput.
Surv., 47(2):18:1–18:51, 2014.

[13] David Cash, Stanislaw Jarecki, Charanjit S. Jutla, Hugo Krawczyk,

Marcel-Catalin Rosu, and Michael Steiner. Highly-scalable
searchable symmetric encryption with support for boolean queries. In
CRYPTO 2013, Part I, pages 353–373, 2013.

[14] Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh,

Deborah A. Wallach, Michael Burrows, Tushar Chandra, Andrew
Fikes, and Robert Gruber. Bigtable: A distributed storage system for
structured data. In OSDI 2006, pages 205–218, 2006.

[15] Nathan Chenette, Kevin Lewi, Stephen A. Weis, and David J. Wu.
Practical order-revealing encryption with limited leakage. In FSE,
pages 474–493. Springer, 2016.

[16] Reza Curtmola, Juan A. Garay, Seny Kamara, and Rafail Ostrovsky.
Searchable symmetric encryption: improved deﬁnitions and efﬁcient
constructions. In ACM CCS 06, pages 79–88, 2006.

[17] Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, Gunavardhan

Kakulapati, Avinash Lakshman, Alex Pilchin, Swaminathan
Sivasubramanian, Peter Vosshall, and Werner Vogels. Dynamo:
Amazon’s highly available key-value store. In SOSP 2007, pages
205–220, 2007.

[18] Srinivas Devadas, Marten van Dijk, Christopher W. Fletcher, Ling

Ren, Elaine Shi, and Daniel Wichs. Onion ORAM: A constant
bandwidth blowup oblivious RAM. Theory of Cryptography
Conference, TCC ’16, 2016.

[19] Sky Faber, Stanislaw Jarecki, Hugo Krawczyk, Quan Nguyen,

Marcel-Catalin Rosu, and Michael Steiner. Rich queries on encrypted
data: Beyond exact matches. In ESORICS 2015, Part II, volume 9327
of LNCS, pages 123–145. Springer, Heidelberg, September 2015.

[20] Sanjam Garg, Craig Gentry, and Shai Halevi. Candidate multilinear

maps from ideal lattices. In EUROCRYPT 2013, volume 7881 of
LNCS, pages 1–17. Springer, Heidelberg, May 2013.

[21] Sanjam Garg, Steve Lu, Rafail Ostrovsky, and Alessandra Scafuro.
Garbled RAM from one-way functions. In 47th ACM STOC, pages
449–458. ACM Press, June 2015.

[22] Craig Gentry. Fully homomorphic encryption using ideal lattices. In

41st ACM STOC, pages 169–178, 2009.

[23] Eu-Jin Goh. Secure indexes. IACR Cryptology ePrint Archive,

2003:216, 2003.

[24] Oded Goldreich. Towards a theory of software protection and

simulation by oblivious RAMs. In 19th ACM STOC, pages 182–194.
ACM Press, May 1987.

[25] Oded Goldreich, Silvio Micali, and Avi Wigderson. Proofs that yield

nothing but their validity and a methodology of cryptographic
protocol design (extended abstract). In 27th FOCS, 1986.

[26] Michael T. Goodrich, Michael Mitzenmacher, Olga Ohrimenko, and

Roberto Tamassia. Practical oblivious storage. In ACM CODASPY
’12, pages 13–24, 2012.

[27] Florian Hahn and Florian Kerschbaum. Searchable encryption with
secure and efﬁcient updates. In ACM CCS 14, pages 310–320, 2014.
[28] C. A. R. Hoare. Algorithm 64: Quicksort. Commun. ACM, 4(7):321–,

July 1961.

[29] Florian Kerschbaum. Frequency-hiding order-preserving encryption.

In ACM CCS 15, pages 656–667, 2015.

[30] Florian Kerschbaum and Axel Schröpfer. Optimal

average-complexity ideal-security order-preserving encryption. In
ACM CCS 14, pages 275–286, 2014.

[31] Rui Li, Alex X. Liu, Ann L. Wang, and Bezawada Bruhadeshwar.

Fast range query processing with strong privacy protection for cloud
computing. Proc. VLDB Endow., 7(14):1953–1964, October 2014.

[32] Tarik Moataz, Travis Mayberry, and Erik-Oliver Blass. Constant

communication ORAM with small blocksize. In ACM CCS 15, pages
862–873, 2015.

[33] Muhammad Naveed, Seny Kamara, and Charles V. Wright. Inference
attacks on property-preserving encrypted databases. In ACM CCS 15,
2015.

[34] Vasilis Pappas, Fernando Krell, Binh Vo, Vladimir Kolesnikov, Tal
Malkin, Seung Geol Choi, Wesley George, Angelos D. Keromytis,
and Steve Bellovin. Blind Seer: A scalable private DBMS. In 2014
IEEE Symposium on Security and Privacy, pages 359–374, 2014.

[35] Raluca A. Popa, Frank H. Li, and Nickolai Zeldovich. An

ideal-security protocol for order-preserving encoding. In 2013 IEEE
Symposium on Security and Privacy, pages 463–477, 2013.

[36] Raluca A. Popa, Catherine M. S. Redﬁeld, Nickolai Zeldovich, and

Hari Balakrishnan. CryptDB: protecting conﬁdentiality with
encrypted query processing. In SOSP 2011, pages 85–100, 2011.

[37] Elaine Shi, John Bethencourt, Hubert T.-H. Chan, Dawn Xiaodong

Song, and Adrian Perrig. Multi-dimensional range query over
encrypted data. In 2007 IEEE Symposium on Security and Privacy,
pages 350–364, 2007.

[38] Elaine Shi, T.-H. Hubert Chan, Emil Stefanov, and Mingfei Li.

Oblivious RAM with o((log n)3) worst-case cost. In
ASIACRYPT 2011, volume 7073 of LNCS, pages 197–214. Springer,
Heidelberg, December 2011.

[39] Dawn Xiaodong Song, David Wagner, and Adrian Perrig. Practical

techniques for searches on encrypted data. In 2000 IEEE Symposium
on Security and Privacy, pages 44–55, 2000.

[40] Emil Stefanov, Marten van Dijk, Elaine Shi, Christopher W. Fletcher,

Ling Ren, Xiangyao Yu, and Srinivas Devadas. Path ORAM: an
extremely simple oblivious RAM protocol. In ACM CCS 13, pages
299–310. ACM Press, November 2013.

[41] The Apache Software Foundation. Accumulo.

https://accumulo.apache.org/. Accessed: 2015-09-24.

[42] The Apache Software Foundation. Cassandra.

https://cassandra.apache.org/. Accessed: 2015-09-24.

[43] The Apache Software Foundation. Hbase. http://hbase.apache.org/.

Accessed: 2015-09-24.

[44] Xiao Wang, Hubert Chan, and Elaine Shi. Circuit ORAM: On

tightness of the Goldreich-Ostrovsky lower bound. In ACM CCS 15,
pages 850–861, 2015.

1141[45] D. Westhoff, J. Girao, and M. Acharya. Concealed data aggregation

for reverse multicast trafﬁc in sensor networks: Encryption, key
distribution, and routing adaptation. Mobile Computing, IEEE
Transactions on, 5(10):1417–1431, Oct 2006.

[46] Andrew Chi-Chih Yao. How to generate and exchange secrets

(extended abstract). In 27th FOCS, pages 162–167, 1986.

8. PROOF OF THEOREM 1

Choose n,  so that L = n > 16. The case of Insert is trivial
to analyze: The server never makes comparison requests. So, we
focus on the case of Search.
An alternative split procedure To simplify our analysis, we in-
troduce an alternative version of the leaf splitting procedure which
discards any split that results in very unbalanced partitions. We ar-
gue that such a split will always (in expectation) be worse than our
original split and thus can be used to bound its costs.

Let z = 2L/ log L. We say that a set of L pivot points is z-
balanced if there are z (out of L) pivots such that partitioning a
node of size k using these z pivots only results in partitions that are
each of size at most 2k/z.

A. As in actual Split, choose L pivots uniformly at random and par-

tition the labels according to these pivots.

B. If the L pivots are not z-balanced, throw out the partition and

recurse on the same node.

C. If these L pivots are z-balanced, promote only the z balanced
pivots (instead of the total L) to the parent, partition the labels
and recurse on the node containing the searched label.

We argue that this procedure is worse than the original split both
in the (expected) number of rounds and the (expected) total band-
width (over all m queries). To see this for the number of rounds,
observe that the alternate procedure chooses its partitions in the
same way as the original, but always drops some (or all) of the
pivot points resulting in larger nodes and a deeper recursion to
reach nodes of size L. For the case of bandwidth most of the cost
comes from streaming labels to the client to partition them when
splitting a leaf, which takes O(k) bandwidth for a node of size k.
Now consider a single label x in the tree. This may get moved be-
tween leaf nodes multiple times during the queries, but each time it
is moved the node it lands in is larger if the alternative split proce-
dure is used as argued above, as compared to the actual Split. Thus,
the total cost of all splits over m queries is larger for the alternative
split as it will require repeatedly streaming these larger nodes to the
client.

For the remainder of this proof, we analyze the alternative pro-

cedure for splitting a leaf to bound the costs of the real one.
Round complexity for a single search. The round complexity for
a search can be computed by considering the round complexity for
splitting at internal nodes (case (i) in Section 3.4) and splitting at a
leaf (case (ii) in Section 3.4).

The round complexity for case (i) is asymptotically the same as
the height of the tree. Since the tree is re-balanced such that each
internal node contains at least L/2 labels, the height of the tree is
O(logL n).

As for case (ii), we ﬁrst need to show that L random pivots are
z-balanced with constant probability, so that there is a successful
split after O(1) many unsuccessful ones. To see this, deﬁne an
imaginary sorted list (X1, . . . , Xz) that contains the k input labels
in sorted order, equally partitioned so each Xi has k/z elements.
Note if each Xi contains at least one pivot (out of the chosen L
pivots), then the pivots must be z-balanced; in particular, one can

ﬁnd such pivots by choosing one from each Xi. By the Coupon
Collector’s Problem, the probability that L pivots hit all the Xis is
constant.

Now, note that, by the deﬁnition of z-balanced, after each suc-
cessful split the size of the largest partition is reduced by a factor of
z/2 = L/ log L. Thus, the total number of successful splits needed
and also the total (expected) recursion depth is O(logL/ log L n),
which simpliﬁes to O(logL n) when n ≥ 16. The total round
complexity of the POPE protocol is therefore O(logL n).
Total bandwidth over m search queries.
Height of the tree. First, we need a tighter analysis on the height
of the POPE tree. For this, we start with counting the total number
of labels in the internal nodes. The total number of Split calls over
all m Search operations is at most O(m logL n), since each search
has O(logL n) recursion depth.

Now, consider the sorted labels in non-leaf nodes of the tree.
Each such label is inserted by a Split operation from a leaf, and
each Split inserts at most z labels. Therefore, the total number
of labels stored in the sorted, non-leaf portion of the tree T is
O(zm logL n), which is O(mL log n). Recall the sorted labels
in the non-leaf nodes of the tree form a B-tree with between L and
L/2 labels per node (after rebalancing). Therefore, the maximum

height of the tree is height(T ) = O(cid:0) logL(mL log n)(cid:1).

Sending sorted labels to client. Recall that the round complexity of
a search is O(logL n). Each round of Search involves uploading at
most L labels to serve as partition indices to the client, incurring a
total bandwidth of B(cid:96) = O(mL logL n).
Sending labels in non-leaf buffers to client. In addition, all the la-
bels in buffers along the search path are sent to the client – some
more than once. Observe that labels in buffers only move to a lower
buffer, or laterally from leaf nodes to leaf nodes during Split op-
erations, which means that any label in non-leaf nodes must be
sent to the client at most height(T ) times. Therefore, the ex-
pected total bandwidth for the labels in non-leaf buffers, across all
Search operations, is Bin = O(n · height(T )) = O(n logL m +
n logL(log n)).
Communication cost of splits. Observe that splitting a leaf node
of size k, through all the recursive calls, takes bandwidth O(k)
since O(k + k/z + k/z2 + . . .) = O(k). So, we consider the
total size of all leaf nodes encountered during Search operations
to compute the costs from splits. The worst-case scenario for the
construction is when all n insertions happen before all m searches,
and each search’s splits land in the largest remaining leaf node(s).
Using the alternative split procedure, the largest possible leaf nodes
the search’s splits will land in (counting only successful splits as
rounds) have the following sizes:

most n, etc.

nodes is at most n.

nodes of size at most n · (2/z).

- (Round 1) 1 node of size n. A split lands on this node, splitting it into
- (Round 2) z nodes of size at most n· (2/z). Note the total size of the
- (Round 3) z2 nodes of size at most n · (2/z)2. The total size is at
i=0 zi ≥ m with w = O(logL m), and m largest
leaf nodes are encountered by round w. Since the total size of the
touched nodes in each round is at most n, the total size of the m
largest leaf nodes is bounded by Bs = nw = O(n logL m).

We have (cid:80)w

By summing up B(cid:96), Bin, Bs, we ﬁnd that the total bandwidth
over all (n + m) operations is at most O(mL logL n + n logL m +
n logL(log n)), and Theorem 1 follows.

1142