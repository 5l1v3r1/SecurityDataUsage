Differential Privacy for Classiﬁer Evaluation

Kendrick Boyd

Eric Lantz

Department of Computer

Department of Computer

Sciences

University of

Sciences

University of

David Page

Department of Biostatistics

and Medical Informatics

University of

Wisconsin-Madison

boyd@cs.wisc.edu

Wisconsin-Madison

lantz@cs.wisc.edu

Wisconsin-Madison

page@biostat.wisc.edu

ABSTRACT
Diﬀerential privacy provides powerful guarantees that indi-
viduals incur minimal additional risk by including their per-
sonal data in a database. Most work in diﬀerential privacy
has focused on diﬀerentially private algorithms that produce
models, counts, and histograms. Nevertheless, even with a
classiﬁcation model produced by a diﬀerentially private al-
gorithm, directly reporting the classiﬁer’s performance on
a database has the potential for disclosure. Thus, diﬀeren-
tially private computation of evaluation metrics for machine
learning is an important research area. We ﬁnd eﬀective
mechanisms for area under the receiver-operating character-
istic (ROC) curve and average precision.

1.

INTRODUCTION

It has long been known that machine learning models can
reveal information about the data used to train them. In
the extreme case, a nearest neighbor model might store the
dataset itself, but more subtle disclosures occur with all
types of models. Even small changes in the training set
can produce detectable changes in the model. This fact has
motivated work to preserve the privacy of the training set
by making it diﬃcult for an adversary to discern informa-
tion about the training data. One popular framework is
diﬀerential privacy [5], which sets bounds on the amount of
change that can occur when any one training dataset row is
modiﬁed.

Several authors have modiﬁed existing machine learning
algorithms such that the algorithms satisfy diﬀerential pri-
vacy, e.g. [3, 7, 16, 19]. The machine learning method is a
kind of query on the training database, and the modiﬁed al-
gorithms output perturbed models. In doing so, the learned
models can be released to the public, and the privacy risk to
the owners of the rows in the database is tightly bounded,
even if the adversary has auxiliary information. However,
these protections only cover the training dataset, not any
latter uses of the model, such as evaluating a model’s per-
formance on another dataset. Evaluation metrics (e.g., accu-

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
AISec’15, October 16, 2015, Denver, Colorado, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3826-4/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2808769.2808775.

racy, area under the ROC curve, average precision) on sepa-
rate test datasets are a crucial part of the machine learning
pipeline, and are just as important as training or interpret-
ing the model. But when a privately trained model is eval-
uated on a dataset and the results are released, the privacy
guarantees from training do not apply to the test dataset.

Consider a scenario in which multiple hospitals are collab-
orating to predict disease onset but are prevented by policy
or law from sharing their data with one another. They may
instead attempt to produce a model using data from one
institution and test the model at other sites in order to
evaluate how well the model generalizes. The institution
generating the model might use a diﬀerentially private al-
gorithm to create the model in order to protect their own
patients, and then distribute the model to the other hospi-
tals. These hospitals in turn run the model on their patients
and produce an evaluation of the model performance such as
accuracy or, more commonly, the area under the ROC curve
(AUC). If the AUC is released, the test datasets at the lat-
ter institutions are not covered by any privacy protection
that might have been used during training. The problem
remains even if the training and test datasets exist at the
same institution. While releasing an evaluation metric may
seem to be a limited potential privacy breach, it has been
demonstrated that data about patients can be reconstructed
from ROC curves if the adversary has access to a subset of
the test data [12].

Our aim in this paper is to expand the scope of diﬀeren-
tial privacy in machine learning to include protection of test
datasets in addition to existing work on protecting training
datasets. While previous work has examined related topics
such as diﬀerentially private density estimation [9, 18] and
parameter selection [4], there has been little focus on the
metrics most commonly used in machine learning evalua-
tion: accuracy, area under the ROC curve (AUC) and aver-
age precision (AP). Without including these in the study of
diﬀerential privacy, we are omitting from privacy consider-
ations a large portion of what machine learning practioners
actually do with real data. Fundamental contributions of
this paper include derivations of the local sensitivity of AUC
and AP and emphasizing that accuracy, speciﬁcity, etc. can
be readily made private via existing methods.

We start in Section 2 by providing background on eval-
uation metrics based on confusion matrices, such as ROC
curves, as well as on diﬀerential privacy.
In Section 3 we
provide mechanisms for diﬀerentially private calculation of
AUC and average precision. Finally we evaluate our mech-
anisms in several experiments in Section 4 and describe ex-

15tensions to reporting symmetric binormal ROC curves in
Section 5.

boring databases (d(D, D′) = 1) is bounded by a constant
ratio.

2. BACKGROUND

2.1 Confusion Matrices and Rank Metrics

Evaluation for binary classiﬁcation tasks begins with the
confusion matrix. A confusion matrix describes the perfor-
mance of a model that outputs positive or negative for every
example in some test set. We characterize the test set by the
number of positive examples n and the number of negative
examples m. The four values in a confusion matrix are the
true positives (tp), false positives (f p), true negatives (tn),
and false negatives (f n). From the confusion matrix, many
common evaluation metrics can be calculated. For example,
accuracy is tp+tn

n+m and precision is

Many models output a probability or score, which cre-
ates a ranking over examples. This creates many diﬀerent
confusion matrices, depending on which value is chosen as
a threshold. To describe the performance of these scoring
classiﬁers, a common technique is to consider all possible
thresholds and plot the trade-oﬀs among confusion matrix
metrics. For example, we have ROC curves when true posi-
tive rate ( tp
m ) is
on the x-axis. As a single summary metric, the area under
the ROC curve is often used.

n ) is on the y-axis and false positive rate ( f p

tp

tp+f p .

Area under ROC curve (AUC) is well-studied in statistics

and is equivalent to the Mann-Whitney U statistic [14].

AUC =

1

nm

m

n

Xi=1

Xj=1

where xi for 1 ≤ i ≤ m are the scores on the negative
examples in the test set, yj for 1 ≤ j ≤ n are the scores on
the positive examples, and I is the indicator function. Note
that neither xi nor yj need be ordered.

Another metric we examine is average precision (AP),
which is commonly used in information retrieval and is equiv-
alent to the area under the precision-recall curve as n+m →
∞ with constant n
m [11]. AP is the average of the precision
values across all values of recall. We use the following for-
mulation:

j
I[xi > yj]

(2)

AP =

1
n

n

Xj=1

i=1

j +Pm

with the same xi and yj as above and the additional as-
sumption that the yj’s (but not xi’s) are sorted.

We assume no ties in the scores assigned to positive and
negative examples when calculating Eq. (1) and Eq. (2).
In case of ties, we impose a complete ordering where all
negative examples are placed before the positive examples
to avoid overestimation of the curves and areas.

2.2 Differential Privacy

Diﬀerential privacy is a privacy mechanism that guaran-
tees the presence or absence of an individual’s information in
the database has little eﬀect on the output of an algorithm.
Thus, an adversary can learn limited information about any
individual. More precisely, for any databases D, D′ ∈ D, let
d(D, D′) be the number of rows that diﬀer between the two
databases. Diﬀerential privacy requires that the probability
an algorithm outputs the same result on any pair of neigh-

I[xi < yj]

(1)

Theorem 1. (Laplace noise [6]): Given a function f :

Definition 1. ((ǫ, δ)-diﬀerential privacy [6]): For any
input database D, a randomized algorithm f : D → Range(f )
is (ǫ, δ)-diﬀerentially private iﬀ for any S ⊆ Range(f ) and
any database D′ where d(D, D′) = 1,

Pr(f (D) ∈ S) ≤ eǫ Pr(f (D′) ∈ S) + δ

(3)

When δ = 0, the stronger guarantee of ǫ-diﬀerential privacy
is met.

To ensure diﬀerential privacy, we must compute the sensi-
tivity of the function we want to privatize. Here, sensitivity
is the largest diﬀerence between the output on any pair of
neighboring databases (not the performance metric tp

n ).

Definition 2. (Global sensitivity [6]): Given a function

f : D → R, the sensitivity of f is:

GSf = max

d(D,D′)=1 |f (D) − f (D′)|

(4)

This is known as global sensitivity because it is a worst-case
bound for the change in f for an arbitrary pair of databases
and to diﬀerentiate it from local sensitivity, introduced in
Deﬁnition 3.

Using Laplace distributed noise to perturb any real-valued

query gives the following result:

D → R, the computation

ǫ (cid:19)
f ′(D) = f (D) + Laplace(cid:18) GSf

(5)

guarantees ǫ-diﬀerential privacy.

The preceding approaches for obtaining diﬀerential pri-
vacy use the worst-case, global sensitivity to scale the noise.
For some functions, such as median, the global sensitivity
may be large, but the diﬀerence between outputs for most
neighboring databases is quite small. This motivates the
work of Nissim et al. [13] to explore uses of local sensitivity.

Definition 3. (Local sensitivity [13]): Given a function

f : D → R, the local sensitivity of f at D is

LSf (D) = max

d(D,D′)=1 |f (D) − f (D′)|.

(6)

Local sensitivity diﬀers from global sensitivity because lo-
cal sensitivity is parameterized by a particular database D,
while global sensitivity is over all databases, i.e., GSf =
maxD LSf (D).

Local sensitivity cannot be directly used to provide dif-
ferential privacy, but a smooth upper bound can be used.

Definition 4. (β-smooth sensitivity [13]): For β > 0,

the β-smooth sensitivity of f is

S∗

f,β(D) = max
D′∈D

LSf (D′)e−βd(D,D′).

(7)

16Using the β-smooth sensitivity and Cauchy-like or Laplace
noise provides diﬀerentially privacy as speciﬁed in the fol-
lowing theorem.

Theorem 2. (Calibrating Noise to Smooth Bounds on
Sensitivity, 1-Dimensional Case [13]): Let f : D → R be
any real-valued function and let S : D → R be the β-smooth
sensitivity of f , then

1. If β ≤ ǫ

2(γ+1)S(D)

2(γ+1) and γ > 1, the algorithm f ′(D) = f (D)+
η, where η is sampled from the distribution
with density h(z) ∝ 1
1+|z|γ , is ǫ-diﬀerentially private.
Note that when γ = 2, η is drawn from a standard
Cauchy distribution.

ǫ

ǫ

2. If β ≤

and δ ∈ (0, 1), the algorithm f ′(D) =
2 ln( 2
δ )
f (D) + 2S(D)
η, where η ∼ Laplace(1), is (ǫ, δ)-diﬀer-
entially private.

ǫ

3. PRIVATE MECHANISMS

Our discussion of diﬀerentially private evaluation will as-
sume that a classiﬁcation model is applied to a private data-
base. The model could be hand-constructed by the submit-
ter, trained on another private database in a diﬀerentially
private way, or trained on a public database. Our goal is to
ensure the evaluation output does not release too much infor-
mation about any particular example in the private database
by requiring a diﬀerentially private evaluation function.

We assume that the size of the database, n + m, is public
information, but that the speciﬁc values of n and m are not
publicly available. Though allowing n and m to be public
would make our analysis for AUC and AP simpler and might
achieve induced neighbors privacy [10], we believe that keep-
ing the number of positive and negative examples private is
a critical aspect of private model evaluation.
If n and m
were public information, the worst-case adversary for diﬀer-
ential privacy that knows all but one row of the database
would be able to trivially infer whether the last row is a
positive or negative. Since the class label is often the most
sensitive piece of information in a prediction task, releas-
ing precise counts of positives and negatives would greatly
weaken the security provided by a privacy framework. To il-
lustrate the diﬀerence in neighboring databases, ROC curves
for two neighboring databases are shown in Figure 1.

What types of evaluation metrics can be released privately
under this framework? Any metric based on a single confu-
sion matrix can be made private by applying the standard
methods, such as Laplace noise, for diﬀerentially private
counts or marginals [5]. Thus, diﬀerentially private accu-
racy, recall, speciﬁcity, precision, etc. can be obtained. We
focus on more complex metrics such as AUC that are both
more useful for classiﬁer evaluation [15] as well as more chal-
lenging to implement privately.

3.1 Reidentifying AUC

Prior work has demonstrated the vulnerability of data
points in ROC curves to reidentiﬁcation [12]; we extend that
to AUC to demonstrate that the problem remains even with
the summary metric. Consider the problem of identifying
the class of an excluded example given the AUC of the full
dataset. Here the adversary has access to all of the data
points but one, and also knows the AUC of the full data.

1.00

e
t
a
r
 
e
v
i
t
i
s
o
p
 
e
u
r
t

0.75

0.50

0.25

0.00

0.00

0.25

0.50

0.75

1.00

false positive rate

D

D'

Figure 1: ROC curves for two neighboring databases
where the diﬀerence between D and D′ is that a
negative was changed to a positive and given a new
score. D contains 15 positives and 15 negatives and
D′ contains 16 positives and 14 negatives. AUC for
D and D′ is 0.796 and 0.772, respectively.

The goal is to predict whether the ﬁnal example is a member
of the positive or negative class. Note that we do not assume
the adversary knows where the target example should go in
the ranking.

The adversary’s algorithm is to attempt to place the tar-
get example at each position in the ranking, and calculate
the resulting AUC under the assumption that the example
is positive and again assuming it is negative. The class that
produces an answer closest to the released AUC for the full
dataset (or the most frequent class in the case of ties) is
guessed as the class of the example. This setup is similar to
the assumptions of diﬀerential privacy in terms of looking
at the inﬂuence on AUC from a single example. However, it
is not a worst case analysis and is concerned with identify-
ing an attribute value of the target example not simply its
presence in the original data.

Figure 2 shows the ability of the attacker to guess the
class of the target example given a sample of data from the
UCI adult dataset. One heuristic method that could be used
to interfere with this attack is to round the released AUC
to a smaller number of decimal places, and is illustrated in
the ﬁgure. When the AUC is given to a high number of
decimal places, the adversary is able to recover the class
value with high probability, though this ability falls as the
number of data points increases. Rounding the AUC value
to fewer decimal places does reduce the adversary’s success,
but comes at a cost to precision.

3.2 AUC

When calculating sensitivity of AUC, each example can
contribute to the sum multiple times. The sensitivity of
AUC is further complicated because the factor 1
nm diﬀers be-
tween neighboring datasets when a positive example changes
to a negative or vice versa. Fortunately, we can bound the

171.00

0.75

)
s
s
e
c
c
u
s
(
p

0.50

0.25

0.00

digits

7

6

5

4

3

2

0

500

1000
dataset size

1500

2000

2500

Figure 2: Adversary’s success rate in identifying the
class of the missing example given AUC of a dataset
containing half positives and half negatives with
speciﬁed signiﬁcant digits. The horizontal black line
at 0.5 denotes performance of randomly guessing the
class.

maximum change in AUC between neighboring datasets to
ﬁnd the local sensitivity.

Theorem 3. Local sensitivity of area under the ROC curve

(AUC) is

LSAUC(n, m) =(

1

min(n,m)
1

if n > 0 and m > 0
otherwise

(8)

where n and m are the number of positive and negative ex-
amples in the test set, respectively.

The full proof can be found in the appendix, but we
present a short proof outline here. For the purposes of cal-
culating AUC, it is suﬃcient to consider the ranking and
class label for rows in the database. This is captured in the
xi and yj variables and indicator function in Eq. (1). Since
most rows do not change, the majority of the indicator func-
tions in the double summation in Eq. (1) remain the same.
So we can decompose Eq. (1) into components that change
and those that do not for each of the four possible cases.
We then bound the maximum diﬀerence possible between
two databases in the components that do change to ﬁnd the
local sensitivity.

Local sensitivity itself is not suitable for creating diﬀeren-
tially private algorithms since adding diﬀerent amounts of
noise for adjacent databases can leak information [13]. In-
stead, we use β-smooth sensitivity which ensures the scale
of noise for adjacent databases is within a factor of eβ.

Theorem 4. β-smooth sensitivity of area under the ROC

curve (AUC) is

S∗

AU C,β = max

0≤i≤n+m

LSAUC(i, n + m − i)e−β|i−n|

(9)

The proof, given in the appendix, is a straight-forward ap-
plication of the deﬁnition of β-smooth sensitivity. Figure 3

1.00

0.75

y
t
i
v
i
t
i
s
n
e
s

0.50

0.25

0.00

beta

0

0.001

0.01

0.1

Infinity

0.00

0.25

0.50

0.75

1.00

prevalence

Figure 3: β-smooth sensitivity for AUC on a dataset
with 1000 examples. The sensitivity changes de-
pending on the prevalence of positive examples,
shown on the x-axis. When β is 0, the smooth sensi-
tivity cannot change so it is always 1, i.e., the global
sensitivity. If β is inﬁnity, there is no smoothness
constraint and the smooth sensitivity is exactly the
local sensitivity.

shows smooth sensitivity given by Eq. (9) for several values
of β demonstrating that the advantages of lower sensitivity
are more pronounced with higher β or balanced datasets.

With the β-smooth sensitivity of AUC, appropriately scaled

Cauchy noise can be used to obtain ǫ-diﬀerential privacy or
Laplace noise can be used to obtain (ǫ, δ)-diﬀerential privacy
as described in Theorem 2. Since the range of AUC is [0, 1],
we truncate the output. The truncation does not violate dif-
ferential privacy [8] because an adversary knows the range
of the true function.

3.3 Average Precision

Once again we want to use smooth sensitivity to produce
diﬀerentially private average precision, but ﬁrst we need to
ﬁnd the local sensitivity of average precision (AP). Precision
at low recall has high variance since changing just a single
row for neighboring datasets can cause precision to go from
1 to 1
2 simply by adding a high-scoring negative example.
Though precision at low recalls can vary substantially be-
tween neighboring datasets, the impact on average precision
is mitigated by the 1
n coeﬃcient in Eq. (2) and the sensitivity
is bounded in the following theorem.

Theorem 5. Local sensitivity of average precision (AP)

is

∆AP =


n

max(cid:16) log(n+1)
+ max(cid:16) log(n+1)

n

, 9+log(n−1)

4(n−1) (cid:17)
4n (cid:17)

, 9+log n

1

(10)

if n > 1
if n ≤ 1

where n is the number of positive examples and log is the
natural logarithm.

The proof approach for AP is similar to that for AUC in
that the summation in Eq. (2) can also be decomposed into
indicator functions that do and do not change. However,

18there are a few diﬀerences to the approach that are outlined
here and the full proof can be found in the appendix. Most
notably, instead of directly considering changing a row of the
database, we derive bounds for adding or removing a positive
or negative example separately as this greatly simpliﬁed the
math. Changing an example is equivalent to removing an
example and then adding an example, so the local sensitivity
is the sum of the bounds for both actions. Additionally,
due to the fraction that is summed in Eq. (2), the bounds
include harmonic terms. We use a simple upper bound of the
harmonic numbers to obtain Eq. (10), but a tighter bound
could be used to get slightly smaller local sensitivities.

Note that the local sensitivity of AP depends only on the
number of positive examples, n, and not the number of neg-
ative examples. This aligns with the notion that AP (and
PR curves) does not give credit for true negatives.

Theorem 6. β-smooth sensitivity of average precision (AP)

is

S∗
AP,β = max

0≤i≤n+m

LSAP(i)e−β|i−n|

(11)

The proof is identical to Theorem 4 with LSAP instead of
LSAUC.

As in AUC, we can use Cauchy or Laplace noise to produce
ǫ- or (ǫ, δ)-diﬀerentially private outputs. The range of AP is
not [0,1] since the the minimum AP for any particular n and
m is strictly greater than zero [2]. Though the minimum AP
can be sizable (about 0.3 when n = m), it depends on the
non-public n and m, so we cannot truncate to the database
speciﬁc minimum AP and just truncate to the overall range
of [0,1].

4. EXPERIMENTS

In this section we apply the algorithms from the previous
section to two datasets. Since our mechanisms operate on
the output of a classiﬁcation model, they should not be inﬂu-
enced by the number of features in the original dataset. The
ﬁrst dataset is the adult dataset from the UCI repository [1].
It contains potentially private information in both the class
label (yearly income greater or less than $50,000) and other
features (e.g., capital gain/loss and work status) that indi-
viduals might be hesitant to provide without privacy guar-
antees. The dataset has 14 features and 48,842 examples.
The second dataset is diabetes – a medical dataset from a
Kaggle competition1 to predict diabetes from anonymized
electronic health records. We processed the dataset to in-
clude age, gender, and binary features for the 50 most com-
mon drugs, diagnoses, and laboratory tests for a total of 152
features. The dataset contains 9,948 patients.

In our experiments, we trained a model on part of each
dataset using logistic regression. We use diﬀerentially pri-
vate evaluation on subsets of the rest of the dataset as a
surrogate for a private database. We investigate the utility
of the diﬀerentially private evaluations using mean absolute
error (MAE). The primary error of interest is between the
diﬀerentially private output and the answer calculated di-
rectly from the private data which we call the “DP error”.
For example, Figure 4 shows the distribution of private out-
puts for repeated queries of the same private dataset and
model.

1http://www.kaggle.com/c/pf2012-diabetes

100

1000

A
U
C

A
P

0.0

0.5

1.0

0.0

0.5

1.0

Differentially Private Output

Figure 4: Histograms of (ǫ, δ)-diﬀerentially private
AUC (top) and AP (bottom) from datasets of size
n = m = 100 (left) and n = m = 1000 (right). The
vertical red line denotes the non-private value and
the histograms are from repeated draws of Laplace
noise and correspond to the DP error. Data are
from the diabetes dataset and the privacy budget is
ǫ = 1 and δ = 0.01.

However, the DP error is not the only source of error or
uncertainty in the process. The private data set is a sample
from a (much) larger population of examples, and what we
as machine learners are most interested in is the performance
on the entire population. But we just have a small data set,
so the metric we calculate from that private data set (be-
fore applying the perturbation for privacy) is itself just an
estimate, with associated error, of the population value. We
refer to the diﬀerence between the population value and the
non-private metric calculated from the sampled data set as
the “estimation error”. The estimation error is a level of
uncertainty that is unavoidable, given the size of the data
set. If the additional noise due to making the output dif-
ferentially private is much smaller then making the output
private has minimal impact on utility. Even if the DP error
is of similar magnitude to the estimation error, the private
metric is still highly useful, adding a bit more noise to the
result, but not changing the scale of uncertainty.

The top part of Figure 5 shows the DP and estimation er-
ror of AUC for several dataset sizes. While the Cauchy noise
when δ = 0 causes the estimation error to be considerably
larger than the DP error, with (ǫ, δ)-diﬀerential privacy the
DP error is similar or smaller than the estimation error. As
dataset size increases, not only does the DP error decrease,
but it decreases relative to the estimation error. Thus, the
lost utility from providing privacy is decreasing more quickly
than the uncertainty due to estimating from a ﬁxed data
set. For the larger dataset on the right side of Figure 4,
the private AUC is clustered tightly around the non-private
version, while the smaller dataset is more spread out due to
the larger amount of noise required by diﬀerential privacy.
For AP, we use the same setup as for AUC, measuring
MAE versus ǫ for several dataset sizes with n = m. The
general trends for DP error of AP in the bottom part of
Figure 5 are similar to those for AUC, but the DP error
is much higher than the estimation error. This matches the
sensitivity theorems where there is an additional log n factor
in the AP sensitivity compared to AUC sensitivity. Thus,
for DP error and estimation error to be equal for AP requires
much larger datasets or ǫ than for AUC. The bottom part
of Figure 4 shows the distribution of outputted private AP
values. When n is small, the Laplace noise often puts the
value outside the valid range of AP. Since we truncate back

19delta=0

delta=1e−06

delta=1e−04

delta=0.01

A
d
u
l
t

n

D
i
a
b
e
t
e
s

30

100

300

1000

0.0

0.5

1.0 0.0

0.5

1.0 0.0

ε

0.5

1.0 0.0

0.5

1.0

delta=0

delta=1e−06

delta=1e−04

delta=0.01

A
d
u
l
t

n

D
i
a
b
e
t
e
s

30

100

300

1000

)
e
l
a
c
s
 
g
o
l
(
 
E
A
M
C
U
A

 

0.10

0.01

0.10

0.01

)
e
l
a
c
s
 

g
o
l
(
 
E
A
M
P
A

 

0.10

0.01

0.10

0.01

0.0

0.5

1.0 0.0

0.5

1.0 0.0

ε

0.5

1.0 0.0

0.5

1.0

Figure 5: Mean average error (log scale) of AUC (top) and AP (bottom) for several dataset sizes (n = m).
The solid lines show the DP error due to the perturbation required for (ǫ, δ)-diﬀerential privacy, while the
horizontal dashed lines show the estimation error. The δ = 0 plots use Cauchy noise to guarantee ǫ-diﬀerential
privacy and the other plots use Laplace noise.

to [0, 1], the mode of the AP distribution for n = 100 is 1
instead of the correct AP of about 0.75.

5. SYMMETRIC BINORMAL CURVES

Extending the work on AUC to ROC curves is challenging.
One possibility is to add noise directly to each of the points
that make up the curve. However, this method requires the
privacy budget to be split among all points in the curve.
The approach was used in [17] to generate diﬀerentially pri-
vate ROC curves, ﬁrst selecting a number of interpolation
points followed by adding noise to each of the points. With
a diﬀerentially private AUC from Section 3.2 and a method
to map from AUC back to an ROC curve, we can produce
diﬀerentially private ROC curves. Unfortunately, numerous
ROC curves have the same area, so we need a way to choose
among the ROC curves with a speciﬁed area. One way of
specifying unique curves for each value of AUC is to use a
symmetric binormal ROC curve [14], a 1-parameter ROC
curve estimator.

Binormal ROC curves assume the scores for negative and
positive examples are drawn from two normal distributions.
The theoretical ROC curve is y = Φ(a+bΦ−1(x)), where Φ is
the cumulative distribution functions of the normal distribu-
tion, a = µY −µX
. To remove the second degree
of freedom, we can set b = 1. The output curves will be sym-
metric binormal curves (symmetric over the line y = 1 − x),
which assumes the standard deviation of the positive and
negative examples is the same. Setting a = √2Φ−1(AUC)
produces the desired curve.

, and b = σX
σY

σY

Figure 6 shows diﬀerentially private ROC curves created
in this manner as well as the true curves being approximated.
The far right subplot is for the adult dataset and the others
are for data simulated from various binormal distributions.
The simulations all use X ∼ N (0, 1) and correspond to the
assumed symmetric form in the far left subplot when Y ∼
N (1, 1) and two diﬀerent non-symmetric misspeciﬁcations
in the other two plots. If a classiﬁer’s predictions (or any
monotonic transformation) are modeled reasonably by two

20Y ~ N(1,1)

Y ~ N(1,0.5)

Y ~ N(1,1.5)

Adult

e
t
a
r
 
e
v
i
t
i
s
o
p
 
e
u
r
t

1.0

0.5

0.0

True

2.5%

50%

97.5%

0.0

0.5

1.0 0.0

0.5

1.0 0.0

0.5

1.0 0.0

0.5

1.0

false positive rate

Figure 6: (ǫ, δ)-diﬀerentially private ROC curves generated using symmetric binormal method. The true ROC
curve with n = m = 1000 and empirical 2.5%, 50% (median) and 97.5% quantiles of the diﬀerentially private
curves are shown. ǫ = 0.1 and δ = 0.01.

normal distributions with the same standard deviation, the
symmetric ROC curves are a good approximation, as in the
adult dataset. Compared to [17], this method produces
smoother curves when the assumptions are met, but is not
ﬂexible to diﬀering variances among the two classes.

6. CONCLUSION

Diﬀerentially private models allow organizations with sen-
sitive data to provide guarantees about the eﬀect of model
release on the privacy of database entries. But for these
models to be eﬀectively evaluated, they must be run on new
data, which may have similar privacy concerns. We pre-
sented methods for providing the same diﬀerential privacy
guarantees for model evaluation, irrespective of the train-
ing setting. We provided high-utility mechanisms for AUC
and AP. Future work includes creating mechanisms for other
evaluation methods, general ROC curves, and investigating
the eﬀect of cross-validation. We hope the discussion of dif-
ferential privacy for model evaluation motivates future work
to enable diﬀerential privacy to be applied more broadly
throughout machine learning.

7. REFERENCES

[1] K. Bache and M. Lichman. UCI machine learning

repository, 2013.

[2] K. Boyd, V. S. Costa, J. Davis, and D. Page.

Unachievable region in precision-recall space and its
eﬀect on empirical evaluation. In ICML, pages
639–646, 2012.

[3] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate.

Diﬀerentially private empirical risk minimization. The
Journal of Machine Learning Research, 12:1069–1109,
2011.

[4] K. Chaudhuri and S. A. Vinterbo. A stability-based

validation procedure for diﬀerentially private machine
learning. In NIPS, pages 2652–2660, 2013.

[5] C. Dwork. Diﬀerential privacy. In ICALP. Springer,

2006.

[6] C. Dwork, F. McSherry, K. Nissim, and A. Smith.

Calibrating noise to sensitivity in private data
analysis. In Theory of Cryptography, pages 265–284.
Springer, 2006.

[7] A. Friedman and A. Schuster. Data mining with

diﬀerential privacy. In KDD, pages 493–502. ACM,
2010.

[8] A. Ghosh, T. Roughgarden, and M. Sundararajan.

Universally utility-maximizing privacy mechanisms. In
STOC, 2009.

[9] R. Hall, A. Rinaldo, and L. Wasserman. Diﬀerential

privacy for functions and functional data. The Journal
of Machine Learning Research, 14(1):703–727, 2013.

[10] D. Kifer and A. Machanavajjhala. Puﬀerﬁsh: A

framework for mathematical privacy deﬁnitions. ACM
Trans. Database Syst., 39(1):3:1–3:36, Jan. 2014.

[11] C. D. Manning, P. Raghavan, and H. Sch¨utze.

Introduction to Information Retrieval. Cambridge
University Press, New York, NY, USA, 2008.

[12] G. J. Matthews and O. Harel. An examination of data

conﬁdentiality and disclosure issues related to
publication of empirical {ROC} curves. Academic
Radiology, 20(7):889 – 896, 2013.

[13] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth
sensitivity and sampling in private data analysis. In
STOC, page 75. ACM Press, 2007.

[14] M. S. Pepe. The statistical evaluation of medical tests

for classiﬁcation and prediction. Oxford University
Press, USA, 2004.

[15] F. J. Provost, T. Fawcett, and R. Kohavi. The case
against accuracy estimation for comparing induction
algorithms. In ICML, volume 98, pages 445–453, 1998.

[16] B. I. Rubinstein, P. L. Bartlett, L. Huang, and

N. Taft. Learning in a large function space:
Privacy-preserving mechanisms for svm learning.
preprint arXiv:0911.5708, 2009.

[17] B. Stoddard, Y. Chen, and A. Machanavajjhala.

Diﬀerentially private algorithms for empirical machine
learning. preprint arXiv:1411.5428, 2014.

[18] L. Wasserman and S. Zhou. A statistical framework

for diﬀerential privacy. Journal of the American
Statistical Association, 105(489):375–389, 2010.

[19] J. Zhang, Z. Zhang, X. Xiao, Y. Yang, and

M. Winslett. Functional mechanism: regression
analysis under diﬀerential privacy. VLDB,
5(11):1364–1375, 2012.

21APPENDIX

B. PROOF OF THEOREM 4

A. PROOF OF THEOREM 3

Proof. Let D and D′ be two neighboring databases that
diﬀer by exactly one row. Let n and m be the number of
positive and negative examples in D, respectively.

We consider the four cases of moving a negative in the
ranking, moving a positive, changing a positive to negative
(and moving), and changing a negative to a positive. Our
analysis of these four cases requires n > 0 and m > 0, so
for completeness we say the local sensitivity of AUC is 1 if
either n = 0 or m = 0.

Case 1) Move negative: D′ has the same xi and yj as D
except for some xk that is now x∗ in D′. The only changes in
Eq. (1) occur when xk is compared in the indicator functions.
xk appears n times and each time the indicator function can
nm = 1
change by at most 1, so in this case sensitivity is n
m .
Case 2) Move positive: Similar to Case 1, D′ is the same
as D except for some yk that changes to y∗. This yk appears
in Eq. (1) m times so the sensitivity is m

Case 3) Change negative to positive: Here, D′ has n + 1
positive and m − 1 negative examples (assume m > 2) with
the same xi and yj except for some xk that has been removed
and a new positive example with score y∗ has been added.
Without loss of generality, assume that k = m. Using C
to collect the unchanged terms, and noting that 0 ≤ C ≤
(m − 1)n, we have
1

n

nm = 1
n .

AUC(D) =

(C +

I[xm < yj])

(12)

nm

Xj=1

1

(n + 1)(m − 1)

(C +

Xi=1
m − n − 1

C

nm(n + 1)(m − 1)
I[xm < yj]
+

1

n

nm

Xj=1

AUC(D) − AUC(D′) =

AUC(D′) =

m−1

I[xi < y∗]).

(13)

AP(D) =

Proof. Let nx and ny be the number of positive exam-
ples in databases x and y, respectively, and similarly mx
and my be the number of negatives. The smallest row dif-
ference between x and y occurs if we just need to change
the positive or negative labels on the minimal number of
examples to ensure the ni and mi counts are correct, hence
d(x, y) ≥ |nx − ny|. Starting from Deﬁnition 2.2 of Nissim
et al. [13], we have,

S∗
AUC,β = max

y∈Dn+m

LSAUC(ny, my)e−β|nx−ny |
LSAUC(i, n + m − i)e−β|nx−i|

(18)

(19)

= max

0≤i≤n+m

since there always exists some y for which d(x, y) = |nx −
ny|.
C. PROOF OF THEOREM 5

Proof. Let x1, x2, ..., xm and y1, y2, ..., yn be the classi-
ﬁer scores on the m negative and n positive examples for
a data set D. To bound the maximum change in AP be-
tween D and a neighboring database, we consider the four
cases of adding or removing a positive example and adding
or removing a negative example.

Case 1) Remove positive: Assume WLOG that y1 > y2 >
... > yn. Consider making D′ by removing a positive exam-
ple yz. Separating out the diﬀerent parts of the AP sum to
facilitate comparison between D and D′, we have

1

n "z−1
Xi=1

i

i + si

+

z

z + sz

+

n

Xi=z+1

i

i + si#

(20)

I[xj > yi]. Removing the yz example for

where si = Pm

D′ yields

j=1

AP(D′) =

1

n − 1 "z−1
Xi=1

i

i + si

+

n

Xi=z+1

i − 1 + si# .
i − 1

(21)

After renormalizing and simplifying,

AP(D) − AP(D′) =

1

−i
i + si

n(n − 1) "z−1
Xi=1
(n − 1)z
Xi=z+1
z + sz

+

n

+

n

+

Xi=z+1

nsi

(i + si)(i − 1 + si)#

−i
i + si

(22)

The two sums of −i
i+si
So we can add and subtract

z

z+sz

to get,

in Eq. (22) include all i’s except i = z.

AP(D) − AP(D′) =

n

1

+

z + sz

n(n − 1) " nz
Xi=z+1

−i
i + si
(i + si)(i − 1 + si)# .

Xi=1

nsi

+

n

(23)

The absolute value of ﬁrst two terms are maximized when
sz = 0 and si = 0, respectively. Both are bounded by 1
n−1 .
The third term is relaxed to 1
(i−1+si)2 . We need

i=z+1

si

n−1 Pn

−

1

(n + 1)(m − 1)

m−1

Xi=1

I[xi < y∗]

(14)

Eq. (14) is maximized when each of the three terms is max-
imized. The ﬁrst term is maximized when m > n and
C = (m − 1)n,

m − n − 1

nm(n + 1)(m − 1)

C ≤

m − n − 1
m(n + 1)

.

(15)

nm = 1
The second and third terms are bounded above by n
m
and 0, respectively. Putting it all together we have an upper
bound of
AUC(D)−AUC(D′) ≤
Similarly, the lower bound for Eq. (14) occurs when n > m
and is
AUC(D) − AUC(D′) ≥
Case 4) Change positive to negative: Symmetric with Case

m − n − 1
m(n + 1) −

m − n − 1
m(n + 1)

n
nm ≤

m
nm

= −

. (16)

n + 1

1
m

(17)

1
n

+

=

1

.

3.

22si

to maximize
(i−1+si)2 for each i where si is free to take any
(integer) value between 0 and m. This function is maximized
when si = i − 1, which is always a valid choice for si, and
gives an upper bound of

1

n − 1

n

Xi=z+1

i − 1

(i − 1 + i − 1)2 =

1

4(n − 1)

n

Xi=z+1

1
i − 1

.

(24)

Since all terms of the sum in Eq. (24) are positive (z ≥ 1,
so i ≥ 2), it is maximized when there are as many terms as
possible, i.e., when z = 1:

1

4(n − 1)

n

Xi=z+1

1

i − 1 ≤

1

4(n − 1)

1
j

n−1

Xj=1

=

Hn−1
4(n − 1)

.

(25)

where Hn−1 is the (n − 1)st harmonic number. Combining
the three terms to bound Eq. (23), we have

∆ =

(26)

2

n − 1

+

Hn−1
4(n − 1)

=

8 + Hn−1
4(n − 1)

Case 2) Add positive: Equivalent to Case 1, but if D has
n positive examples then D′ has n + 1, so the sensitivity is

∆ =

8 + Hn

4n

.

(27)

Case 3) Remove negative: Consider removing a negative

example xk.

AP(D) =

AP(D′) =

1
n

1
n

n

n

Xi=1
Xi=1

i

i + si

i

i + si + δi

(28)

(29)

where si = Pm

I[xj > yi] and δi = −I[xk > yi] is the
change in false positive counts between D and D′. The
diﬀerence in AP is

j=1

AP(D) − AP(D′) =

1
n

n

Xi=1

iδi

(i + si)(i + si + δi)

.

(30)

δi ∈ {0, −1}, so the absolute value of Eq. (30) is maximized
when δi = −1 and si = 1 ∀ i (si 6= 0 because there must be
an existing false positive to remove).

|AP(D) − AP(D′)| ≤

=

1
n

1
n

n

n

Xi=1
Xi=1

i

(i + 1)i

1

i + 1

=

Hn+1 − 1

n

(31)

(32)

Case 4) Add negative: If we add a negative example in-
stead of removing it, we again get to Eq. (30), but now
δi ∈ {0, 1} and the absolute value is maximized when δi = 1
and si = 0 ∀ i. Using these values also gives Eq. (32).

Putting the cases together,

∆AP = max(cid:18) Hn+1 − 1
+ max(cid:18) Hn+1 − 1

n

n

,

8 + Hn−1

4(n − 1) (cid:19)
4n (cid:19)

8 + Hn

,

(33)

is our tightest bound on the local sensitivity. Using the
bound Hn < 1 + log n gives Eq. (10).

23