The Million-Key Question—Investigating the 

Origins of RSA Public Keys

Petr Švenda, Matúš Nemec, Peter Sekan, Rudolf Kvašňovský, David Formánek,  

David Komárek, and Vashek Matyáš, Masaryk University

 https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/svenda

This paper is included in the Proceedings of the 25th USENIX Security SymposiumAugust 10–12, 2016 • Austin, TXISBN 978-1-931971-32-4Open access to the Proceedings of the 25th USENIX Security Symposium is sponsored by USENIX The Million-Key Question – Investigating the Origins of RSA Public Keys

Petr ˇSvenda, Mat´uˇs Nemec, Peter Sekan, Rudolf Kvaˇsˇnovsk´y,

David Form´anek, David Kom´arek and Vashek Maty´aˇs

Masaryk University, Czech Republic

Abstract
Can bits of an RSA public key leak information about
design and implementation choices such as the prime
generation algorithm? We analysed over 60 million
freshly generated key pairs from 22 open- and closed-
source libraries and from 16 different smartcards, reveal-
ing signiﬁcant leakage. The bias introduced by different
choices is sufﬁciently large to classify a probable library
or smartcard with high accuracy based only on the val-
ues of public keys. Such a classiﬁcation can be used to
decrease the anonymity set of users of anonymous mail-
ers or operators of linked Tor hidden services, to quickly
detect keys from the same vulnerable library or to ver-
ify a claim of use of secure hardware by a remote party.
The classiﬁcation of the key origins of more than 10 mil-
lion RSA-based IPv4 TLS keys and 1.4 million PGP keys
also provides an independent estimation of the libraries
that are most commonly used to generate the keys found
on the Internet.

Our broad inspection provides a sanity check and
deep insight regarding which of the recommendations for
RSA key pair generation are followed in practice, includ-
ing closed-source libraries and smartcards1.

1

Introduction

The RSA key pair generation process is a crucial part of
RSA algorithm usage, and there are many existing (and
sometimes conﬂicting) recommendations regarding how
to select suitable primes p and q [11, 13, 14, 17, 18] to
be later used to compute the private key and public mod-
ulus. Once these primes have been selected, modulus
computation is very simple: n = p · q, with the public
exponent usually ﬁxed to the value 65 537. But can the
modulus n itself leak information about the design and
implementation choices previously used to generate the

1Full details, paper supplementary material, datasets and author
contact information can be found at http://crcs.cz/papers/usenix2016.

primes p and q? Trivially, the length of the used primes
is directly observable. Interestingly, more subtle leakage
was also discovered by Mironov [20] for primes gener-
ated by the OpenSSL library, which unwantedly avoids
small factors of up to 17 863 from p−1 because of a cod-
ing omission. Such a property itself is not a security vul-
nerability (the key space is decreased only negligibly),
but it results in sufﬁciently signiﬁcant ﬁngerprinting of
all generated primes that OpenSSL can be identiﬁed as
their origin with high conﬁdence. Mironov used this ob-
servation to identify the sources of the primes of factor-
izable keys found by [12]. But can the origins of keys be
identiﬁed only from the modulus n, even when n cannot
be factorized and the values of the corresponding primes
are not known?

To answer this question, we generated a large number
of RSA key pairs from 22 software libraries (both open-
source and closed-source) and 16 different cryptographic
smartcards from 6 different manufacturers, exported both
the private and public components, and analysed the ob-
tained values in detail. As a result, we identiﬁed seven
design and implementation decisions that directly ﬁnger-
print not only the primes but also the resulting public
modulus: 1) Direct manipulation of the primes’ high-
est bits. 2) Use of a speciﬁc method to construct strong
or provable primes instead of randomly selected or uni-
formly generated primes. 3) Avoidance of small factors
in p−1 and q−1. 4) Requirement for moduli to be Blum
integers. 5) Restriction of the primes’ bit length. 6) Type
of action after candidate prime rejection. 7) Use of an-
other non-traditional algorithm – functionally unknown,
but statistically observable.

As different design and implementation choices are
made for different libraries and smartcards (cards) with
regard to these criteria, a cumulative ﬁngerprint is suf-
ﬁcient to identify a probable key origin even when only
the public key modulus is available. The average clas-
siﬁcation accuracy on the test set was greater than 73%
even for a single classiﬁed key modulus when a hit within

USENIX Association  

25th USENIX Security Symposium  893

the top 3 matches was accepted2. When more keys from
the same (unknown) source were classiﬁed together, the
analysis of as few as ten keys allowed the correct ori-
gin to be identiﬁed as the top single match in more than
85% of cases. When ﬁve keys from the same source were
available and a hit within the top 3 matches was accepted,
the classiﬁcation accuracy was over 97%.

We used the proposed probabilistic classiﬁer to clas-
sify RSA keys collected from the IPv4 HTTPS/TLS [9],
Certiﬁcate Transparency [10] and PGP [30] datasets and
achieved remarkably close match to the current market
share of web servers for TLS dataset.

The optimal and most secure way of generating RSA
key pairs is still under discussion. Our wide-scale analy-
sis also provides a sanity check concerning how closely
the various recommendations are followed in practice for
software libraries and smartcards and what the impact on
the resulting prime values is, even when this impact is not
observably manifested in the public key value. We iden-
tiﬁed multiple cases of unnecessarily decreased entropy
in the generated keys (although this was not exploitable
for practical factorization) and a generic implementation
error pattern leading to predictable keys in a small per-
centage (0.05%) of cases for one type of card.

Surprisingly little has been published regarding how
key pairs are generated on cryptographic cards. In the
case of open-source libraries such as OpenSSL, one
can inspect the source code. However, this option is
not available for cards, for which the documentation of
the generation algorithm is conﬁdential and neither the
source code nor the binary is available for review. To in-
spect these black-box implementations, we utilized the
side channels of time and power consumption (in addi-
tion to the exported raw key values). When this side-
channel information was combined with the available
knowledge and observed characteristics of open-source
libraries, the approximate key pair generation process
could also be established for these black-box implemen-
tations.

This paper is organized as follows: After a brief sum-
mary of the RSA cryptosystem, Section 2 describes the
methodology used in this study and the dataset of RSA
keys collected from software libraries and cryptographic
cards. Section 3 provides a discussion of the observed
properties of the generated keys. Section 4 describes the
modulus classiﬁcation method and its results on large
real-world key sets, the practical impact and mitigation
of which are discussed in Section 5. Additional analysis
performed for black-box implementations on cards and a
discussion of the practical impact of a faulty/biased ran-
dom number generator are presented in Section 6. Fi-
nally, conclusions are offered in Section 7.

2The correct library is listed within the ﬁrst three most probable

groups of distinct sources identiﬁed by the classiﬁcation algorithm.

2 RSA key pairs

To use the RSA algorithm, one must generate a key:

1. Select two distinct large primes3 p and q.
2. Compute n = p· q and ϕ(n) = (p− 1)(q− 1).
3. Choose a public exponent4 e < ϕ(n) that is coprime

to ϕ(n).

4. Compute the private exponent d as e−1 mod ϕ(n).

The pair (e,n) is the public key; either (d,n) serves as
the secret private key, or (p,q) can be used ((d,n) can be
calculated from (p,q,e) and vice versa).

2.1 Attacks against the RSA cryptosystem
The basic form of attack on the RSA cryptosystem is
modulus factorization, which is currently computation-
ally unfeasible or at least extremely difﬁcult if p and q
are sufﬁciently large (512 bits or more) and a general al-
gorithm such as the number ﬁeld sieve (NFS) or the older
quadratic sieve (MPQS) is used. However, special prop-
erties of the primes enable more efﬁcient factorization,
and measures may be taken in the key pair generation
process to attempt to prevent the use of such primes.

The primes used to generate the modulus should be
of approximately the same size because the factorization
time typically depends on the smallest factor. However,
if the primes are too close in value, then they will also
be close to the square root of n and Fermat factorization
can be used to factor n efﬁciently [16].

Pollard’s p − 1 method outperforms general algo-
rithms if for one of the primes p, p− 1 is B-smooth (all
factors are ≤ B) for some small B (which must usually be
guessed in advance). The modulus can be factored using
Williams’ p +1 method if p +1 has no large factors [27].
Despite the existence of many special-purpose algo-
rithms, the easiest way to factor a modulus created as
the product of two randomly generated primes is usually
to use the NFS algorithm. Nevertheless, using special
primes may potentially thwart such factorization attacks,
and some standards, such as ANSI X9.31 [28] and FIPS
186-4 [14], require the use of primes with certain prop-
erties (e.g., p− 1 and p + 1 must have at least one large
factor). Other special algorithms, such as Pollard’s rho
method and the Lenstra elliptic curve method, are im-
practical for factoring a product of two large primes.

Although RSA factorization is considered to be an NP-
hard problem if keys that fulﬁl the above conditions are
used, practical attacks, often relying on a faulty random

3Generated randomly, but possibly with certain required properties,

as we will see later.

4Usually with a low Hamming weight for faster encryption.

894  25th USENIX Security Symposium 

USENIX Association

Insufﬁcient entropy, pri-
generator, nevertheless exist.
marily in routers and embedded devices, leads to weak
and factorizable keys [12]. A faulty card random num-
ber generator has produced weak keys for Taiwanese cit-
izens [3], and supposedly secure cryptographic tokens
have been known to produce corrupted or signiﬁcantly
biased keys and random streams [6].

Implementation attacks can also compromise private
keys based on leakage in side channels of timing [8] or
power [15]. Active attacks based on fault induction [26]
or exploits aimed at message formatting [2, 5] enable the
recovery of private key values. We largely excluded these
classes of attacks from the scope of our analysis, focus-
ing only on key generation.

2.2 Analysis methodology
Our purpose was to verify whether the RSA key pairs
generated from software libraries and on cards provide
the desired quality and security with respect to the ex-
pectations of randomness and resilience to common at-
tacks. We attempted to identify the characteristics of the
generated keys and deduce the process responsible for
introducing them. The impact of the techniques used on
the properties of the produced public keys was also in-
vestigated. We used the following methodology:

1. Establish the characteristics of keys generated from
open-source cryptographic libraries with known im-
plementations.

2. Gather a large number of RSA key pairs from cryp-
tographic software libraries and cards (one million
from each).

3. Compare the keys originating from open-source li-
braries and black-box implementations and discuss
the causes of any observed similarities and differ-
ences (e.g., the distribution of the prime factors of
p− 1).
4. Analyse the generated keys using multiple statisti-
cal techniques (e.g., calculate the distribution of the
most signiﬁcant bytes of the primes).

Throughout this paper, we will use the term source (of
keys) when referring to both software libraries and cards.

2.3 Source code and literature
We examined the source codes of 19 open-source cryp-
tographic libraries variants5 and match it to the relevant
algorithms for primality testing, prime generation and

5We inspected multiple versions of libraries (though not all exhaus-
tively) to detect code changes relevant to the key generation process. If
such a change was detected, both versions were included in the analy-
sis.

RSA key generation from standards and literature. We
then examined how the different methods affected the
distributions of the primes and moduli. Summary results
together for all sources are available in Table 1.

2.3.1 Prime generation
Probable primes. Random numbers (or numbers from
a sequence) are tested for primality using probabilis-
tic primality (compositeness) tests. Different libraries
use different combinations of the Fermat, Miller-Rabin,
Solovay-Strassen and Lucas tests. None of the tests re-
jects prime numbers if implemented correctly; hence,
they do not affect
the distribution of the generated
primes. GNU Crypto uses a ﬂawed implementation of
the Miller-Rabin test. As a result, it permits only Blum
primes6. No other library generates such primes exclu-
sively (however, some cards do).

In the random sampling method, large integers (can-
didates) are generated until a prime is found.
If the
candidates are chosen uniformly, the distribution is not
biased (case of GNU Crypto 2.0.1, LibTomCrypt 1.17
and WolfSSL 3.9.0). An incremental search algorithm
selects a random candidate and then increments it un-
til a prime is found (Botan 1.11.29, Bouncy Castle 1.54,
Cryptix 20050328, cryptlib 3.4.3, Crypto++ 5.6.3, Flex-
iProvider 1.7p7, mbedTLS 2.2.1, SunRsaSign – Open-
JDK 1.8.0, OpenSSL 1.0.2g, and PGPSDK4). Primes
preceded by larger “gaps” will be selected with slightly
higher probability; however, this bias is not observable
from the distribution of the primes.

Large random integers are likely to have some small
prime divisors. Before time-consuming primality tests
are performed, compositeness can be revealed through
trial division with small primes or the computation of the
greatest common divisor (GCD) with a product of a few
hundred primes. In the case of incremental search, the
sieve of Eratosthenes or a table of remainders that is up-
dated when the candidate is incremented can be used. If
implemented correctly, these efﬁciency improvements do
not affect the distribution of the prime generator.

OpenSSL creates a table of remainders by dividing a
candidate by small primes. When a composite candi-
date is incremented, this table is efﬁciently updated using
only operations with small integers. Interestingly, candi-
dates for p for which p− 1 is divisible by a small prime
up to 17 863 (except 2) are also rejected. Such a com-
putational step is useful to speed up the search for a safe
prime; however, (p− 1)/2 is not required (as would be
for safe prime) to be prime by the library. This strange
behaviour was ﬁrst reported by Mironov [20] and can be
used to classify the source if the primes are known.

6A prime p is a Blum prime if p ≡ 3 (mod 4). When both p and q

are Blum primes, the modulus n is a Blum integer n ≡ 1 (mod 4).

USENIX Association  

25th USENIX Security Symposium  895

1
+
k
2

k
2

1
−
k
2

Maximal region
Mod. size
2k+2
2k+1
2k
2k−1
2k−2

q

k
2

Rejection sampling
Accept
Reject

q

1
−
k
2

k
2

2
−
k
2
+

1
−
k
2

1
−
k
2

Square regions

Arbitrary region

q

k
2

1
−
k
2

2k−1

2k

2k+1

2k−1

2k

2k−1

22k−1

2k

2k−1

2k

Figure 1: RSA key generation. The maximal region for the generated primes is deﬁned by the precise length of the
modulus and equal lengths of the primes. Such keys can be generated through rejection sampling. To avoid generating
short moduli (which must be discarded), alternative square regions may be used. Several implementations, such as
that of the NXP J2A080 card, generate primes from arbitrary combinations of square regions.

Provable primes. Primes are constructed recursively
from smaller primes, such that their primality can be de-
duced mathematically (using Pocklington’s theorem or
related facts). This process is randomized; hence, a dif-
ferent prime is obtained each time. An algorithm for
constructing provable primes was ﬁrst proposed by Mau-
rer [18] (used by Nettle 3.2). For each prime p, p − 1
must have a large factor (≥ √p for Maurer’s algorithm
or ≥ 3√p for an improved version thereof). Factors of
p + 1 are not affected.
Strong primes. A prime p is strong if both p− 1 and
p +1 have a large prime factor (used by libgcrypt 1.65 in
FIPS mode and by the OpenSSL 2.0.12 FIPS module).
We also refer to these primes as FIPS-compliant, as FIPS
186-4 requires such primes for 1024-bit keys (larger keys
may use probable primes). Differing deﬁnitions of strong
primes are given in the literature; often, the large factor
of p−1 itself (minus one) should also have a large prime
factor (PGPSDK4 in FIPS mode). Large random primes
are not “weak” by comparison, as their prime factors are
sufﬁciently large, with sufﬁcient probability, to be safe
from relevant attacks.

Strong primes are constructed from large prime fac-
tors. They can be generated uniformly (as in ANSI
X9.31, FIPS 186-4, and IEEE 1363-2000) or with a visi-
bly biased distribution (as in a version of Gordon’s algo-
rithm [11] used in PGPSDK4).

2.3.2 Key generation – prime pairs
The key size is the bit length of the modulus. Typically,
an algorithm generates keys of an exact bit length (the
only exception being PGPSDK4 in FIPS mode). The
primes are thus generated with a size equal to half of the
modulus length. These two measures deﬁne the maximal
region for RSA primes. The product of two k-bit primes
is either 2k or 2k − 1 bits long. There are two princi-

Rejection sampling.

pal methods of solving the problem of short (2k− 1)-bit
moduli, as illustrated in Figure 1.
In this method, pairs of k-bit
primes are generated until their product has the cor-
rect length. To produce an unbiased distribution, two
new primes should be generated each time (Cryptix
20050328, FlexiProvider 1.7p7, and mbedTLS 2.2.1). If
the greater prime is kept and only one new prime is gen-
erated, some bias can be observed in the resulting distri-
bution of RSA moduli (Bouncy Castle up to version 1.53
and SunRsaSign in OpenJDK 1.8.0). If the ﬁrst prime is
kept (without regard to its size) and the second prime is
re-generated, small moduli will be much more probable
than large values (GNU Crypto 2.0.1).

“Square” regions. This technique avoids the genera-
tion of moduli of incorrect length that must be discarded
by generating only larger primes such that their product
has the correct length. Typically, both primes are se-
lected from identical intervals. When the prime pairs are
plotted in two dimensions, this produces a square region.
The smallest k-bit numbers that produce a 2k-bit mod-

ulus are close to √2·2k−1. Because random numbers can

easily be uniformly generated from intervals bounded
by powers of two, the distribution must be additionally
transformed to ﬁt such an interval. We refer to prime

pairs generated from the interval (cid:31)√2· 2k−1,2k − 1(cid:30)

as being generated from the maximal square region
(Bouncy Castle since version 1.54, Crypto++ 5.6.3, and
the Microsoft cryptography providers used in Cryp-
toAPI, CNG and .NET). Crypto++ approximates this in-
terval by generating the most signiﬁcant byte of primes
from 182 to 255.

A more practical square region, which works well for
candidates generated uniformly from intervals bounded
by powers of two, is achieved by ﬁxing the two most sig-
niﬁcant bits of a candidate to 112 (Botan 1.11.29, cryptlib

896  25th USENIX Security Symposium 

USENIX Association

libgcrypt 1.6.5, LibTomCrypt 1.17, OpenSSL
3.4.3,
1.0.2g, PGPSDK4, and WolfSSL 3.9.0). Addition-
ally, the provable primes generated in Nettle 3.2 and
the strong primes generated in libgcrypt 1.6.5 (in FIPS
mode) and in the OpenSSL 2.0.12 FIPS module are pro-
duced from this region.

2.4 Analysis of black-box implementations
To obtain representative results of the key generation
procedures used in cards (for which we could not in-
spect the source codes), we investigated 16 different
types of cards from 6 different established card manufac-
turers (2×Gemalto, 6×NXP, 1×Inﬁneon, 3×Giesecke
& Devrient (G&D), 2×Feitian and 2×Oberthur) devel-
oped using the widely used JavaCard platform. The
key pair generation process itself is implemented at a
lower level, with JavaCard merely providing an inter-
face for calling relevant methods. For each type of
card (e.g., NXP J2D081),
three physical cards were
tested to detect any potential differences among physi-
cal cards of the same type (throughout the entire analy-
sis, no such difference was ever detected). Each card
was programmed with an application enabling the gen-
eration and export of an RSA key pair (using the
KeyPair.generateKey() method) and truly random
data (using the RandomData.generate() method).

We focused primarily on the analysis of RSA keys of
three different lengths – 512, 1024 and 2048 bits. Each
card was repeatedly asked to generate new RSA 512-bit
key pairs until one million key pairs had been generated
or the card stopped responding. The time required to
create these key pairs was measured, and both the pub-
lic (the modulus n and the exponent e) and private (the
primes p and q and the private exponent d) components
were exported from the card for subsequent analyses. No
card reset was performed between key pair generations.
In the ideal case, three times one million key pairs were
extracted for every card type. The same process was re-
peated for RSA key pairs with 1024-bit moduli but for
only 50 000 key pairs, as the key generation process takes
progressively longer for longer keys. The patterns ob-
served from the analyses performed on the 512-bit keys
was used to verify the key set with longer keys7.

Surprisingly, we found substantial differences in the
intervals from which primes were chosen. In some cases,
non-uniform distributions of the primes hinted that the
prime generation algorithms are also different to those
used in the software libraries. Several methods adopted
in software libraries, such as incremental search, seem
to be suitable even for limited-resource systems. This

7For example, one can quickly verify whether a smaller number of
factorized values of p− 1 from 1024-bit RSA keys ﬁt the distribution
extrapolated from 512-bit keys.

argument is supported by a patent application [21] by
G&D, one of the manufacturers of the examined cards.
All tested cards from this manufacturer produced Blum
integers, as described in the patent, and these integers
were distributed uniformly, as expected from the incre-
mental search method.

A duration of approximately 2-3 weeks was typically
required to generate one million key pairs from a single
card, and we used up to 20 card readers gathering keys in
parallel. Not all cards were able to generate all required
keys or random data, stopping with a non-speciﬁc error
(0x6F00) or becoming permanently non-responsive after
a certain period. In total, we gathered more than 30 mil-
lion card-generated RSA key pairs8. Power consumption
traces were captured for a small number of instances of
the key pair generation process.

In addition, 100 MB streams of truly random data were
extracted from each card for tests of statistical random-
ness. When a problem was detected (i.e., the data failed
one or more statistical tests), a 1 GB stream was gener-
ated for ﬁne-grained veriﬁcation tests.

3 Analysis of the generated RSA key pairs

The key pairs extracted from both the software libraries
and the cards were examined using a similar set of ana-
lytical techniques. The goal was to identify sources with
the same behaviour, investigate the impact on the public
key values and infer the probable key generation algo-
rithm used based on similarities and differences in the
observed properties.

3.1 Distributions of the primes
To visualize the regions from which pairs of primes were
chosen, we plotted the most signiﬁcant byte (MSB) of
each prime on a heat map. It is possible to observe the in-
tervals for prime generation, as discussed in Section 2.3.
Figure 2 shows a small subset of the observed non-
uniform distributions. Surprisingly, the MSB patterns
were signiﬁcantly different for the cards and the software
implementations. The patterns were identical among dif-
ferent physical cards of the same type and were also
shared between some (but not all) types of cards from the
same manufacturer (probably because of a shared code
base). We did not encounter any library that produced
outputs comparable to those of the ﬁrst two cards from
the examples shown in Figure 2. The third example could
be reproduced by generating primes alternately and uni-
formly from 14 different regions, each characterized by a
pattern in the top four bits of the primes. By comparison,
it was rarer for a bias to be introduced by a library.

8The entire dataset is available for further research at [31].

USENIX Association  

25th USENIX Security Symposium  897

11111111
11111000
11110000
11101000
11100000
11011000
11010000
11001000
11000000
10111000
10110000
10101000
10100000
10011000
10010000
10001000
10000000

Q

11111111
11111000
11110000
11101000
11100000
11011000
11010000
11001000
11000000
10111000
10110000
10101000
10100000
10011000
10010000
10001000
10000000

Q

11111111
11111000
11110000
11101000
11100000
11011000
11010000
11001000
11000000
10111000
10110000
10101000
10100000
10011000
10010000
10001000
10000000

Q

11111111
11111000
11110000
11101000
11100000
11011000
11010000
11001000
11000000
10111000
10110000
10101000
10100000
10011000
10010000
10001000
10000000

Q

11111111
11111000
11110000
11101000
11100000
11011000
11010000
11001000
11000000
10111000
10110000
10101000
10100000
10011000
10010000
10001000
10000000

Q

11111111
11110100
11101000
11011100
11010000
11000100
10111000
10101100
10100000
10010100
10001000
01111100
01110000
01100100
01011000
01001100
01000000

Q

Card: Infineon JTOP 80K

Card: Gemalto GXP E64

Card: NXP J2A080

0
.
0

2
.
0

4
.
0

6
.
0

8
.
0

Density (%)

Pmin = 11000000
Pmax = 11001111
Qmin = 11000000
Qmax = 11001111
Nmin = 10010000
Nmax = 10101000
P = Q

0
0
.
0

2
0
.
0

4
0
.
0

6
0
.
0

8
0
.
0

0
1
.
0

Density (%)

Pmin = 10110101
Pmax = 11111111
Qmin = 10110101
Qmax = 11111111
Nmin = 10000100
Nmax = 11110101
P = Q

0
0
.
0

1
0
.
0

2
0
.
0

3
0
.
0

4
0
.
0

5
0
.
0

Density (%)

Pmin = 11000000
Pmax = 11111111
Qmin = 10010000
Qmax = 11101111
Nmin = 10000010
Nmax = 11101111
P = Q

0
0
0
0
0
0
0
1

0
0
0
1
0
0
0
1

0
0
0
0
1
0
0
1

0
0
0
1
1
0
0
1

0
0
0
0
0
1
0
1

0
0
0
1
0
1
0
1

0
0
0
0
1
1
0
1

0
0
0
1
1
1
0
1

0
0
0
0
0
0
1
1

0
0
0
1
0
0
1
1

0
0
0
0
1
0
1
1

0
0
0
1
1
0
1
1

0
0
0
0
0
1
1
1

0
0
0
1
0
1
1
1

0
0
0
0
1
1
1
1

0
0
0
1
1
1
1
1

1
1
1
1
1
1
1
1

P

0
0
0
0
0
0
0
1

0
0
0
1
0
0
0
1

0
0
0
0
1
0
0
1

0
0
0
1
1
0
0
1

0
0
0
0
0
1
0
1

0
0
0
1
0
1
0
1

0
0
0
0
1
1
0
1

0
0
0
1
1
1
0
1

0
0
0
0
0
0
1
1

0
0
0
1
0
0
1
1

0
0
0
0
1
0
1
1

0
0
0
1
1
0
1
1

0
0
0
0
0
1
1
1

0
0
0
1
0
1
1
1

0
0
0
0
1
1
1
1

0
0
0
1
1
1
1
1

1
1
1
1
1
1
1
1

P

0
0
0
0
0
0
0
1

0
0
0
1
0
0
0
1

0
0
0
0
1
0
0
1

0
0
0
1
1
0
0
1

0
0
0
0
0
1
0
1

0
0
0
1
0
1
0
1

0
0
0
0
1
1
0
1

0
0
0
1
1
1
0
1

0
0
0
0
0
0
1
1

0
0
0
1
0
0
1
1

0
0
0
0
1
0
1
1

0
0
0
1
1
0
1
1

0
0
0
0
0
1
1
1

0
0
0
1
0
1
1
1

0
0
0
0
1
1
1
1

0
0
0
1
1
1
1
1

1
1
1
1
1
1
1
1

P

Library: OpenSSL 1.0.2g

Library: Microsoft CryptoAPI

Library: PGP SDK 4 FIPS

0
0
.
0

1
0
.
0

2
0
.
0

3
0
.
0

4
0
.
0

5
0
.
0

Density (%)

Pmin = 11000000
Pmax = 11111111
Qmin = 11000000
Qmax = 11111111
Nmin = 10010000
Nmax = 11111111
P = Q

0
0
0
.
0

5
0
0
.
0

0
1
0
.
0

5
1
0
.
0

0
2
0
.
0

Density (%)

Pmin = 10110101
Pmax = 11111111
Qmin = 10110101
Qmax = 11111111
Nmin = 10000000
Nmax = 11111111
P = Q

0
0
.
0

1
0
.
0

2
0
.
0

3
0
.
0

4
0
.
0

5
0
.
0

Density (%)

Pmin = 01100000
Pmax = 11101111
Qmin = 01100100
Qmax = 11111111
Nmin = 00100110
Nmax = 11101000
P = Q

0
0
0
0
0
0
0
1

0
0
0
1
0
0
0
1

0
0
0
0
1
0
0
1

0
0
0
1
1
0
0
1

0
0
0
0
0
1
0
1

0
0
0
1
0
1
0
1

0
0
0
0
1
1
0
1

0
0
0
1
1
1
0
1

0
0
0
0
0
0
1
1

0
0
0
1
0
0
1
1

0
0
0
0
1
0
1
1

0
0
0
1
1
0
1
1

0
0
0
0
0
1
1
1

0
0
0
1
0
1
1
1

0
0
0
0
1
1
1
1

0
0
0
1
1
1
1
1

1
1
1
1
1
1
1
1

P

0
0
0
0
0
0
0
1

0
0
0
1
0
0
0
1

0
0
0
0
1
0
0
1

0
0
0
1
1
0
0
1

0
0
0
0
0
1
0
1

0
0
0
1
0
1
0
1

0
0
0
0
1
1
0
1

0
0
0
1
1
1
0
1

0
0
0
0
0
0
1
1

0
0
0
1
0
0
1
1

0
0
0
0
1
0
1
1

0
0
0
1
1
0
1
1

0
0
0
0
0
1
1
1

0
0
0
1
0
1
1
1

0
0
0
0
1
1
1
1

0
0
0
1
1
1
1
1

1
1
1
1
1
1
1
1

P

P

0
0
0
0
0
0
1
0

0
0
1
1
0
0
1
0

0
0
0
1
1
0
1
0

0
0
1
0
0
1
1
0

0
0
0
0
1
1
1
0

0
0
1
1
1
1
1
0

0
0
0
1
0
0
0
1

0
0
1
0
1
0
0
1

0
0
0
0
0
1
0
1

0
0
1
1
0
1
0
1

0
0
0
1
1
1
0
1

0
0
1
0
0
0
1
1

0
0
0
0
1
0
1
1

0
0
1
1
1
0
1
1

0
0
0
1
0
1
1
1

0
0
1
0
1
1
1
1

1
1
1
1
1
1
1
1

Figure 2: Example distributions of the most signiﬁcant byte of the prime p and the corresponding prime q from 8
million RSA key pairs generated by three software libraries and three types of cards. The histograms on the top and
the side of the graph represent the marginal distributions of p and q, respectively. The colour scheme expresses the
likelihood that primes of a randomly generated key will have speciﬁc high order bytes, ranging from white (not likely)
over orange to red (more likely). For distributions of all sources from the dataset, see our technical report [25].

The relation between the values of p and q reveals
additional conditions placed on the primes, such as a
minimal size of the difference p − q (PGPSDK4, NXP
J2D081, and NXP J2E145G).
It is possible to verify whether small factors of p− 1
are being avoided (e.g., OpenSSL or NXP J2D081) or
whether the primes generally do not exhibit same distri-
bution as randomly generated numbers (Inﬁneon JTOP
80K) by computing the distributions of the primes, mod-
ulo small primes. It follows from Dirichlet’s theorem that
the remainders should be distributed uniformly among
the φ (n) congruence classes in Z∗n [19, Fact 4.2].

The patterns observed for the 512-bit keys were found
to be identical to those for the stronger keys of 1024 and
2048 bits. For the software implementations, we checked
the source codes to conﬁrm that there were no differ-
ences in the algorithms used to generate keys of different
lengths. For the cards, we assume the same and gen-

eralize the results obtained for 512-bit RSA keys to the
longer (and much more common) keys.

3.2 Distributions of the moduli
The MSB of a modulus is directly dependent on the
MSBs of the corresponding primes p and q. As seen in
Figure 3, if an observable pattern exists in the distribu-
tions of the MSBs of primes p and q, a noticeable pattern
also appears in the MSB of the modulus. The preserva-
tion of shared patterns was observed for all tested types
of software libraries and cards. The algorithm used for
prime pair selection can often be observed from the dis-
tribution of the moduli. If a source uses an atypical al-
gorithm, it is possible to detect it with greater precision,
even if we do not know the actual method used.

Non-randomness with respect to small factors of p−1
can also be observed from the modulus, especially for

898  25th USENIX Security Symposium 

USENIX Association

Figure 3: The visible preservation of the MSB distribu-
tions of the primes p and q in the MSB distribution of the
modulus n = p· q. This example is from the NXP J2A081
card.

small divisors. Whereas random primes are equiproba-
bly congruent to 1 and 2 modulo 3, OpenSSL primes are
always congruent to 2. As a result, an OpenSSL mod-
ulus is always congruent to 1 modulo 3. This property
is progressively more difﬁcult to detect for larger prime
divisors. The moduli are more probably congruent to 1
modulo all small primes, which are avoided from p− 1
by OpenSSL. However, the bias is barely noticeable for
prime factors of 19 and more, even in an analysis of a
million keys. OpenSSL primes are congruent to 1 mod-
ulo 5 with probability 1/3 (as opposed to 1/4 for random
primes), to 1 modulo 7 with probability 1/5 (as opposed
to 1/6), and to 1 modulo 11 with probability 1/9 (as op-
posed to 1/10). For the practical classiﬁcation of only a
few keys (see Section 4), we use only the remainder of
division by 3.

The use of Blum integers can also be detected from
the moduli with a high precision, as random moduli are
equiprobably congruent to 1 and 3 modulo 4, whereas
Blum integers are always congruent to 1 modulo 4. The
probability that k random moduli will be Blum integers
is 2−k.

Neither libraries nor cards attempt to achieve a uni-
form distribution of moduli. Existing algorithms [13, 17]
have the disadvantage that sometimes a prime will be one
bit larger than half of the modulus length. All sources
sacriﬁce uniformity in the most signiﬁcant bits of the
modulus to beneﬁt from more efﬁcient methods of prime
and key generation.

We veriﬁed that the distribution of the other bytes of
the moduli is otherwise uniform. The second least signif-
icant bit is biased in the case of Blum integers. Sources
that use the same algorithm are not mutually distinguish-
able from the distributions of their moduli.

3.3 Factorization of p− 1 and p + 1
It is possible to verify whether strong primes are be-
ing used. Most algorithms generate strong primes from
uniform distributions (ANSI X9.31, FIPS 186-4, IEEE
1363, OpenSSL FIPS, libgcrypt FIPS, Microsoft and
Gemalto GCX4 72K), matching the distribution of ran-

dom primes, although PGPSDK4 FIPS produces a highly
atypical distribution of primes and moduli, such that this
source can be detected even from public keys. Hence,
we were obliged to search for the sizes of the prime fac-
tors of p− 1 and p + 1 directly9 by factoring them using
the YAFU software package [29]. We then extended the
results obtained for 512-bit keys to the primes of 1024-
bit key pairs (though based on fewer factorized values
because of the longer factorization time). Finally, we
extrapolated the results to keys of 2048 bits and longer
based on the known patterns for shorter keys.

As conﬁrmed by the source code, large factors of p±1
generated in OpenSSL FIPS and by libgcrypt in FIPS
this value is hardcoded.
mode always have 101 bits;
PGPSDK4 in FIPS mode also generates prime factors of
ﬁxed length; however, their size depends on the size of
the prime.

Additionally, we detected strong primes in some of our
black-box sources. Gemalto GCX4 72K generates strong
primes uniformly, but the large prime factors always have
101 bits. The strong primes of Gemalto GXP E64, which
have 112 bits, are not drawn from a uniform distribution.
The libraries that use Microsoft cryptography providers
(CryptoAPI, CNG, and .NET) produce prime factors of
randomized length, ranging from 101 bits to 120 bits, as
required by ANSI X9.31.

For large primes, p± 1 has a large prime factor with
high probability. A random integer p will not have a fac-
tor larger than p1/u with a probability of approximately
u−u [19]. Approximately 10% of 256-bit primes do not
have factors larger than 100 bits, but 512-bit keys are
not widely used. For 512-bit primes, the probability is
less than 0.05%. Therefore, the requirement of a large
factor does not seem to considerably decrease the num-
ber of possible primes. However, many sources construct
strong primes with factors of exact length (e.g., 101 bits).
Using the approximation of the prime-counting function
π(n) ≈ n
ln(n) [19], we estimate that the interval required
by ANSI X9.31 (prime factors from 101 to 120 bits) con-
tains approximately 220 times more primes than the num-
ber of 101-bit primes. Hence, there is a loss of entropy
when strong primes are generated in this way, although
we are not aware of an attack that would exploit this fact.
For every choice of an auxiliary prime, 293 possible val-
ues are considered instead of 2113, which implies the loss
of almost 20 bits of entropy. If the primes are to be (p−,
p+)-safe, then 2 auxiliary primes must be generated. Be-
cause we require two primes p and q for every RSA key,
we double the estimated loss of entropy compared with
ANSI-compliant keys to 80 bits for 1024-bit keys.

When p− 1 is guaranteed to have a large prime factor
but p + 1 is not, the source is most likely using provable
9By p− 1, we always refer to both p− 1 and q− 1, as we found no

relevant difference between p and q in the factorization results.

USENIX Association  

25th USENIX Security Symposium  899

P128255Q128255128255Na) Random prime p

Bit lengths of the largest prime factors of p−1
c) No factors 3 to 17863

b) No factors 3 to 251

d) Some factors less probable

)
h

t

g
n
e

l
 
t
i

b
(
 
r
o

t
c
a

f
 

e
m

i
r
p

 
t
s
e
g
r
a

l
 

d
n
o
c
e
s
 

e
h
T

0
2
1

0
8

0
4

0

0
2
1

0
8

0
4

0

100

50
250
e) At least one 101−bit factor

150

200

0
2
1

0
8

0
4

0

0
2
1

0
8

0
4

0

100

50
250
f) At least one 112−bit factor

150

200

0
2
1

0
8

0
4

0

0
2
1

0
8

0
4

0

50

100

150

200

250

50

100

200

150
The largest prime factor (bit length)

250

100

50

150

0
2
1

0
8

0
4

0

100

50
250
g) At least one 129−bit factor

150

200

100

50
250
h) 101 to 120−bit prime factors

150

200

0
2
1

0
8

0
4

0

200

250

50

100

150

200

250

Figure 4: Scatter graphs with all combinations of two biggest factors of p− 1 for 512-bit RSA. The tested sources
fall into following categories: a) Botan 1.11.29, Bouncy Castle 1.53 & 1.54, Cryptix JCE 20050328, cryptlib 3.4.3,
Crypto++ 5.6.3, FlexiProvider 1.7p7, GNU Crypto 2.0.1, (GPG) libgcrypt 1.6.5, LibTomCrypt 1.17, mbedTLS 2.2.1,
PGPSDK4, SunRsaSign (OpenJDK 1.8), G&D SmartCafe 3.2, Feitian JavaCOS A22, Feitian JavaCOS A40, NXP
J2A080, NXP J2A081, NXP J3A081, NXP JCOP 41 V2.2.1, Oberthur Cosmo Dual 72K; b) NXP J2D081, NXP
J2E145G; c) OpenSSL 1.0.2g; d) Inﬁneon JTOP 80K; e) (GPG) libgcrypt 1.6.5 FIPS, OpenSSL FIPS 2.0.12, Gemalto
GCX4 72K; f) Gemalto GXP E64; g) Nettle 3.2; h) MS CNG, MS CryptoAPI, MS .NET.

primes, as in the case of the Nettle library. Techniques
for generating provable primes construct p using a large
prime factor of p−1 (at least √p for Maurer’s algorithm
or 3√p for an improved version thereof). The size of the
prime factors of p + 1 is not affected by Maurer’s algo-
rithm.

Factorization also completes the picture with regard
to the avoidance of small factors in p− 1. Sources that
avoid small factors in p− 1 achieve a smaller number of
factors on average (and therefore also a higher average
length of the largest factor). No small factors are present
in keys from NXP J2D081 and J2E145G (values from
3 to 251 are avoided), from OpenSSL (values from 3 to
17 863 are avoided) and from G&D Smartcafe 4.x and
G&D Smartcafe 6.0 (values 3 and 5 are avoided). Small
factors in p + 1 are not avoided by any source.

Concerning the distribution of factors, most of the
software libraries (14) and card types (8) yield distri-
butions comparable to that of randomly generated num-
bers of a given length (see Figure 4). The Inﬁneon JTOP
80K card produces signiﬁcantly more small factors than
usual (compared with both random numbers and other
sources). This decreases the probability of having a large
factor.

We estimated the percentage of 512-bit RSA keys
that are susceptible to Pollard p− 1 factorization within
280 operations. This percentage ranges from 0% (FIPS-
compliant sources) to 4.35% (Inﬁneon JCOP 80K), with

an average of 3.38%. Although the NFS algorithm
would still be faster in most cases of keys of 512 bits
and larger, we found a card-generated key (with a small
maximal factor of p− 1) that was factorized via Pollard
p− 1 method in 19 minutes, whereas the NFS algorithm
would require more than 2 000 CPU hours. Note that for
1024-bit keys, the probability of such a key being pro-
duced is negligible.

3.4 Sanity check
Based on the exported private and public components of
the generated RSA keys obtained from all sources, we
can summarize their basic properties as follows (see also
Table 1):

• All values p and q are primes and are not close

enough for Fermat factorization to be practical.

• All card-generated keys use a public exponent equal
to 0x10001 (65 537), and all software libraries ei-
ther use this value as the default or support a user-
supplied exponent.

• Most modulus values are of an exactly required
length (e.g., 1024 bits). The only exception is PG-
PSDK4 in FIPS mode, which also generates moduli
that are shorter than the speciﬁed length by one or
two bits.

900  25th USENIX Security Symposium 

USENIX Association

• Neither libraries nor cards ensure that p is a safe

prime (p = 2· q + 1, where q is also prime).

• Some sources construct strong primes according to
the stricter deﬁnition or at least comply with the
requirements deﬁned in the FIPS 186-4 and ANSI
X9.31 standards, such that p−1 and p+1 both have
a large prime factor. Other libraries are not FIPS-
compliant; however, keys of 1024 bits and larger
resist p− 1 and p + 1 attacks for practical values of
the smoothness bound.

• Some libraries (5) and most card types (12) order
the primes such that p > q, which seems to be a
convention for CRT RSA keys. PGPSDK4 (in both
regular and FIPS modes) and libgcrypt (used by
GnuPG) in both modes order the primes in the op-
posite manner, q > p. In some sources, the ordering
is a side effect of the primes having ﬁxed (and dif-
ferent) most signiﬁcant bits (e.g., 4 bits of p and
q are ﬁxed to 1111 and 1001, respectively, by all
G&D cards).

• All generated primes were unique for all libraries
and all types of cards except one (Oberthur Cosmo
Dual 72K).

• All G&D and NXP cards,

the Oberthur Cosmo
Dual 72K card and the GNU Crypto library gen-
erate Blum integers. As seen from a bug in the
implementation of the Miller-Rabin test in GNU
Crypto, a simpler version of the test sufﬁces for test-
ing Blum primes. However, we hypothesize that the
card manufacturers have a different motivation for
using such primes.

4 Key source detection

The distinct distributions of speciﬁc bits of primes and
moduli enable probabilistic estimation of the source li-
brary or card from which a given public RSA key was
generated. Intuitively, classiﬁcation works as follows: 1)
Bits of moduli known to carry bias are identiﬁed with
additional bits derived from the modulus value (a mask,
6 + 3 bits in our method). 2) The frequencies of all pos-
sible mask combinations (29) for a given source in the
learning set are computed. 3) For classiﬁcation of an un-
known public key, the bits selected by the mask are ex-
tracted as a particular value v. The source with the high-
est computed frequency of value v (step 2) is identiﬁed
as the most probable source. When more keys from the
same source are available (multiple values vi), a higher
classiﬁcation accuracy can be achieved through element-
wise multiplication of the probabilities of the individual
keys.

We ﬁrst describe the creation of a classiﬁcation ma-
trix and report the classiﬁcation success rate as evaluated
on our test set [31]. Later, classiﬁcation is applied to
three real-world datasets: the IPv4 HTTPS handshakes
set [9], Certiﬁcate Transparency set [10] and the PGP
key set [30].

4.1 The classiﬁcation process
The classiﬁcation process is reasonably straightforward.
For the full details of the algorithm, please refer to our
technical report [25].

1. All modulus bits identiﬁed through previous analy-
sis as non-uniform for at least one source are in-
cluded in a mask. We included the 2nd − 7th most
signiﬁcant bits inﬂuenced by the prime manipula-
tions described in Section 3.1, the second least sig-
niﬁcant bit (which is zero for sources that use Blum
integers), the result of the modulus modulo 3 (which
is inﬂuenced by the avoidance of factor 3) and the
overall modulus length (which indicates whether an
exact length is enforced).

2. A large number of keys (learning set) from known
generating sources are used to create a classiﬁcation
matrix. For every possible mask value (of which
there are 29 in our case) and every source, the rela-
tive frequency of the given mask value in the learn-
ing set for the given source is computed.

3. During the classiﬁcation phase for key K with mod-
ulus m, the value v obtained after the application of
mask to modulus m is extracted. The row (proba-
bility vector) of the classiﬁcation matrix that corre-
sponds to the value v contains, as its ith element, the
probability of K being produced by source i.

4. When a batch of multiple keys that are known to
have been produced by the same (unknown) source
is classiﬁed, the probability vectors for every key
obtained in step 3 are multiplied element-wise and
normalized to obtain the source probabilities pb for
the entire batch, and the source with the highest
probability is selected.

Note that the described algorithm cannot distinguish be-
tween sources with very similar characteristics, e.g., be-
tween the NXP J2D081 and NXP J2E145G cards, which
likely share the same implementation. For this reason, if
two sources have the same or very similar proﬁles, they
are placed in the same group. Figure 5 shows the cluster-
ing and (dis-)similarity of all sources considered in this
study. If the particular source of one or more key(s) is
missing from our analysis (relevant for the classiﬁcation

USENIX Association  

25th USENIX Security Symposium  901

p
u
o
r
g
n
o
i
t
a
c
ﬁ

i
s
s
a
l
C

XI
VIII
X
VIII
XI
X
VIII
II
XI
XI
XI
VIII
XI
V
XI
XI
IV
VIII
XI

X
X
X

XI
XI
XIII

I
I
XI
IX
XII
VII
VII
III
III
VII
VII
VI
XI

Version

1.11.29
1.53
1.54
20050328
3.4.3
5.6.3
1.7p7
2.0.1
1.6.5
1.6.5 FIPS mode
1.17
2.2.1
3.2
1.0.2g
2.0.12
PGP Desktop 10.0.1
FIPS mode

Source
Open-source libraries
Botan
Bouncy Castle
Bouncy Castle
Cryptix JCE
cryptlib
Crypto++
FlexiProvider
GNU Crypto
GPG Libgcrypt
GPG Libgcrypt
LibTomCrypt
mbedTLS
Nettle
OpenSSL
OpenSSL FIPS
PGP SDK 4.x
PGP SDK 4.x
SunRsaSign Provider OpenJDK 1.8
3.9.0
WolfSSL
Black-box implementations
Microsoft CNG
Windows 10
Microsoft CryptoAPI Windows 10
Microsoft .NET
Windows 10
Smartcards
Feitian JavaCOS A22
Feitian JavaCOS A40
G&D SmartCafe 3.2
G&D SmartCafe 4.x
G&D SmartCafe 6.0
Gemalto GCX4 72K
Gemalto GXP E64
Inﬁneon JTOP 80K
NXP J2A080
NXP J2A081
NXP J2D081
NXP J2E145G
NXP J3A081
NXP JCOP 41 V2.2.1
Oberthur Cosmo Dual 72K
Oberthur Cosmo 64

1
−
p

1
−
p

f
o

s
r
o
t
c
a
f

l
l
a
m
S

f
o
r
o
t
c
a
f

e
g
r
a
L

1
+
p

f
o
r
o
t
c
a
f

e
g
r
a
L

s
r
e
g
e
t
n

i

m
u
B

l

k
c
e
h
c

|
q
−
p
|

k
c
e
h
c

|
d
|

Notes

×

×

×

× (cid:31) × ×
× (cid:31) × × (cid:31) (cid:31) Rejection sampling is less biased
× (cid:31) × × (cid:31) (cid:31) Checks Hamming weight of the modulus
× (cid:31) × ×
× Rejection sampling is not biased
× (cid:31) × × (cid:31) (cid:31)
× 255 ≥ MSB of prime ≥ 182 = (cid:26)√2· 128(cid:24)
× (cid:31) × ×
×
× (cid:31) × ×
× Rejection sampling is not biased
×
(cid:31) (cid:31) × ×
× Rejection sampling is more biased
×
× (cid:31) × ×
× Used by GnuPG 2.0.30
×
× (cid:31) (cid:31) (cid:31) (cid:31)
× 101-bit prime factors of p± 1
× (cid:31) × ×
×
×
× (cid:31) × ×
× Rejection sampling is not biased
×
× (cid:31) (cid:31) ×
× Prime factor of p− 1 has (|n|/4 + 1) bits
×
× No prime factors 3 to 17 863 in p− 1
× × × ×
×
× (cid:31) (cid:31) (cid:31) (cid:31)
× 101-bit prime factors of p± 1
× (cid:31) × × (cid:31)
× p and q differ in their top 6 bits
× (cid:31) (cid:31) (cid:31) (cid:31)
× Prime factors of p± 1 have (|n|/4− 32) bits
× (cid:31) × ×
× Rejection sampling is less biased
×
× (cid:31) × ×
×
×

d
o
h
t
e
m
h
c
r
a
e
s

e
m
i
r
P

Incr.
Incr.
Incr.
Incr.
Incr.
Incr.
Incr.
Rand.
Incr.
FIPS
Rand.
Incr.
Maurer
Incr.
FIPS
Incr.
PGP
Incr.
Rand.

FIPS
FIPS
FIPS

n
o
i
t
c
e
l
e
s

r
i
a
p
e
m
i
r
P

112
RS
√2
RS
112
√2
RS
RS
112
112
112
RS
112
112
112
112
112
RS
112

√2
√2
√2

× (cid:31) (cid:31) (cid:31)
× (cid:31) (cid:31) (cid:31)
× (cid:31) (cid:31) (cid:31)

?
?
?

Incr./Rand.
Incr./Rand.
Incr./Rand.
Incr./Rand.
Incr./Rand.

FIPS
Gem.
Inf.

Incr./Rand.
Incr./Rand.
Incr./Rand.
Incr./Rand.
Incr./Rand.
Incr./Rand.

Incr.

Incr./Rand.

× (cid:31) × ×
× (cid:31) × ×

112
?
112
?
FX×9X (cid:31) (cid:31) × × (cid:31)*
FX×9X (cid:31) × × × (cid:31)*
FX×9X (cid:31) × × × (cid:31)*
× (cid:31) (cid:31) (cid:31)
112
?
× (cid:31) (cid:31) (cid:31)
?
Gem.
× (cid:31) × ×
?
Inf.
(cid:31) (cid:31) × ×
?
NXP
(cid:31) (cid:31) × ×
?
NXP
(cid:31) × × × (cid:31)
RS
(cid:31) × × × (cid:31)
RS
(cid:31) (cid:31) × ×
?
NXP
(cid:31) (cid:31) × ×
?
NXP
(cid:31) (cid:31) × ×
?
112
× (cid:31) ?
?
112
?

?
?
?

?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?

Prime factors of p± 1 have 101 to 120 bits
Prime factors of p± 1 have 101 to 120 bits
Prime factors of p± 1 have 101 to 120 bits

*Size of |p− q| guaranteed by prime intervals
No prime factors 3 and 5 in p− 1
No prime factors 3 and 5 in p− 1
101-bit prime factors of p± 1
112-bit prime factors of p± 1

No prime factors 3 to 251 in p− 1
No prime factors 3 to 251 in p− 1

512-bit keys not supported

Table 1: Comparison of cryptographic libraries and smartcards. The algorithms are explained in Section 2.3. Prime
search method: incremental search (Incr.); random sampling (Rand.); FIPS 186-4 Appendix B.3.6 or equivalent
algorithm for strong primes (FIPS); Maurer’s algorithm for provable primes (Maurer); PGP strong primes (PGP);
Gemalto strong primes (Gem.); Inﬁneon algorithm (Inf.); unknown prime generator with almost uniform distribution,
possibly incremental or random search (Incr./Rand.). Prime pair selection: practical square region (112); rejection
sampling (RS); maximal square region (√2); the primes p and q have a ﬁxed pattern in their top four bits, 11112 and
10012, respectively (FX×9X); Gemalto non-uniform strong primes (Gem.); Inﬁneon algorithm (Inf.); NXP regions –
14 distinct square regions characterized by patterns in the top four bits of p and q (NXP). Blum integers: the modulus
n is always a Blum integer n ≡ 1 (mod 4) ((cid:31)); the modulus is n ≡ 1 (mod 4) and n ≡ 3 (mod 4) with equal probability
(×). Small factors of p− 1: p− 1 contains small prime factors ((cid:31)); some prime factors are avoided in p− 1 (×).
Large factors of p− 1: p− 1 is guaranteed to have a large prime factor – provable and strong primes ((cid:31)); size of the
prime factors of p− 1 is random (×). Large factors of p + 1: similar as for p− 1, typically strong primes are ((cid:31));
random and provable primes are (×). |p− q| check: p and q differ somewhere in their top bits ((cid:31)); the property is
not guaranteed (×); the check may be performed, but the negative case occurs with a negligible probability (?). |d|
check: sufﬁcient bit length of the private exponent d is guaranteed ((cid:31)); not guaranteed (×); possibly guaranteed, but
not detectable (?).

902  25th USENIX Security Symposium 

USENIX Association

i

e
c
n
a
t
s
d
 
n
a
e
d

i
l

c
u
E

0
3

.

0

5
1

.

0

0
0

.

0

Similarity of analyzed sources (classification groups)

Group separation threshold

1
8
0
A
2
J
 
P
X
N

0
8
0
A
2
J
 
P
X
N

1
8
0
A
3
J
 
P
X
N

I

 

 

1
8
0
D
2
J
 
P
X
N

G
5
4
1
E
2
J
 
P
X
N

S
P
F
4
K
D
S
P
G
P

 

x
.

0

1

.

.

4

6

0

 

 

.

2

 

o

t

p
y
r

 

C
U
N
G

f

f

e
a
C

e
a
C

t
r
a
m
S
D
&
G

t
r
a
m
S
D
&
G

 

 

g
2

.

0

.

1

.

2

.

K
2
7

 
l

 

 

1

a
u
D
o
m
s
o
C

L
S
S
n
e
p
O

 
r
u
h

2
v
 

 

1
4
P
O
C
J
 
P
X
N

1

.

2

.

7
p
7

3
5

8

.

8
2
3
0
5
0
0
2
E
C
J
 
x
i
t

 

p
y
r

C

.

1

 

.

 

2
S
L
T
d
e
b
m

1

e

i

 
r
e
d
v
o
r
P
x
e
F

l
t
s
a
C
 
y
c
n
u
o
B

l

i

 

 

1
K
D
J
n
e
p
O
n
g
S
a
s
R
n
u
S

i

t
r
e
b
O
VIVIV

7
1

5

2

.

.

6

3

2
1

.

.

.

 

1

e

0

.

3

.

6

.

 

5
+
+
o

 

4
6
E
P
X
G
o

 

t
l

p
y
r

t

C

a
m
e
G

4
5

.

I

P
A
o

G
N
C

1

 

t

p
y
r

C

 
t
f

o
s
o
r
c
M

i

 
t
f

e

o
s
o
r
c
M

i

l
t
s
a
C
 
y
c
n
u
o
B

T
E
N

.
 
t
f

o
s
o
r
c
M

i

 

 

4
K
D
S
P
G
P

 

4
6

 

 

 

K
2
7
X
C
G
o

2
2
A
S
O
C
a
v
a
J
 

0
4
A
S
O
C
a
v
a
J
 

t
l

 

o
m
s
o
C

 
r
u
h

a
m
e
G

t
r
e
b
O

n
a

n
a

i
t
i

i
t
i

e
F

e
F

1

 
t

p
y
r

C
m
o
T
b
L

i

 
t

i

p
y
r
c
g
b
L
G
P
G

 

l
t
t

e
N

 

I

 

2
S
P
F
L
S
S
n
e
p
O

0

3

.

.

9

4

.

.

3

3

 

 

I

S
P
F
5

 

b

9
2

.

1
1

.

L
S
S

f
l

o
W

i
l
t

p
y
r
c

.

1

 

6

.

1

n
a

t

o
B

2

.

3

 

 

K
0
8
P
O
T
J
 

n
o
e
n

i
f

n

I

f

e
a
C

t
r
a
m
S
D
&
G

 

 
t

i

p
y
r
c
g
b
L
G
P
G

 

Group:

I

II

III

VII

VIII

IX

X

XI

XII

XIII

Figure 5: Clustering of all inspected sources based on the 9 bits of the mask. The separation line shows which sources
were put by us into the same classiﬁcation category. Finer separation is still possible (e.g., SunRsaSign vs mbedTLS),
but the number of the keys from same source needs to be high enough to distinguish these very similar sources.

Top 1 match

Top 2 match

Top 3 match

# keys in batch
Group I
Group II
Group III
Group IV
Group V
Group VI
Group VII
Group VIII
Group IX
Group X
Group XI
Group XII
Group XIII
Average

5

2

1

5

2

1

5

2

1

10

10

10

100

100

100
95.39% 98.42% 99.38% 99.75% 100.00% 98.41% 99.57% 99.92% 100.00% 100.00% 98.41% 99.84% 100.00% 100.00% 100.00%
17.75% 32.50% 58.00% 69.50% 98.00% 35.58% 60.88% 84.15% 93.80% 100.00% 42.85% 71.58% 91.45% 98.40% 100.00%
45.36% 72.28% 93.17% 98.55% 100.00% 54.34% 78.31% 95.23% 99.35% 100.00% 82.45% 94.59% 99.25% 99.90% 100.00%
90.14% 97.58% 99.80% 100.00% 100.00% 92.22% 98.14% 99.90% 100.00% 100.00% 94.42% 99.02% 100.00% 100.00% 100.00%
63.38% 81.04% 97.50% 99.60% 100.00% 84.14% 90.88% 99.25% 99.90% 100.00% 90.01% 96.62% 99.95% 100.00% 100.00%
54.68% 69.22% 88.45% 94.60% 100.00% 80.31% 89.70% 97.90% 99.80% 100.00% 90.40% 96.34% 99.55% 100.00% 100.00%
7.58% 31.69% 64.21% 82.35% 99.75% 32.67% 69.48% 95.33% 98.60% 100.00% 63.99% 88.70% 98.89% 99.70% 100.00%
15.65% 40.30% 68.46% 76.60% 85.20% 30.29% 52.81% 79.54% 92.38% 100.00% 39.32% 66.45% 90.34% 97.92% 100.00%
22.22% 45.12% 76.35% 83.00% 83.00% 54.57% 71.86% 85.25% 86.80% 88.00% 61.77% 81.96% 94.35% 95.00% 99.00%
0.63% 6.33% 27.42% 42.74% 69.60% 15.05% 43.84% 78.83% 84.62% 91.00% 41.46% 70.54% 96.78% 99.88% 100.00%
11.77% 28.40% 55.56% 65.28% 77.69% 29.94% 56.09% 86.43% 96.19% 100.00% 55.35% 78.48% 97.04% 99.77% 100.00%
60.36% 79.56% 97.20% 99.40% 100.00% 82.96% 93.58% 99.60% 99.90% 100.00% 94.48% 97.62% 99.75% 100.00% 100.00%
39.56% 70.32% 96.20% 99.70% 100.00% 84.52% 95.54% 99.85% 100.00% 100.00% 95.22% 99.00% 99.95% 100.00% 100.00%
40.34% 57.90% 78.59% 85.47% 93.33% 59.62% 76.98% 92.40% 96.26% 98.38% 73.09% 87.75% 97.48% 99.27% 99.92%

Table 2: The classiﬁcation success rate of 13 groups created from all 38 analyzed sources using test set with same prior
probability of sources (see Figure 5 for libraries and cards in particular group). Columns corresponds to different
number of keys (1, 2, 5, 10 and 100) classiﬁed together from same (unknown) source.

of real-world datasets), any such key will be misclassi-
ﬁed as belonging to a group with a similar mask proba-
bility vector.

Both the construction of the classiﬁcation matrix and
the actual classiﬁcation are then performed using these
groups instead of the original single sources. The ob-
served similarities split the examined sources into 13 dif-
ferent groups (labelled I to XIII and listed in Figure 5).
The resulting classiﬁcation matrix10 has dimensions of
13× 512.
10Because of its large size, the resulting matrix is available in our

technical report [25] and at http://crcs.cz/papers/usenix2016.

4.1.1 Evaluation of the classiﬁcation accuracy

To evaluate the classiﬁcation success of our method, we
randomly selected 10 000 keys from the collected dataset
(that were not used to construct the classiﬁcation matrix)
for every source, thereby endowing the test set with equal
prior probability for every source.

A single organization may use the same source library
to generate multiple keys for its web servers. The clas-
siﬁcation accuracy was therefore evaluated not only for
one key (step 3 of the algorithm) but also for ﬁve, ten
and one hundred keys (step 4) originating from the same
(unknown) source. We evaluated not only the ability to

USENIX Association  

25th USENIX Security Symposium  903

achieve the “best match” with the correct source group
but also the ability to identify the correct source group
within the top two and top three most probable matches
(top-n match).

As shown in Table 2, the average accuracy on the test
set of the most probable source group was over 40% for
single keys and improved to greater than 93% when we
used batches of 100 keys from the same source for classi-
ﬁcation. When 10 keys from the same source were clas-
siﬁed in a batch, the most probable classiﬁed group was
correct in more than 85% of cases and was almost always
(99%) included in the top three most probable sources.

A signiﬁcant variability in classiﬁcation success was
observed among the different groups. Groups I (G&D
cards) and IV (PGPSDK4 FIPS) could be correctly iden-
tiﬁed from even a single key because of their distinct dis-
tributions of possible mask values. By contrast, group X
(Microsoft providers) was frequently misclassiﬁed when
only a single key was used because of the wider range of
possible mask values, resulting in a lower probability of
each individual mask value.

We conclude that our classiﬁcation method is moder-
ately successful even for a single key and very accurate
when a batch of at least 10 keys from the same source is
classiﬁed simultaneously.

Further leakage in other bits of public moduli might
be found by applying machine learning methods to the
learning set, potentially leading to an improvement of the
classiﬁcation accuracy. Moreover, although we have al-
ready tested a wide range of software libraries and cards,
more sources could also be incorporated, such as addi-
tional commercial libraries, various hardware security
modules and additional types of cards and security to-
kens.

4.2 Classifying real-world keys
One can attempt to classify keys from suitable public
datasets using the described method. However, the clas-
siﬁcation of keys observed in the real world may differ
from the classiﬁcation scenario evaluated above in two
respects:

1. The prior probabilities of real-world sources can
differ signiﬁcantly (e.g., OpenSSL is a more prob-
able source for TLS keys than is any card), and the
resulting posterior probabilities from the classiﬁca-
tion matrix will then also be different.

2. Our classiﬁcation matrix does not include all ex-
isting sources (e.g., we have not tested high-speed
hardware security modules), and such sources will
therefore always be misclassiﬁed.

The classiﬁcation success rate can be signiﬁcantly im-
proved if the prior distribution of possible sources can be

estimated. Such an estimate can be performed based on
meta information such as statistics concerning the popu-
larity of various software libraries or sales ﬁgures for a
particular card model. Note that the prior distributions
may also signiﬁcantly differ for different application ar-
eas, e.g., PGP keys are generated by a narrower set of
libraries and devices than are TLS keys. In this work, we
did not perform any prior probability estimations.

4.2.1 Sources of Internet TLS keys

We used IPv4 HTTPS handshakes collected from the
Internet-Wide Scan Data Repository [9] as our source of
real-world TLS keys. The complete set contains approxi-
mately 50 million handshakes; the relevant subset, which
consists of handshakes using RSA keys with a public
exponent of 65 537, contains 33.5M handshakes. This
set reduces to 10.7M unique keys based on the modu-
lus values. The keys in this set can be further divided
into batches with the same subject and issue date (as ex-
tracted from their certiﬁcates), where the same under-
lying library is assumed to be responsible for the gen-
eration of all keys in a given batch. As the classiﬁca-
tion accuracy improves with the inclusion of more keys
in a batch, we obtained classiﬁcation results separately
for batches consisting of a single key only (users with a
single HTTPS server), 2-9 keys, 10-99 keys (users with
a moderate number of servers) and 100 and more keys
(users with a large number of servers).

Intuitively, batches with 100+ keys will yield very ac-
curate classiﬁcation results but will capture only the be-
haviour of users with a large number of HTTPS servers.
Conversely, batches consisting of only a single key will
result in low accuracy but can capture the behaviours of
different types of users.

The frequency of a given source in a dataset (for a
particular range of batch sizes) is computed as follows:
1) The classiﬁcation probability vector pb for a given
batch is computed according to the algorithm from Sec-
tion 4.1. 2) The element-wise sum of pb · nb over all
batches b (weighted by the actual number of keys nb in
the given batch) is computed and normalized to obtain
the relative proportion vector, which can be found as a
row in Table 3.

As shown in Section 4.1.1, a batch of 10 keys origi-
nating from the same source should provide an average
classiﬁcation accuracy of greater than 85% – sufﬁciently
high to enable reasonable conclusions to be drawn re-
garding the observed distribution. Using batches of 10-
99 keys, the highest proportion of keys generated for
TLS IPv4 (82%) were classiﬁed as belonging to group
V, which contains a single library – OpenSSL. This pro-
portion increased to almost 90% for batches with 100+
keys. The second largest proportion of these keys (ap-

904  25th USENIX Security Symposium 

USENIX Association

Group of sources

Dataset (size of included batches)

#keys

I

II

III

IV

V

VI

VII

VIII

IX

X

XI

XII

XIII

Multiple keys classiﬁed in single batch, likely accurate results (see discussion in Section 4.1.1)

TLS IPv4 (10-99 keys) [9]
TLS IPv4 (100+ keys) [9]
Cert.Transparency (10-99 keys) [10]
PGP keyset (10-99 keys) [30]

518K
973K
23K
1.7K

-
-
-
-

0.00%

-

0.00%

-

-
-
-
-

0.01% 82.84%
0.01% 89.92%
0.07% 26.14%
6.87% 11.95%

-
-
-
-

-
-
-
-

1.09% 0.28% 10.18% 5.61%
4.68% 0.00% 3.46% 1.93%
6.90% 2.79% 47.70% 16.41%
36.11% 2.09% 5.73% 37.25%

-
-
-
-

-
-
-
-

Classiﬁcation based on batches with 2-9 keys only, likely lower accuracy results

TLS IPv4 (2-9 keys) [9]
Cert. Transparency (2-9 keys) [10]
PGP keyset (2-9 keys) [30]

237K 0.02% 0.79% 2.06% 0.11% 54.14% 3.26% 1.73% 7.03% 7.98% 11.34% 11.17% 0.36% 0.05%
794K 0.03% 1.12% 3.21% 0.14% 43.89% 5.03% 2.64% 6.59% 10.52% 12.10% 14.18% 0.49% 0.06%
83K 0.02% 1.47% 1.40% 2.07% 14.36% 7.90% 3.91% 7.74% 16.10% 18.80% 25.86% 0.35% 0.03%

Classiﬁcation based on single key only, likely low accuracy results

TLS IPv4 (1 key) [9]
Cert. Transparency (1 key) [10]
PGP keyset (1 key) [30]

8.8M 0.98% 4.02% 6.47% 1.94% 21.01% 8.63% 6.13% 8.65% 12.22% 11.95% 13.48% 3.49% 1.03%
12.7M 0.88% 3.75% 6.90% 1.49% 23.10% 8.69% 6.04% 7.99% 12.08% 11.78% 13.50% 3.04% 0.77%
1.35M 0.44% 4.24% 4.09% 2.17% 13.91% 10.55% 7.18% 8.83% 14.34% 14.22% 16.79% 2.64% 0.59%

Table 3: The ratio of resulting source groups identiﬁed by the classiﬁcation method described in Section 4. Datasets
are split into subsets based on the number of keys that can be attributed to a single source (batch). ‘-’ means no key
was classiﬁed for the target group. ‘0.00%’ means that some keys were classiﬁed, but less than 0.005%.

These estimates can be compared against

proximately 10.2%) was assigned to group X, which con-
tains the Microsoft providers (CAPI, CNG, and .NET).
the es-
timated distribution of commonly used web servers.
Apache, Nginx, LiteSpeed, and Google servers with the
OpenSSL library as the default option have a cumula-
tive market share of 86% [32]. This value exhibits a re-
markably close match to the classiﬁcation rate obtained
for OpenSSL (group V). MS Internet Information Ser-
vices (IIS) is included with Microsoft’s cryptographic
providers (group X) and has a market share of approx-
imately 12%. Again, a close match is observed with
the classiﬁcation value of 10.2% obtained for users with
10-99 certiﬁcates certiﬁed within the same day (batch).
Users with 100 and more keys certiﬁed within the
same day show an even stronger preference for OpenSSL
library (89.9%; group V) and also for group VIII (4.6%;
this group contains popular libraries such as OpenJDK’s
SunRsaSign, Bouncy Castle and mbedTLS) at the ex-
pense of groups X and XI.

The classiﬁcation accuracy for users with only single-
key batches or a small number of keys per batch is sig-
niﬁcantly less certain, but the general trends observed
for larger batches persist. Group V (OpenSSL) is most
popular, with group X (Microsoft providers) being the
second most common. Although we cannot obtain
the exact proportions of keys generated using particular
sources/groups, we can easily determine the proportion
of keys that certainly could not have been generated by
a given source by means of the occurrence of impossi-
ble values produced by the bit mask, i.e., values that are
never produced by the given source. Using this method,

we can conclude for certain that 19%, 25%, 17% and
10% of keys for users with 1, 2-9, 10-99 and 100+ keys
per batch, respectively, could not have been generated by
the OpenSSL library (see [25] for details).

Another dataset of TLS keys was collected from
Google’s Pilot Certiﬁcate Transparency server [10]. The
dataset processing was the same as that for the previous
TLS dataset [9]. For users with small numbers of keys
(1 and 2-9), the general trends observed from the TLS
IPv4 dataset were preserved. Interestingly, however, Cer-
tiﬁcate Transparency dataset indicates that group X (Mi-
crosoft) is signiﬁcantly more popular (47%) than group
V (OpenSSL) for users with 10-99 keys.

4.2.2 Sources of PGP keys

A different set of real-world keys can be obtained from
PGP key servers [30]. We used a dump containing nearly
4.2 million keys, of which approximately 1.4 million
were RSA keys suitable for classiﬁcation using the same
processing as for the TLS datasets.
In contrast to the
TLS handshakes, signiﬁcantly fewer PGP keys could be
attributed to the same batch (i.e., could be identiﬁed as
originating from the same unknown source) based on the
subject name and certiﬁcation date. Still, 84 thousand
unique keys were extracted in batches of 2-9 keys and
1 732 for batches of 10-99 keys.

The most proliﬁc source group is group XI (which
contains both libgcrypt from the GnuPG software dis-
tribution and the PGPSDK4 library), as seen in Ta-
ble 3.
This is intuitively expected because of the
widespread use of these two software libraries. Group

USENIX Association  

25th USENIX Security Symposium  905

VIII, consisting of the Bouncy Castle library (contain-
ing the org.bouncycastle.openpgp package), is also very
common (36%) for batches of 10-99 keys.

Because of the lower accuracy of classiﬁcation for
users with smaller numbers of keys (1 and 2-9), it is fea-
sible only to consider the general properties of these key
batches and their comparison with the TLS case rather
than being concerned with the exact percentage values in
these settings. The results for the PGP dataset indicate a
signiﬁcant drop in the proportion of keys generated us-
ing the OpenSSL library. According to an analysis of the
keys that certainly could not have been obtained from a
given source, at least 47% of the single-key batches were
certainly not generated by OpenSSL, and this percent-
age increases to 72% for batches of 2-9 keys. PGPSDK4
in FIPS mode (group IV) was found to be signiﬁcantly
more common than in the TLS datasets.

Note that an exported public PGP key usually contains
a Version string that identiﬁes the software used. Unfor-
tunately, however, this might be not the software used
to generate the original key pair but merely the software
that was used to export the public key. If the public key
was obtained via a PGP keyserver (as was the case for
our dataset), then the Version string indicates the version
of the keyserver software itself (e.g., Version: SKS 1.1.5)
and cannot be used to identify the ratios of the different
libraries used to generate the keys11.

5 Practical impact of origin detection

The possibility of accurately identifying the originating
library or card for an RSA key is not solely of theoretical
or statistical interest. If some library or card is found to
produce weak keys, then an attacker can quickly scan for
other keys from the same vulnerable source. The possi-
bility of detection is especially helpful when a successful
attack against a weak key requires a large but practically
achievable amount of computational resources. Prese-
lecting potentially vulnerable keys saves an attacker from
spending resources on all public keys.

The identiﬁcation of the implementations responsible
for the weak keys found in [3, 12] was a difﬁcult prob-
lem. In such cases, origin classiﬁcation can quickly pro-
vide one or a few of the most probable sources for further
manual inspection. Additionally, a set of already identi-
ﬁed weak keys can be used to construct a new classiﬁ-
cation group, which either will match an already known
one (for which the underlying sources are known) or can
be used to search for other keys that belong to this new
group in the remainder of a larger dataset (even when the
source is unknown).

11A dataset with the original Version strings could be used to test

these predictions.

Another practical impact is the decreased anonymity
set of the users of a service that utilizes the RSA al-
gorithm whose users are not
intended to be distin-
guishable (such as the Tor network). Using differ-
ent sources of generated keys will separate users into
smaller anonymity groups, effectively decreasing their
anonymity sets. The resulting anonymity sets will be es-
pecially small when individual users decides to use cryp-
tographic hardware to generate and protect their private
keys (if selected device does not fall into into group with
widely used libraries). Note that most users of the Tor
project use the default client, and hence the same im-
plementation, for the generation of the keys they use.
However, the preservation of indistinguishability should
be considered in the development of future alternative
clients.

Tor hidden services

sometimes utilize ordinary
HTTPS certiﬁcates for TLS [1], which can be then linked
(via classiﬁcation of their public keys) with other ser-
vices of the same (unknown) operator.

Mixnets such as mixmaster and mixminion use RSA
public keys to encrypt messages for target recipient
and/or intermediate mix. If key ID is preserved, one may
try to obtain corresponding public key from PGP key-
server and search for keys with the same source to nar-
row that user’s anonymity set in addition to analysis like
one already performed on alt.anonymous.messages [22].
Same as for Tor network, multiple seemingly indepen-
dent mixes can be linked together if uncommon source is
used to generate their’s RSA keys.

A related use is in a forensic investigation in which
a public key needs to be matched to a suspect key-
generating application. Again, secure hardware will
more strongly ﬁngerprint its user because of its relative
rarity.

An interesting use is to verify the claims of remote
providers of Cryptography as a Service [4] regarding
whether a particular secure hardware is used as claimed.
As the secure hardware (cards) used in our analysis
mostly exhibit distinct properties of their generated keys,
the use of such hardware can be distinguished from the
use of a common software library such as OpenSSL.

5.1 How to mitigate origin classiﬁcation
The impact of successful classiﬁcation can be mitigated
on two fronts: by library maintainers and by library
users. The root cause lies with the different design and
implementation choices for key generation that inﬂuence
the statistical distributions of the resulting public keys.
A maintainer can modify the code of a library to elimi-
nate differences with respect to the approach used by all
other sources (or at least the most common one, which is
OpenSSL in most cases). However, although this might

906  25th USENIX Security Symposium 

USENIX Association

work for one speciﬁc library (mimicking OpenSSL), it is
not likely to be effective on a wider scale. Changes to
all major libraries by its maintainers are unlikely to oc-
cur, and many users will continue to use older versions
of libraries for legacy reasons.

More pragmatic and immediate mitigation can be
achieved by the users of these libraries. A user may re-
peatedly generate candidate key pairs from his or her li-
brary or device of choice and reject it if its classiﬁcation
is too successful. Expected number of trials differs based
on the library used and the prior probability of sources
within the targeted domain. For example, if TLS is the
targeted domain, ﬁve or less key generation trials are ex-
pected for most libraries to produce “indecisive” key.

The weakness of the second approach lies in the un-
known extent of public modulus leakage. Although we
have described seven different causes of leakage, others
might yet remain unknown – allowing for potential fu-
ture classiﬁcation of keys even after they have been op-
timized for maximal indecisiveness against these seven
known causes.

This strategy can be extended when more keys are to
be generated. All previously generated keys should be
included in a trial classiﬁcation together with the new
candidate key. The selection process should also be ran-
domized to some extent; otherwise, a new classiﬁcation
group of “suspiciously indecisive” keys might be formed.

6 Key generation process on cards

The algorithms used in open-source libraries can be in-
spected and directly correlated to the biases detected in
their outputs. To similarly attribute the biased keys pro-
duced by cards to their unknown underlying algorithms,
we ﬁrst veriﬁed whether the random number generator
might instead be responsible for the observed bias. We
also examined the time- and power-consumption side
channels of the cards to gain insight into the processes
responsible for key generation.

Truly random data generated on-card are a crucial in-
put for the primes used in RSA key pair generation. A
bias in these data would inﬂuence the predictability of
the primes. If a highly biased or malfunctioning gener-
ator is used, factorization is not necessary (only a small
number of ﬁxed values can be taken as primes) or is fea-
sible even for RSA keys with lengths otherwise deemed
to be secure [3, 6, 12].

6.1 Biased random number generator
The output of an on-card truly random number genera-
tor (TRNG) can be tested using statistical batteries, and
deviances are occasionally detected in commercial secu-
rity tokens [6]. We generated a 100 MB stream of ran-

y
c
n
e
u
q
e
r
f
 

 

n
r
e

t
t

a
p

 
f

o

 
y
c
n
e
u
q
e
r
f
 

d
e
v
r
e
s
b
O

d
e

t
c
e
p
x
e

 
f

o

 

e
p

l

i
t
l

u
m
 
s
a

 

1.02
0.96
0.91
0.85
0.80
0.74
0.68
0.63

Infineon JTOP 80K − serial test 16−bit

(1)
(2)

(3)

(2) Patterns xy(xy XOR 02), xy(xy XOR 03), xy(xy XOR 40), xy(xy XOR C0)

(3) Patterns xy(xy XOR 01), xy(xy XOR 80)

(1)
(2)

(3)

(1) Patterns xy(xy XOR 04), xy(xy XOR 05), xy(xy XOR 06), xy(xy XOR 07), 

 xy(xy XOR 20), xy(xy XOR 60), xy(xy XOR A0), xy(xy XOR E0)

(4)

0000

2492

(4) Patterns xyxy, xxxx

(4)

4924
16−bit pattern (hexadecimal)

6DB6

9248

B6DA DB6C FFFF

Figure 6: The frequencies of different patterns with the
length of 16 bits computed from 1 GB random data
stream generated by the Inﬁneon JTOP 80K card. At
least ﬁve distinct patterns can be identiﬁed where all pat-
terns should exhibit an uniform distribution instead.

dom data from one card of each type and tested these
data streams using the common default settings of the
NIST STS and the Dieharder battery of statistical tests
[7, 23] as well as our alternative EACirc distinguisher
[24]. All types of cards except two (Inﬁneon JTOP 80K
and Oberthur Cosmo Dual 72K) passed the tests with the
expected number of failures at a conﬁdence level of 1%.
The Inﬁneon JTOP 80K failed the NIST STS Approxi-
mate Entropy test (85/100, expected entropy contained in
the data) at a signiﬁcant level and also failed the group of
Serial tests from the Dieharder suite (39/100, frequency
of overlapping n-bit patterns).
Interestingly, the serial
tests began to fail only for patterns with lengths of 9 bits
and longer (lengths of up to 16 bits were tested), sug-
gesting a correlation between two consecutive random
bytes generated by the TRNG. As shown in Figure 6, for
16-bit patterns, all bytes in the form of xyxy (where x
and y denote 4-bit values) were 37% less likely to oc-
cur than other combinations. At least three more distinct
groups of inputs with smaller-than-average probabilities
were also identiﬁed. Note that deviating distributions
were observed in all three physical Inﬁneon JTOP 80K
cards that were tested and thus were probably caused by
a systematic defect in the entire family of cards rather
than a single malfunctioning device. The detected bias
is probably not sufﬁcient to enable faster factorization
by guessing potential primes according to the slightly bi-
ased distribution. However, it may be used to identify
this type of card as the source of a sufﬁciently large (e.g.,
1KB) random data stream (i.e., to ﬁngerprint such a ran-
dom stream).

The Oberthur Cosmo Dual 72K failed more visibly,
as two cards were blocked after the generation of only
several MB of random data. The statistical tests then fre-
quently failed because of the signiﬁcant bias in the data.
Several speciﬁc byte values were never produced in the
“random” stream.

USENIX Association  

25th USENIX Security Symposium  907

We also generated data streams directly from the con-
catenated exported primes with the two most signiﬁ-
cant bytes and the least two bits dropped, as the previ-
ous analysis had revealed a non-uniform distribution in
these bits. Interestingly, both the Inﬁneon JTOP 80K and
the Oberthur Cosmo Dual 72K failed only for their ran-
dom data streams (as described above) but successfully
passed12 for the streams generated from the concatenated
primes, hinting at the possibility that either random data
are generated differently during prime generation or (un-
likely) the prime selection process is able to mask the
bias observed in the raw random data.

6.1.1 Malfunctioning generator

All primes for the card-generated 512- and 1024-bit keys
were tested for uniqueness. All tested card types ex-
cept one generated unique primes.
In the exceptional
case of the Oberthur Cosmo Dual 72K cards, approx-
imately 0.05% of the generated keys shared a speciﬁc
value of prime q. The ﬂaw was discovered in all three
tested physical cards for both 512-bit and 1024-bit keys.
The repeated prime value was equal to 0xC000...0077
for 512-bit RSA keys and 0xC000...00E9B for 1024-
bit RSA keys. These prime values correspond to the
ﬁrst Blum prime generated when starting from the value
0xC000...000 in each case.

The probable cause of such an error is the following
sequence of events during prime generation: 1) The ran-
dom number generator of the card was called but failed
to produce a random number, either by returning a value
with all bits set to zero or by returning nothing into the
output memory, which had previously been zeroed. 2)
The candidate prime value q (equal to 0 at the time) had
its highest four bits ﬁxed to 11002 (to obtain a modulus
of the required length13 when multiplied by the prime p),
resulting in a value of 0xC0 in the most signiﬁcant byte.
3) The candidate prime value was tested for primality and
increased until the ﬁrst prime with the required properties
(a Blum prime in the case of the Oberthur Cosmo Dual
72K) was found (0xC000...0077 in the case of 512-bit
RSA).

The faulty process described above that leads to the
observed predictable primes may also occur for other
cards or software libraries as a result of multiple causes
(e.g., an ignored exception in random number genera-
tion or a programming error). We therefore inspected our
key pair dataset, the TLS IPv4 dataset [9] and the PGP
dataset [30] for the appearance of such primes relevant to
key lengths of 512, 1024 and 2048 bits. Interestingly, no
such corrupt keys were detected except for those already
described.

12Except for the Oberthur nearly zero keys (see Section 6.1.1).
13As was observed for the dataset analysed in Section 3.

Note that a random search for a prime is much less
likely to fail in this mode. Even if some of the top bits
and the lowest bit are set to one, the resulting value is not
a prime for common MSB masks. New values will be
generated if the starting value contains only zeroes.

6.2 Power analysis of key generation
Analysis of power consumption traces is a frequently
used technique for card inspection. The baseline power
trace expected should cover at least the generation of ran-
dom numbers of potential primes, primality testing, com-
putation of the private exponent and storage of generated
values into a persistent key pair object. We utilized the
simple power analysis to reveal signiﬁcant features like
random number generation, RSA encryption, and RSA
decryption operation, separately. By programming a card
to call only the desired operation (generate random data,
encrypt, decrypt), the feature pattern for the given oper-
ation is obtained. These basic operations were identiﬁed
in all tested types of cards. Once identiﬁed, the opera-
tions can be searched for inside a more complex opera-
tions like the RSA key pair generation.

A typical trace of the RSA key pair generation process
(although feature patterns may differ with card hardware)
contains: 1) Power consumption increases after the gen-
erating key pair method is called (cryptographic RSA co-
processor turned on). 2) Candidate values for primes p
and q are generated (usage of a TRNG can be observed
from the power trace) and tested. 3) The modulus and
the private exponent are generated (assumed, not distin-
guishable from the power trace). 4) Operation with a pri-
vate key is executed (decryption, in 7 out of 16 types of
cards) to verify key usability. 5) Operation with a public
key is executed (encryption, 3 types of cards only).

Note that even when the key generation process is cor-
rectly guessed, it is not possible to simply implement it
again and compare the resulting power traces – as only
the card’s main CPU is available for user-deﬁned opera-
tions, instead of a coprocessor used by the original pro-
cess. Additional side-channel and fault induction pro-
tection techniques may be also applied. Therefore, one
cannot obtain an exactly matching power trace from a
given card due to unavailability of low-level program-
ming interfaces and additionally executed operations for
veriﬁcation of key generation hypothesis.

Whereas some steps of the key generation, such as the
randomness generation, take an equal time across mul-
tiple runs of the process, the time required to generate
a prime differs greatly as can be also seen from the ex-
ample given in Figure 7, where timing is extracted from
the power trace. The variability can be attributed to the
randomized process of the prime generation. Incremen-
tal search will ﬁnd the ﬁrst prime greater than a random

908  25th USENIX Security Symposium 

USENIX Association

number selected as the base of the search. Since both
primes p and q are distributed as distances from a random
point to a prime number, the resulting time distribution
will be affected by a mixture of these two distributions.
In samples collected from 12 out of 16 types of cards,
the distribution of time is concentrated at evenly spaced
points14 as seen in Figure 7. The distance between a pair
of points is interpreted as the duration of a single primal-
ity test, whereas their amount corresponds to the number
of candidates that were ruled out by the test as a compos-
ite. Then it is possible to obtain a histogram of number
of tested candidates, e.g., by binning the distribution with
breaks placed in the midpoints of the empty intervals.

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

)

%

(
 
y
t
i
s
n
e
D

Time of key generation

  512b keys (smartcard)
1024b keys (smartcard)
  512b keys (sieving 11b)
1024b keys (sieving 6b)

)

%

(
 
y
t
i
s
n
e
D

0
.
3

0
.
2

0
.
1

0
.
0

0

50

100

150

Number of candidates tested for primality

6.3 Time distribution
We experimentally obtained distributions for a number
of needed primality tests for different parameters of trial
division. Then we were able to match them with dis-
tributions from several cards, obtaining a likely estimate
for the number of primes used by the card in the trial
division (sieving) phase. For some types of cards, a sin-
gle parameter did not match distributions of neither 512-
bit nor 1024-bit keys. There may exist a different op-
timal value of trial division tests and primality tests for
different key lengths. Notably, in some cases of card-
generated 512-bit keys, the number of primality tests
would have to be halved to exactly match a referential
distribution. However, we are not aware of a mechanism
that would perform two primality tests in parallel or at
least in the same time, as is required for testing a candi-
date of double bit length.

The exact time distribution for software implementa-
tions is of less concern since the key generation process
tends to be much faster on an ordinary CPU. The source
code can be modiﬁed to accommodate for counting the
number of tests directly (as shown in the inlay in Figure
7) without relying on time measurement that may be in-
ﬂuenced by other factors speciﬁc to the implementation.

7 Conclusions

This paper presents a thorough analysis of key pairs gen-
erated and extracted from 38 different sources encom-
passing open-source and proprietary software libraries
and cryptographic cards. This broad analysis allowed us
to assess current trends in RSA key pair generation even
when the source codes for key generation were not avail-
able, as in the case of proprietary libraries and cards. The

14Due to small differences in duration of key generation and round-
ing caused by precision of the measurement, the times belonging to the
same group will not be identical to one millisecond. The peaks were
highlighted by summing adjoining milliseconds, but only in the case
when large (almost) empty spaces exist in the distribution.

500

0
2500
Time of key generation (ms, difference from fastest run)

1000

1500

2000

Figure 7: An example of the histogram of times neces-
sary to generate a large number of 512 and 1024-bit
RSA keys generated from an NXP J2D081 card. Left –
the distribution of key generation times is concentrated
around evenly spaced points, with the distance represent-
ing the duration of a single primality test. The times were
normalized to begin at zero, therefore they represent dif-
ference from the fastest run. Inlay – the distribution of
number of candidates tested by primality tests obtained
from a software implementation. 512-bit keys are gener-
ated with trial division up to 11-bit primes, 1024-bit keys
used 6-bit primes. The results show a clear correlation
between the generation time and an expected number of
primality tests.

range of approaches identiﬁed indicates that the question
of how to generate an optimal RSA key has not yet been
settled.

The tested keys were generally found to contain a high
level of entropy, sufﬁcient to protect against known fac-
torization attacks. However, the source-speciﬁc prime
selection algorithms, postprocessing techniques and en-
forcement of speciﬁc properties (e.g., Blum primes)
make the resulting primes slightly biased, and these bi-
ases serve as ﬁngerprints of the sources. Our paper there-
fore shows that public moduli leak signiﬁcantly more in-
formation than previously assumed. We identiﬁed seven
properties of the generated primes that are propagated
into the public moduli of the generated keys. As a result,
accurate identiﬁcation of originating library or smartcard
is possible based only on knowledge of the public keys.
Such an unexpected property can be used to decrease the
anonymity set of RSA keys users, to search for keys gen-
erated by vulnerable libraries, to assess claims regarding
the utilization of secure hardware by remote parties, and
for other practical uses. We classiﬁed the probable ori-
gins of keys in two large datasets consisting of 10 and

USENIX Association  

25th USENIX Security Symposium  909

15 million (mostly) TLS RSA keys and 1.4 million PGP
RSA keys to obtain an estimate of the sources used in
real-world applications.

The random number generator is a crucial component
for the generation of strong keys. We identiﬁed a generic
failure scenario that produces weak keys and occasion-
ally detected such keys in our dataset obtained from the
tested cards. Luckily, no such weak key was identiﬁed in
the datasets of publicly used RSA keys.

Acknowledgements: We acknowledge the support of
the Czech Science Foundation, project GA16-08565S.
The access to the computing and storage resources of Na-
tional Grid Infrastructure MetaCentrum (LM2010005) is
greatly appreciated. We would like to thank all anony-
mous reviewers and our colleagues for their helpful com-
ments and fruitful discussions.

References
[1] ARMA. Tor blog: Facebook, hidden services, and https certs.
Available from https://blog.torproject.org/blog/facebook-hidden-
services-and-https-certs, cit. [2016-06-26].

[2] BARDOU, R., FOCARDI, R., KAWAMOTO, Y., SIMIONATO, L.,
STEEL, G., AND TSAY, J.-K. Efﬁcient Padding Oracle At-
tacks on Cryptographic Hardware. In Advances in Cryptology –
CRYPTO 2012: 32nd Annual Cryptology Conference. Proceed-
ings. Springer-Verlag, 2012, pp. 608–625.

[3] BERNSTEIN, D. J., CHANG, Y.-A., CHENG, C.-M., CHOU, L.-
P., HENINGER, N., LANGE, T., AND SOMEREN, N. Factoring
RSA Keys from Certiﬁed Smart Cards: Coppersmith in the Wild.
In Advances in Cryptology – ASIACRYPT 2013. Springer-Verlag,
2013, pp. 341–360.

[4] BERSON, T., DEAN, D., FRANKLIN, M., SMETTERS, D., AND
SPREITZER, M. Cryptography as a network service.
In Pro-
ceedings of the ISOC Network and Distributed System Security
Symposium (NDSS) (2001).

[5] BLEICHENBACHER, D. Chosen ciphertext attacks against pro-
tocols based on the RSA encryption standard PKCS #1.
In
Advances in Cryptology — CRYPTO ’98: 18th Annual Inter-
national Cryptology Conference. Proceedings. Springer-Verlag,
1998, pp. 1–12.

[6] BOORGHANY, A., SARMADI, S., YOUSEFI, P., GORJI, P., AND
JALILI, R. Random data and key generation evaluation of some
commercial tokens and smart cards. In Information Security and
Cryptology (ISCISC), 11th International ISC Conference. Pro-
ceedings. IEEE, 2014, pp. 49–54.

[7] BROWN, R. G.

Dieharder:

suite,

version

test
Available
http://www.phy.duke.edu/∼rgb/General/dieharder.php,
[2016-06-26].

3.31.1,

2004.

A random number
from:
cit.

[8] BRUMLEY, B. B., AND TUVERI, N. Remote Timing Attacks
Are Still Practical. In Computer Security – ESORICS 2011: 16th
European Symposium on Research in Computer Security. Pro-
ceedings. Springer-Verlag, 2011, pp. 355–371.

[9] DURUMERIC, Z., ET AL. Internet-Wide Scan Data Repository:
Full IPv4 HTTPS Handshakes, dump from June 02, 2016. Avail-
able from: https://scans.io/, [cit. 2016-06-02].

[10] GOOGLE. Certiﬁcate Transparency dump from June 07, 2016.

https://www.certiﬁcate-transparency.org, cit. [2016-06-07].

[11] GORDON, J. Strong Primes are Easy to Find. Springer-Verlag,

1985, pp. 216–223.

[12] HENINGER, N., DURUMERIC, Z., WUSTROW, E., AND HAL-
DERMAN, J. A. Mining Your Ps and Qs: Detection of
Widespread Weak Keys in Network Devices.
In 21st USENIX
Security Symposium. Proceedings. USENIX, 2012, pp. 205–220.
[13] IEEE. Standard Speciﬁcations for Public-Key Cryptography.

IEEE Std 1363, 2000.

[14] KERRY, C. F., AND ROMINE, C. FIPS PUB 186-4 Digital Sig-

nature Standard (DSS), 2013.

[15] KOCHER, P., JAFFE, J., AND JUN, B. Differential Power Analy-
sis. In Advances in Cryptology – CRYPTO’99: 19th Annual Inter-
national Cryptology Conference. Proceedings. Springer-Verlag,
1999, pp. 388–397.

[16] LEHMAN, R. S. Factoring large integers.

In Mathematics of
Computation, vol. 28. American Mathematical Society, 1974,
pp. 637–646.

[17] LOEBENBERGER, D., AND N ¨USKEN, M. Notions for RSA In-
tegers. In International Journal of Applied Cryptography. Inder-
science Publishers, 2014, pp. 116–138.

[18] MAURER, U. M. Fast generation of prime numbers and secure
public-key cryptographic parameters. Journal of Cryptology 8, 3
(1995), 123–155.

[19] MENEZES, A. J., OORSCHOT, P. C. V., VANSTONE, S. A., AND
RIVEST, R. L. Handbook of Applied Cryptography, 1st ed. CRC
Press, 1996.
[20] MIRONOV,

Factoring RSA Moduli II. Available from

I.

https://windowsontheory.org/2012/05/17/factoring-rsa-moduli-
part-ii/, cit. [2016-06-26].

[21] PULKUS, J. Efﬁcient Prime-Number Check, Oct. 2 2014. US

Patent App. 14/354,455.

[22] RITTER, T. De-anonymizing alt.anonymous.messages. Available

from https://ritter.vg/p/AAM-defcon13.pdf, cit. [2016-06-26].

[23] RUKHIN, A., ET AL. A Statistical Test Suite for Random and
Pseudorandom Number Generators for Cryptographic Applica-
tions. In NIST Special Publication 800-22rev1a. NIST, 2010.

[25]

[24] S ´YS, M., ˇSVENDA, P., UKROP, M., AND MATY ´AˇS, V. Con-
In SECRYPT 2014,

structing empirical tests of randomness.
SCITEPRESS (2014), pp. 229–237.
ˇSVENDA, P., NEMEC, M., SEKAN, P., KVAˇS ˇNOVSK ´Y, R.,
FORM ´ANEK, D., KOM ´AREK, D., AND MATY ´AˇS, V.
The
Million-Key Question Investigating the Origins of RSA Public
Keys, Technical report FIMU-RS-2016-03. Masaryk University,
Czech Republic, 2016.

[26] WAGNER, D. Cryptanalysis of a Provably Secure CRT-RSA Al-
gorithm. In 11th ACM Conference on Computer and Communi-
cations Security. Proceedings. ACM, 2004, pp. 92–97.

[27] WILLIAMS, H. C. A p +1 Method of Factoring. In Mathematics
of Computation, vol. 39. American Mathematical Society, 1982,
pp. 225–234.

[28] ANSI X9.31-1998: Public Key Cryptography Using Reversible

Algorithms for the Financial Services Industry (rDSA), 1998.

[29] YAFU: Yet Another Factorization Utility, 2013. Available from:

http://sourceforge.net/projects/yafu/, cit. [2016-06-26].

[30] PGP keydump from June 02, 2016.

Available from:

http://pgp.key-server.io/dump/current/, cit. [2016-06-02].

[31] Keys collected in 1MRSA project, 2016. Available from:

http://crcs.cz/papers/usenix2016/1mrsaset.

[32] W3Techs Web

Technology

Surveys:

Usage

servers

for websites,

web
Available
http://w3techs.com/technologies/overview/web server/all,
cit. [2016-06-26].

2016.

of
from

910  25th USENIX Security Symposium 

USENIX Association

