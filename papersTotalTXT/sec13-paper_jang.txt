Towards Automatic Software Lineage Inference

Jiyong Jang, Maverick Woo, and David Brumley, Carnegie Mellon University

Open access to the Proceedings of the 22nd USENIX Security Symposium is sponsored by USENIXThis paper is included in the Proceedings of the 22nd USENIX Security Symposium.August 14–16, 2013 • Washington, D.C., USAISBN 978-1-931971-03-4Towards Automatic Software Lineage Inference

Jiyong Jang, Maverick Woo, and David Brumley

{jiyongj, pooh, dbrumley}@cmu.edu

Carnegie Mellon University

Abstract
Software lineage refers to the evolutionary relationship
among a collection of software. The goal of software
lineage inference is to recover the lineage given a set of
program binaries. Software lineage can provide extremely
useful information in many security scenarios such as
malware triage and software vulnerability tracking.

In this paper, we systematically study software lineage
inference by exploring four fundamental questions not
addressed by prior work. First, how do we automatically
infer software lineage from program binaries? Second,
how do we measure the quality of lineage inference al-
gorithms? Third, how useful are existing approaches to
binary similarity analysis for inferring lineage in reality,
and how about in an idealized setting? Fourth, what are
the limitations that any software lineage inference algo-
rithm must cope with?

Towards these goals we build ILINE, a system for auto-
matic software lineage inference of program binaries, and
also IEVAL, a system for scientiﬁc assessment of lineage
quality. We evaluated ILINE on two types of lineage—
straight line and directed acyclic graph—with large-scale
real-world programs: 1,777 goodware spanning over a
combined 110 years of development history and 114 mal-
ware with known lineage collected by the DARPA Cyber
Genome program. We used IEVAL to study seven metrics
to assess the diverse properties of lineage. Our results
reveal that partial order mismatches and graph arc edit
distance often yield the most meaningful comparisons in
our experiments. Even without assuming any prior infor-
mation about the data sets, ILINE proved to be effective
in lineage inference—it achieves a mean accuracy of over
84% for goodware and over 72% for malware in our data
sets.
1
Software evolves to adapt to changing needs, bug ﬁxes,
and feature additions [28]. As such, software lineage—the
evolutionary relationship among a set of software—can
be a rich source of information for a number of security
questions. Indeed, the literature is replete with analyses
of known or manually recovered software lineages. For
example, software engineering researchers have analyzed

Introduction

the histories of open source projects and the Linux kernel
to understand software evolution [14, 45] as well as its
effect on vulnerabilities in Firefox [33]. The security com-
munity has also studied malware evolution based upon the
observation that the majority of newly detected malware
are tweaked variants of well-known malware [2, 18, 20].
With over 1.1 million malware appearing daily [43], re-
searchers have exploited such evolutionary relationships
to identify new malware families [23, 31], create models
of provenance and lineage [9], and generate phylogeny
models based upon the notion of code similarity [22].

The wealth of existing research demonstrating the util-
ity of software lineage immediately raises the question—
“Can we infer software lineage automatically?” We fore-
see a large number of security-related applications once
this becomes feasible. In forensics, lineage can help de-
termine software provenance. For example, if we know
that a closed-source program pA is written by author X
and another program pB is derived from pA, then we may
deduce that the author of pB is likely to be related to X.
In malware triage [2, 18, 20], lineage can help malware
analysts understand trends over time and make informed
decisions about which malware to analyze ﬁrst. This is
particularly important since the order in which the vari-
ants of a malware are captured does not necessarily mirror
its evolution. In software security, lineage can help track
vulnerabilities in software of which we do not have source
code. For example, if we know a vulnerability exists in
an earlier version of an application, then it may also exist
in applications that are derived from it. Such logic has
been fruitfully applied at the source level in our previous
work [19]. Indeed, these and related applications are im-
portant enough that the US Defense Advanced Research
Projects Agency (DARPA) is funding a $43-million Cyber
Genome program [6] to study them.

Having established that automatically and accurately
infer software lineage is an important open problem, let
us look at how to formalize it. Software lineage inference
is the task of inferring a temporal ordering and ances-
tor/descendant relationships among programs. We model
software lineage by a lineage graph:
Deﬁnition 1.1. A lineage graph G = (N,A) is a directed
acyclic graph (DAG) comprising a set of nodes N and a
set of arcs A. A node n ∈ N represents a program, and

USENIX Association  

22nd USENIX Security Symposium  81

Straight(Line(Lineage
Set Distances

DAG(Lineage

5

Features 2

4

Datasets
Contiguous Revisions
Released Versions
Released Binaries
Malware

Symmetric Distance
Weighted Symmetric Distance
Dice Coefficient
Jaccard Distance
Jaccard Containment

Features 12
Section Size
File Size
Cyclomatic Complexity
n-grams
S/D Instructions
S/D Mnemonics
S/D Normalized Mnemonics
S/D Multi-resolution
Root Revision
2
Inferred Root
Real Root

Metrics

2
Inversions
Edit Distance to Monotonicity

S/D Multi-resolution

Root Revision
Inferred Root
Real Root

2

Timestamp

3
No Timestamp
Pseudo Timestamp
Real Timestamp

Datasets

2

DAG Revisions
Malware

Metrics

5

LCA Mismatches
Avg Distance to True LCA
Partial Order
Graph Arc Edit Distance
k-Cone

Figure 1: Design space in software lineage inference (S/D represents static/dynamic analysis-based features.)

an arc (x,y) ∈ A denotes that program y is a derivative of
program x. We say that x is a parent of y and y is a child
of x.

A root is a node that has no incoming arc and a leaf is
a node that has no outgoing arc. The set of ancestors of a
node n is the set of nodes that can reach n. Note that n is
an ancestor of itself. The set of common ancestors of x
and y is the intersection of the two sets of ancestors. The
set of lowest common ancestors (LCAs) of x and y is the
set of common ancestors of x and y that are not ancestors
of other common ancestors of x and y [4]. Notice that in
a tree each pair of nodes must have a unique LCA, but in
a DAG some pair of nodes can have multiple LCAs.
In this paper, we ask four basic research questions:

1. Can we automatically infer software lineage? Exist-
ing research focused on studying known software history
and lineage [14, 33, 45], not creating lineage. Creating
lineage is different from building a dendrogram based
upon similarity [22, 23, 31]. A dendrogram can be used
to identify families; however it does not provide any infor-
mation about a temporal ordering, e.g., root identiﬁcation.
In order to infer a temporal ordering and evolution-
ary relationships among programs, we develop new algo-
rithms to automatically infer lineage of programs for two
types of lineage: straight line lineage (§4.1) and directed
acyclic graph (DAG) lineage (§4.2). In addition, we ex-
tend our approach for straight line lineage to k-straight
line lineage (§4.1.4). We build ILINE to systematically
evaluate the effectiveness of our lineage inference algo-
rithms using twelve software feature sets (§2), ﬁve dis-
tance measures between feature sets (§3), two policies on
the root identiﬁcation (§4.1.1), and three policies on the
use of timestamps (§4.2.2).

Without any prior information about data sets, for
straight line linage, the mean accuracies of ILINE are
95.8% for goodware and 97.8% for malware. For DAG
lineage, the mean accuracies are 84.0% for goodware and
72.0% for malware.
2. What are good metrics? Existing research focused on
building a phylogenetic tree of malware [22, 23], but did
not provide quantitative metrics to scientiﬁcally measure

the quality of their output. Good metrics are necessary
to quantify how good our approach is with respect to the
ground truth. Good metrics also allow us to compare
different approaches. To this end, we build IEVAL to
assess our lineage inference algorithms using multiple
metrics, each of which represents a different perspective
of lineage.

IEVAL uses two metrics for straight line lineage (§5.1).
Given an inferred lineage graph G and the ground truth
G∗, the number of inversions measures how often we
make a mistake when answering the question “which one
of programs pi and p j comes ﬁrst”. The edit distance
to monotonicity asks “how many nodes do we need to
remove in G so that the remaining nodes are in the sorted
order (and thus respect G∗)”.

IEVAL also utilizes ﬁve metrics to measure the accuracy
of DAG lineage (§5.2). An LCA mismatch is a generalized
version of an inversion because the LCA of two nodes
in a straight line is the earlier node. We also measure
the average pairwise distance between true LCA(s) and
derived LCA(s) in G∗. The partial order mismatches in
a DAG asks the same question as inversions in a straight
line. The graph arc edit distance for (labeled) graphs mea-
sures “what is the minimum number of arcs we need to
delete from G and G∗ to make both graphs equivalent”. A
k-Cone mismatch asks “how many nodes have the correct
set of descendants counting up to depth k”.

Among the above seven metrics, we recommend two
metrics—partial order mismatches and graph arc edit dis-
tance. In §5.3, we discuss how the metrics are related,
which metric is useful to measure which aspect of a lin-
eage graph, which metric is efﬁcient to compute, and
which metric is deducible from other metrics.
3. How well are we doing now? We would like to un-
derstand the limits of our techniques even in ideal cases,
meaning we have (i) control over variables affecting the
compilation of programs, (ii) reliable feature extraction
techniques to abstract program binaries accurately and
precisely, and (iii) the ground truth with which we can
compare our results to measure accuracy and to spot error
cases. We discuss the effectiveness of different feature

82  22nd USENIX Security Symposium 

USENIX Association

sets and distance measures on lineage inference in §8.

We argue that it is necessary to also systematically val-
idate a lineage inference technique with “goodware”, e.g.,
open source projects. Since malware is often surrepti-
tiously developed by adversaries, it is typically hard or
even impossible to obtain the ground truth. More funda-
mentally, we simply cannot hope to understand the evo-
lution of adversarial programs unless we ﬁrst understand
the limits of our approach in our idealized setting.

We systematically evaluated ILINE with both good-
ware and malware that we have the ground truth on: 1,777
goodware spanning over a combined 110 years of devel-
opment history and 114 malware collected by the DARPA
Cyber Genome program.
4. What are the limitations? We investigate error cases
in G constructed by ILINE and highlight some of the
difﬁcult cases where ILINE failed to recover the correct
evolutionary relationships. Since some of our experiments
are conducted on goodware with access to source code,
we are able to pinpoint challenging issues that must be ad-
dressed before we can improve the accuracy in software
lineage inference. We discuss such challenging issues
including reverting/refactoring, root identiﬁcation, clus-
tering, and feature extraction in §9. This is important
because we may not be able to understand malware evo-
lution without understanding limits of our approach with
goodware.
2 Software Features for Lineage
In this study, we use three program analysis methods:
syntax-based analysis, static analysis, and dynamic analy-
sis. Given a set of program binaries P, various features
fi are extracted from each pi ∈ P to evaluate different
abstractions of program binaries. Source code or meta-
data such as comments, commit messages or debugging
information is not used as we are interested in results in
security scenarios where source code is typically not avail-
able, e.g., forensics, proprietary software, and malware.
2.1 Using Previous Observations
Previous work analyzed software release histories to un-
derstand a software evolution process. It has been often
observed that program size and complexity tend to in-
crease as new revisions are released [14, 28, 45]. This
observation also carries over to security scenarios, e.g.,
the complexity of malware is likely to grow as new vari-
ants appear [8]. We measured code section size, ﬁle size,
and code complexity to assess how useful these features
are in inferring lineage of program binaries.
• Section size: ILINE ﬁrst identiﬁes executable sections
in binary code, e.g., .text section, which contain exe-
cutable program code, and calculates the size.
• File size: Besides the section size, ILINE also calcu-
lates the ﬁle size, including code and data.

d485db75
83c42c5b
5dc383c4
e9adf8ff

db750783
2c5b5e5d
5b5e5de9

5dd485db
0783c42c
5e5dc383
5de9adf8

85db7507
c42c5b5e
c383c42c
adf8ffff

8b5dd485db750783c42c5b5e5dc383c42c5b5e5de9adf8ffff
(a) Byte sequence of program code
8b5dd485
750783c4
5b5e5dc3
5e5de91d
(b) 4-grams
mov -0x2c(%ebp),%ebx;test %ebx,%ebx;jne 805e198
add $0x2c,%esp;pop %ebx;pop %esi;pop %ebp;ret
add $0x2c,%esp;pop %ebx;pop %esi;pop %ebp;jmp 805da50
(c) Disassembled instructions
mov mem,reg;test reg,reg;jne imm
add imm,reg;pop reg;pop reg;pop reg;ret
add imm,reg;pop reg;pop reg;pop reg;jmp imm
(d) Instructions mnemonics with operands type
mov mem,reg;test reg,reg;jcc imm
add imm,reg;pop reg;pop reg;pop reg;ret
add imm,reg;pop reg;pop reg;pop reg;jmp imm
(e) Normalized mnemonics with operands type

Figure 2: Example of feature extraction

• Cyclomatic complexity: Cyclomatic complexity [34]
is a common metric that indicates code complexity by
measuring the number of linearly independent paths.
From the control ﬂow graph (CFG) of a program, the
complexity M is deﬁned as M = E − N + 2P where E is
the number of edges, N is the number of nodes, and P is
the number of connected components of the CFG.
2.2 Using Syntax-Based Feature
While syntax-based analysis may lack semantic under-
standing of a program, previous work has shown its ef-
fectiveness on classifying unpacked programs. Indeed,
n-gram analysis is widely adopted in software similarity
detection, e.g., [20, 22, 26, 40]. The beneﬁt of syntax-
based analysis is that it is fast because it does not require
disassembling.
• n-grams: An n-gram is a consecutive subsequence of
length n in a sequence. From the identiﬁed executable
sections, ILINE extracts program code into a hexadeci-
mal sequence. Then, n-grams are obtained by sliding a
window of n bytes over it. For example, Figure 2b shows
4-grams extracted from Figure 2a.

2.3 Using Static Features
Existing work utilized semantically richer features by
ﬁrst disassembling a binary. After reconstructing a con-
trol ﬂow graph for each function, each basic block can
be considered as a feature [12]. In order to maximize
the probability of identifying similar programs, previous
work also normalized disassembly outputs by considering
instruction mnemonics without operands [23, 46] or in-
struction mnemonics with only the types of each operand
(such as memory, a register or an immediate value) [39].
In our experiments, we introduce an additional nor-
malization step of normalizing the instruction mnemonics
themselves. This was motivated by our observations when

USENIX Association  

22nd USENIX Security Symposium  83

we analyzed the error cases in the lineages constructed
using the above techniques. Our results indicate that this
normalization notably improves lineage inference quality.
We also evaluate binary abstraction methods in an ide-
alized setting in which we can deploy reliable feature
extraction techniques. The limitation with static analysis
comes from the difﬁculty of getting precise disassem-
bly outputs from program binaries [27, 30]. In order to
exclude the errors introduced at the feature extraction
step and focus on evaluating the performance of software
lineage inference algorithms, we also leverage assembly
generated using gcc -S (not source code itself) to ob-
tain basic blocks more accurately. Note that we use this
to simulate what the results would be with ideal disassem-
bling, which is in line with our goal of understanding the
limits of the selected approaches.
• Basic blocks comprising disassembly instructions:
ILINE disassembles a binary and identiﬁes its basic blocks.
Each feature is a sequence of instructions in a basic block.
For example, in Figure 2c, each line is a series of instruc-
tions in a basic block; and each line is considered as an
individual feature. This feature set is semantically richer
than n-grams.
• Basic blocks comprising instruction mnemonics:
For each disassembled instruction, ILINE retains only its
mnemonic and the types of its operands (immediate, reg-
ister, and memory). For example, add $0x2c, %esp
is transformed into add imm, reg in Figure 2d. By
normalizing the operands, this feature set helps us miti-
gate errors from syntactical differences, e.g., changes in
offsets and jump target addresses, and register renaming.
• Basic blocks comprising normalized mnemonics:
ILINE also normalizes mnemonics. First, mnemonics for
all conditional jumps, e.g., je, jne and jg, are normal-
ized into jcc because the same branching condition can
be represented by ﬂipped conditional jumps. For exam-
ple, program p1 uses cmp eax, 1; jz addr1 while
program p2 has cmp eax, 1; jnz addr2. Second,
ILINE removes the nop instruction.
2.4 Using Dynamic Features
Modern malware is often found in a packed binary for-
mat [15, 21, 32, 38] and it is often not easy to analyze such
packed/obfuscated programs with static analysis tools.
In order to mitigate such difﬁculties, dynamic analysis
has been proposed to monitor program executions and
changes made to a system at run time [1, 2, 13, 35]. The
idea of dynamic analysis is to run a program to make it
disclose its “behaviors”. Dynamic analysis on malware is
typically performed in controlled environments such as
virtual machines and isolated networks to prevent infec-
tions spreading to other machines [37].
• Instructions executed at run time: For malware
speciﬁcally, ILINE traces an execution using a binary in-

strumentation tool and collects a set of instruction traces.
Similar to static features, ILINE also generates additional
sets of features by normalizing operands and mnemonics.
2.5 Using Multi-Resolution Features
Besides considering each feature set individually, ILINE
utilizes multiple feature sets to beneﬁt from normalized
and speciﬁc features. Speciﬁcally, ILINE ﬁrst uses the
most normalized feature set to detect similar programs
and gradually employs less-normalized feature sets to
distinguish highly similar programs. This ensures that
less similar programs (e.g., major version changes) will
be connected only after more similar programs (e.g., only
changes of constant values) have been connected.
3 Distance Measures Between Feature Sets
To measure the distance between two programs p1 and
p2, ILINE uses the symmetric difference between their
feature sets, which captures both additions and deletions
made between p1 and p2. Let f1 and f2 denote the two
feature sets extracted from p1 and p2, respectively. The
symmetric distance between f1 and f2 is deﬁned to be

SD( f1, f2) =| f1(cid:31) f2| +| f2(cid:31) f1|,

(1)

which denotes the cardinality of the set of features that are
in f1 or f2 but not both. The symmetric distance basically
measures the number of unique features in p1 and p2.

Distance metrics other than symmetric distance may
be used for lineage inference as well. For example, the
Dice coefﬁcient distance DC( f1, f2) = 1 − 2| f1∩ f2|
, the
| f1|+| f2|
Jaccard distance JD( f1, f2) =1 − | f1∩ f2|
, and the Jaccard
| f1∪ f2|
containment distance JC( f1, f2) =1 − | f1∩ f2|
min(| f1|,| f2|) can all
be used to calculate the dissimilarity between two sets.
Besides the above four distance measures, which are all
symmetric, i.e., distance( f1, f2) = distance( f2, f1),
we have also evaluated an asymmetric distance measure
to determine the direction of derivation between p1 and
p2. We call it the weighted symmetric distance, denoted
WSD( f1, f2) =| f1(cid:31) f2| ×C del + | f2(cid:31) f1| ×C add where
Cdel and Cadd denote the cost for each deletion and each
addition, respectively. Note that WSD( f1, f2) = SD( f1, f2)
when Cdel = Cadd = 1.

Our hypothesis is that additions and deletions should
have different costs in a software evolution process, and
we should be able to infer the derivative direction be-
tween two programs more accurately using the weighted
symmetric distance. For example, in many open source
projects and malware, code size usually grows over
time [8, 45]. In other words, addition of new code is pre-
ferred to deletion of existing code. Differentiating Cdel
and Cadd can help us to decide a direction of derivation. In
this paper, we set Cdel = 2 and Cadd = 1. (We leave it as

84  22nd USENIX Security Symposium 

USENIX Association

future work to investigate the effect of these values.) Sup-
pose program pi has feature set fi = {m1,m2,m3}, and
program p j contains feature set f j = {m1,m2,m4,m5}.
By introducing asymmetry, evolving from pi to p j has a
distance of 4 (deletion of m3 and addition of m4 and m5),
while the opposite direction has a distance of 5 (deletion
of m4 and m5 and addition of m3). Since pi → p j has a
smaller distance, we conclude that it is the more plausible
scenario.

For the rest of our paper, we use SD as a representative
distance metric when we explain our lineage inference
algorithms. We evaluated the effectiveness of all ﬁve dis-
tance measures on inferring lineage using SD as a baseline
(see §8). Regarding metric-based features, e.g., section
size, we measure the distance between two samples as the
difference of their metric values.
4 Software Lineage Inference
Our goal is to automatically infer software lineage of
program binaries. We build ILINE to systematically ex-
plore the design space illustrated in Figure 1 to understand
advantages and disadvantages of our algorithms for infer-
ring software lineage. We applied our algorithms to two
types of lineage: straight line lineage (§4.1) and directed
acyclic graph (DAG) lineage (§4.2). In particular, this is
motivated by the observation that there are two common
development models: serial/mainline development and
parallel development. In serial development, every devel-
oper makes a series of check-ins on a single branch; and
this forms straight line lineage. In parallel development,
several branches are created for different tasks and are
merged when needed, which results in DAG lineage.
4.1 Straight Line Lineage
The ﬁrst scenario that we have investigated is 1-straight
line lineage, i.e., a program source tree that has no branch-
ing/merging history. This is a common development
history for smaller programs. We have also extended
our technique to handle multiple straight line lineages
(§4.1.4).

Software lineage inference in this setting is a problem
of determining a temporal ordering. Given N unlabeled
revisions of program p, the goal is to output label “1” for
the 1st revision, “2” for the 2nd revision, and so on. For
example, if we are given 100 revisions of program p and
we have no timestamp of the revisions (or 100 revisions
are randomly permuted), we want to rearrange them in
the correct order starting from the 1st revision p1 to the
100th revision p100.
4.1.1 Identifying the Root Revision
In order to identify the root/ﬁrst revision that has no parent
in lineage, we explore two different choices: (i) inferring

the root/earliest revision, and (ii) using the real root revi-
sion from the ground truth.

ILINE picks the root revision based upon Lehman’s
observation [28]. The revision that has the minimum code
complexity (the 2nd software evolution law) and the min-
imum size (the 6th software evolution law) is selected
as the root revision. The hypothesis is that developers
are likely to add more code to previous revisions rather
than delete other developers’ code, which can increase
code complexity and/or code size. This is also reﬂected
in security scenarios, e.g., malware authors are also likely
to add more modules to make it look different to bypass
anti-virus detection, which leads to high code complex-
ity [8]. In addition, provenance information such as ﬁrst
seen date [10] and tool-chain components [36] can be
leveraged to infer the root.

Inferring Order

We also evaluate ILINE with the real root revision given
from the ground truth in case the inferred root revision
was not correct. By comparing the accuracy of the lin-
eage with the real root revision to the accuracy of the
lineage with the inferred root revision, we can assess the
importance of identifying the correct root revision.
4.1.2
From the selected root revision, ILINE greedily picks the
closest revision in terms of the symmetric distance as the
next revision. Suppose we have three contiguous revi-
sions: p1, p2, and p3. One hypothesis is SD(p1, p2) <
SD(p1, p3), i.e., the symmetric distance between two adja-
cent revisions would be smaller. This hypothesis follows
logically from Lehman’s software evolution laws.

There may be cases where the symmetric distance be-
tween two different pairs are the same, i.e., a tie. Suppose
SD(p1, p2) = SD(p1, p3). Then both p2 and p3 become
candidates for the next revision of p1. Using normalized
features can cause more ties than using speciﬁc features
because of the information loss.

ILINE utilizes more speciﬁc features in order to break
ties more correctly (see §2.5). For example, if the symmet-
ric distances using normalized mnemonics are the same,
then the symmetric distances using instruction mnemon-
ics are used to break ties. ILINE gradually reduces nor-
malization strength to break ties.
4.1.3 Handling Outliers
As an optional step, ILINE handles outliers in our recov-
ered ordering, if any. Since ILINE constructs lineage
in a greedy way, if one revision is not selected mistak-
enly, the revision may not be selected until the very last
round. To see this, suppose we have 5 revisions p1, p2,
p3, p4, and p5. If ILINE falsely selects p3 as the next revi-
sion of p1 (p1 → p3) and SD(p3, p4) < SD(p3, p2), then
p4 will be chosen as the next revision (p1 → p3 → p4).
It is likely that SD(p4, p5) < SD(p4, p2) holds because

USENIX Association  

22nd USENIX Security Symposium  85

p4 and p5 are neighboring revisions, and then p5 will
be selected (p1 → p3 → p4 → p5). The probability
of selecting p2 is getting lower and lower if we have
more revisions. At last p2 is added as the last revision
(p1 → p3 → p4 → p5 → p2) and becomes an outlier.
In order to handle such outliers, ILINE monitors the
symmetric distance between every adjacent pair in the
constructed lineage G. Since the symmetric distance at
an outlier is the accumulation of changes from multiple
revisions, it would be much larger than the difference be-
tween two contiguous revisions. (See Figure 10 for a real
life example.) ILINE detects outliers by detecting peaks
among the symmetric distances between consecutive pairs
by means of a user-conﬁgurable threshold.

Once an outlier r has been identiﬁed, ILINE eliminates
it in two steps. First, ILINE locates the revision y that has
the minimum distance with r. Then, ILINE places r im-
mediately next to y, favoring the side with a gap that has a
larger symmetric distance. In our example, suppose p3 is
the closest revision to p2. ILINE will compare SD(p1, p3)
(before) with SD(p3, p4) (after) and then insert p2 into
the bigger of the two gaps. Therefore, in the case when
SD(p1, p3) is larger than SD(p3, p4), we will recover the
correct lineage, i.e., p1 → p2 → p3 → p4 → p5.
4.1.4 k-Straight Line Lineage
We consider k-straight line lineage where we have a mixed
data set of k different programs instead of a single pro-
gram, and each program has straight line lineage.

For k-straight line lineage, ILINE ﬁrst performs clus-
tering on a given data set P to group the same (similar)
programs into the same cluster Pk ⊆ P. Programs are sim-
ilar if D(pi, p j) (cid:31) t where D(·) means a distance measure-
ment between two programs and t is a distance threshold
to be considered as a group. After we isolate distinct
program groups between each other, ILINE identiﬁes the
earliest revision p1
k and infers straight line lineage for each
program group Pk using the straight line lineage method.
We denote the r-th revision of the program k as pr
k. One
caveat with the use of clustering as a preprocessing step
is that more precise clustering may require reliable “com-
ponents” extraction from program binaries, which is out
of our scope.

Given a collection of programs and revisions, previ-
ous work shows that clustering can effectively separate
them [5, 18, 20, 46]. ILINE uses hierarchical clustering
because the number of variants k is not determined in ad-
vance. Other clustering methods like k-means clustering
require that k is set at the beginning. ILINE groups two
programs if JD( f1, f2) (cid:31) t where t is a distance threshold
(0 (cid:31) t (cid:31) 1). In order to decide an appropriate distance
threshold t, we explore entire range of t and ﬁnd the value
where the resulting number of clusters becomes stable
(see Figure 7 for an example).

4.2 Directed Acyclic Graph Lineage
The second scenario we studied is directed acyclic graph
(DAG) lineage. This generalizes straight line lineage to
include branching and merging histories. Branching and
merging are common in large scale software development
because branches allow developers to modify and test
code without affecting others.

Identifying the Root Revision

In a lineage graph G, branching is represented by a
node with more than one outgoing arcs, i.e., a revision
with multiple children. Merging is denoted by a node
with more than one incoming arcs, i.e., a revision with
multiple parents.
4.2.1
In order to identify the root revision in lineage, we explore
two different choices: (i) inferring the root/earliest revi-
sion and (ii) using the real root revision from the ground
truth as discussed in §4.1.1.
4.2.2 Building Spanning Tree Lineage
ILINE builds (directed) spanning tree lineage by greedy
selection. This step is similar to, but different from the
ordering recovery step of the straight line lineage method.
In order to recover an ordering, ILINE only allows the last
revision in the recovered lineage G to have an outgoing
arc so that the lineage graph becomes a straight line. For
DAG lineage, however, ILINE allows all revisions in the
recovered lineage G to have an outgoing arc so that a
revision can have multiple children.

For example, given three revisions p1, p2, and p3, if
p1 is selected as a root and SD(p1, p2) < SD(p1, p3), then
ILINE connects p1 and p2 (p1 → p2). If SD(p1, p3) <
SD(p2, p3) holds, p1 will have another child p3 and a
lineage graph looks like the following:

p1

p2

p3

We evaluate three different policies on the use of a
timestamp in DAG lineage: no timestamp, the pseudo
timestamp from the recovered straight line lineage, and
the real timestamp from the ground truth. Without a times-
tamp, the revision p j to be added to G is determined by
the minimum symmetric distance min{SD(pi, p j) : pi ∈
ˆN, p j ∈ ˆNc} where ˆN ⊆ N represents a set of nodes al-
ready inserted into G and ˆNc denotes a complement of
ˆN; and an arc (pi, p j) is added. However, with the use
of a timestamp, the revision p j ∈ ˆNc to be inserted is de-
termined by the earliest timestamp and an arc is drawn
based upon the minimum symmetric distance. In other
words, we insert nodes in the order of timestamps.
4.2.3 Adding Non-Tree Arcs
While building (directed) spanning tree lineage, ILINE
identiﬁes branching points by allowing the revisions pi ∈

86  22nd USENIX Security Symposium 

USENIX Association

ˆN to have more than one outgoing arcs—revisions with
multiple children. In order to pinpoint merging points,
ILINE adds non-tree arcs also known as cross arcs to
spanning tree lineage.

For every non-root node pi, ILINE identiﬁes a unique
feature set ui that does not come from its parent p j, i.e.,
ui = {x : x ∈ f i and x (cid:29)∈ f j}. Then ILINE identiﬁes possi-
ble parents pk ∈ N as follows:
i) if real/pseudo timestamps are given, pk with earlier

timestamps than the timestamp of pi

ii) for symmetric distance measures such as SD, DC, JD,

and JC, non-ancestors pk added to G before pi

iii) for the asymmetric distance measure WSD, non-
ancestors pk satisfying WSD(pk, pi) < WSD(pi, pk)

become possible parents. Among the identiﬁed possible
parents pk, if ui and f k extracted from pk have common
features, then ILINE adds a non-tree arc from pk to pi.
Consequently, pi becomes a merging point of p j and pk
and a lineage graph looks like the following:

p j

pk

pi

After adding non-tree arcs, ILINE outputs DAG lineage
showing both branching and merging.
5 Software Lineage Metrics
We build IEVAL to scientiﬁcally measure the quality of
our constructed lineage with respect to the ground truth.
5.1 Straight Line Lineage
We use dates of commit histories and version numbers as
the ground truth of ordering G∗ = (N,A∗), and compare
the recovered ordering by ILINE G = (N,A) with the
ground truth to measure how close G is to G∗.

IEVAL measures the accuracy of the constructed lin-
eage graph G using two metrics: number of inversions
and edit distance to monotonicity (EDTM). An inversion
happens if ILINE gives a wrong ordering for a chosen pair
of revisions. The total number of inversions is the number

of wrong ordering for all (cid:31)|N|2 (cid:30) pairs. The EDTM is the

minimum number of revisions that need to be removed to
make the remaining nodes in the lineage graph G in the
correct order. The longest increasing subsequence (LIS)
can be computed in G, which is the longest (not necessar-
ily contiguous) subsequence in the sorted order. Then the
EDTM is calculated by |N|−|LIS|, which depicts how
many nodes are out-of-place in G.

p1

p2

p3
p4
(a) Lineage 1

p5

p1

p4

p3
p5
(b) Lineage 2

p2

Figure 3: Inversions and edit distance to monotonicity

For example, we have 5 revisions of a program and
ILINE outputs lineage 1 in Figure 3a and lineage 2 in
Figure 3b. Lineage 1 has 1 inversion (a pair of p3 − p2)
and 1 EDTM (delete p2). Lineage 2 has 3 inversions
(p3 − p2, p4 − p2, and p5 − p2) and 1 EDTM (delete p2).
As shown in both cases, the number of inversions can be
different even when the EDTM is the same.
5.2 Directed Acyclic Graph Lineage
We evaluate the practical use of ﬁve metrics for measuring
the accuracy of the constructed DAG lineage: number of
LCA mismatches, average pairwise distance to true LCA,
partial order mismatches, graph arc edit distance, and
k-Cone mismatches.
p2
p3

p6
p7
Figure 4: Lowest common ancestors

p4
p5

p1

We deﬁne SLCA(x,y) to be the set of LCAs of x and y
because there can be multiple LCAs. For example, in Fig-
ure 4, SLCA(p4, p5) = {p2, p3}, while SLCA(p6, p7) =
{p4}. Given SLCA(x,y) in G and the true SLCA∗(x,y)
in G∗, we can evaluate the correct LCA score of (x,y)
L(SLCA(x,y), SLCA∗(x,y)) in the following four ways.

i) 1 point (correct) if SLCA(x,y) = SLCA∗(x,y)
ii) 1 point (correct) if SLCA(x,y) ⊆ SLCA∗(x,y)
iii) 1 point (correct) if SLCA(x,y) ⊇ SLCA∗(x,y)
iv) 1− JD(SLCA(x,y), SLCA∗(x,y)) point
Then the number of LCA mismatches is

|N × N|− ∑

(x,y)∈N×N

L(SLCA(x,y), SLCA∗(x,y)).

The 1st policy is sound and complete, i.e., we only con-
sider an exact match of SLCA. However, even small errors
can lead to a large number of LCA mismatches. The 2nd
policy is sound, i.e., every node in SLCA is indeed a true
LCA (no false positive). Nonetheless, including any extra
node will result in a mismatch. The 3rd policy is com-
plete, i.e., SLCA must contain all true LCAs (no false
negative). However, missing any true LCA will result in
a mismatch. The 4th policy uses the Jaccard distance to
measure dissimilarity between SLCA and SLCA∗. In our
evaluation, ILINE followed the 4th policy since it allows
us to attain a more ﬁne-grained measure.

We also measure the distance between the true LCA(s)
and reported LCA(s). For example, if ILINE falsely re-
ports p5 as an LCA of p6 and p7 in Figure 4, then the
pairwise distance to the true LCA is 2 (=distance between
p4 and p5). Formally, let D(u,v) represent the distance
between nodes u and v in the ground truth G∗. Given
SLCA(x,y) and SLCA∗(x,y), we deﬁne the pairwise dis-
tance to true LCA T (SLCA(x,y), SLCA∗(x,y)) to be

USENIX Association  

22nd USENIX Security Symposium  87

SPECIAL CASE ← ··············· → GENERAL CASE
Straight Line
Inversions
EDTM

DAG

PO

SLCA
GAED
k-Cone

Property measured

Order/Topology

Out-of-place nodes/arcs

Descendants within depth k

Table 1: Relationships among metrics

∑

(l,l∗)∈SLCA(x,y)×SLCA∗(x,y)

|SLCA(x,y)× SLCA∗(x,y)|
and the average pairwise distance to true LCA to be

D(l,l∗)

∑

(x,y)∈N×N

T (SLCA(x,y), SLCA∗(x,y))

|N × N|

.

A partial order (PO) of x and y is to identify which one
of x and y comes ﬁrst: either x or y, or incomparable if they
are not each other’s ancestors. For example, in Figure 4,
the PO of p3 and p7 is p3, while the PO of p6 and p7 is
incomparable. The total number of PO mismatches is the

number of wrong ordering for all (cid:31)|N|2 (cid:30) pairs.

A graph arc edit distance (GAED) measures how many
arcs need to be deleted from G and G∗ to make both G
and G∗ identical. For every node x, we calculate E(x) =
SD(Adj(x), Adj∗(x)) where Adj(x) and Adj∗(x) denotes
the adjacency list of x in G and G∗ respectively. Then
GAED becomes ∑x∈N E(x).
We deﬁne k-CONE(x) to be the set of descendants
within depth k from node x. For example,
in Fig-
ure 4, 2-CONE of p1 is {p2, p3, p4, p5}. Then the given
k-CONE(x) in G and the true k-CONE∗(x) in G∗, we can
evaluate the correct k-CONE score of x R(k-CONE(x))
using four different ways of set comparisons: an exact
match, a subset match, a superset match, or the Jaccard in-
dex. In our evaluation, ILINE used the Jaccard index for a
more ﬁne-grained measure. Then the number of k-CONE
mismatches is |N|− ∑x∈N R(k-CONE(x)). With smaller k,
we can measure the accuracy of nearest descendants.
5.3 Relationships among Metrics
Table 1 shows the relationships among different metrics
and a property measured by each metric. A PO mismatch
is a special case of an LCA mismatch because when x and
y are in different branches, an LCA mismatch measures
the accuracy of SLCA while a PO mismatch just says two
nodes are incomparable. An inversion is also a special
case of an LCA mismatch because querying the LCA of x
and y in a straight line is the same as asking which one of
x and y comes ﬁrst. Essentially, a PO mismatch in a DAG
is equal to an inversion in a straight line.

EDTM is a special case of GAED and an upper bound
of GAED in a straight line is GAED ≤ EDTM×6. One
out-of-place node can cause up to six arcs errors. For
example, p1 → p2 → p4 → p3 → p5 has 1 EDTM (delete

p3 or p4) and 6 GAED (delete p2 → p4, p4 → p3, and
p3 → p5 in G and p2 → p3, p3 → p4, and p4 → p5 in
G∗).
A k-Cone mismatch is a local metric to assess the cor-
rectness of nearest descendants of nodes while the other
six metrics are global metrics to evaluate the correctness
of the order of nodes and to count out-of-place nodes/arcs.

What are good metrics? Among the seven metrics, we
recommend two metrics—partial order mismatches and
graph arc edit distance. PO mismatches and GAED are
both desirable because they evaluate different properties
of lineage and are not deducible from each other.

To see this, observe that PO mismatches and SLCA
mismatches measure the same property of lineage and
have similar accuracy results in our evaluation. However,
PO mismatches are more efﬁcient to compute than SLCA
mismatches; moreover, PO gives an answer for a more in-
tuitive question, “which one of these two programs comes
ﬁrst”. Thus, PO mismatches are preferred. Average dis-
tance to true LCA is supplementary to SLCA mismatches
and so this metric is not necessary if we exclude SLCA
mismatches. The number of inversions and edit distance
to monotonicity can be respectively seen as special cases
of PO mismatches and GAED in the case of straight line
lineages. k-Cone mismatches can be extremely useful to
an analyst during manual analysis, but it can be difﬁcult
to pick the right value of k automatically.
6
ILINE is implemented using C (2.5 KLoC) and
IDAPython plugin (100 LoC). We use the IDA Pro disas-
sembler1 to disassemble program binaries and to identify
basic blocks. As discussed in §2.3, gcc -S output is
used to compensate the errors introduced at the disas-
sembling step. We utilize Cuckoo Sandbox2 to monitor
native functions, API calls and network activities of mal-
ware. On top of Cuckoo Sandbox, we use malwasm3 with
pintool4, which allows us to obtain more ﬁne-grained in-
struction level of traces. Since some kinds of malicious
activities require “live” connections, we also employ IN-
etSim5 to simulate various network services, e.g., web,

Implementation

1http://www.hex-rays.com/products/ida/index.shtml
2http://cuckoosandbox.org/
3http://code.google.com/p/malwasm/
4http://software.intel.com/en-us/articles/pintool
5http://www.inetsim.org/

88  22nd USENIX Security Symposium 

USENIX Association

[software]

iLine

[k code bases]

[lineage output]

iEval

Extract
Features

Perform
Clustering

Construct
Lineage

Measure
Accuracy

ground
truth

[accuracy]
Metric1
Metric2
89.2%
99.5%
100% 93.2%
89.3%
85.1%

Figure 5: Software lineage inference overview

email, DNS, FTP, IRC, and so on. For example, Blaster-
Worm in our data set sent exploit packets and propagated
itself via TFTP only when there were (simulated) live
vulnerable hosts.

For the scalability reason, we use the feature hash-
ing technique [20, 44] to encode extracted features into
bit-vectors. For example, let bv1 and bv2 denote two
bit-vectors generated from f1 and f2 using feature hash-
ing. Then the symmetric distance in Equation 1 can be
calculated by:

SDbv(bv1,bv2) =S (bv1 ⊗ bv2)

(2)
where ⊗ denotes bitwise-XOR and S(·) means the number
of bits set to one.
7 Evaluation
As depicted in Figure 5, we systematically evaluated our
lineage inference algorithms using (i) ILINE to explore
all the design spaces described in Figure 1 with a variety
of data sets and (ii) IEVAL to measure the accuracy of our
outputs with respect to the ground truth.
7.1 Straight Line Lineage
7.1.1 Data sets
For straight line lineage experiments, we have collected
three different kinds of goodware data sets, e.g., con-
tiguous revisions, released versions, and actual release
binaries, and malware data sets.
i) Contiguous Revisions: Using a commit history
from a version control system, e.g., subversion and git,
we downloaded contiguous revisions of a program. The
time gap between two adjacent commits varies a lot, from
<10 minutes to more than a month. We excluded some
revisions that changed only comments because they did
not affect the resulting program binaries.

Programs # revisions

memcached
redis
redislite

Last rev

First rev

Period
124 2008-10-14 2012-02-02 3.3 yr
158 2011-09-29 2012-03-28 0.5 yr
89 2011-06-02 2012-01-18 0.6 yr

Table 2: Data sets of contiguous revisions

In order to set up idealized experiment environments, we
compiled every revision with the same compiler and the
same compiling options. We excluded variations that can
come from the use of different compilers.

ii) Released Versions: We downloaded only released
versions of a program meant to be distributed to end users.
For example, subversion maintains them under the tags
folder. The difference with contiguous revisions is that
contiguous revisions may have program bugs (commit-
ted before testing) or experimental functionalities that
would be excluded in released versions. In other words,
released versions are more controlled data sets. We com-
piled source code with the same compiler and the same
compiling options for ideal settings.

Programs #

grep
nano
redis
sendmail
openssh

First release
Date

Last release
Date

Ver

Ver
2.0

releases
19
1993-05-22 2.11 2012-03-02 18.8 yr
114 0.7.4 2000-01-09 2.3.1 2011-05-10 11.3 yr
48
2.6 yr
38 8.10.0 2000-03-03 8.14.5 2011-05-15 11.2 yr
52 2.0.0 2000-05-02 5.9p1 2011-09-06 11.4 yr

2009-09-03 2.4.10 2012-03-30

1.0

Period

Table 3: Data sets of released versions

iii) Actual Release Binaries: We collected binaries (not
source code) of released versions from rpm or deb pack-
age ﬁles.

Programs #

First release
Date

Last release
Date

Period

Ver

Ver
2.0-3

grep
nano
redis
sendmail
openssh
FileZilla
p7zip

2009-08-02 2.11-3 2012-04-17

ﬁles
37
2.7 yr
69 0.7.9-1 2000-01-24 2.2.6-1 2010-11-22 10.8 yr
2.9 yr
39 0.094-1 2009-05-06 2.4.9-1 2012-03-26
6.1 yr
41 8.13.3-6 2005-03-12 8.14.4-2 2011-04-21
7.1 yr
75 3.9p1-2 2005-03-12 5.9p1-5 2012-04-02
2007-09-13
62
2012-01-08
4.3 yr
2004-08-21 9.20.1 2011-03-16
6.6 yr
32
Table 4: Data sets of actual release binaries

3.0.0
0.91

3.5.3

The difference is that we did not have any control over the
compiling process of the program, i.e., different programs
may be compiled with different versions of compilers
and/or optimization options. This data set is a represen-
tative of real-world scenarios where we do not have any
information about development environments.
iv) Malware: We used 84 samples with known lineage
collected by the Cyber Genome program. The data set
includes bots, worms, and Trojan horses and contains 7
clusters.
Cluster # samples
Cluster # samples
10
MC1
MC5
17 BlasterWorm MC6
MC2
15 MiniPanzer.A MC7
MC3
7 CleanRoom.A
MC4
Table 5: Data sets of malware

10 CleanRoom.B
15 MiniPanzer.B
10 CleanRoom.C

Family
KBot

Family

USENIX Association  

22nd USENIX Security Symposium  89

File Size
Cyclomatic Complexity

)

B
K

(
 
e
z
i

S
 
e
l
i

F

 250

 245

 240

 235

 230

 225

 220

 215

 210

 205

 200

 1700

 1650

 1600

 1550

 1500

 1450

 1400

 1350

y
t
i
x
e
l
p
m
o
C
 
c
i
t
a
m
o
l
c
y
C

 2850

 2800

)

B
K

(
 
e
z
i

S
 
e
l
i

F

 2750

 2700

 2650

 2600

 2550

File Size
Cyclomatic Complexity

File Size
Cyclomatic Complexity

 10400

 10200

 10000

 9800

 9600

 9400

 9200

 9000

y
t
i
x
e
l
p
m
o
C
 
c
i
t
a
m
o
l
c
y
C

)

B
K

(
 
e
z
i

S
 
e
l
i

F

 290
 280
 270
 260
 250
 240
 230
 220
 210
 200
 190
 180

 2000

 1900

 1800

 1700

 1600

 1500

 1400

 1300

 1200

 1100

y
t
i
x
e
l
p
m
o
C
 
c
i
t
a
m
o
l
c
y
C

 0

 20

 40

 60

 80

 100

 120

 0

 20

 40

 60

 80

 100  120  140

 0

 10

 20

 30

Revision

(a) memcached

Revision

(b) redis

Figure 6: File size and complexity for contiguous revisions

 40
 50
Revision

 60

 70

 80

(c) redislite

7.1.2 Results
What selection of features provides the best lineage graph
with respect to the ground truth? We evaluated different
feature sets on diverse data sets.
i) Contiguous Revisions: In order to identify the ﬁrst
revision of each program, code complexity and code size
of every revision were measured. As shown in Figure 6,
both ﬁle size and cyclomatic complexity generally in-
creased as new revisions were released. For these three
data sets, the ﬁrst revisions were correctly identiﬁed by
selecting the revision that had the minimum ﬁle size and
cyclomatic complexity.
A lineage for each program was constructed as described
in §4.1. Although section/ﬁle size achieved high accu-
racies, e.g., 95.5%–99.5%, they are not reliable features
because many ties can decrease/increase the accuracies de-
pending on random guesses. n-grams over byte sequences
generally achieved better accuracies; however, 2-grams
(small size of n) were relatively unreliable features, e.g.,
6.3% inversion error in redis. In our experiments, n=4
bytes worked reasonably well for these three data sets.
The use of disassembly instructions had up to 5% in-
version error in redislite. Most errors came from
syntactical differences, e.g., changes in offsets and jump
target addresses. After normalizing operands, instruc-
tion mnemonics with operands types decreased the errors
substantially, e.g., from 5% to 0.4%. With additional
normalization, normalized instruction mnemonics with
operands types achieved the same or better accuracies.
Note that more normalized features can result in better or
worse accuracies because there may be more ties where
random guesses are involved.
In order to break ties, more speciﬁc features were used
in multi-resolution features. For example, all 10 tie cases
in memcached were correctly resolved by using more
speciﬁc features. This demonstrated the effectiveness of
using multi-resolution features for breaking ties.
ii) Released Versions:The ﬁrst/root revisions were also
correctly identiﬁed by selecting the revision that had the
minimum code size. In some cases, simple feature sets,
e.g., section/ﬁle size, could achieve higher accuracies than
semantically rich feature sets (requiring more expensive

process), e.g., instruction sequences. For example, ILINE
with section size yielded 88.3% accuracy, while ILINE
with instructions achieved 77.8% accuracy in grep. This,
however, was improved to 100% with normalization. Like
the experiments on contiguous revisions, 2-grams per-
formed worse in the experiments on released versions,
e.g., 18.9% accuracy in sendmail. Among various fea-
ture sets, multi-resolution features outperformed the other
feature sets, e.g., 99.3%–100%.
iii) Actual Release Binaries: The ﬁrst/root revisions for
nano and openssh were correctly identiﬁed by select-
ing the revision that had the minimum code size. For the
other ﬁve data sets, we performed the experiments both
with the wrong inferred root and with the correct root
given from the ground truth.
Overall accuracy of the constructed lineage was fairly
high across all the data sets even though we did not control
the variables of the compiling process, e.g., 83.3%–99.8%
accuracy with the correct root. One possible explanation
is that closer revisions (developed around the same time)
might be compiled with the same version of compiler
(available around the same time), which can make neigh-
boring revisions look related to each other at the binary
code level.
It was conﬁrmed that lineage inference can be improved
with the knowledge of the correct root. For example,
ILINE picked a wrong revision as the ﬁrst revision in
FileZilla, which resulted in 51.6% accuracy; in con-
trast, the accuracy increased to 99.8% with the correct
root revision.
iv) Malware: The ﬁrst/root samples for all seven clus-
ters were correctly identiﬁed by selecting the sample that
had the minimum code size. Section size achieved high
accuracies, e.g., 93.3–100%, which showed new variants
were likely to add more code to previous malware. File
size was not a good feature to infer a lineage of MC2
because all samples in MC2 had the same ﬁle size. The
multi-resolution feature yielded 94.9–100% accuracy.
Dynamic instrumentations at the instruction level enabled
us to catch minor updates between two adjacent vari-
ants. For example, subsequent BlasterWorm samples
add more checks for virtual environments to hide its ma-
licious activities if it is being monitored, e.g., examin-

90  22nd USENIX Security Symposium 

USENIX Association

 100

 10

s
r
e
t
s
u
l
C

 
f
o

 
r
e
b
m
u
N

 1

 0

 0.1

 0.2

 0.3

 0.4

 0.5

Distance Threshold

Figure 7: Clustering mixed data set of 2 and 3 programs

ing user names (sandbox, vmware, honey), running pro-
cesses (VBoxService.exe, joeboxserver.exe), and current
ﬁle names (C:\sample.exe). Dynamic feature sets yielded
worse accuracy in MC1, MC2, MC3, MC5, and MC6
while achieving the same accuracy in MC4 and better ac-
curacy in MC7. One main reason of the differences in
accuracy is that dynamic analysis followed a speciﬁc ex-
ecution path depending on the context. For example, in
MC2, some variants exited immediately when they de-
tected a VirtualBox service process, and produced limited
execution traces.
v) k-Straight Line Lineage: We evaluated ILINE on
mixed data sets including k different programs. For
2-straight line lineage, we mixed memcached and
redislite in that both programs have the same func-
tionality and similar code section sizes. Figure 7 shows
the resulting number of clusters with various distance
threshold values. From 0.2 to 0.5 distance threshold, the
resulting number of clusters was 2. This means ILINE
can ﬁrst perform clustering to divide the data set into two
groups, then build a straight line lineage for each group.
The resulting number of clusters of the mixed data set of
3 programs including memcached, redislite, and
redis became stabilized to 3 from 0.2 to 0.5 distance
threshold, which means they were successfully clustered
for the subsequent straight line lineage building process.
We have also evaluated ILINE on three mixed malware
data sets, each of which is a combination of different
clusters in Table 5: {MC2+MC5}, {MC4+MC6}, and
{MC2+MC3+MC7}. For each mixed data set, ILINE also
clustered malware samples correctly for the subsequent
straight line lineage inference. We discuss inferring lin-
eage on incorrect clusters in §9.

7.2 Directed Acyclic Graph Lineage
7.2.1 Data sets
For DAG lineage experiments, we also evaluated ILINE
on both goodware and malware.
i) Goodware: We have collected 10 data sets for di-
rected acyclic graph lineage experiments from github6.
We used github because we know when a project is forked
from a network graph showing the development history
as a graph including branching and merging.

6https://github.com/

3 sets of programs
2 sets of programs

We downloaded DAG revisions that had multiple times of
branching and merging histories, and compiled with the
same compilers and optimization options.

Programs # revisions

Last rev

http-parser
libgit2
redis
redislite
shell-fm
stud
tig
uzbl
webdis
yajl

First rev

Period
55 2010-11-05 2012-07-27 1.7 yr
61 2012-06-25 2012-07-17 0.1 yr
98 2010-04-29 2010-06-04 0.1 yr
97 2011-04-19 2011-06-12 0.1 yr
107 2008-10-01 2012-06-26 3.7 yr
73 2011-06-09 2012-06-01 1.0 yr
58 2006-06-06 2007-06-19 1.0 yr
73 2011-08-07 2012-07-01 0.9 yr
96 2011-01-01 2012-07-20 1.6 yr
62 2010-07-21 2011-12-19 1.4 yr

Table 6: Goodware data sets for DAG lineage

ii) Malware: We used two malware families with
known DAG lineage collected by the Cyber Genome pro-
gram. They contain 30 samples in total.
Cluster # samples Family
21 WormBot
9 MinBot

MC8
MC9

Table 7: Malware data sets for DAG lineage

7.2.2 Results
We set two policies for DAG lineage experiments: the
use of timestamp (none/pseudo/real) and the use of the
real root (none/real). The real timestamp implies the real
root so that we explored 3× 2− 1 = 5 different setups.
We used multi-resolution feature sets for DAG lineage
experiments because multi-resolution feature sets attained
the best accuracy in constructing straight line lineage.
i) Goodware: Without having any prior knowledge,
ILINE achieved 71.5%–94.1% PO accuracies. By using
the real root revision, the accuracies increased to 71.5%–
96.1%. For example, in case of tig, ILINE gained about
20% increase in the accuracy.
With pseudo timestamps, accuracies were worse even
with the real root revisions for most of data sets, e.g.,
64.0%–90.9% (see §8). By using the real timestamps,
ILINE achieved higher accuracies of 84.1%–96.7%. This
means that the recovered DAG lineages were very close
to the true DAG lineages.
ii) Malware: ILINE achieved 68.6%–75.0% accuracies
without any prior knowledge. Using the correct times-
tamps, the accuracies increased notably to 86.2%–91.7%.
While we obtained the real timestamps from the ground
truth in our experiments, we can also leverage ﬁrst seen
date of malware, e.g., Symantec’s Worldwide Intelligence
Network Environment [10].
With dynamic features, ILINE achieved 59.0%–75.0% ac-
curacies without any prior knowledge, and 68.6%–80.6%
accuracies with real timestamps, which is a bit lower than
the accuracies based upon static features.

USENIX Association  

22nd USENIX Security Symposium  91

7.3 Performance
Given N binaries with their features already extracted,
the complexity of constructing lineage is O(N2) due to

the computation of the (cid:31)|N|2 (cid:30) pairwise distances. To give

concrete values, we measured the time to construct lin-
eage with multi-resolution features, SD, and 32 KB of
bit-vectors on a Linux 3.2.0 machine with a 3.40 GHz i7
CPU utilizing a single core. Depending on the size of the
data sets, it took 0.002–1.431s for straight line lineage and
0.005–0.385s for DAG lineage with the help of feature
hashing. On average, this translates to 146 samples/s and
180 samples/s for straight line lineage and DAG lineage,
respectively. As a comparison, our BitShred malware
clustering system [20], which represents the state of the
art at the time of its publication in 2011, can process
257 samples per second using a single core on the same
machine. Since the running times of malware cluster-
ing and lineage inference are both dominated by distance
comparisons, and since ILINE needs to resolve ties us-
ing multi-resolution features whereas BitShred needs not,
we conclude that our current implementation of ILINE is
competitive in terms of performance.
8 Discussion & Findings
Features. File/section size features yielded 94.6–95.5%
mean accuracy in straight line lineage on goodware. Such
high accuracy supports Lehman’s laws of software evolu-
tion, e.g., continuing growth. However, size is not a reli-
able feature to infer malware lineage where malware au-
thors can obfuscate a feature, e.g., samples with the same
ﬁle size in MC2. As simple syntactic features, 4/8/16-
grams achieved 95.3–96.3% mean accuracy in straight
line lineage on goodware, whereas 2-grams achieved only
82.4% mean accuracy. This is because 2-grams are not
distinctive enough to differentiate between samples and
cause too many ties. Basic blocks as semantic features
achieved 94.0–95.6% mean accuracy in straight line lin-
eage on goodware. This slightly lower accuracy when
compared to n-grams was due to ties. Multi-resolution fea-
tures performed best, e.g., it achieved 95.8–98.4% mean
accuracy in straight line lineage on goodware. This is due
to its use of both syntactic and semantic features.

Distance Metrics. Our evaluation indicates that our lin-
eage inference algorithms perform similarly regardless
of the distance metrics except for the Jaccard contain-
ment (JC) distance. JC turns out to be inappropriate for
lineage inference because it cannot capture evolution-
ary changes effectively. Suppose there are three contigu-
ous revisions p1, p2, and p3; and p2 adds 10 lines of
code to p1 and p3 adds 10 lines of code to p2. Then,
JC(p1, p2) = JC(p1, p3) = JC(p2, p3) = 0 because one re-
vision is a subset of another revision. Such ties result

p22

p23

p30
p29
p41
p40
(a) Ground truth

p35
p42

p36
p43

p22

p23

p29

p30

p35

p36

p41

p40
(b) Constructed lineage with the use of pseudo timestamps

p42

p43

Figure 8: Error caused by pseudo timestamps in uzbl

 25000

 20000

 15000

 10000

 5000

d
e
t
a
l
u
m
u
c
c
A

e
c
n
a
t
s
i
D
 
c
i
r
t
e
m
m
y
S

2.0.0 2.1.0

2.2.0 2.3.0

1.3.0

1.2.0

1.0.01.1.0

0.7.40.8.00.9.0
 0

01/01/00
Figure 9: Development history of nano

01/01/04

01/01/08

Date

in low accuracy. For example, JC yielded 74.5% mean
accuracy, whereas SD yielded 84.0% mean accuracy in
DAG lineage on goodware.

Pseudo Timestamp.
ILINE computes pseudo times-
tamps by ﬁrst building a straight line lineage and then
use the recovered ordering as timestamps. Since ILINE
achieved fairly high accuracy in straight line lineage, at
ﬁrst we expected this approach to do well in DAG lin-
eage. To our initial surprise, ILINE with pseudo times-
tamps actually performed worse. In retrospect, we ob-
served that since each branch had been developed sep-
arately, it is challenging to determine the precise order-
ing between samples from different branches. For ex-
ample, Figure 8 shows the partial ground truth and the
constructed lineage by ILINE for uzbl with pseudo
timestamps. Although ILINE without pseudo times-
tamps successfully recovered the ground truth lineage,
the use of pseudo timestamps resulted in poor perfor-
mance. The recovered ordering, i.e., pseudo timestamps
were p22, p40, p41, p42, p43, p23, p29, p30, p35, p36. Due to
the imprecise timestamps, the derivative relationships in
the constructed lineage were not accurate.

Revision History vs. Release Date. Correct software
lineage inference on a revision history may not corre-
spond with software release date lineage. For example,
Figure 9 shows the accumulated symmetric distance be-
tween two neighboring releases where a development
branch of nano-1.3 and a stable branch of nano-1.2
are developed in parallel. ILINE infers software lineage
consistent with a revision history.

92  22nd USENIX Security Symposium 

USENIX Association

outlier

 500

Symmetric Distance

 400

 300

e
c
n
a
t
s
i
D
 
c
i
r
t
e
m
m
y
S

 200

 100

 0

 20

 0
 120
Figure 10: An outlier in memcached

 100

 40

 60

 80

 100000

d
e
t
a
l
u
m
u
c
c
A

e
c
n
a
t
s
i
D
 
c
i
r
t
e
m
m
y
S

 80000

 60000

 40000

 20000

Order

sendmail

openssh

grep

redis

redislite

nano

memcached

Threats to Validity. Our malware experiments were
performed on a relatively small data set because of difﬁ-
culties in obtaining the ground truth. Although it is hard
to indicate a representative of modern malware due to its
surreptitious nature, we evaluated our methods on com-
mon malware categories such as bots, worms, and Trojan
horses. To the best of our knowledge, we are the ﬁrst to
take a systematic approach towards software lineage infer-
ence to provide scientiﬁc evidence instead of speculative
remarks.
9 Limitations
Reverting/Refactoring. Regression of code is a chal-
lenging problem in software lineage inference. A revision
adding new functionalities is sometimes followed by sta-
bilizing phases including bug ﬁxes. Bug ﬁxes might be
done by reverting to the previous revision, i.e., undoing
the modiﬁcations of the code.

Some revisions can become outliers because of ILINE’s
greedy construction and reverting/refactoring issues.
In §4.1.3, we propose a technique to detect and process
outliers by looking for peaks of the distance between
two contiguous revisions. For example, ILINE had 70
inversions and 1 EDTM for the contiguous revisions of
memcached. The error came from the 53rd revision that
was incorrectly located at the end of the lineage. Figure 10
shows the symmetric distance between two adjacent revi-
sions in the recovered lineage before we process outliers.
The outlier caused an exceptional peak of the symmetric
distance at the rightmost of the Figure 10. ILINE iden-
tiﬁed such possible outliers by looking for peaks, then
generated the perfect lineage of memcached after han-
dling the outlier.

There can also be false positives among detected out-
liers, i.e., a peak is identiﬁed even revisions are in the
correct order. For example, a peak can be identiﬁed be-
tween two contiguous revisions when there is a huge
update like major version changes. However, such false
positives do not affect overall accuracy of ILINE because
the original (correct) position will be chosen again when
minimizing the overall distance.

Although our technique improves lineage inference, it
may not be able to resolve every case. Unless we design a

 0

 0

 100

 200

 300

 400

 500

Order

Figure 11: Recovered ordering of mixed data set

precise model describing the developers’ reverting/refac-
toring activity, no reasonable algorithm may be able to
recover the same lineage as the ground truth. Rather, the
constructed lineage can be considered as a more practi-
cal/pragmatic representation of the truth.

Root Identiﬁcation.
It is a challenging problem to iden-
tify the correct roots of data sets where we do not have
any knowledge about the compilation process. ILINE suc-
cessfully identiﬁed the correct roots based upon code size
and complexity in all data sets except for some data sets
of actual release binaries. This shows that the Lehman’s
laws of software evolution are generally applicable to root
identiﬁcation, but with a caveat. For example, with actual
release binaries data sets, ILINE achieved 77.8% mean
accuracy with the inferred roots. The accuracy increased
to 91.8% with the knowledge of the correct ﬁrst revision.
In order to improve lineage inference, we can lever-
age “ﬁrst seen” date of malware, e.g., Symantec’s World-
wide Intelligence Network Environment [10] or tool-
chain provenance such as compilers and compilation op-
tions [36].

Clustering. Clustering may not be able to group pro-
gram accurately due to noise or algorithmic limitations. In
order to simulate cases where clustering failed, we mixed
binaries from seven programs including memcached,
redis, redislite, grep, nano, sendmail, and
openssh into one set and ran our lineage inference algo-
rithm on it. As shown in Figure 11, revisions from each
program group located next to each other in the recovered
order (each program is marked in a different color). This
shows ILINE can identify close relationships within the
same program group even with high noise in a data set.
There are multiple intra-program gaps and inter-program
gaps. Relatively big intra-program gaps corresponded to
major version changes of a program where the Jaccard
distances were 0.28–0.66. The Jaccard distances at the
inter-program gaps were much higher, e.g., 0.9–0.95. This
means we can separate the mixed data set into different
program groups based on the inter-program gaps.

USENIX Association  

22nd USENIX Security Symposium  93

Feature Extraction. Although ILINE achieved an over-
all 95.8% mean accuracy in straight line lineage of good-
ware, ILINE achieved only 77.8% mean accuracy with
actual released binaries. In order to improve lineage infer-
ence, future work may choose to leverage better features.
For example, we may use recovered high-level abstraction
of program binaries [41], or we may detect similar code
that was compiled with different compilers and optimiza-
tion options [24].
10 Related Work
While previous research focuses on studying known soft-
ware lineage or development history, our focus is on de-
signing algorithms to create lineage and evaluating met-
rics to assess the quality of constructed lineage.

Belady and Lehman studied software evolution of IBM
OS/360 [3], and Lehman and Ramil formulated eight
laws describing software evolution process [28]. Xie et al.
analyzed histories of open source projects in order to ver-
ify Lehman’s laws of software evolution [45], and God-
frey and Tu investigated the Linux kernel to understand a
software evolution process in open source development
systems [14]. Shihab et al. evaluated the effects of branch-
ing in software development on software quality with
Windows Vista and Windows 7 [42]. Kim et al. studied
the history of code clones to evaluate the effectiveness
of refactoring on software improvement with respect to
clones [25].

Massacci et al. studied the effect of software evolution,
e.g., patching and releasing new versions, on vulnerabil-
ities in Firefox [33], and Jang et al. proposed a method
to track known vulnerabilities in modern OS distribu-
tions [19]. Edwards and Chen statistically veriﬁed that
an increase of security issues identiﬁed by a source code
analyzer in a new release may indicate an increase of
exploitable bugs in a release [11]. Davies et al. proposed
a signature-based matching of a binary against a known
library repository to identify library version information,
which can be potentially used for security vulnerabilities
scans [7].

Gupta et al. studied malware metadata collected by an
anti-virus vendor to describe evolutionary relationships
among malware [16]. Dumitras and Neamtiu studied
malware evolution to ﬁnd new variants of well-known
malware [9]. Karim et al. generated phylogeny models
based upon code similarity to understand how new mal-
ware related to previously seen malware [22]. Khoo and
Lio investigated FakeAV-DO and Skyhoo malware fami-
lies using phylogenetic methods to understand statistical
relationships and to identify families [23]. Ma et al. stud-
ied diversity of exploits used by notorious worms and
constructed dendrograms to identify families and found
non-trivial code sharing among different families [31].
Lindorfer et al. investigated the malware evolution process

by comparing subsequent versions of malware samples
that were collected by exploiting embedded auto-update
functionality [29]. Hayes et al. pointed out the necessity
of systematic evaluation in malware phylogeny systems
and proposed two models to artiﬁcially generate refer-
ence sets of samples: mutation-based model and feature
accretion-based model [17].
11 Conclusion
In this paper, we proposed new algorithms to infer soft-
ware lineage of program binaries for two types of lineage:
straight line lineage and directed acyclic graph (DAG)
lineage. We built ILINE to systematically explore the
entire design space depicted in Figure 1 for software lin-
eage inference and performed over 2,000 different experi-
ments on large scale real-world programs—1,777 good-
ware spanning over a combined 110 years of development
history and 114 malware with known lineage. We also
built IEVAL to scientiﬁcally measure lineage quality with
respect to the ground truth. Using IEVAL, we evaluated
seven different metrics to assess diverse properties of lin-
eage, and recommended two metrics—partial order mis-
matches and graph arc edit distance. We showed ILINE
effectively extracted evolutionary relationships among
program binaries with over 84% mean accuracy for good-
ware and over 72% for malware.
12 Acknowledgment
We would like to thank our shepherd Fabian Monrose
for his support in ﬁnalizing this paper. We also would
like to thank the anonymous reviewers for their insightful
comments. This material is based upon work supported by
Lockheed Martin and DARPA under the Cyber Genome
Project grant FA975010C0170. Any opinions, ﬁndings
and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily
reﬂect the views of Lockheed Martin or DARPA.
References
[1] M. Bailey, J. Oberheide, J. Andersen, F. J. Z. Morley Mao, and
J. Nazario. Automated classiﬁcation and analysis of internet
malware.
In International Symposium on Recent Advances in
Intrusion Detection, September 2007.

[2] U. Bayer, P. M. Comparetti, C. Hlauschek, C. Kruegel, and
E. Kirda. Scalable, behavior-based malware clustering. In Network
and Distributed System Security Symposium, 2009.

[3] L. A. Belady and M. M. Lehman. A model of large program

development. IBM Systems Journal, 15(3):225–252, 1976.

[4] M. Bender, M. Farach-Colton, G. Pemmasani, S. Skiena, and
P. Sumazin. Lowest common ancestors in trees and directed
acyclic graphs. Journal of Algorithms, 57(2):75–94, 2005.

[5] zynamics BinDiff.

http://www.zynamics.com/

bindiff.html. Page checked 5/23/2013.

[6] DARPA-BAA-10-36, Cyber Genome Program. https://www.

fbo.gov/spg/ODA/DARPA/CMO/DARPA-BAA-10-36/
listing.html. Page checked 5/23/2013.

94  22nd USENIX Security Symposium 

USENIX Association

[7] J. Davies, D. M. German, M. W. Godfrey, and A. Hindle. Software
bertillonage: ﬁnding the provenance of an entity.
In Working
Conference on Mining Software Repositories, New York, New
York, USA, 2011.

[8] F. de la Cuadra. The geneology of malware. Network Security,

2007(4):17–20, 2007.

[9] T. Dumitras and I. Neamtiu. Experimental challenges in cyber
security: a story of provenance and lineage for malware. In Cyber
Security Experimentation and Test, 2011.

[10] T. Dumitras and D. Shou. Toward a standard benchmark for
computer security research: the worldwide intelligence network
environment (WINE). In Building Analysis Datasets and Gather-
ing Experience Returns for Security, 2011.

[11] N. Edwards and L. Chen. An historical examination of open
source releases and their vulnerabilities. In ACM Conference on
Computer and Communications Security, 2012.

[12] H. Flake. Structural comparison of executable objects. In IEEE
Conference on Detection of Intrusions, Malware, and Vulnerability
Assessment, 2004.

[13] M. Fredrikson, S. Jha, M. Christodorescu, R. Sailer, and X. Yan.
Synthesizing Near-Optimal Malware Speciﬁcations from Suspi-
cious Behaviors. In IEEE Symposium on Security and Privacy,
2010.

[14] M. W. Godfrey and Q. Tu. Evolution in open source software: A
case study. In International Conference on Software Maintenance,
2000.

[15] F. Guo, P. Ferrie, and T.-C. Chiueh. A study of the packer problem
and its solutions. In International Symposium on Recent Advances
in Intrusion Detection, 2008.

[16] A. Gupta, P. Kuppili, A. Akella, and P. Barford. An empirical
In International Communication

study of malware evolution.
Systems and Networks and Workshops, 2009.

[17] M. Hayes, A. Walenstein, and A. Lakhotia. Evaluation of malware
phylogeny modelling systems using automated variant generation.
Journal in Computer Virology, 5(4):335–343, July 2008.

[18] X. Hu, T. Chiueh, and K. G. Shin. Large-scale malware indexing
using function call graphs. In ACM Conference on Computer and
Communications Security, 2009.

[19] J. Jang, A. Agrawal, and D. Brumley. ReDeBug: ﬁnding un-
patched code clones in entire os distributions. In IEEE Symposium
on Security and Privacy, 2012.

[20] J. Jang, D. Brumley, and S. Venkataraman. BitShred: feature
hashing malware for scalable triage and semantic analysis. In
ACM Conference on Computer and Communications Security,
2011.

[21] M. G. Kang, P. Poosankam, and H. Yin. Renovo: A hidden code
extractor for packed executables. In ACM Workshop on Rapid
Malcode, 2007.

[22] M. E. Karim, A. Walenstein, A. Lakhotia, and L. Parida. Malware
phylogeny generation using permutations of code. Journal in
Computer Virology, 1:13–23, 2005.

[23] W. M. Khoo and P. Lio. Unity in diversity: Phylogenetic-inspired
techniques for reverse engineering and detection of malware fami-
lies. In SysSec Workshop, 2011.

[24] W. M. Khoo, A. Mycroft, and R. Anderson. Rendezvous: A
search engine for binary code. In Working Conference on Mining
Software Repositories, 2013.

[25] M. Kim, V. Sazawal, D. Notkin, and G. C. Murphy. An empirical
study of code clone genealogies. In European software engineer-
ing conference - Foundations of software engineering, 2005.

[26] J. Z. Kolter and M. A. Maloof. Learning to detect and classify
malicious executables in the wild. Journal of Machine Learning
Research, 7:2721–2744, 2006.

[27] C. Kruegel, W. Robertson, F. Valeur, and G. Vigna. Static disas-

sembly of obfuscated binaries. In USENIX Security Symposium,
2004.

[28] M. M. Lehman and J. F. Ramil. Rules and tools for software evo-
lution planning and management. Annals of Software Engineering,
11(1):15–44, 2001.

[29] M. Lindorfer, A. Di Federico, F. Maggi, P. M. Comparetti, and
S. Zanero. Lines of malicious code: insights into the malicious
software industry.
In Annual Computer Security Applications
Conference, 2012.

[30] C. Linn and S. Debray. Obfuscation of executable code to improve
resistance to static disassembly. In ACM Conference on Computer
and Communications Security, 2003.

[31] J. Ma, J. Dunagan, H. J. Wang, S. Savage, and G. M. Voelker.
Finding diversity in remote code injection exploits. In ACM SIG-
COMM on Internet Measurement, 2006.

[32] L. Martignoni, M. Christodorescu, and S. Jha. OmniUnpack: fast,
generic, and safe unpacking of malware. In Annual Computer
Security Applications Conference, 2007.

[33] F. Massacci, S. Neuhaus, and V. H. Nguyen. After-life vulnera-
bilities: a study on ﬁrefox evolution, its vulnerabilities, and ﬁxes.
In International Conference on Engineering Secure Software and
Systems, 2011.

[34] T. J. McCabe. A complexity measure. IEEE Transactions on

Software Engineering, SE-2(4):308–320, 1976.

[35] K. Rieck, P. Trinius, C. Willems, and T. Holz. Automatic analysis
of malware behavior using machine learning. Journal of Computer
Security, 19(4):639–668, 2011.

[36] N. Rosenblum, B. P. Miller, and X. Zhu. Recovering the toolchain
provenance of binary code. In International Symposium on Soft-
ware Testing and Analysis, 2011.

[37] C. Rossow, C. J. Dietrich, C. Grier, C. Kreibich, V. Paxson,
N. Pohlmann, H. Bos, and M. V. Steen. Prudent Practices for
Designing Malware Experiments: Status Quo and Outlook. In
IEEE Symposium on Security and Privacy, 2012.

[38] P. Royal, M. Halpin, D. Dagon, R. Edmonds, and W. Lee. PolyUn-
pack: automating the hidden-code extraction of unpack-executing
malware. In Computer Security Applications Conference, Decem-
ber 2006.

[39] A. Sæ bjørnsen, J. Willcock, T. Panas, D. Quinlan, and Z. Su.
In International

Detecting code clones in binary executables.
Symposium on Software Testing and Analysis, 2009.

[40] S. Schleimer, D. Wilkerson, and A. Aiken. Winnowing: Local
algorithms for document ﬁngerprinting. In ACM SIGMOD/PODS
Conference, 2003.

[41] E. J. Schwartz, J. Lee, M. Woo, and D. Brumley. Native x86
decompilation using semantics-preserving structural analysis and
iterative control-ﬂow structuring. In USENIX Security Symposium,
2013.

[42] E. Shihab, C. Bird, and T. Zimmermann. The effect of branch-
ing strategies on software quality. In ACM/IEEE International
Symposium on Empirical Software Engineering and Measurement,
2012.

[43] Symantec.

Symantec internet security threat report, vol-
ume 17. http://www.symantec.com/threatreport/,
2012. Page checked 5/23/2013.

[44] K. Weinberger, A. Dasgupta, J. Langford, A. Smola, and J. At-
tenberg. Feature hashing for large scale multitask learning. In
International Conference on Machine Learning, 2009.

[45] G. Xie, J. Chen, and I. Neamtiu. Towards a better understanding of
software evolution: An empirical study on open source software.
In IEEE International Conference on Software Maintenance, 2009.
[46] Y. Ye, T. Li, Y. Chen, and Q. Jiang. Automatic malware catego-
rization using cluster ensemble. In ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, 2010.

USENIX Association  

22nd USENIX Security Symposium  95

A Appendix
A.1 Straight Line Lineage

Distance Metric

Features

SD
WSD
DC
JD
JC

Multi

Mean accuracy with the inferred root Mean accuracy with the real root
Inversion Accuracy
ED
6.0
95.8%
6.7
95.4%
8.4
93.7%
8.4
93.7%
93.0%
9.1

Inversion Accuracy
98.4%
98.1%
97.1%
97.1%
97.1%

ED
8.6
9.0
9.7
9.7
12.2

Table 8: Mean accuracy for straight line lineage on goodware

Distance Metric

Features

SD
WSD
DC
JD
JC
SD
WSD
DC
JD
JC

Static
Multi

Dynamic

Multi

Mean accuracy with the inferred root (=real root)
ED
0.9
1.3
0.9
0.9
3.1
2.6
2.9
2.9
2.9
4.1

Inversion Accuracy
97.8%
94.2%
98.2%
98.2%
84.3%
86.7%
80.0%
85.5%
85.5%
70.9%

Table 9: Mean accuracy for straight line lineage on malware

A.2 DAG Lineage

Distance Metric

Features

SD
WSD
DC
JD
JC

Multi

Mean accuracy with no prior information Mean accuracy with real timestamp
GAED
20.3
23.0
20.0
20.0
35.0

PO Accuracy
84.0%
82.6%
83.8%
83.8%
74.5%

GAED
52.4
57.3
56.1
56.1
90.0

PO Accuracy
91.1%
90.0%
91.1%
91.1%
90.6%

Table 10: Mean accuracy for DAG lineage on goodware

Distance Metric

Features

SD
WSD
DC
JD
JC
SD
WSD
DC
JD
JC

Static
Multi

Dynamic

Multi

Mean accuracy with no prior information Mean accuracy with real timestamp
GAED
6.0
5.5
6.0
6.0
9.5
13.0
12.5
12.5
12.5
12.5

PO Accuracy
69.5%
72.0%
69.5%
69.5%
50.8%
61.4%
62.2%
59.8%
59.8%
55.3%

PO Accuracy
87.0%
90.2%
87.0%
87.0%
86.6%
70.3%
76.4%
72.8%
72.8%
72.8%

GAED
8.5
8.5
8.5
8.5
19.5
17.0
17.0
19.0
19.0
17.5

Table 11: Mean accuracy for DAG lineage on malware

96  22nd USENIX Security Symposium 

USENIX Association

