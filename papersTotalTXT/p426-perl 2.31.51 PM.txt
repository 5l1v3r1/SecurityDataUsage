VCCFinder: Finding Potential Vulnerabilities in
Open-Source Projects to Assist Code Audits

Henning Perl*,

Sergej Dechand†,
Matthew Smith*†

* Fraunhofer FKIE, Germany
† University of Bonn, Germany

Daniel Arp,

Fabian Yamaguchi,

Konrad Rieck

University of Göttingen,

Germany

Sascha Fahl,
Yasemin Acar

Saarland University, Germany

ABSTRACT
Despite the security community’s best eﬀort, the number
of serious vulnerabilities discovered in software is increasing
rapidly. In theory, security audits should ﬁnd and remove
the vulnerabilities before the code ever gets deployed. How-
ever, due to the enormous amount of code being produced,
as well as a the lack of manpower and expertise, not all code
is suﬃciently audited. Thus, many vulnerabilities slip into
production systems. A best-practice approach is to use a
code metric analysis tool, such as Flawﬁnder, to ﬂag poten-
tially dangerous code so that it can receive special attention.
However, because these tools have a very high false-positive
rate, the manual eﬀort needed to ﬁnd vulnerabilities remains
overwhelming.

In this paper, we present a new method of ﬁnding poten-
tially dangerous code in code repositories with a signiﬁcantly
lower false-positive rate than comparable systems. We com-
bine code-metric analysis with metadata gathered from code
repositories to help code review teams prioritize their work.
The paper makes three contributions. First, we conducted
the ﬁrst large-scale mapping of CVEs to GitHub commits
in order to create a vulnerable commit database. Second,
based on this database, we trained a SVM classiﬁer to ﬂag
suspicious commits. Compared to Flawﬁnder, our approach
reduces the amount of false alarms by over 99 % at the same
level of recall. Finally, we present a thorough quantitative
and qualitative analysis of our approach and discuss lessons
learned from the results. We will share the database as
a benchmark for future research and will also provide our
analysis tool as a web service.

Categories and Subject Descriptors
D.2.4 [Software Engineering]: Software/Program Veriﬁ-
cation; K.6.5 [Management of Computing and Infor-
mation Systems]: Security and Protection

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
c(cid:13) 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813604.

Keywords
Vulnerabilities; Static Analysis; Machine Learning

1.

INTRODUCTION

Despite the best eﬀort of the security community, the
number of serious vulnerabilities discovered in deployed soft-
ware is on the rise. The Common Vulnerabilities and Expo-
sures (CVE) database operated by MITRE tracks the most
serious vulnerabilities.
In 2000, around 1,000 CVEs were
registered. By 2010, there were about 4,500. In 2014, al-
most 8,000 CVEs were registered. The trend seems to be
increasing in speed.

While it is considered a best practice to perform code
reviews before code is released, as well as to retroactively
checking old code, there is often not enough manpower to
rigorously review all the code that should be reviewed. Al-
though open-source projects have the advantage that any-
body can, in theory, look at all the source code, and although
bug-bounty programs create incentives to do so, usually only
a small team of core developers reviews the code.

In order to support code reviewers in ﬁnding vulnerabili-
ties, tools and methodologies that ﬂag potentially dangerous
code are used to narrow down the search. For C-like lan-
guages, a wide variety of code metrics can raise warning
ﬂags, such as a variable assigned inside an if-statement or
unreachable cases in a switch-statement. The Clang static
analyzer [1] as well as the dynamic analyzer Valgrind [3] and
others, can pinpoint further pitfalls such as invalid mem-
ory access. For the Linux kernel, the Trinity system call-
fuzzer [2] has found and continues to ﬁnd many bugs. Fi-
nally, static analysis tools like Flawﬁnder [34] help ﬁnd pos-
sible security vulnerabilities.

Most of these approaches operate on an entire software
project and deliver a (frequently very large) list of poten-
tially unsafe code. However, software grows incrementally
and it is desirable to have tools to assist in reviewing these
increments as well as tools to check entire projects. Most
open-source projects manage their source code with version
control systems (VCS) such as Git, Mercurial, CVS or Sub-
version. In such systems, code – including vulnerable code
– is inserted into the software in the form of commits to the
repository. Therefore, the natural unit upon which to check
whether new code is dangerous is the commit. However,
most existing tools cannot simply be executed on code snip-
pets contained within a commit. Thus, if a code reviewer
wants to check the security of a commit, the reviewer must

426execute the analysis software on the entire project and then
check if any of the warnings relate to the commit. This can
be a considerable amount of work, especially since many
tools require source code to be annotated and dynamic tests
would have to be constructed in a way that triggers the
commit.

Static and dynamic code analysis tools focus exclusively
on the code without the context of who wrote the code and
how it was committed. However, code repositories contain
a wealth of metadata which can be highly relevant to the
code quality. For instance, it can be seen whether a com-
mitter is new to the project or if they are one of the core
contributors. It is possible to see the time of day or night
at which code was submitted and to monitor the activity of
development in certain regions of code. Moreover, most ex-
isting code-metric-based tools have very high false-positive
rates, creating a (sometimes impossibly) high workload and
undermining trust in the eﬀectiveness of the tools. For in-
stance, Flawﬁnder tool created 5,460 false positives warnings
for only 53 true positives on the dataset used in this paper.
It is intuitively clear that code reviewers who want to ﬁnd
53 vulnerabilities in a set of 5,513 ﬂagged commits have a
tough time ahead of them.

In this paper, we present a classiﬁer that can identify po-
tentially vulnerable commits with a signiﬁcantly lower false-
positive rate while retain high recall rates. Therefore, unlike
most existing tools for vulnerability ﬁnding, we don’t focus
solely on code metrics, but also leverage the rich metadata
contained in code repositories.

To evaluate the eﬀectiveness of our approach, we conduct
a large-scale evaluation of 66 GitHub projects with 170,860
commits, gathering both metadata about the commits as
well as mapping CVEs to commits to create a database of
vulnerability-contributing commits (VCCs) and a benchmark
for future research.

We conducted a statistical analysis of these VCCs and
trained a Support Vector Machine (SVM) to detect them
based on the combination of code metric analysis and Git-
Hub metadata. For our evaluation we trained our classiﬁer
only on data up to December 31, 2010 and ran our tests
against CVEs discovered in 2011–2014.

In this dataset, our approach, called VCCFinder, pro-
duces only 36 false positives compared to Flawﬁnder’s 5,460
at the same level of recall. This is a reduction of over 99 %
and signiﬁcantly eases the workload of code reviewers.
1.1 Our Contributions

In summary, we make the following contributions in this

paper:

• We present VCCFinder, a code analysis tool that ﬂags
suspicious commits by using a SVM-based detection
model. Our method outperforms Flawﬁnder by a great
margin, reducing the false positives by over 99 % at the
same level of recall. Our methodology is suited to work
on code snippets, enabling us to analyse code at the
commit level and making a lightweight analysis of new
code far easier than requiring a full build environment
to be set up for each test.

• We construct the ﬁrst large-scale database mapping
CVEs to vulnerability-contributing commits (VCCs).
The database contains 66 GitHub projects, 170,860
commits and 640 VCCs. We conduct an extensive eval-

uation of the methodology used to create this database
to ascertain its quality as a benchmark for future re-
search.

• We present an extensive quantitative and qualitative
evaluation of VCCFinder and discuss take-aways, in-
cluding, for instance that, from a security perspective,
gotos are not generally harmful but in combination
with error-handling code they are responsible for a sig-
niﬁcant number of VCCs.
2. RELATED WORK

The discovery of vulnerabilities in program code is a fun-
damental problem of computer security. Consequently, it
has received much attention in the past. In the following,
we give a sample of the prior work most closely related to
our approach.

Static analysis.

The set of static analysis tools can be thought of as a spec-
trum ranging from faster, lightweight approaches to slower
but more thorough techniques. With VCCFinder being a
lightweight tool, we compare ourselves to FlawFinder [34],
a prominent representative of this class of tools. Other
lightweight approaches include Rats [9], Prefast [8] as well
as Splint [10], the later requiring manual annotations.

Regarding more thorough approaches Bandhakavi et al.
[11] search for vulnerabilities in browser extensions by apply-
ing static information-ﬂow analysis to the JavaScript code.
Dahse and Holz [15] introduced a static analyzer for PHP
that can detect sophisticated attacks against web applica-
tions. Finally, commercial tools like Coventry [5], Fortify [6],
CodeSonar [4], and IBM Security AppScan Source (formerly
Rational) [7] focus on a thorough analysis with conﬁgurable
rulesets and consequently long run times.

Symbolic execution.

Cadar et al. [12] present KLEE, a symbolic execution tool,
which requires manual annotation and modiﬁcation of the
source code. Also the runtime grows exponentially with the
number of paths in the program, which limits the size of
project which can be tested with KLEE. Thus it is not fea-
sible to execute KLEE on the same scale as VCCFinder.
However, it is an interesting area of future work to execute
KLEE as a second step after VCCFinder. Klee would then
only be used on the commits ﬂagged by VCCFinder which
hopefully would signiﬁcantly reduce the eﬀort needed to run
KLEE. We see these tools as complementary and separate
steps in the tool chain.

Dynamic analysis.

Cho et al. [14] use a combination of symbolic and con-
crete execution to build an abstract model of the analyzed
application and ﬁnd vulnerabilities in several open-source
projects. Yamaguchi et al. [37] provide an analysis plat-
form oﬀering fuzzy parsing of code that generates a graph
representing code suitable to be mined with graph-database
queries. This approach allows application-speciﬁc vulnera-
bility patterns to be expressed; however, in contrast to our
approach, it requires manual speciﬁcation of these patterns
by the analyst. Holler et al. [20] used fuzzing on code frag-
ments to ﬁnd vulnerabilities in the Mozilla JavaScript inter-
preter and the PHP interpreter.

427Software metrics.

Several authors have proposed to employ software met-
rics to home in on regions of code more likely to contain
vulnerabilities. For example, Zimmermann et al. [38] per-
form a large-scale empirical study on Windows Vista, indi-
cating that metrics such as code churn, code complexity [see
22, 19] and organizational measures allow vulnerabilities to
be detected with high precision at low recall rates, while
code dependency measures achieve low precision at high re-
call rates. However, Graylin et al. [18] point out that many
of these metrics may be highly correlated with lines of code.
In particular, they show empirically that the relation be-
tween cyclomatic complexity and lines of code is near-linear,
meaning that no reduction in the amount of code to read is
achieved in this way.

Repository analysis.

There is a range of research work looking at software repo-
sitories in relation to software vulnerabilities. The most rel-
evant with respect to our project can be divided into two
groups: those that look at code metrics and those that look
at metadata.

Neuhaus et al. [26] use the vulnerability database of the
Mozilla project to extract which software components have
had vulnerabilities in the past and which imports and func-
tion calls were involved. They use this to predict which soft-
ware components of the Mozilla Internet suite are most likely
to contain more vulnerabilities. Unlike our approach, they
do not use any metadata in their analysis and the results
ﬂag entire software components rather than single commits.
The results are thus more generic in the sense that they can
say only that one set of software components is more worth
checking than others.

On the other side, work conducted by Meneely et al. and
Shin et al. analyzes diﬀerent code repository metadata in
relation to CVEs [25, 23, 24]. Speciﬁcally, they check how
features such as code churn, lines of code, or the number of
reviewers from a project’s repository and review system data
correlate to reported vulnerabilities. They do this manually
for the Mozilla Firefox Browser, Apache HTTP server and
an excerpt of the RHEL Linux kernel. Unlike the work above
and our work, they do not use this data to predict vulnera-
bilities; moreover, unlike our work, they do not combine the
features but look at each separately.

Sadeghi et al. [28] aim to reduce the number of rules used
by static analysis software. For this they looked at “catego-
rized software repositories” (i.e. the Google Play Store) and
evaluated how knowledge of the app’s category can reduce
the number of static analysis rules needed to still retain full
coverage. For this, they compared Java programs on Source-
Forge (without a framework) to Android apps on F-Droid
(written with an application development framework). From
the app’s category they were able to build a predictor that
helps pick a subset of static analyzer rules to apply; therefore
reducing the time the static analyzer needs. Their method
works especially well with apps using a framework, such as
Android apps. In contrast, while this work reduces the num-
ber of rules used for analysis, we prioritize the code needed
to be analysed. These approaches are complementary.

Wijayasekara et al. [35] used bug-trackers to study bugs
that afterwards have been identiﬁed as vulnerabilities. The
work does not deal with ﬁnding unknown vulnerabilities.

While this does not directly relate to our work, bug-trackers
are an interesting additional source of information.

Kim et al. [21] mined logs of a project’s SCM repository for
bug-introducing changes using ﬁxed keywords (such as “ﬁx”
or “bug”). They then extracted features from these commits
and trained an SVM. Our approach diﬀers from the authors’
in three ways: ﬁrst, we use as a base of our research the much
smaller and thus harder set of critical vulnerabilities mined
from the CVE database; second, we use additional features
gathered from the SCM history such as past/future diﬀerent
authors; third we use a historical split to evaluate our system
opposed to a (random) ten-fold cross validation. The later
is important since it guarantees that our system was not
trained on future commits to decide if some past commit
introduced a bug. Unfortunately neither the code base nor
the data is available so a direct comparison is not possible.
We re-ran our experiments using random cross validation
and found that it increased the precision for around 15 %
with a recall between 0.4 and 0.6.

´Sliwerski et al. [32] present preliminary results on a sta-
tistical analysis of the Eclipse and Mozilla projects. They
mined the Eclipse project’s bug database for ﬁx-inducing
changes. Then, they do a statistical analysis on their data,
showing that most bugs were committed on Fridays. Our
work goes beyond these results in several ways. The sta-
tistical analysis is more extensive, is done on a much larger
dataset and is only the ﬁrst step in our system. Based on
the results we train a SVM and create an adaptable system
to predict vulnerability inducing commits.

Thus, our work goes beyond the above approaches in sev-
eral ways. We combine both code metrics as well as meta-
data in our analysis and use a machine-learning approach
to extract and combine relevant features as well as to cre-
ate a classiﬁcation engine to predict which commits are more
likely to be vulnerable. In contrast to the work above, we do
this for a large set of projects in an automated way instead
of hand-picking features and analyzing single projects.

Machine-learning techniques.

Machine-learning and data-mining approaches have been
proposed by several authors for ﬁnding vulnerabilities. For
example, Scandariato et al. [31] train a classiﬁer on textual
features extracted from source code to determine vulnera-
ble software components. Moreover, several unsupervised
machine-learning approaches have been presented to assist
in the discovery of vulnerabilities. For example, Yamaguchi
et al. [36] introduce a method to expose missing checks in
C source code by combining static tainting and techniques
for anomaly detection. Similarly, Chang et al. [13] present
a data-mining approach to reveal neglected conditions and
discover implicit conditional rules and their violations.

However, in order to identify vulnerabilities, these ap-
proaches concentrate only on features extracted from source
code. In contrast, we show that additional meta informa-
tion, such as the experience of a developer, are valuable
features that improve detection performance.
3. METHODOLOGY

In this section, we describe how we created a database
of commits that introduced known vulnerabilities in open-
source projects and which features we extracted from the
commits. We will share this database with the research
community as a baseline to enable a scientiﬁc comparison

428between competing approaches. We focus on 66 C and
C++ projects using the version control system Git (see ap-
pendix A for the list). These 66 projects contain 170,860
commits and 718 vulnerabilities reported by CVEs.
3.1 Vulnerability-contributing Commits

In order to analyze the common features of commits that
introduce vulnerabilities, we ﬁrst needed to ﬁnd out which
commits actually introduced vulnerabilities. To the best
of our knowledge, no large-scale database exists that maps
vulnerabilities as reported by CVEs to commits. Meneely
et al. and Shin et al.
[25, 23, 24] manually created such
mappings for the Mozilla Firefox Browser, Apache HTTP
server and parts of the RHEL Linux kernel. We contacted
the authors to inquire whether they would share this data,
since we could have used that as a baseline for our larger
analysis. Unfortunately, this was not possible at the time,
although the data might be released in the future. To create
a freely available database for ourselves and the research
community, we set out to create a method to automatically
map CVEs to vulnerability-contributing commits.1

Since at this point we are only interested in CVEs relating
to projects hosted on Github, we utilized two data sources
as starting points for our mapping. As a ﬁrst source, we
selected all CVEs containing a link to a commit of one of the
66 projects ﬁxing a vulnerability as part of the “proof”. As a
second source for ﬁxing commits, we created a crawler that
searches commit messages of the 66 projects for mentions of
CVE IDs. To check the accuracy of our mapping we took a
random sample of 10 % and manually checked the mapping
and found no incorrectly mapped CVEs. This gave us a
list of 718 CVEs. This list is potentially not complete since
there might be CVEs that do not link to the ﬁxing commit
and which are also not mentioned in the commit messages.
However, this does not represent a problem for our approach
since 718 is a large enough sample to train our classiﬁer.

We then developed and tested a heuristic to proceed from
these ﬁxing commits to the vulnerability-contributing com-
mits (VCCs). Recall that we are operating on Git commits,
which means that we have access to the whole history of a
given project. One (appropriately named) Git subcommand
is git blame, which, given a ﬁle, for each line names the
commit that last changed the line. The heuristic for ﬁnding
the commit that introduced a vulnerability given a commit
that ﬁxed it is as follows:

1. Ignore changes in documentation such as release notes

or change logs.

2. For each deletion, blame the line that was deleted.

Rationale: If the ﬁx needed to change the line, that
often means that it was part of the vulnerability. Note
that Git diﬀs only know of added and deleted lines. If
a line was changed, it shows up as a deletion and an
addition in the diﬀ.

uploaded

the

have

anonymously

1We
database
to
https://www.dropbox.com/s/x1shbyw0nmd2x45/
vcc-database.dump?dl=0 so the reviewers can access
the raw data during the review process. We will release the
data to the community together with the paper. The ﬁle
was created using pg_dump and can be read into a database
using pg_restore. The dump ﬁle will create the three
tables cves, commits and repositories in the schema export.

3. For every continuous block of code inserted in the ﬁx-
ing commit, blame the lines before and after the block

Rationale: Security ﬁxes are often done by adding ex-
tra checks, often right before an access or after a func-
tion call.

4. Finally, mark the commit vulnerable that was blamed
most in the steps above. If two commits were blamed
for the same amount of lines, blame both.

Our heuristic maps the 718 CVEs of our dataset to 640
VCCs. The reason we have fewer VCCs than CVEs is that
a single commit can induce multiple CVEs. To estimate the
accuracy of our heuristic, we took a 15 % random sample
of all VCCs ﬂagged by our heuristic (i.e. 96 VCCs) and
manually checked them. We found only three cases (i.e.
3.1 %) where our heuristic blamed a wrong commit for the
vulnerability. All three of the mis-mappings occurred in
very large commits. For example, one commit of libtiﬀ2
that ﬁxes CVE-2010-1411 also upgrades libtool to version
2.2.8. The method we propose for VCCFinder is capable of
dealing with noisy datasets, so for the purpose of this work,
an error rate of 3.1 % is acceptable. However, improving our
blame heuristics further is an interesting avenue for future
research.

Apart from the 640 VCCs, we have a large set of 169,502
unclassiﬁed commits. We name these commits unclassiﬁed,
since, while no CVE points to them, they might still contain
unknown vulnerabilities.

At this point we now have a large dataset mapping CVEs
to vulnerability-contributing commits. Our goal now is to
extract features from these VCCs in order to detect further
potential VCCs in the large number of unclassiﬁed commits.
3.2 Features

First we extracted a list of characteristics that we hypoth-
esized could distinguish commits. One of our central hy-
potheses is that combining code metrics with GitHub meta-
data features is beneﬁcial for ﬁnding VCCs. First, we test
each feature separately using statistical analysis, e.g. for
each feature we measured whether the distribution of this
feature within the class of vulnerable commits was statisti-
cally diﬀerent from the distribution within all unclassiﬁed
commits.

Here is a list of hypotheses concerning metadata we started

with:

• New committers are more likely to introduce security

bugs than frequent contributors.

• It is good to “commit early and often” according to
the Git Best Practices3. Therefore, longer commits
may be more suspicious than shorter ones.

• Code that has been iterated over frequently, possibly
by many diﬀerent authors,
is more suspicious than
code that doesn’t change often. Meneely and Williams
[23] already analyzed these code churn features in their
work. We integrate and combine these features below.

Table 1 shows a list of all features along with a statistical
evaluation (cf. Section 3.4) of all numerical features except
2https://GitHub.com/vadz/libtiﬀ/commit/31040a39
3http://sethrobertson.GitHub.io/GitBestPractices/
#commit

429Feature

Scope

mean
VCCs

mean
others

U eﬀect size

Number of commits
Repository
Number of unique contributors Repository

282 171.39
524.99

103 980.95
236.90

32143126*
30528184*

Contributions in project

Additions
Deletions
Past changes
Future changes
Past diﬀerent authors
Future diﬀerent authors
Hunk count
Commit message 1
Commit patch 1
Keywords 2

Added functions
Deleted functions
Modiﬁed functions

Author

Commit
Commit
Commit
Commit
Commit
Commit
Commit
Commit
Commit
Commit

Function
Function
Function

5 %

306.19
73.93
627.17
792.46
40.16
136.58
17.68
—
—
—

6.51
1.07
6.79

15 % 31263040*

71.54
37.46
385.53
396.63
22.70
51.44
9.88
—
—
—

1.03
0.49
3.59

20215148*
42983290*
40715632*
36261346*
40292116*
29534644*
32348343*
—
—
—

28724694*
50084674*
41446509*

40 %
43 %

42 %

62 %
20 %
24 %
33 %
25 %
45 %
40 %
—
—
—

46 %
7 %
23 %

1 These features are text-based and thus not considered in the statistical analysis.
2 See Table 2 for a statisical analyis of each keyword.

Table 1: Overview of the features and results of the statistical analysis of the numeric
features. Mann–Whitney U test signiﬁcant (*) if p < 0.00059.

for project-scoped features. In the following, we discuss the
features. For brevity reasons, we omit the discussion of self-
explaining features here.All our analyses are based on com-
mits. A commit can contain changes to one or more ﬁles.
The metrics about ﬁles and functions are aggregated in the
corresponding commit.

Features scoped by project are obviously the same for ev-
ery commit in that project. However, in combination with
other commit-based features, these can still become relevant.
3.2.1 Features Scoped by Project

Programming language The primary language the pro-
ject is written in, as determined by GitHub through
their open-source linguist library. In our analysis, we
focused on projects written in either C or C++. The
main reason for limiting our focus to one language was
that we wanted to ensure comparability between the
features extracted from the commit patches. When
mixing diﬀerent languages and syntaxes, this can’t be
ensured. We chose C and C++ speciﬁcally since many
security-relevant projects (Linux, Kerberos, OpenSSL,
etc.) are written in these languages.

Star count (number) The number of stars the project
has received on GitHub. Stars are a user’s way of keep-
ing track of interesting projects, as starred projects
show up on the own proﬁle page.

Fork count (number) To fork a project on GitHub means
copying the repository under your personal namespace.
This is often the ﬁrst step to contributing back to the
project by then making changes under the personal
namespace and sending a pull request to the oﬃcial
repository.

Number of commits (number) We counted the number
of commits that are reachable from the main branches

HEAD. The canonical main branch is “master”, but
some projects like bestpractical/rt use “stable” as the
default branch. In those cases we used the branch set
at GitHub by the maintainer of the project.

3.2.2 Features Scoped by Author

Contributions (percentage) How many commits the au-
thor has made in this project in percent, i.e. the num-
ber of commits authored divided by the number of to-
tal commits.

3.2.3 Features Scoped by Commit

Number of Hunks (number) As a hunk is a continuous
block of changes in a diﬀ, this number assesses how
fragmented the commit is (i.e. lots of changes all over
the project versus one big change in one ﬁle or func-
tion).

Patch (text) All changes made by the commit as text rep-

resented as a bag of words.

Patch keywords (number) For each patch, we counted
the number of occurrences of each C/C++ keyword.
See Table 2 for a statistical analysis of the diﬀerent
distributions of each keyword.

3.2.4 Features Scoped by File

Future changes (number) If the commit at hand is not
the most current one, this is the number of times the
ﬁle will be changed by later commits. We only use
this feature for our historical analysis and not for the
classiﬁer, since this feature is naturally not available
for new commits.

430Keyword VCCs

mean mean
others

eﬀect
size

U

if
int
struct
return
static
void
unsigned
goto
sizeof
break
char

39.00
31.30
32.38
18.76
15.17
12.52
8.66
5.92
4.37
5.56
6.71

7.82
7.02
3.66
3.60
3.58
4.31
1.51
0.43
0.78
0.84
2.68

37013390*
39930128*
39729656*
41342834*
45382955*
63935365*
64440969*
64798818*
66764357*
74389604*
93400907*

70 %
68 %
68 %
67 %
64 %
49 %
48 %
48 %
46 %
40 %
25 %

Table 2: Statistical analysis of C/C++ keywords
sorted by eﬀect size [33], Mann–Whitney U test sig-
niﬁcant (*) if p < 0.000357.

3.3 Excluded Features

As can be seen in Table 1, the vast majority of the fea-
tures depend only on data gathered from the version control
system and not from additional information on GitHub or
any other platform. In fact, we left out some features that
were only available on some projects or for few commits since
the data was too sparse to reveal anything reliable. We will
brieﬂy discuss why we excluded some features which might
seem counter-intuitive.

One feature that would be promising but which we did
not include was issue tracker information. GitHub provides
an issue tracker and even links texts like “ﬁxes #123” in the
commit message to the corresponding issue. However, the
projects which use this feature tend to be smaller projects,
while the older and larger projects for which we have a rich
set of CVE data predominantly use an external issue tracker.
Thus, this feature is not useful for us at this time.

Another piece of information that is interesting – but un-
fortunately too sparse at the moment – is the content of the
discussion surrounding the inclusion of a change into the
main repository. For this information, features could be the
length of the discussion, the number of people involved, or
the mean experience (in terms of contributions) of the people
involved. Projects that use GitHub’s functionalities exten-
sively often do this through “pull requests”. A contributor
submits a commit to his own, unoﬃcial repository and sub-
sequently notiﬁes the maintainer of the oﬃcial repository to
pull in the changes he made. GitHub provides good support
for this work ﬂow, including the ability to make comments
on a pending pull request. Although this data could be use-
ful for the classiﬁcation of commits, at this point, too few
projects use this work ﬂow to be useful.
3.4 Statistical Analysis of Features

For each numerical feature, we wanted to assess its ﬁt-
ness with respect to distinguishing VCCs from unclassiﬁed
commits. We used the Mann–Whitney U test4 in order to
compare the distribution of a given feature within the set
of commits with vulnerabilities against the set of all unclas-
siﬁed commits. The null hypothesis states that the feature
is distributed independently from whether the commit con-

4The Mann–Whitney U test is used to test whether a value
is distributed diﬀerently between two populations.

tained a bug or not. If we can reject the null hypothesis,
the feature is distributed diﬀerently in each set and thus
is a promising candidate as input for the machine-learning
algorithms.

We used the Bonferroni correction to correct for multi-
ple testing for the 17 features we tested. Therefore, we test
against the stricter signiﬁcance level of 0.00059, which corre-
sponds to a non-corrected p ≤ 0.01 for each individual test.
The date and time features (project age and commit with
time zone) were converted to numerical features based on
seconds that have elapsed since January 1, 1970 UTC (Unix
epoch).

3.4.1 Features Scoped by Project
These features were attributed to the commit depending
on the project the commit was taken from. Since all commits
from a repository, whether containing vulnerabilities or not,
have the same features, these features are too broad to ac-
tually distinguish commits. However, they can be valuable
in combination with other features later on. For brevity, we
do not discuss the features on their own here, though the
table shows the signiﬁcance testing.

3.4.2 Patch Keyword Features
For each commit we counted the occurrences of each of the
following 28 C/C++ keywords: bool, char, const, extern,
false, float, for, if, int, long, namespace, new, opera-
tor, private, protected, sizeof, static, static, struct,
switch, template, throw, typedef, typename, union, un-
signed, virtual, and volatile. We then used the Mann–
Whitney U test to ﬁnd out whether the given keyword is
used more or less frequently in VCCs compared to unclassi-
ﬁed commits. Table 2 shows a subset of those keywords with
high signiﬁcance and high eﬀect. We say that an eﬀect is
signiﬁcant if p < 0.000357, corresponding to 0.01/28, again
accounting for a Bonferroni correction for multiple testing
for the 28 keywords.

The eﬀect size measures the percentage of pairs that sup-
port the hypothesis. For example, for the keyword if, the
vulnerable commits contain more ifs than the unclassiﬁed
commits in 70 % of the cases. As can be seen by looking at
the mean values for each distribution, if there is a statistical
eﬀect, the VCCs are more likely to contain those keywords
compared to unclassiﬁed commits.

3.4.3 Features Scoped by Commit or File
All remaining features except for the number of deleted
lines are distributed diﬀerently over VCC versus unclassiﬁed
commits, with p = 3.9 × 10−6 the number of hunks being the
least signiﬁcant result. We note that the fact that a feature
is distributed diﬀerently does not mean that this feature
can be used to distinguish between the two sets. However,
these results provide some hint as to why a machine-learning
approach that uses a combination of these features can be
successful.
The only feature where the diﬀerence was not signiﬁcant
was the number of deleted lines (p = 4.6 × 10−4), contrary
to the number of added lines (p = 3.9 × 10−37), for which
there is a signiﬁcant diﬀerence in the distribution. When
we manually looked at commits with known vulnerabilities
and compared them to unclassiﬁed commits, we saw that
the former often added a great deal of code, whereas the
number of deleted or edited lines were the same as for un-

431classiﬁed commits. This ﬁnding conﬁrms the intuition that
security bugs are not commonly introduced by code edits or
refactoring, but that new code is a more likely entry points
for vulnerabilities. To the best of our knowledge this fact
has not been used to ease the workload of code reviewers.
3.4.4 Text-Based Features
One of the central tenets of our work is that combining
code metrics with GitHub metadata can help with the de-
tection of VCCs. While both the code and the metadata
features detailed above are “hard” numerical features, there
are also a number “soft” features contained in GitHub that
can be helpful. These text-based features, like the commit
message, cannot be evaluated using statistical tests as above,
but will be integrated into the machine-learning algorithm
using a generalized bag-of-words model as we will discuss in
Section 4.1.

4. LEARNING-BASED DETECTION

The diﬀerent features presented in the previous sections
provide information for analyzing the search for suspicious
commits and the discovery of potential vulnerabilities. As
the large number of these features renders the manual con-
struction of detection rules diﬃcult, we apply techniques
from the area of machine-learning to automatically analyze
the commits and rank them so code-reviewers can prioritise
their work. The construction of a learning-based classiﬁer,
however, poses several challenges that need to be addressed
to make our approach useful in practice:

1. Generality: Our features comprise information that
range from numerical code metrics to structured meta-
data, such as words in commit messages and keywords
in code. Consequently, we strive for a classiﬁer that is
capable of jointly analyzing these heterogeneous fea-
tures and inferring a combined detection model.

2. Scalability: To analyze large code repositories with
thousands of source ﬁles and commits, we require a
very eﬃcient learning method which is able to operate
on the large amount of available features in reasonable
time.

3. Explainability: To help an analyst in practice, it is ben-
eﬁcial if the classiﬁer can give a human-comprehensible
explanation as to why the commit was ﬂagged, instead
of requiring an analyst to blindly trust a black-box de-
cision.

We address these challenges by combining two concepts
from the domains of machine-learning and information re-
trieval. In particular, we ﬁrst create a joint representation
for the heterogeneous features using a generalized bag-of-
words model and then apply a linear Support Vector Ma-
chine (SVM)—a learning method that can be extended to
provide explanations for its decisions and which is also eﬃ-
cient enough to cope with the large number of features which
need to be analysed.
4.1 Generalized Bag-of-Words Models

Bag-of-word models have been initially designed for anal-
ysis of text documents [30, 29]. In order to combine both
code metric based numerical features with GitHub meta-
data features, we generalize these models by considering a

generic set of tokens S for our analysis. This set can contain
textual words from commit messages as well as keywords,
identiﬁers and other tokens from the code of a commit. In
particular, we obtain these tokens by splitting the commit
message and its code using spaces and newlines. Further-
more, we ignore certain tokens, such as author names and
email addresses, since they might bias the generality of our
classiﬁer and could compromise privacy.

Formally, we deﬁne the mapping ϕ from a commit to a

vector space as

ϕ : X −→ R|S|

, ϕ : x (cid:55)−→(cid:0)b(x, s)(cid:1)

,

s∈S

where X is the set of all commits and x ∈ X an individual
commit to be embedded in the vector space. The auxiliary
function b(x, s) returns a binary ﬂag for the presence of a
token s in x and is given by

(cid:40)

b(x, s) =

1 if token s is contained in x
0 otherwise.

To also incorporate numerical features like the author con-
tribution into this model, we additionally convert all nu-
merical features into strings. This enables us to add all
arbitrary numbers to S and thereby treat both kinds of fea-
tures equally. However, when using a string representation
for numerical features we have to ensure that similar values
are still identiﬁed as being similar. This is obviously not
the case for a naive mapping, as “1.01” and “0.99” represent
totally diﬀerent strings.

We tackle this problem by mapping all numerical features
to a discrete grid of bins prior to the vector space embed-
ding. This quantization ensures that similar values fall into
the same bins. We choose diﬀerent bin sizes depending on
the type of the feature. If the numerical values are rather
evenly distributed, we apply a uniform grid, whereas for
features with skewed distribution we a apply a logarithmic
partitioning. For the latter, we apply the logarithmic func-
tion to its values and cut oﬀ all digits after the ﬁrst decimal
place.

To better understand this generalized bag-of-words model,
let us consider a ﬁctitious commit x, where a patch has been
written by a user who did not contribute to a project before.
The committed patch is written in C and contains a call
to an API function which is associated with a buﬀer write
operation. The corresponding vector representation of the
commit x looks as follows
···





···
1

0···

1

0···

ϕ(x) (cid:55)→

AUTHOR_CONTRIBUTION:0.0
AUTHOR_CONTRIBUTION:10.0

···

buf_write_func();
some_other_func();

···

The two tokens indicative of the commit are reﬂected by
non-zero dimensions, while all unrelated tokens are associ-
ated with zero dimensions. Note that the resulting vector
space is high-dimensional and may contain several thousands
of dimensions. For a concrete commit x, however, the vast
majority of these dimensions are zero and thus the vector
ϕ(x) can be stored in a sparse data structure. We make use
of the open-source tool Sally [27] for this purpose, which
implements diﬀerent strategies for extracting and storing
sparse feature vectors.

4324.2 Classiﬁcation and Explainability

While in principle a wide range of methods are available
for learning a classiﬁer for the detection of vulnerability
contributing commits, only few methods scale with larger
amount of data while also providing explanations for their
decisions. One technique satisfying both properties are lin-
ear Support Vector Machines (SVM). This variant of classic
SVMs does not apply the kernel trick for learning, but in-
stead directly operates in the input space. As a result, the
run-time complexity of a linear SVM scales linearly in the
number of vectors and features.

We implement our classiﬁer for commits using the open-
source tool LibLinear [17] that provides diﬀerent optimiza-
tion algorithms for linear SVMs. Each of these algorithms
seeks a hyperplane w that separates two given classes with
maximum margin, in our case corresponding to unclassi-
ﬁed commits and vulnerability-contributing commits. As
the learning is performed in the input space, we can use
this hyperplane vector w for explaining the decisions of our
classiﬁer.

(a) Detection performance
CFinder using diﬀerent feature sets.

of VC-

By calculating the inner product between ϕ(x) and the
vector w, we obtain a score which describes the distance from
ϕ(x) to the hyperplane; that is, how likely the commit intro-

duces a vulnerability, f (x) = (cid:104)ϕ(x), w(cid:105) =(cid:80)

As this inner product is computed using a summation over
each feature, we can simply test which features provide the
biggest contribution to this distance and thus are causal for
the decision.

s∈S ws b(x, s).

Finally, to calibrate free parameters of the linear SVM,
namely the regularization parameter C and the class weight
W , we perform a standard cross-validation on the training
data. We then pick the best values corresponding to a reg-
ularization cost C = 1 and a weight W = 100 for the class
of suspicious commits.

5. EVALUATION

We evaluate the eﬀectiveness of our approach in several
diﬀerent ways. First, we use a temporal split between the
training and test data to evaluate the predictiveness of the
SVM. We picked 2011 as the split data to have the rela-
tion of two-thirds to one-thirds training vs test data.5 Since
we have the ground truth for the years 2011 to 2014 this
method is the allows us to realistically and reliably test the
eﬀectiveness of VCCFinder.

Dataset

Historical

Test

Total

CVEs
VCCs
Unclassiﬁed commits

469
421
90,282

249
219
79,220

718
640
169,502

Table 3: Distribution of commits, CVEs, and VCCs.

5This is a standard approach to evaluate classiﬁers. The
ﬁrst dataset contains all commit data up until the 31st of
December 2010. We use this dataset for the design and
training of our classiﬁer. The dataset can be considered the
‘historical’ dataset. The second ‘testing’ dataset contains
all commit data from 2011 to 2014 which is then used to
evaluate our approach. This simulates VCCFinder being
used in the beginning of 2011 having being trained on all
existing data at the time and then trying to predict the
unkown VCCs of the future (2011 to 2014).

(b) Detection performance of our ap-
proach and FlawFinder as precision-
recall curve.

Figure 1: Detection performance of VCCFinder.

Second, we discuss the features learnt by the SVM as well
as the true positives, i.e. vulnerabilities our classiﬁer found
in the test set. Third, we discuss the commits that are
ﬂagged by our classiﬁer but lie outside the ground truth
we have based on the CVEs. These could either be false
positives or point to previously undetected vulnerabilities.
Finally, we compare our approach to Flawﬁnder, an open-
source static code analyzer.

Comparing feature sets.

We start with an evaluation of the impact of diﬀerent
feature sets and their combination on the detection perfor-
mance of our classiﬁer. Figure 1(a) shows the precision-
recall curves for these experiments. To this end, we train
a classiﬁer on code metric features and meta-information.
As can be seen, the classiﬁer that combines all the features
(shown in blue) out-performs the classiﬁers which only oper-
ate on a sub-set of the features, showing that combining the
diﬀerent features is beneﬁcial. Figure 1(b) shows the preci-
sion recall curve of our VCCFinder compared to Flawﬁnder,
which only operates on code metrics. The comparison with
Flawﬁnder will be discussed in greater depth in section 5.3.

0.00.20.40.60.81.0Recall0.00.20.40.60.81.0Precisioncombinedcode metricscodemeta-data0.00.20.40.60.81.0Recall0.00.20.40.60.81.0PrecisionVCCFinderFlawfinder4335.1 Case Study

The previous section shows the precision of our approach
for the diﬀerent levels of recall. In practice, developers can
simply decide how many commits they can aﬀord (time-
and cost-wise) to review and VCCFinder will improve their
chances of ﬁnding vulnerabilities. For the sake of comparison
with Flawﬁnder, we now set VCCFinder’s recall to the same
as that of Flawﬁnder (i.e. 0.24 cf. Table 4) and discuss
some examples of the VCCs which would have been ﬂagged
by VCCFinder if it had been run from 2011 to 20146. In
these four years, VCCFinder would only have ﬂagged 89
out of 79688 commits for manual review compared to 5,513
commits ﬂagged by Flawﬁnder. We believe this is a very
manageable amount of code reviews to ask reviewers to do
for a high return. Additionally, projects can increase the
number of commits to review at any time. In the following,
we present an excerpt of the vulnerabilities that VCCFinder
found, when set at the very conservative level of Flawﬁnder’s
recall. We also discuss which features our classiﬁer used to
spot the VCCs.

CVE-2012-2119.

Commit 97bc3633be includes a buﬀer overﬂow in the mac-
vtap device driver in the Linux kernel before 3.4.5, when run-
ning in certain conﬁgurations, allows privileged KVM guest
users to cause a denial of service (crash) via a long descrip-
tor with a long vector length 7. Considering metadata, our
SVM detects this commit because of the edited ﬁle’s high
code churn, and because the author made few contributions
to the Kernel in combination with the fact the the developer
used sockets.

CVE-2013-0862.

FFmpeg commit 69254f4628 introduces multiple integer
overﬂows in the process frame obj function in libavcodec /
sanm.c in FFmpeg before 1.1.2 that allow remote attackers
to have an unspeciﬁed impact via crafted image dimensions
in LucasArts Smush video data, which triggers an out-of-
bounds array access 8. The SVM detected that the author
contributed little to the project before as well as that the
commit inserted a large chunk of code at once.

CVE-2014-1438.

In commit 1361b83a13, the restore fpu checking function
in arch/x86/include/asm/fpu-internal.h in the Linux ker-
nel before 3.12.8 on the AMD K7 and K8 platforms does
not clear pending exceptions before proceeding to an EMMS
instruction, which allows local users to cause a denial of
service (task kill) or possibly gain privileges via a crafted
application.9 The SVM detected a high amount of excep-
tions, a high number of changed code, inline ASM code, and
variables containing user input such as __input and user.

6As previously mentioned we use the years 2011–2014 as the
test dataset, since we have ground truth data on which to
base the discussion.
7http://cve.mitre.org/cgi-bin/cvename.cgi?name=
CVE-2012-2119
8http://cve.mitre.org/cgi-bin/cvename.cgi?name=
CVE-2013-0862
9https://cve.mitre.org/cgi-bin/cvename.cgi?name=
CVE-2014-1438

CVE-2014-0148.

In commit e8d4e5ffdb of Qemu the block driver for Hyper-
V VHDX Images is vulnerable to inﬁnite loops and other po-
tential issues when calculating BAT entries. This is due to
missing bounds checks for block size and logical sector size
variables 10. The SVM found that the patch of the VCC
included many keywords indicating errorprone byte manip-
ulation, such as “opaque”, “*bs”, or “bytes”.
5.2 Flagged Unclassiﬁed Commits

While we discussed the known true positive hits of our
classiﬁer for the years 2011 to 2014 above, we also have
36 commits that were ﬂagged as potentially dangerous, for
which we have no known CVE. These are commits that need
be checked by code reviewers. We have shared our results
with several code reviewing teams and will follow responsible
disclosure in all cases, so we cannot discuss the ﬂagged com-
mits at this time. However, we can already talk about one
vulnerability found by VCCFinder in commit d08d7142fd
of the FFmpeg project, since this vulnerability was ﬁxed in
commit cca1a42653 before it ever was released. Thus, dis-
cussing the ﬁndings poses no harm to the FFmpeg project.
Commit d08d7142fd of FFmpeg introduces a new codec
for Sierra Online audio ﬁles and Apple QuickDraw and was
ﬂagged in the 101 commits, but is not associated with a
CVE. However, we discovered that in the newly created ﬁle
libavcodec/qdrw.c, starting at line 72, the author does not
check the size of an integer read from an adversary-supplied
buﬀer.

f o r ( i = 0 ;

i <= c o l o r s ;

i ++) {

i n d e x ∗/

in t i d x ;
i d x = BE 16 ( buf ) ; /∗ c o l o r
buf += 2 ;
a−>p a l e t t e [ i d x ∗ 3 + 0 ] = ∗ buf++;
buf++;
a−>p a l e t t e [ i d x ∗ 3 + 1 ] = ∗ buf++;
buf++;
a−>p a l e t t e [ i d x ∗ 3 + 2 ] = ∗ buf++;
buf++;

}

The macro BE_16() reads two bytes from the argument
and returns an unsigned 16 bit integer. This means that an
adversary controlling buf (e.g. through a malicious video)
could address 3 · 65535 bytes of memory which will be ﬁlled
by data from buf itself.

The SVM classiﬁed the commit because of raw byte ma-
nipulation, indicated by uses of “buf” as well as an inexpe-
rienced committer pushing a large chunk of code at once.
5.3 Comparison to Flawﬁnder

We compare our ﬁndings against Flawﬁnder [34] version
1.31, a static source code scanner. Flawﬁnder is a mature
open-source tool that has been under active development
since 2001 and fulﬁlls the requirements of being able to pro-
cess C and C++ code on the level of commits. When given
a source ﬁle, Flawﬁnder returns lines with suspected vul-
nerabilities. It oﬀers a short explanation of the ﬁnding as
well as a link to the Common Weakness Enumeration (CVE)
database.11 For the comparison, we run Flawﬁnder on each

10https://bugzilla.redhat.com/show_bug.cgi?id=
1078212
11http://cwe.mitre.org/

434added or modiﬁed ﬁle of a commit. We then record the
lines which Flawﬁnder ﬂags that were inserted by the com-
mit. Consequently, we say that Flawﬁnder marked a commit
if it found a ﬂaw in one of the lines the commit inserted.

We then evaluated both our tool and Flawﬁnder against
the test dataset. Table 4 shows the contingency table, pre-
cision and recall for both tools. We argue that precision is
the most important metric in this table and the one which
should be used to compare Flawﬁnder and VCCFinder, as
this value determines how many code locations a security
researcher needs to look at in order to ﬁnd a vulnerabil-
ity. While a higher recall would theoretically mean that
more vulnerabilities can be found, in practice they would be
buried in a large amount of false positives. So for now, we
accept that we will not ﬁnd all vulnerabilities but create an
environment in which it is realistic for a reviewer to check all
ﬂagged commits and achieve a decent success rate. Each row
compares VCCFinder to Flawﬁnder with a diﬀerent conﬁgu-
ration. In the ﬁrst row, we set VCCFinder’s recall to that of
Flawﬁnder’s. As can be seen, VCCFinder’s precision is sig-
niﬁcantly higher. Our approach improves the false positive
rate by over 99 %! This is the most realistic conﬁguration,
since this conﬁguration can be used in a real world setting.
For the next comparison, we set VCCFinder’s false posi-
tives to the same number as Flawﬁnder’s. While of course
the number of false positives is then prohibitively high, VC-
CFinder does ﬁnd almost three times as many VCCs as
Flawﬁnder.
In the ﬁnal comparison, we set VCCFinder’s
precision to Flawﬁnder’s very poor value. While the num-
ber of false positives is prohibitively high, VCCFinder ﬁnds
almost 90% of all VCCs compared to Flawﬁnder’s 24%.

In Table 5 we also compare VCCFinder and Flawﬁnder
based on their top results. In the ﬁrst row, we select the top
100 ﬂagged commits, then 500 and ﬁnally 1000. Among the
top 100 commits, VCCFinder identiﬁes 56 VCCs correctly,
signiﬁcantly reducing the amount of commits a security re-
searcher would need to review before ﬁnding a commit con-
taining a vulnerability. Compared to FlawFinder, its preci-
sion is more than 50 times higher and it already identiﬁes
more than 25% of all VCCs in the data set at this point.

VCCFinder signiﬁcantly outperforms Flawﬁnder in all pa-
rameter conﬁgurations. Importantly, we were able to reduce
the number of false positives to the point where it becomes
realistic for reviewers to carefully check all ﬂagged commits.
This represents a signiﬁcant improvement over the current
state-of-the-art.

We would have liked to compare our approach to more
alternatives; however, since most research papers have not
published the datasets they worked on and since their tools
are not applicable to commits at the scale at which we tested
VCCFinder, this was not possible. We are releasing our
VCC database and results to the community, so that fu-
ture researchers have a benchmark against which diﬀerent
approaches can be compared.

6. TAKE-AWAYS

As the results above show, the performance of VCCFinder
means that it can realistically be used in production en-
vironments without overburdening developers with a huge
number of reviews. Since it can work on code snippets it
can used automatically when new commits come in without
requiring a complex test environment.

TP FP FN TN

Flawﬁnder
Top 100
Top 500
Top 1000

1
6
13

VCCFinder
Top 100
Top 500
Top 1000

56
88
105

99
494
987

44
412
895

218
213
206

163
131
114

79121
78726
78233

79176
78808
78325

n
o
i
s
i
c
e
r
P

0.01
0.01
0.01

0.56
0.18
0.11

l
l
a
c
e
R

0.00
0.03
0.06

0.26
0.40
0.48

Table 5: Confusion matrix of the tools by top X
commits. T: True, F: False, P: Positive, N: Negative.

Apart from this, we would like to present some qualitative
take-aways we found while developing and evaluating VC-
CFinder, which can be useful even without using the tool.
While some of these take-aways conﬁrm well-known beliefs,
we found it interesting to see that our machine-learning ap-
proach also came to these conclusions and backed them up
with quantitative data, but also generated new insights.

Error handling is hard.

When looking at the features the SVM learnt by classify-
ing VCCs, we saw that the adage “gotos considered harmful”
[16] still holds true today, as amongst others the keyword
goto and the according jump labels such as out: and er-
ror: increase the likelihood of vulnerable code. We can
conﬁrm this by looking at Table 2. However, we found that
the SVM also ﬂags returning error values such as -EINVAL
as potentially dangerous. Combined with gotos, these are
common C mechanisms for error and exception handling. So
unlike Dijkstra’s argument that gotos are harmful because
they lead to unreadable code, in our context gotos are con-
sidered harmful because they frequently occur in an error-
handling context. So instead of merely detecting gotos, our
SVM gives exception and error-handling code a higher po-
tential vulnerability ranking. Our explanation of this eﬀect
is as follows: because it is easy to miss some cases, exception
handling is easy to get wrong (e.g. Apple’s goto fail bug in
their TLS implementation12).

Variable Usage and Memory Management.

When examining highly ranked features of the SVM, we
noticed that some memory management constructs lead to a
higher vulnerability ranking. For instance, sizeof(struct,
a high usage of sizeof in general, len and length as vari-
able names occurred more often in vulnerable commits. In
addition, we observed that variable names consisting of spe-
ciﬁc strings often occurred in VCCs: buf, net, socket and
sk. While the presented keywords and variables alone do
not lead to vulnerabilities, they may indicate more critical
areas of the code.

12https://www.imperialviolet.org/2014/02/22/
applebug.html

435True

False

False

True

positive positive negative negative Precision Recall

Flawﬁnder

VCCFinder
– with same recall/true positives
– with same number of false positives
– with same precision

53

5,460

53
144
185

36

5460
24288

166

166
75
34

73760

0.01

0.24

79184
73760
54932

0.60
0.03
0.01

0.24
0.66
0.84

Table 4: Comparison of the tools

Help new contributors.

We found that new contributors, i.e. contributors with
less than 1 % of all commits in a given project, are about
ﬁve times as likely to commit a vulnerability as their coun-
terparts who frequently contribute. While new contributors
authored 470 of 95,621 VCCs, or 0.49 %, frequent contrib-
utors authored only 244 of 255,074 VCCs, or 0.10 % (Pear-
son’s χ2: p < 0.0001). While this is of course also no big
surprise, we hope quantifying this risk will help convince
projects to introduce more stringent review policies.

Final Thoughts.

As both the evaluation and the take-aways show, commits
have a myriad of possible reasons for being ﬂagged as VCCs.
These reasons can be code-based or metadata-based or, im-
portantly, a combination of the two. The examples above
give us an intuitive understanding of why this is. While
our main recommendation is to use VCCFinder to classify
potentially vulnerable commits to prioritize reviews, there
also general recommendation which can be extracted from
the classiﬁer results.
7. LIMITATIONS

Our approach has several limitations. We selected 66
open-source projects written in C or C++ that created at
least one CVE but otherwise varied in numbers of contribu-
tors, commits, or governance. We believe that applying our
results to other projects using C or C++ should not threaten
the validity. However, we can make no predictions on how
VCCFinder performs on projects which to date have not re-
ceived any CVEs. For generalization to other programming
languages, the feature extraction and training will need to
be re-done per language, so that the SVM does not mis-train
based on diﬀerences in syntax.

We used a heuristic to map CVEs to VCCs. Our manual
analysis of 15 % of these mappings showed that we have an
error rate of 3.1 %. This needs to be taken into account by
any project building on this dataset.

While we were able to map CVEs to VCCs, it is of course
unknown how many unknown vulnerabilities are contained
in our annotated database. Thus, our true positives must be
considered a lower bound and the false positives an upper
bound. So both VCCFinder’s and Flawﬁnder’s results might
be better than reported and the relation between them could
change. However, since VCCFinder outperforms Flawﬁnder
by a large margin, it seems unlikely that the outcome would
change.

Our experiments demonstrate that VCCFinder is able to
automatically spot vulnerability-contributing commits with
high precision; yet this alone does not ensure that an under-

lying vulnerability will be uncovered. Signiﬁcant work and
expertise is still necessary to audit commits for potential
security ﬂaws. However, our approach reduces the amount
of code to inspect considerably and thus helps increase the
eﬀectiveness of code audits.

8. CONCLUSION

In this paper, we present and evaluate VCCFinder, an
approach to improve code audits. Our approach combines
code-metric analysis with meta data gathered from code
repositories using machine-learning techniques. Our results
show that our approach signiﬁcantly outperforms the vulner-
ability ﬁnder Flawﬁnder. We created a large test database
containing 66 C and C++ project with 170,860 commits on
which to evaluate and compare our approach. Training our
classiﬁer on data up until 2010 and testing it against data
from 2011 to 2014, VCCFinder produced 99% fewer false
positives than Flawﬁnder, detecting 53 of the 219 known
vulnerabilities and only producing 36 false positives com-
pared to Flawﬁnder’s 5,460 false positives.

To enable future research in this area, we will release
our annotated VCC database and results so that future ap-
proaches can use this database both as a training set and as
a benchmark to compare themselves to existing approaches.
The community is currently lacking such a baseline and we
hope to spur more comparable research in this domain.

We see a very large amount of interesting future work.
While the results are already signiﬁcantly better than the
Flawﬁnder tool, we believe that we have only begun to
scratch the surface of what can be ascertained by combin-
ing the diﬀerent features. Further analyzing the results of
the classiﬁer will likely allow us to make more general rec-
ommendations on how to minimize the likelihood that vul-
nerabilities make it from the initial vulnerable commit into
deployed software.

References
[1] Clang static analyzer. http://clang-analyzer.llvm.org/.

Accessed: 2015-05-08.

[2] Trinity: A linux system call fuzzer.

http://codemonkey.org.uk/projects/trinity/. Accessed:
2015-05-08.

[3] Valgrind. http://valgrind.org/. Accessed: 2015-05-08.
[4] CodeSonar R(cid:13) | GrammaTech static analysis.

https://www.grammatech.com/codesonar/, visited August,
2015.

[5] Coverity Scan — static analysis.

https://scan.coverity.com/, visited August, 2015.

[6] HP Fortify. https://www.hpfod.com/, visited August, 2015.

436[7] IBM Security AppScan Source. https:

//www.ibm.com/software/products/en/appscan-source/,
visited August, 2015.

[8] PREfast analysis tool. https:

//msdn.microsoft.com/en-us/library/ms933794.aspx,
visited January, 2015.

[9] Rough auditing tool for security (RATS). https://code.

google.com/p/rough-auditing-tool-for-security/,
visited January, 2015.

[10] Splint – annotation-assisted lightweight static checking.

http://splint.org/, visited January, 2015.

[11] S. Bandhakavi, S. T. King, P. Madhusudan, and

M. Winslett. VEX: Vetting browser extensions for security
vulnerabilities. In USENIX Security Symposium,
volume 10, pages 339–354, 2010.

[26] S. Neuhaus, T. Zimmermann, C. Holler, and A. Zeller.

Predicting vulnerable software components. In Proceedings
of the 14th ACM conference on Computer and
communications security, pages 529–540. ACM, 2007.

[27] K. Rieck, C. Wressnegger, and A. Bikadorov. Sally: A tool
for embedding strings in vector spaces. Journal of Machine
Learning Research (JMLR), 13(Nov):3247–3251, Nov. 2012.

[28] A. Sadeghi, N. Esfahani, and S. Malek. Mining the

categorized software repositories to improve the analysis of
security vulnerabilities. In Fundamental Approaches to
Software Engineering, pages 155–169. Springer, 2014.

[29] G. Salton. Mathematics and information retrieval. Journal

of Documentation, 35(1):1–29, 1979.

[30] G. Salton and M. J. McGill. Introduction to Modern

Information Retrieval. McGraw-Hill, 1986.

[12] C. Cadar, D. Dunbar, and D. R. Engler. KLEE: Unassisted

[31] R. Scandariato, J. Walden, A. Hovsepyan, and W. Joosen.

and automatic generation of high-coverage tests for
complex systems programs. In OSDI, volume 8, pages
209–224, 2008.

[13] R.-Y. Chang, A. Podgurski, and J. Yang. Discovering

neglected conditions in software by mining dependence
graphs. Software Engineering, IEEE Transactions on, 34
(5):579–596, Sept 2008.

[14] C. Y. Cho, D. Babic, P. Poosankam, K. Z. Chen, E. X. Wu,

and D. Song. MACE: Model-inference-assisted concolic
exploration for protocol and vulnerability discovery. In
USENIX Security Symposium, pages 139–154, 2011.

[15] J. Dahse and T. Holz. Static detection of second-order

vulnerabilities in web applications. In 23rd USENIX
Security Symposium (USENIX Security 14), pages
989–1003, San Diego, CA, Aug. 2014. USENIX Association.

[16] E. W. Dijkstra. Letters to the editor: go to statement

considered harmful. Communications of the ACM, 11(3):
147–148, 1968.

[17] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and

C.-J. Lin. LIBLINEAR: A library for large linear
classiﬁcation. Journal of Machine Learning Research
(JMLR), 9:1871–1874, 2008.

[18] J. Graylin, J. E. Hale, R. K. Smith, H. David, N. A. Kraft,
W. Charles, et al. Cyclomatic complexity and lines of code:
empirical evidence of a stable linear relationship. Journal
of Software Engineering and Applications, 2(03):137, 2009.

[19] M. H. Halstead. Elements of software science. Elsevier

computer science library : operational programming
systems series. North-Holland, New York, NY, 1977.

[20] C. Holler, K. Herzig, and A. Zeller. Fuzzing with code

fragments. In Presented as part of the 21st USENIX
Security Symposium (USENIX Security 12), pages
445–458, Bellevue, WA, 2012. USENIX.

[21] S. Kim, E. J. Whitehead Jr, and Y. Zhang. Classifying

software changes: Clean or buggy? Software Engineering,
IEEE Transactions on, 34(2):181–196, 2008.

[22] T. J. McCabe. A complexity measure. Software

Engineering, IEEE Transactions on, (4):308–320, 1976.
[23] A. Meneely and O. Williams. Interactive churn metrics:
Socio-technical variants of code churn. SIGSOFT Softw.
Eng. Notes, 37(6):1–6, Nov. 2012.

[24] A. Meneely, H. Srinivasan, A. Musa, A. Rodriguez Tejeda,

M. Mokary, and B. Spates. When a patch goes bad:
Exploring the properties of vulnerability-contributing
commits. In Empirical Software Engineering and
Measurement, 2013 ACM / IEEE International
Symposium on, pages 65–74, Oct 2013.

[25] A. Meneely, A. C. R. Tejeda, B. Spates, S. Trudeau,

D. Neuberger, K. Whitlock, C. Ketant, and K. Davis. An
empirical investigation of socio-technical code review
metrics and security vulnerabilities. In Proceedings of the
6th International Workshop on Social Software
Engineering, SSE 2014, pages 37–44. ACM, 2014.

Predicting vulnerable software components via text mining.
Software Engineering, IEEE Transactions on, 40(10):
993–1006, Oct 2014.

[32] J. ´Sliwerski, T. Zimmermann, and A. Zeller. When do

changes induce ﬁxes? ACM Sigsoft Software Engineering
Notes, 30(4):1–5, 2005.

[33] H. W. Wendt. Dealing with a common problem in social

science: A simpliﬁed rank-biserial coeﬃcient of correlation
based on the u statistic. European Journal of Social
Psychology, 2(4):463–465, 1972.

[34] D. A. Wheeler. Flawﬁnder.

http://www.dwheeler.com/flawfinder/, visited January,
2015.

[35] D. Wijayasekara, M. Manic, J. L. Wright, and M. McQueen.

Mining bug databases for unidentiﬁed software
vulnerabilities. In Human System Interactions (HSI), 2012
5th International Conference on, pages 89–96. IEEE, 2012.

[36] F. Yamaguchi, C. Wressnegger, H. Gascon, and K. Rieck.

Chucky: Exposing missing checks in source code for
vulnerability discovery. In Proceedings of the 2013 ACM
SIGSAC conference on Computer & communications
security, pages 499–510. ACM, 2013.

[37] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck. Modeling
and discovering vulnerabilities with code property graphs.
In Security and Privacy (SP), 2014 IEEE Symposium on.
IEEE, 2014.

[38] T. Zimmermann, N. Nagappan, and L. Williams. Searching

for a needle in a haystack: Predicting security
vulnerabilities for windows vista. In Software Testing,
Veriﬁcation and Validation (ICST), 2010 Third
International Conference on, pages 421–428. IEEE, 2010.

APPENDIX
A. LIST OF REPOSITORIES
We used the following list of repositories: Portspoof,
GnuPG, Kerberos, PHP, MapServer, HHVM, Mozilla
Gecko, Quagga, libav, Libreswan, Redland Raptor RDF
syntax library, charybdis, Jabberd2, ClusterLabs
pacemaker, bdwgc, pango, qemu, glibc, OpenVPN, torque,
curl, jansson, PostgreSQL, corosync, tinc, FFmpeg,
nedmalloc, mosh, trojita, inspircd, nspluginwrapper,
cherokee webserver, openssl, libfep, quassel, polarssl,
radvd, tntnet, Android Platform Bionic, uzbl, LibRaw,
znc, nbd, Pidgin, V8, SpiderLabs ModSecurity, ﬁle,
graphviz, Linux Kernel, libtiﬀ, ZRTPCPP, taglib, suhosin,
Phusion passenger, monkey, memcached, lxc, libguestfs,
libarchive, Beanstalkd, Flac, libX11, Xen, libvirt,
Wireshark, and Apache HTTPD.

437