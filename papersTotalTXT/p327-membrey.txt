Time to Measure the Pi

Peter Membrey

Darryl Veitch

Department of Computing
The Hong Kong Polytechnic

School of Computing and

Communications

University

Hunghom, Hong Kong
peter@membrey.hk

University of Technology

Sydney, Australia

Darryl.Veitch@uts.edu.au

Rocky K. C. Chang

Department of Computing
The Hong Kong Polytechnic

University

Hunghom, Hong Kong

csrchang@comp.polyu.edu.hk

ABSTRACT
The Raspberry Pi platform is increasingly being used
for network measurement due to its low cost, ease of de-
ployment, and ability to run Linux. Timestamps are a
critical part of measurement data, yet the suitability of
the Pi for timing has not been established. We use ref-
erence hardware to characterize the Pi’s STC hardware
counter, and to evaluate its performance when paired
with a low cost yet powerful GPS ‘hat’. We ﬁnd that
the platform can support timing adequate for most mea-
surement purposes, but with some caveats.
Categories and Subject Descriptors
C.2.3 [Computer Communications]: Network oper-
ations—Network monitoring
Keywords
Raspberry Pi, GPS hat, network measurement, PPS,
clock synchronization, Internet of Things.
1.

INTRODUCTION

The Raspberry Pi (or Pi) has become a widely used
computing platform due to its low cost, small form fac-
tor, and support for the Linux operating system [3].
For network measurement, its ease of deployment has
made it an attractive way to scale out monitoring net-
works. For example both CAIDA’s Ark platform [1],
and the BISmark project [2], increasingly employ Pi-
based nodes, and Pi’s equipped with GPS are now also
receiving attention [8].

Although relatively powerful, the processing, memory
and networking limitations of the Pi are well recognized

Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
IMC 2016, November 14-16, 2016, Santa Monica, CA, USA
c(cid:13) 2016 ACM. ISBN 978-1-4503-4526-2/16/11. . . $15.00
DOI: http://dx.doi.org/10.1145/2987443.2987476

[12, 13]. These naturally place constraints on the mea-
surement regimes a Pi-based infrastructure can support,
but the range of applications is nonetheless wide, par-
ticularly since Pi’s or other IoT devices could be used
primarily for data collection, with data analysis occur-
ring elsewhere. There is however a fundamental issue
that relates directly to the collection platform. Much
measurement data involves, if not literally consists of,
timestamps. How suitable then is the Pi as a platform
for reliable timing? Underlying hardware factors such
as low cost oscillators, small thermal mass, low resolu-
tion counters, and inexpensive networking architecture,
will all impact on its ﬁnal timekeeping and timestamp-
ing performance.

In this paper we examine the suitability of the Pi

platform for timing. Our main goals are as follows.
(1) Characterization of the System Timer Counter.
The STC, which ticks nominally at a rate of 1Mhz, is
the Pi’s only readily accessible hardware counter. It is
therefore the basis of any software clock one could deﬁne
on the platform (including the system clock under the
Raspbian operating system). Clock performance will
therefore be limited by the STC’s stability and resolu-
tion. We use specialist hardware and careful method-
ology to measure this stability accurately for the ﬁrst
time for each of the Pi-1(B), Pi-2, and the newly re-
leased Pi-3.

An alternative counter of higher resolution is that
underlying the CPU, however it is not readily accessi-
ble under Raspbian, nor used by it (see [15] for access
notes). We believe this is because its nominal rate is
unreliable, rendering it unsuitable for timekeeping.

The remaining goals concern the situation where Pulse
Per Second (PPS) input is available. The main example
is when the PPS is provided by a GPS receiver, where
the pulses fall on the seconds of Coordinated Universal
Time (UTC). These pulses are used by system clocks
to lock to UTC. Our study of ‘Pi+PPS’ is motivated by
the fact that low cost GPS boards are now available for
the Pi, opening up the possibility of using it as a low
cost precision timing platform with particular beneﬁts
for measurement, in particular where accurate absolute

327time is essential. These include studies based on la-
tency comparisons, such as latency benchmarking and
mapping, reliable event ordering (needed for example
in time-aware distributed databases [9]), or the health
checking of public timing infrastructure [21, 20].
(2) Characterization of a low cost GPS hat.
We examine a high quality uBLOX M8Q GNSS module
with a custom ﬁt to the Pi, and compare its PPS output
to a GPS synchronized atomic clock reference.
(3) Examination of latency pitfalls in PPS triggering.
The availability of PPS to a network of Pi measure-
ment points enables their use as precise distributed trig-
gers. This would increase the accuracy and resolution
of applications such as Internet coordinate systems, and
moreover can be achieved independently of potentially
inaccurate system clocks. We report on some potential
pitfalls of this idea.
(4) Performance of Pi+hat timing.
By comparing ‘identical twin’ Pi+PPS systems in par-
allel we report on the expected best case underlying
performance of the Pi+hat platform.

It is beyond the scope of this paper to systematically
evaluate all elements impacting on the end performance
of Pi-based timing. For example we do not evaluate the
latency proﬁles of network hardware, nor evaluate in
detail the reaction of ntpd (the default system clock
synchronization algorithm) to failures in a PPS, a key
problem in opportunistic ‘antenna out the window’ de-
ployments. Nonetheless, in the conclusion we describe
the implications of our ﬁndings in terms of the end per-
formance of both the Pi+hat and Pi platforms.

Our work provides a strong foundation for future
studies.
In particular our focus on the availability of
PPS is not only about a desire to evaluate a Pi+hat
platform. It is the basis of our methodology for the ac-
curate measurement of the goals above, which can also
be used and/or adapted for the benchmarking of alter-
native platforms, or future counters on the Pi which
may become available. For example it may be possi-
ble to synthesize a reliable counter of high resolution
by combining the STC and the CPU counter in a simi-
lar way to Xen Clocksource [6]. Our methodology also
allows for the benchmarking of methods to estimate
counter stability when PPS is not available. This can
be done by exploiting RADclock [19], and will also form
the object of our future work.
2. TESTBED HARDWARE

Our testbed consists of a number of Pi’s, six units of
a particular GPS hat, GPS receivers (more accurately,
GNSS, using both GPS and GLONASS satellite constel-
lations), an atomic clock, and additional measurement
equipment.
2.1 The Pi and its Hat

We work with all three generations of the Raspberry
Pi, including the recently released Pi-3. Although there

Figure 1: The reference testbed equipment.

have been some variations within these (we use Pi-1(B)),
they mainly aﬀect CPU, RAM and additional features
such as wireless networking, bluetooth capabilities and
the like, rather than the core hardware including the
STC hardware counter that we focus on here. The Pi’s
all have General Purpose Input Output (GPIO) pins,
which can be used to input a PPS signal.

To provide the GPS functionality, a GPS module
from uputronics (previously HAB Supplies) was used.
This device was chosen since, despite being a low cost
board (£34.99 at the time of writing) and designed to
physically ﬁt (like a hat) to the Pi, it uses a very high
quality uBLOX M8Q GPS receiver which features a
PPS output enabling high precision locking to UTC.
Cheaper GPSes rely solely on messages sent using the
NMEA protocol [14], which do not arrive with the same
precision as pulse edges. The chipset supports various
modes to improve its reliability for time synchroniza-
tion, such as limiting the number of NMEA messages
it sends, and allowing Stationary mode: telling the re-
ceiver it is physically stationary reduces the degrees of
freedom and so improves the time ﬁx. Finally, the M8Q
model provides a TCXO (Temperature Compensated
Crystal Oscillator), which has a higher stability than of
a standard crystal, though still below that of an atomic
clock. This cost to performance ratio makes it an ideal
board for developing larger IoT devices.

2.2 Reference Testbed

Benchmarking a GPS system requires an accurate
and robust reference against which to compare. To cre-
ate such a reference, industry standard equipment was
chosen from leading manufacturers, widely known and
respected in the industry.

We cascade three devices to produce, ﬁnally, a PPS
output which is extremely stable both at small and long
time-scales, and which oﬀered ﬂexible conﬁguration for
our needs. This cascade, shown in Figure 1, consists of:
GPS receiver → atomic clock → signal generator.

328Figure 2: Pulse errors E(i) of the hat over 99 hours. Left: timeseries, Right: histogram of all values.

The ﬁrst device is a Microsemi S600 GPS Network
Time Server, which provides a UTC synchronized PPS
which is highly stable on long time-scales.1 To im-
prove stability over shorter time-scales, and to provide
holdover protection in case of satellite reception prob-
lems, we use the GPS’s PPS output to discipline a SRS
(Stanford Research Systems) FS-725 Rubidium Desk-
top Reference. This atomic clock produces a reference
PPS which will converge to a more stable form of the
output of the S600. Finally, because the 1.2 V output
level of the FS-725’s PPS is potentially too low for the
Pi to reliably detect (especially when daisy chaining and
testing a number of Pi’s in parallel), we use the 10Mhz
output of the FS-725 to precisely discipline a Tektronix
AGF1022 signal generator, whose own output can be
easily conﬁgured to generate a PPS of our choosing.
This PPS source was used for experiments with the Pi.
The Pi’s GPIO interrupt trigger level is usually around
1.8 V , but can range from 0.8 V to 2.0 V [5]. Whilst the
Pi-2 and Pi-3 triggered reliably at 1.2 V , the Pi-1 did
not. We set the signal generator’s output to 2 V , which
allowed reliable triggering for all Pi’s.

Each GPS receiver (both the Microsemi references
and the hats) has it own antenna, which was placed out-
side a nearby window. The number of satellites visible
during experiments was monitored. It was well above
the minimum required for each receiver to maintain a
reliable and accurate time ﬁx. For example the S600
saw typically 6 and 8 satellites from the GPS (L1) con-
stellation.
2.3 Measurement Equipment

In addition to our PPS reference, a Rigol DS1104Z
Oscilloscope and a Keysight 53230A digital frequency
counter were used for performing visualization, com-
parison, and measurement of pulse trains.

The counter was disciplined (in hardware via phase
locked loop) by the 10Mhz output of the FS-725. The
counter’s time-interval measurements are estimated to
have an accuracy of 50 ps (1 sigma) for our signals. The
oscilloscope does not support external synchronization,
however it contains a TCXO and was only used for com-
paring signals over sub-µs intervals, where errors due to
the TCXO drift would be in the pico-second range.

1In some cases a higher end Microsemi S650 was used
instead of the S600. The improvement is negligible for
our purposes. For simplicity we refer to the S600 below.

3. CHARACTERIZING THE HAT’S PPS
We begin with the analysis of the hat performance
(Goal 2), as this is the logical ﬁrst step, being indepen-
dent of the Pi. We characterize the pulse train of the
hat in three ways: via per-pulse errors, stability analy-
sis, and pulse shape.
3.1 Experiment Design

To measure per-pulse errors, we feed the pulse trains
from the hat and our atomic clock reference into sepa-
rate channels of the digital counter. The counter is set
to trigger on each hat pulse and return the time interval
to the next reference pulse.
We oﬀset (via a hat conﬁguration option) the hat
pulses by δ = −10µs (to sub-ns error) to ensure they al-
ways arrive before their corresponding reference pulses.
This oﬀset is removed in post-processing.

The resulting data is the error in the arrival time th(i)

of the i-th hat pulse:

E(i) = th(i) − tr(i), i = 1, 2, . . .

compared to the reference tr(i).
Since (by necessity) our reference is assumed perfect,
we know that tr(i) = tr(1) − 1 + i. We can therefore
recover the hat pulse times as

th(i) = tr(i) + E(i) = tr(1) − 1 + i + E(i)

and obtain the Inter-Pulse Time sequence, used in sta-
bility analysis, as

IPT(i) = th(i + 1) − th(i) = 1 + E(i + 1) − E(i).

To examine pulse shapes we employ the oscilloscope,
collecting summary statistics on rise and fall times, and
pulse widths for the two pulse trains separately. The
scope was set to trigger at 1.2 V for the SRS and 0.86 V
for the hats (50% of the recorded output voltage at
50 ohms). The trigger level, together with the choice
of edge, in fact deﬁnes where pulses ‘are’. The rise/fall
times can therefore be read as bounds on variations in
pulse position arising from alternative trigger choices
that may be found in equipment accepting PPS input.
We collect data from our ‘main’ hat in an experiment
99 hours long, and also from four other hats of the same
type, each 12 hours long.

pulse #i×1050.511.522.533.5E(i) [ns]-2000200400600[ns]-2000200400600med= 73, iqr= 1.2e+02 [ns]3293.2 Results
Errors: The error histogram shown in Figure 2 for the
main hat is well behaved. With a median error of 116ns
and an inter-quartile range of only 73ns, by itself this
would suggest that the hat is a more than adequate
hardware basis upon which to build a software clock
with sub-µs precision. The other hats are similar, with
median errors of [−83,−35,−81,−74] ns, correspond-
ing inter-quartile ranges of [105, 134, 119, 121] ns, and
ranges all under 1µs (recall range= max−min). How-
ever, the time series in Figure 2 shows that the errors
E(i) are temporally correlated, with oscillatory behav-
ior on multiple timescales, motivating a deeper stability
analysis.
Stability: The IPT(i) sequence contains the variations
in pulse rate about the ideal of a perfect 1PPS. Under-
standing how this rate ﬂuctuates over time reveals and
quantiﬁes the impact of diverse inﬂuences that operate
at diﬀerent timescales.

It is traditional to characterize the variability or sta-
bility of clock oscillators via the Allan Variance [11].
This can be interpreted as a robust measure of the vari-

Figure 3: Allan Deviation of IPT(i) as a function
of timescale τ .

ance of rate, averaged over timescale τ , as a function
of τ (see [18] for a more detailed discussion). Simple
time domain approaches such as the empirical auto-
correlation function are unsuitable as they are highly
biased on non-stationary data. The Allan Variance is in
fact equivalent to a Haar wavelet energy spectrum plot,
and therefore enjoys a number of statistical robustness
features of wavelet analysis (see [4]).

The Allan Deviation (square root of the Allan Vari-
ance) as a function of τ for IPT(i) is shown in the log-
log plot of Figure 3 for all ﬁve hats (the main hat from
Figure 2 stands out as the longer curve).

The results in each case were very similar. As ex-
pected, the value drops as the averaging interval τ grows.
Even at the smallest scale, here τ = 2s, the relative rate

Metric

rise time [ns]
fall time [ns]

pulse width [ms]

min
2.8
3.7
100

1st median
2.8
4.1
100

3.3
5.1
100

99th max
4.1
4.0
6.2
6.5
100
100

Table 1: Pulse shape statistics for the hat.

Metric

rise time [ns]
fall time [ns]

width [ns]

min
5.1
2.6
9900

1st median
5.2
2.6
9950

5.6
2.8

10000

99th
6.4
3.5

10050

max
6.6
3.9
10050

Table 2: Pulse shape statistics for the SRS.

error/deviation is under 6 × 10−9. (At small scales the
sawtooth on the PPS input is being averaged out. The
slope past τ = 100 s is −1, indicating white phase noise.)
This plot is used below to put into context the Allan

Deviation of the Pi’s hardware counter, the STC.

The above analyses were performed on IPT(i) se-
quences from which any outliers were replaced by neu-
tral surrogate values. This is essential to avoid signif-
icant distortion of the Allan Deviation, which is sensi-
tive to impulses. An eﬀective surrogate is to replicate
a neighbor: we set IPT(i) = IPT(i−1) for each outlier
i. In our data we encountered only a single such out-
lier, with E(i) ≈ 0.9s, due to a counter triggering error.
If any such outliers were to reach the Pi, it is critical
that they can be eﬀectively dealt with by the upstream
software clock algorithm or signiﬁcant errors can result.
We return to this general point in the conclusion.
Pulse Shape: Tables 1 and 2 give results obtained
from 1,000 pulses from the hat and our reference respec-
tively. In both cases the pulses are extremely consistent
in shape, with tight rise and fall times of comparable
size. The pulse width of the hat is much wider than
that of the reference (so wide it results in limited mea-
surement resolution), but this is not important when
rising edges are used in triggering, as here.

4. STABLITY OF THE STC

We now address Goal 1, the stability of the STC on
the Pi. As the core system hardware varies over the
Pi-1, Pi-2 and Pi-3, we provide results for each.
4.1 Experiment Design

We would, ideally, like to read the STC (we call this
raw timestamping) at precisely periodic timepoints with
small period, however performing this within an oper-
ating system is challenging. There are three issues, all
of which concern latency: timestamp triggering, times-
tamping location, and counter reading.

We deal with counter reading by accessing the counter
directly via memory mapping. This provides a minimal
access latency uniformly across kernel and user-space.
Our approach to triggering is to exploit the almost per-
fectly periodic pulses from our reference PPS. Finally, to

τ  [s]100101102103104105Allan deviation10-1210-1110-1010-910-8330minimize the latency between the delivery of pulses to
the GPIO pin and their timestamping, the timestamp
location is chosen to be at the beginning of the IRQ
routine responding to pulse interrupts. Kernel modiﬁ-
cations were added to pass the raw timestamps up to a
data logging application.

The resulting data are the raw timestamps STC(ti),
where ti is the true time at which the i-th pulse was
timestamped. We denote the corresponding raw inter-
pulse values by

C(i) = STC(ti+1) − STC(ti).

The timeseries C(i) combines variability from three
sources: (i) the underlying pulses; (ii) the latency be-
tween pulse arrival and timestamping; (iii) the variabil-
ity of the STC itself. It is not possible to separate these
within the Pi, which is why it is so important to send
in the reference quality pulse train to minimize the ﬁrst
component, and to make every eﬀort to minimize the
second as described above, so that C(i) is a low noise
measurement of the underlying STC behavior. It is diﬃ-
cult to provide an accurate ﬁgure for the error bound for
the error in our methodology, that is of latency source
(ii) above, as it depends on hardware characteristics
which can be diﬃcult to obtain. We believe it however
to be typically under 1µs.

We conduct a 20-day long experiment where each Pi
is fed a copy of the same reference pulse from the signal
generator. Hence the pulse and temperature conditions
are identical for each.
4.2 Results

Because the STC is a free running counter, the time-
series STC(ti) displays drift, and so, unlike the case of
the hat’s PPS, C(i) is not centered precisely around its
expected value (namely 106, since the STC is nominally
a 1MHz counter). The actual average period of C(i)
will vary from board to board and can (and must) be
measured, together with the stability about that value.
For each of the Pi’s the variation in C(i) is small: the
inter-quartile range is exactly 1µs in each case, a reﬂec-
tion of the limited resolution of the counter. Accord-
ingly, we use a robust mean rather than the median to
determine the ‘central’ value of the counter period. We
ﬁnd the STC period for the Pi-1, Pi-2 and Pi-3 to diﬀer
from the nominal 1µs by 19.3, −6.76, and −8.13 parts
per million (PPM) respectively. Here and below missing
pulses, and other rare anomalies/outliers in C(i), have
been replaced by neutral surrogates, again to avoid dis-
torting the Allan Deviation.

The Allan Deviation plots for each Pi are shown in
Figure 4. At small scales up to 100s the results are
ordered as one might expect according to generation,
with the older Pi-1 having the most variability and the
Pi-3 the least. At intermediate scales between 100s and
1000s temperature eﬀects enter in, which can aﬀect dif-
ferent platforms diﬀerently (this is more apparent when
looking at subsets of the full trace, which have some

Figure 4: Allan Deviation of ST C(i) as a function
of timescale τ for each generation of Pi.

overlap over this scale range. The full traces are for-
tuitiously well nested here). Finally, at scales beyond
1000s the variability actually starts to increase as diur-
nal temperature proﬁles rather than hardware or soft-
ware eﬀects dominate, and so the platforms become in-
creasingly similar.

The Allan Deviation plot for Pi-3 is very similar in
general terms to that seen in our prior characterization
of commodity PCs and servers [19, 17, 7]. The method-
ology here however, being based on PPS triggering, is
superior, resulting in lower spurious variability. It fol-
lows that the STC is in fact more variable than the
counters previously studied, as one might expect from
a smaller platform, even more so for the Pi-2 and Pi-
1. What is important however is that in each case two
key characterizations: (1) beyond short timescales the
variability drops below 10−7 and stays there, and (2),
the location of the minimum point being of the order
of τ = 1000s, agrees with that of our prior work. This
indicates that the Pi’s are suitable for timekeeping from
the stability point of view, implying underlying limits to
ﬁnal clock quality which are not much worse than those
for PC platforms (see [19, 17]).

The accuracy and reliability of a ﬁnal clock solution
however, depends not only on the underlying counter
hardware, but also on system latencies elsewhere in the
system, and how successfully a synchronization algo-
rithm can ﬁlter these to create a software clock which
is not just accurate, but robust to anomalous events.
5. PITFALLS IN PPS TRIGGERS

Precise coordinated triggering can increase the accu-
racy of distributed measurement applications such as
latency-based Internet coordinate systems, by allowing
precise relative ranging of multiple sources to landmark
nodes [10]. Triggering based on synchronized PPS has
the important advantage that it can be virtually in-
dependent of host system clocks, in particular of their

τ  [s]100101102103104105Allan deviation10-810-710-6Pi-1Pi-2Pi-3331This would represent a critical failure to a measurement
infrastructure intending to exploit PPS availability as a
means to coordinate distributed measurement.

In conclusion, although dedicated support exists to
access PPS from user space, for both reliability (failed
callbacks) and accuracy (user-space latency) reasons a
more direct kernel based raw timestamping is needed to
ensure the hat’s potential for well under 1µs triggering
accuracy, exhibited in Figure 2, is realized. This is par-
ticularly true for the special application of disciplining
the system clock itself.

6. PERFORMANCE OF THE PI + HAT
In this section we ask an obvious but important ques-
tion, how good in practical terms is the performance of
the Pi+hat compared to a Pi connected to an expensive,
reference PPS?
6.1 Experiment Design

We compare twin Pi-3’s in a single experiment 28hrs
long where everything is identical across the twins: the
Pi-3 hardware, software image, the hat and its satellite
visibility, experiment duration and temperature envi-
ronment. The only diﬀerence is the source of the PPS:
in one it comes from the hat itself, in the other this has
been replaced by our signal generator reference PPS.

For the ﬁrst time we will examine timestamps from
the system clock, which is disciplined by the ntpd dae-
mon synchronizing to the input PPS in a standard con-
ﬁguration for each twin. Timestamps are made within
the IRQ routine responding to pulses (just after our
raw timestamping location). We compare the system
clock timestamps Sr(i) and Sh(i) for the twin with the
reference and hat PPS respectively.
6.2 Results

We examine the Inter-Pulse Time sequence

IPTx(i) = Sx(i + 1) − Sx(i)

for each Pi, where x is one of h or r.

Figure 6 gives a representative subset 3000 pulses
wide of each IPTx timeseries, and histograms of all
values between the 0.1 and 99.9-th percentiles. The
two series are very similar, however the outliers in the
hat case are larger than those of the underlying pulses
themselves, which have a range under 1µs (Figure 2).
This shows that the system clock built on hat pulses
can have, overall, similar accuracy to one built on a
high end reference system (provided satellite reception
is maintained). However, it also highlights the impor-
tance of algorithm stability, as small instabilities at the
pulse level are being ampliﬁed rather than suppressed
by ntpd, albeit slightly, despite the ideal conditions.

Whereas IPTx(i) looks at errors in measuring 1 sec-
ond intervals, the error of the system clock as it evolves
across the entire trace is given by examining Sx(i) −
(Sx(1) + i − 1). For each twin we observe a ﬂuctuating

Figure 5: Kernel to user-space latency L(i).

errors and potentially limited resolution. The low cost
of Pi+Hat nodes makes it feasible to assemble a sub-
stantial measurement network with this capability. Its
potential could however be derailed by latency issues,
in particular those due to crossing the user and kernel
space divide. We examine this question here.
5.1 Experiment Design

Linux provides a standard way to access a PPS signal
via the Kernel PPS interface. We wrote a user-space
data logging application that uses this call to block on
each pulse. Upon each callback we immediately read the
STC using memory mapping, and also retrieve and log
our IRQ-based STC timestamp. For each generation of
Pi we collected several hours of data, including periods
of minimal load, and high load induced by ﬁle transfers.
5.2 Results

We ﬁrst examined the Inter-Pulse values C(i) taken
at each timestamping location separately. Take for ex-
ample results for the Pi-2. Although having identi-
cal median values, they exhibited markedly diﬀerent
variabilities, with [kernel, user] standard deviations of
[3.1, 418]µs, and ranges of [0.11, 32.5]ms. Note that this
is despite the beneﬁt of the low latency memory-mapped
STC reads.

i )− STC(tk
i in the kernel and tu
i

A more direct approach is to examine the latency
L(i) = STC(tu
i ) between the raw timestamps
read at time tk
in user-space. The
histogram of L(i) is given in Figure 5 for each Pi gen-
eration, showing in each case values from the minimum
to the 99.9-th percentile. Though a clear generational
improvement is seen, the gap between the arrival of the
pulse and its receipt in user-space can be signiﬁcant,
indeed outliers continue out into the ms range.

The experiment also collected instances where the
pulse was seen by the kernel, but the callback failed.

6080100120140160Pi -1:   med = 81, iqr=7 µs405060708090100Pi -2:   med = 51, iqr=3 µsµs2030405060Pi -3:   med = 21, iqr=4 µs332Figure 6: Inter-Pulse Intervals from the twin experiment. Left: timeseries, Right: histogram.

periodic wander in a 30µs band in this quantity, (indi-
cating that the software clock (in)stability is dominating
the small diﬀerences in pulse quality. For reference, the
best case wander on a PC platform is closer to 10µs.

7. CONCLUSION

We conclude by synthesizing our ﬁndings into a re-
sponse to the question, can we trust the Pi for timing?
In terms of timing hardware fundamentals, results
from Goal 1 showed that the STC’s stability was an ad-
equate basis for a software clock, not very diﬀerent from
that of a larger PC platform [17]. The 1µs resolution
however is a current limitation that will be inherited by
any software clock based on it. As pointed out in the in-
troduction however, this could potentially be overcome.
The results from Goal 2 showed that a well chosen,
inexpensive GPS source can be almost indistinguishable
from an expensive reference for timing needs below 1µs,
however this is, crucially, predicated on a reliable PPS
which implies in particular consistent satellite coverage.
Moreover, Goals 3 and 4 each point to the need to man-
age access latencies properly to fully beneﬁt from a PPS.
Operating system and other latencies are more signiﬁ-
cant on a Pi than on a larger platform.

Beyond hardware fundamentals comes synchroniza-
tion fundamentals, which depend critically on the syn-
chronization algorithm, and its ability to manage la-
tency variability both in the underlying reference, and
in timestamps thereof. With PPS available, Goal 4 shows
that Pi+hat using ntpd on Raspbian can perform to
within a small factor of a Stratum-1 server on a larger
platform, but erratic behavior if the PPS stability were
for some reason degraded cannot be ruled out, partic-
ular under high system load. We performed some sim-
ple tests of system clock robustness, by repeatedly dis-
abling the hat’s pulse for a few seconds at a time and
then restoring it. We found that the system clock suf-

fered immediate disruption following each missing pulse
event, and took minutes to recover.

For a Pi without PPS, the timestamps used for sys-
tem clock deﬁnition/synchronization would be of pack-
ets exchanged with remote timeservers, which is prob-
lematic as each Pi has a high latency network interface.
For example the Pi-3 has a USB based interface which
is polled from the operating system rather than inter-
rupt driven. Under ideal conditions latencies lie in the
10’s of µs range, but can easily reach 100’s of µs or
even the ms level under heavy load.
It is well docu-
mented that ntpd has signiﬁcant stability issues [19, 17,
16]. When network and system latency variability reach
certain ‘tipping points’, stability is lost, and errors can
jump from a best case value of the order of 1ms to a
wandering error with an amplitude of 10’s or 100’s of
ms, or even beyond. Because latencies are higher on a
Pi, it is more vulnerable to this occuring. Whether any
given error is acceptable is entirely application depen-
dent, however reliability is always important, as without
it error bounds cannot be placed on measurements.

To give an idea of the best possible performance of the
Pi without a PPS based on ntpd, we performed an addi-
tional 30 hour experiment where a Pi-3 was a Stratum-2
client, synchronizing over the LAN to a Stratum-1 ref-
erence NTP server (the SRS) with no other network
traﬃc or system load, while system clock timestamps
were still collected based on in-kernel PPS triggers as
per Section 6. The IPTh(i) timeseries now exhibited
spikes in the ±20µs range and periodicity with am-
plitude 5µs. More importantly, the accumulated error
Sx(i) − (Sx(1) + i − 1) exhibited a wander in a 370µs
band, compared to 30µs previously. This is approach-
ing the ms level, even under these very unrealistic ideal
conditions.

Feedforward synchronization algorithms such as RAD-
clock [19] do not suﬀer from the feedback instabilities

2000250030003500400045005000 IPT - 1 [µs]-6-4-20246reference PPS[µs]-2-1012med= 0, iqr= 0.48 [µs]inter-pulse #i2000250030003500400045005000 IPT - 1 [µs]-6-4-20246hat PPS[µs]-505med= 0, iqr= 0.48 [µs]333above and can deliver high accuracy (wander under
1ms under reasonable assumptions). Moreover, because
RADclock treats time diﬀerence measurement diﬀer-
ently from absolute time at a fundamental level, it can
provide PPS-like accuracy for quantities such as round-
trip and inter-packet times, and delay variation, with-
out the need for PPS [19]. This is ideal for measurement
eﬀorts, such as CAIDA’s Internet topology mapping,
which are based on RTTs. Of course, individual packet
timestamps will still suﬀer from the latencies of the net-
work hardware, even with a perfect system clock.

To conclude, timing suﬃcient for most network mea-
surement purposes is at a fundamental level possible on
a Pi, but to achieve it may require replacing ntpd to en-
sure accuracy, reliability, and meaningful error bounds.
With the right clock synchronization approach, the lim-
itation to timestamp accuracy on the platform is likely
to be the latency of the network interface.
Acknowledgment
Partially supported by Australian Research Council’s
Linkage Projects funding scheme #LP120100073, in part-
nership with Symmetricom (now Microsemi), an ITSP
Tier-2 project grant (ref. no. GHP/027/11) from the
Innovation Technology Fund in Hong Kong, and a re-
search grant from the Joint Universities Computer Cen-
ter of Hong Kong (ref. no. H-ZL17). Thanks also to
Lewis Masters, Nick Gustafson, Brendan Horan, and
Michael Wouters for their insights, and the reviewers
and our shepherd for helpful comments.
8. REFERENCES
[1] Archipelago monitor locations.

http://www.caida.org/projects/ark/locations/.

[2] BISmark. http://www.projectbismark.net.
[3] Raspberry Pi. https://www.raspberrypi.org/.
[4] P. Abry, D. Veitch, and P. Flandrin. Long-range

dependence: revisiting aggregation with wavelets.
Journal of Time Series Analysis (Bernoulli
Society), 19(3):253–266, May 1998.

[5] ARM. RealView Platform Baseboard for

ARM1176JZF- S User Guide., 2011.

[6] P. Barham, B. Dragovic, K. Fraser, S. Hand,

T. Harris, A. Ho, R. Neugebauer, I. Pratt, and
A. Warﬁeld. Xen and the Art of Virtualization. In
SOSP ’03: Proceedings of the nineteenth ACM
symposium on Operating systems principles, pages
164–177, New York, NY, USA, 2003. ACM.

[7] T. Broomhead, J. Ridoux, and D. Veitch. Counter

Availability and Characteristics for Feed-forward
Based Synchronization. In Int. IEEE ISPCS’09,
pages 29–34, Brescia, Italy, Oct. 12-16 2009.

[8] R. Calvo-Palomino, D. Giustiniano, and

V. Lenders. Collaborative Signal Monitoring and
Decoding with Low-Cost Software-Deﬁned Radio.
In Proc. ACM SIGCOMM Internet Measurement
Conf., Santa Monica, Nov. 14-16 2016.

[9] J. C. Corbett, J. Dean, M. Epstein, A. Fikes,

C. Frost, J. J. Furman, S. Ghemawat,
A. Gubarev, C. Heiser, P. Hochschild, W. Hsieh,
S. Kanthak, E. Kogan, H. Li, A. Lloyd, S. Melnik,
D. Mwaura, D. Nagle, S. Quinlan, R. Rao,
L. Rolig, Y. Saito, M. Szymaniak, C. Taylor,
R. Wang, and D. Woodford. Spanner: Google’s
globally-distributed database. In Proceedings of
the 10th USENIX, OSDI’12, pages 251–264,
Berkeley, CA, USA, 2012.

[10] E. K. Lua, T. Griﬃn, M. Pias, H. Zheng, and

J. Crowcroft. On the accuracy of embeddings for
internet coordinate systems. In IMC’05:
Proc. ACM Internet Measurement Conference,
pages 11–11, Berkeley, CA, USA, 2005.

[11] D. L. Mills. The Network Computer as Precision
Timekeeper. In Proc. Precision Time and Time
Interval (PTTI) Applications and Planning
Meeting, pages 96–108, Reston VA, Dec. 1996.
[12] R. Mok, W. Li, and R. Chang. Improving the

packet send-time accuracy in embedded devices.
In Proc. PAM, 2015.

[13] R. Mok, W. Li, R. Chang, K.-W. Yung, C.-H.

Chan, and Y.-S. Poon. An automated testbed for
proﬁling the packet send-time accuracy of
embedded devices. In Proc. TRIDENTCOM
(Poster Session), 2015.

[14] NMEA. Publications and Standards from the

National Marine Electronics Association (NMEA)
/ NMEA 0183, Nov. 2008.

[15] J. Regehr. High-Resolution Timing on the

Raspberry Pi.
http://blog.regehr.org/archives/794.

[16] J. Ridoux and D. Veitch. Ten Microseconds Over

LAN, for Free (Extended). IEEE Trans.
Instrumentation and Measurement (TIM),
58(6):1841–1848, June 2009.

[17] J. Ridoux, D. Veitch, and T. Broomhead. The
Case for Feed-Forward Clock Synchronization.
IEEE/ACM Transactions on Networking,
20(1):231–242, Feb. 2012.

[18] D. Veitch, S. Babu, and A. P´asztor. Robust

Synchronization of Software Clocks Across the
Internet. In Proc. ACM SIGCOMM Internet
Measurement Conf., pages 219–232, Taormina,
Italy, Oct. 2004.

[19] D. Veitch, J. Ridoux, and S. B. Korada. Robust

Synchronization of Absolute and Diﬀerence
Clocks over Networks. IEEE/ACM Transactions
on Networking, 17(2):417–430, April 2009.

[20] D. Veitch and K. Vijayalayan. Network Timing

and the 2015 Leap Second. In Proc. of PAM 2016,
Heraklion, Crete, Greece, March 31 - April 1 2016.

[21] K. Vijayalayan and D. Veitch. Rot at the Roots?
Examining Public Timing Infrastructure. In Proc.
of IEEE INFOCOM 2016, San Francisco, CA,
USA, April 10-15 2016.

334