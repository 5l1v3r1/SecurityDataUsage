Causality-based Sensemaking of Network Trafﬁc

for Android Application Security∗

Hao Zhang, Danfeng (Daphne) Yao, and Naren Ramakrishnan

Department of Computer Science, Virginia Tech

Blacksburg, VA, USA

{haozhang, danfeng, naren}@cs.vt.edu

ABSTRACT
Malicious Android applications pose serious threats to mo-
bile security. They threaten the data conﬁdentiality and sys-
tem integrity on Android devices. Monitoring runtime activ-
ities serves as an important technique for analyzing dynamic
app behaviors. We design a triggering relation model for dy-
namically analyzing network traﬃc on Android devices. Our
model enables one to infer the dependency of outbound net-
work requests from the device. We describe a new machine
learning approach for discovering the dependency of network
requests. These request-level dependence relations are used
to detect stealthy malware activities. Malicious requests are
identiﬁed due to the lack of dependency with legitimate trig-
gers. Our prototype is evaluated on 14GB network traﬃc
data and system logs collected from an Android tablet. Ex-
perimental results show that our solution achieves a high
accuracy (99.1%) in detecting malicious requests sent from
new malicious apps.

1.

INTRODUCTION

Similar to PC malware, researchers found that malicious
mobile applications usually fetch and run code on-the-ﬂy
without the user’s knowledge [49]. Their purposes are often
to stealthily collect and exﬁltrate sensitive information.

Static analysis solutions typically inspect the source code,
binaries or call sequences for detecting anomalies. For ex-
ample, Drebin [4] extracts features including APIs, permis-
sions, and app components to characterize and classify apps.
SCSDroid [20] identiﬁes malicious apps by extracting sub-
sequences of system calls. However, dynamic code load-
ing, Java reﬂection-based method invocation, data encryp-
tion, and self-veriﬁcation of signatures are commonly seen in
the malware code [11, 38]. These types of code obfuscation
make static analysis based detection challenging. Dynamic
analysis, as a complementary to the static analysis, detects
the runtime behaviors of the malicious apps. For example,

∗This work has been supported in part by NSF grant CAREER

CNS-0953638 and ARO YIP W911NF-14-1-0535.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
AISec’16, October 28 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4573-6/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2996758.2996760

TaintDroid [10] is a solution to monitor and verify the sensi-
tive information ﬂows through the apps, but it is infeasible
to learn the origin of sensitive data leaking (e.g., triggered
by users or malicious code fetching from a remote host). Our
solution provides the logic insights between the user’s inter-
action and malicious traﬃc. Besides, as we treat the apps
as blackbox, our solution is lightweight and low-overhead, as
opposed to monitoring the apps as whitebox in TaintDroid.
In this paper, we aim at identifying malicious apps by
analyzing their dynamic behaviors, speciﬁcally the network
traﬃc generated by apps. We proﬁle the traﬃc pattern of
benign apps to detect malicious network requests and en-
hance the sensemaking process.

Sensemaking is an analysis process including the tasks
of investigating the complex data, exploring their connec-
tions, and gaining insights [13]. In this paper, we propose a
learning-based approach to discover dependencies of network
requests and thus detect malicious ones. The problem of de-
pendence discovery on network events has been introduced
in [44]. The dependence of network requests is deﬁned as
the triggering relationship (TR). The triggering relation be-
tween two network requests ri and rj exists if rj cannot be
issued unless ri is sent out ﬁrst (denoted by ri → rj). The
triggering relations on app-generated requests capture how
one network event is related to other events. They model
the underlying logical and semantic relations among net-
work events, which is useful for human experts’ cognition,
reasoning, and decision making in cyber security [12, 43].

The temporal and triggering relationship can be repre-
sented in a directed graph that is referred to as triggering
relation graph (TRG). We refer the ﬁrst request that triggers
others as the root-trigger request. It may be caused by legit-
imate user activities or generated by benign apps. The TRG
is composed of a list of trees, which are rooted by their root-
triggers and can be sorted chronologically. Each tree depicts
one scenario that how user requests the network resources
(e.g., web-browsing, downloading a song). The legitimacy
of the requests relies on the legitimacy of their root-triggers.
Therefore, dependence structure of network requests oﬀers
important insights for identifying network anomalies, and
thus supporting the sensemaking process.

Our Android traﬃc causal analysis targets at the stealthy
network activities via HTTP. HTTP is the protocol cho-
sen by most app developers to implement communication
with remote servers [37] and is hardly blocked by anti-virus
tools. Repackaged apps and drive-by download attacks are
the common initial infection vectors [49]. After the mali-
cious apps are installed, the requests sent to remote hosts

47could leak user’s information or conduct bot activities for
proﬁts. Our proposed solution can detect these types of ac-
tivities without knowing any signatures of the malware, and
thus applicable to detect the new (zero-day) malicious apps.
An important technical enabler of our solution is the abil-
ity to discover the triggering relation of pairs of requests.
We refer to it as the pairwise triggering relation discovery.
Pairwise relations are used to construct the full triggering
relation graph (TRG). The graph depicts the causality of re-
quests and allows one to quickly identify the root triggers of
observed events. Our solution classiﬁes the root triggers to
identify malicious requests based on their dependency fea-
tures. The new capability of our solution is to distinguish
malicious root triggers from legitimate ones in Android net-
work traﬃc.
It allows us to detect the stealthy malware
activities that may not be detected by existing traﬃc anal-
ysis methods (e.g., [9, 37]).

Dependence analysis was proposed for detecting anoma-
lies on hosts in [44]. Our work systematically diﬀers from
theirs in the problem scope and implementations. First, we
extend the triggering relation discovery problem for all types
of apps on mobiles. The deﬁnition of triggering relationship
is studied using the delay injection approach. In comparison,
theirs only handles the browser-generated requests by using
the referrer-based heuristic. Second, their solution requires
a predeﬁned whitelist to ﬁlter out the non-user triggered
benign requests, while we leverage the two-stage learning-
based approach to classify the benign requests.
Our contributions are summarized as follows.
• We utilize the triggering relation model to formalize
the dependency of mobile network requests and traﬃc-
generating user inputs. The discovery of triggering re-
lations on pairwise network events enables us to con-
struct a triggering relation graph. We present a two-
stage learning-based solution and use it to classify the
abnormal requests.
• We conduct our experiments on 14GB network and
system data. Our results show that the triggering re-
lations of network traﬃc on Android can be inferred
at the accuracy of 98.2%. Without knowing any pri-
ori knowledge, our solution enables the detection of
malicious requests sent from the newly released apps.
Results conﬁrm that we can detect 99.1% malicious
requests from all malicious apps.

The signiﬁcance of our work is to provide insights of the
traﬃc dependency for Android security and demonstrate
the use of structural and semantic information in reasoning
about network behaviors and detecting stealthy anomalies.
Dependency analysis is a promising security approach that
is capable of describing how events are related to each other
and further identify the anomalies isolating from others. The
pairwise relationship is designed to compare the features of
two events, which are also applicable to system calls, services
in a network, and events in Internet of Things (IoT). The
key challenges of obtaining pairwise relationship are i) the
features need to suﬃciently characterize the logic relation
between events, and ii) the classiﬁcation needs to be scalable
for voluminous data. In this paper, we present our prototype
to address the problem in the context of Android network
traﬃc. We show its feasibility in detecting zero-day network
anomalies based on the dependencies of network requests.

2. MOTIVATION AND OVERVIEW

We discuss the challenges of extracting network traﬃc de-
pendency in Android and the security applications of our
analysis model. We also give an overview of our solution.
2.1 New Challenges of Triggering Relation Dis-

covery on Mobile Devices

The problem of detecting malicious requests on mobile de-
vices is challenging. We summarize the technical diﬃculties
on the Android platform as follows.

• Lack of referrer. In our study, 83% network traﬃc
sent from Android device is generated by non-browser
apps. The app developers create diverse apps that
communicate with the remote servers via HTTP pro-
tocol. However, the HTTP requests sent from non-
browser apps rarely contain correct and meaningful re-
ferrers. Some apps attach uniﬁed referrer information
for all outbound requests. Therefore, existing solu-
tions including the referrer-based inference [36, 45] or
the instrumented browser [25, 26] do not work for the
general apps.
• Diverse network traﬃc from apps. Compared
with browser-generated traﬃc, requests sent from apps
have varying patterns. How to use a uniﬁed framework
to infer the triggering relationship has not been stud-
ied yet. Processing massive data also demands scalable
solutions.
• Automatic notiﬁcations and updates. Mobile de-
vices constantly alert users with notiﬁcations, resulting
in plentiful network requests in the background. Prop-
erly handling the notiﬁcations is crucial in achieving a
high accuracy and low false positives. The tradition
whitelisting created by human experts is no longer ad-
equate. Hence, to automatically generate whitelisting
using learning-based methods is desirable.

We infer triggering relations by analyzing how the delays are
propagated among the network requests of an app. As a re-
sult, we discover the dependency without requiring the refer-
rer information. We also design a learning-based approach
to classify the triggering relationship of requests, which is
scalable and suitable for all types of Android apps. We build
the TRG based on classiﬁcation results. The graph oﬀers
rich structural and context-aware features which are further
used to identify malicious requests and aid the sensemaking
process. The whitelisting generated from machine learning
outcomes saves human eﬀorts and improves the detection
accuracy.
2.2 Security Applications of Our Detection

Our model detects the malicious network behaviors by
discovering the dependency of network packets on Android
devices. The behaviors of malware include the unauthorized
network activities, e.g., stealthy outbound requests without
user’s awareness, piggyback requests containing malicious
code, and other types of out-of-order requests. These be-
haviors existing in a wide range of malware families cause
sensitive information leaking and system abusing. Malicious
apps may not immediately trigger its malicious behaviors,
because the timing or logic bombs (e.g., debugger-checks,
rooting-checks) could delay the triggering. We list some ex-
amples of our security applications as follows.

48Figure 1: The workﬂow of our triggering relation discovery based malware detection.

• Repackaged apps refer to the malicious apps created
by repackaging the existing benign ones [49]. They are
known to contain malign payloads that may cause ma-
licious requests, which are usually generated without
user’s consent, e.g., AnserverBot malware family.
• Drive-by download apps fetch malicious payloads
at runtime. In our context, drive-by downloading refers
to the unintentional malicious download. For example,
it may lure users and stealthily install a new malicious
app or create a shortcut icon to some malicious or ad-
vertisement sites, e.g., com.Punda.Free.
• Android bots, being controlled by bot masters through
the network, can be used to conducted remote attacks.
Android bots are not exclusive to the other two cate-
gories. Their behaviors often include the stealthy net-
work communication to remote command and control
(C&C) servers.

The benign apps sometimes send out outbound requests
without user’s awareness. As an open problem, these “gray”
requests are diﬃcult to be classiﬁed, because they may be
sent with user’s consents. Our model labels these requests
as suspicious.

We target at the malicious apps running on the user-
space, therefore malware with root access is not in our threat
model. We trust the system logs and keylogger apps to infer
the triggering relationship and root-triggers.

2.3 Overview of Our Approach

A triggering relation graph is composed of network events
and the edges that link them. The problem of triggering re-
lation discovery on a set of network events can be solved by
inferring the triggering relations of pairs of events, which is
deﬁned as the pairwise triggering relation. Given a sequence
of network requests R = {r1, r2, . . . , rn}, the purpose of pair-
wise comparison is to generate a set of pairs P = {P (ri, rj)},
where 1 ≤ i < j ≤ n, ri and rj have a high likelihood to
have a triggering relation. The pairwise comparison method
has been proposed to solve the general relation discovery
problem [16, 17, 44]. In this work, we further advance it in
the Android context by improving the pairing eﬃciency and
using the multinomial classiﬁcation.
Utilizing predicted results from the classiﬁcation, we build
the TRG G = {T1, T2, . . . , Tm}, each Ti is a tree rooted
at request rti . We classify the root-triggers RT = {rti}
(1 ≤ i ≤ m), based on the extracted features from TRG
G. The root-trigger request rti determines the legitimacy

of each tree, i.e., if it is generated from a benign app, the
entire tree Ti is legitimate. The anomalies are the requests
of trees that are not triggered by users, nor belong to the
auto-generated notiﬁcations/updates from benign apps.

We show an overview of the workﬂow in Figure 1. Our
approach requires the data labeling (in training phase) based
on the requests R, details of which are given in §5.

The main operations in our analysis are pairing, TR analysis,
and detection. The ﬁrst two operations are designed to model
the triggering relations and build a TRG. Both TR analysis
and detection include training and testing, as the standard
operations for the machine learning approach. Speciﬁcally,
we adopt a multinomial classiﬁcation in TR analysis and use
a binary classiﬁer in detection.
A) Pairing takes the R as an input and outputs a set of paired

B) TR analysis takes the predicted pairs P as inputs and

requests P = {P (ri, rj)}. (§3.1)
outputs a constructed TRG G. (§3.2)
C) Detection takes a TRG G and its root-trigger set RT as in-
puts. It extracts features of RT and builds classiﬁcations
to predict the legitimacy of root-triggers. (§4)

3. TRIGGERING RELATION MODELING
We describe our triggering relation modeling in this sec-
tion. The goal of this modeling is to build a complete and
accurate TRG for the later detection phase.

# Paired items Relation labels

(1)
(2)
(3)
(4)
(5)
(6)

P (r1, r2)
P (r1, r3)
P (r1, r4)
P (r2, r3)
P (r2, r4)
P (r3, r4)

(b)

Parent-child
Parent-child

Same-tree

Sibling

Parent-child

Same-tree

(a)

Figure 2: A sample tree (a) and its pairing results (b). In
(a), nodes and edges denote the network requests and their
triggering relations respectively.

We introduce the multi-class to describe the structural
relations of two requests ri and rj: (1) parent-child: ri →
rj, (2) sibling: ri and rj have the same parent (i.e., rk →
ri and rk → rj), (3) same-tree: ri and rj are situated on
the same tree (ri ∈ Ti and rj ∈ Ti) without parent-child
and sibling relations, and (4) no-relation: ri and rj on

Training PhaseKnown statetablet trafficData StorageTriggering Relation Modeling PhasePairing(w/ adaptive window)Detection PhaseTR AnalysisCross Validation(to select parameters)TriggeringrelationinformationDetection (on root-triggers) Identify TPsGeneraterules Retrain the model for new dataTraining & Testtraces logsData Labeling(w/ human effort)Real-worldtablet trafficData Storagetraces logsr1r2r3r449two diﬀerent trees. The quad-class oﬀers rich information
to identify the relations for requests in a TRG. Figure 2
illustrates how we generate pairs from a sample tree based
on quad-class labeling and how these relations determine the
pattern of a tree in a TRG.
3.1 Pairwise Feature Extraction

The pairing operation is performed on any two requests ri
and rj in R for whose time diﬀerence is less than a threshold
τ . Let the request ri have p attributes and each is denoted
as ri.attr, attr ∈ {time, IP, port, host, ..., URL}. Then, the
pairing operation is performed on every attribute of the re-
quests. The pairwise comparison can be written as P (ri, rj) =
{f1(ai.time, aj.time), . . . , fp(ai.URL, aj.URL)}, where fk is the
pairing function for the k-th attribute. Shown in Table 1,
diﬀerent types of attributes require diﬀerent pairing strate-
gies (fk).

Attribute

Pairwise Function (fk)

Numerical Compute the diﬀerence.
Nominal

(Sub)string/equality test.
String similarity comparison.
Composite Comparison of sub-attributes.

String

Example(s)

Timestamp.
Protocol.
Host, URL.
IP address.

Table 1: Pairing functions used for diﬀerent attribute types.

Adaptive Pairing. To avoid the O(n2) complexity of
pairing n requests, we only analyze network requests whose
timestamps are within a small time window (threshold). Let
d(i,j) be the time diﬀerence between a pair of requests ri
and rj that have triggering relation. We obtain the 3-day
moving average of d(i,j) as the µ and its corresponding σ2
in the learning phase, and calculate the threshold using the
Chebyshev’s inequality (τ = µ + 10 ∗ σ). The probability of
the diﬀerence between d(i,j) and µ exceeds 10∗σ is no greater
than 1%. The Chebyshev’s inequality oﬀers an upper bound
on the probability of d(i,j), regardless of its distribution.
3.2 Quad-class Classiﬁcation and TRG Con-

struction

In the TR analysis operation, we train classiﬁers to predict
triggering relations on pairs P = {P (i, j)|i < j}. The pre-
dicted results are in the form of (ri, rj) → lij, where lij is
one of the relations in the quad-class.

We analyze three types of relationship: parent-child, sib-
ling, and same-tree and assign weights wp, ws, and wt to
them, respectively.1 The purpose of the weights is to con-
struct the TRG based on the classiﬁed pairwise triggering
relations. Given ri and rj, we deﬁne w(ri,rj ) in Equation 1
as a score of the pair (ri, rj), which is the summation of the
weights that indicate their triggering relationship.

(cid:88)

w

w(ri,rj ) =



wp,
ws,

wt,

if (ri, rj) → parent-child.
if (ri, rk) → parent-child
and (rk, rj) → sibling.

if (ri, rj) → same-tree.

(1)

We show the procedure to calculate the weight for the
classiﬁed pair (ri, rj) in Algorithm 1. The inputs of Algo-
rithm 1 are the predicted results P and the weights (wp, ws,

1In our design, the weights are calculated using multivari-
able linear regression.

and wt). In lines 6-9, there may be multiple triggering candi-
dates (parents) for one request, thus we update the weights
for each node of its parents (denoted by Parent(ri)). The
output of Algorithm 1 is a dictionary of keys D, which con-
tains the calculated weights of pairs that (may) have trig-
gering relationship.

{(ri, rj ) → pij} (1 ≤ i < j ≤ n), and weights wp, ws, wt.

Algorithm 1 Triggering Relation Weights Calculation
Input: Requests R = {r1, r2, . . . , rn}, classiﬁed pairs P =
Output: A dictionary of keys D = {(ri, rj ) = w(ri,rj )}, where
ri and rj may have triggering relationship according to the
classiﬁcation, and its weight is w(ri,rj ).

else if pij = sibling relation then

if pij = parent-child relation then

weight w(ri,rj ) += wp, and update w(ri,rj ) in D
Parent(ri) ← ri’s trigger(s) according to P
for rp in Parent(ri) do

1: deﬁne D to store the weight w(ri,rj ) for the edge ri → rj
2: for each pair (ri, rj ) → pij ∈ P do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
end if
16: end for
17: return D

end for
Root(ri) ← ri’s root request(s) according to P
for each rr in Root(ri) do

weight w(rp,rj ) += ws, and update w(ri,rj ) in D

weight w(rr, rj ) += wt, and update w(ri,rj ) in D

else if pij = same-tree relation then

end for

Building a TRG from D is equivalent to ﬁnding the max-
imal spanning tree in a graph. In our approach, we adopt
Prim’s algorithm, which suggests starting from one vertex
and selects the highest weight from the adjacent neighbors.2
Prim’s algorithm ﬁts our needs of starting from known root-
triggers. The prerequisite in Prim’s algorithm is the weight
calculation. During the process of selecting the largest weight
in Prim’s algorithm, there may be equal weights in D. We
design rules to select the best triggering parent for a given
request from a group of candidates whose weights are equal,
i.e., w(ri,rk) = w(rj ,rk). The following rules are presented in
the order of the precedence.
Rule 1) If ri ∈ Ti, rj ∈ Tj, and rti .time > rtj .time, then
ri → rk. (The rule is in favor of the request on the
most recent dependency tree.)
Rule 2) If ri, rj ∈ Ti, ri.level < rj.level, then ri → rk.
(The rule is in favor of the request on the upper
level of the tree.)
Rule 3) If ri, rj ∈ Ti, ri.level = rj.level, ri.time > rj.time,
then ri → rk.
(The rule is in favor of the most
recent requests on the same level of the tree.)

We make these design choices based on the empirical study
and domain knowledge of the Android network traﬃc. Prim’s
algorithm outputs the TRG G by picking the best candidate
trigger from the equal-weight candidates. As the classiﬁca-
tion results may contain redundant or conﬂicting informa-
tion, it is indispensable to solve the tie-breaking of weights.

2Prim’s algorithm is usually used to ﬁnd the minimal span-
ning tree. However, in our scenario, the larger weights mean
the higher possibility to have triggering relationship. There-
fore, we ﬁnd the maximal spanning tree for G. The same
situation applies when using Kruskal’s algorithm.

50# Predicted results

(1)
(2)
(3)
(4)
(5)
(6)

(r1, r2) → p-c
(r1, r3) → p-c
(r2, r3) → sib
(r3, r4) → p-c
(r3, r5) → p-c
(r4, r5) → p-c

w(ri ,rj )
wp + ws
wp + ws
0
wp
wp
wp

—

Tie-break

(cid:27)
(cid:27) r3 is on upper

No tie.

—

level than r4.

Built TR
r1 → r2
r1 → r3
r3 → r4
r3 → r5

Table 2: The triggering relations are built from the predicted
results. In predicted results, p-c and sib denote the parent-
child and sibling relations respectively. w(ri, rj) shows the
calculated weights using Algorithm 1. The tie-break column
shows the rule to resolve ties. Note that the weight wp + ws
in (1) and (2) is due to the predicted result from (3).

We illustrate our TRG construction using an example in
Table 2. First, we calculate the weights by parsing the pre-
dicted results. According to formulas (1-3), we observe that
r1 has parent-child relation with r2 and r3. Furthermore,
the knowledge of formula (3) is redundant, only formulas
(1-2) are enough to deduce the triggering relations. In the
latter formulas (4-6), the parent of request r5 has two candi-
dates r3 and r4 based on the calculated weights. We resolve
this conﬂict by using our tie-breaking rules.
The complexity of Algorithms 1 is O(|P|2). Due to the
sparse triggering relations in a TRG (i.e., |node| ≈ |edge|),
the complexity of Prim’s algorithm is O(n ∗ log(n)), where
n is the number of requests in R.
Security discussion. The threshold τ in §3.1 determines
the eﬃciency and accuracy in pairing operation. A large τ
causes unnecessary pairs between two irrelevant requests,
resulting in unbalanced datasets, which negatively impacts
the inference accuracy. A small τ leads to insuﬃcient pairs,
resulting in a fragmented TRG. For example, synchronous
requests (e.g., AJAX), retrieved much later than normal re-
quests, could be regarded as vagabond requests, and causes
false positives in detection. Therefore, the adaptive window
size can improve the pairing accuracy by adjusting the τ
according to the recent user’s behaviors.

The TRG construction relies on the weights calculation
algorithm and tie-breaking rules. Malicious requests tend
to hide themselves by building dependencies to benign re-
quests. Wrong triggering relationship results in the wrong
root-trigger, which causes the false positives of malware de-
tection. Our algorithm and rules build correct TRs by ex-
hausting all predicted weights and reinforce the sensemaking
based on the parent-child and slinging relationships.

4. DETECTION ON ROOT-TRIGGERS

In the detection phase, we identify the root triggers RT for
each network request based on the constructed TRG G =
{T1, T2, . . . , Tm}. Most requests are directly or indirectly
triggered by the legitimate user’s inputs (UT). However,
there may exist requests that cannot be linked with any
user’s activities, e.g., notiﬁcation requests from benign apps
(AT), malicious requests from the malware (MT). Therefore,
RT can be formalized as the disjoint sets of the three.

RT = {UT ∪ AT ∪ MT}

(2)

In our design, we use binary classiﬁers to detect the ma-
licious requests (MT) in two steps: (1) In a clean/labeled

dataset, we distinguish the notiﬁcations or update requests
(AT) from other benign ones (UT). A whitelisting can be
generated from the learning outcomes (AT). (2) In a real-
world dataset, we ﬁlter out the requests that belong to the
whitelist, and then diﬀerentiate the malicious requests (MT)
from the benign ones (UT).
§4.1 and the root-trigger identiﬁcation/labeling in §4.2.

We describe the features extraction for root-triggers in

4.1 Feature Extraction for Detection

For a given root-trigger rt ∈ RT, we extract features as
described in Table 3. We train and classify the data using
common supervised machine learning classiﬁers (e.g., ran-
dom forest, logistic regression). The output of the classiﬁ-
cation in detection is the partition of RT.

Type

Feature Deﬁnition / Description

F1

F2

F3

F4

‡

F1 describes the number of similar requests in the
previous user-triggered trees.
f (rt) = |r|, where r ∈ UT, rt.time − r.time ≤ τ∗ †,
fu(ar.attr, art .attr) ≤ τu
F2 describes the number of similar requests in the
previous non-user-triggered trees.
f (rt) = |r|, where r ∈ {AT ∪ MT}, rt.time − r.time ≤ τ∗ †
fu(ar.attr, art .attr) ≤ τu
F3 describes the statistics from the previous user-
triggered trees in G.
f (rt) = |r|, where r ∈ UT, rt.time − r.time ≤ τ∗ †
F4 describes the temporal relation of rt and previous
events. E.g., up is the most recent user event prior to rt,
then f (rt) = |rt.time − up.time|.

‡

† τ∗ is a time threshold used in feature extraction (not the τ for
pairing). τ∗ could be 1 second, 1 minute, 1 hour, 12 hours and etc.
‡ fu is the function to compute the similarity between attributes.
ar.attr denotes a request r’s (semantic) features, e.g., Host, Re-
ferrer, request URL and destination IP.

Table 3: Features extracted for the detection operation.
In this operation, the dependency-based feature set FD =
{F1,F2,F3,F4} contain the rich semantic and structural in-
formation of G and aid the sensemaking in detecting anoma-
lies. The design of FD features is based on our empirical
observations on malicious and benign requests. F1 features
are extracted to characterize the frequently visit websites
and some benign requests. Because these websites can be
cached by apps to save the bandwidth, therefore, the newly
generated requests may not contain referrers. F2 features
are used to characterize the auto-generated benign requests.
Because Android apps may pull notiﬁcations from the re-
mote servers periodically, the IP, host and referrer informa-
tion of the notiﬁcations have usually seen before. F3 and
F4 features are designed to quantify the pattern of UT in G.
The heuristics behind these features is that: 1) requests are
usually sent during the highly interactive periods; 2) user-
trigger requests are inclined to be sent immediately after
the user’s actions, while the idle between two root-trigger
requests is a noticeable long interval.
4.2 Root-trigger Identiﬁcation

To ﬁnd the root-trigger is an essential task in identifying
the normalcy of the requests, as the requests triggered by
the root remain the same legitimacy in our model.

In our solution, we take advantage of the Android system
debug logs (Logcat) and the KidLogger app [18] to accurately
infer the root-triggers. The Logcat serves as a default means

51to view the system debug outputs, and thus can be used
to identify if the request is triggered by user’s requests or
by system/software updates. We focus on the logging levels
information and debug, as they provide the caller app name
and its status information. The KidLogger app is designed to
monitor and track on the Android system as a parental con-
trol tool. We obtain the following information by processing
the Logcat and KidLogger data.

• We identify the notiﬁcation requests (AT) by parsing
the ActivityManager logs from Logcat. The Activity-
Manager is a major class that interacts with overall ac-
tivities running on the Android device. It provides the
information about when a particular service is awake
and invokes network connections.
• When users interact with an app, debug logs reveal
details on when and how the app responses. Integrated
with the information from Kidlogger, we determine the
foreground apps. Then, we correlate the root-trigger
request (UT) with the closest user’s activity prior to it.

4.3 Security Analysis and Limitations

Our dependence analysis model aims at analyzing the net-
work activities of Android devices, while it has certain limi-
tations and possible evasions. Our discussions are as follows.
• Non-HTTP protocol. Our solution targets at the stealthy
network activities via HTTP, because HTTP is the
protocol of choice for most app developers to imple-
ment communication with remote servers [37] and is
hardly blocked by anti-virus tools. By combining other
security tools and policies, one can set up ﬁrewall rules
for apps that use other protocols.
• Data authenticity. As an anti-analysis technique, at-
tackers may forge the user and system events so that
the malicious requests have triggers. Advanced solu-
tions (e.g., [2, 24, 28]) can be used to ensure the au-
thenticity of the user inputs and system events.
• Rooting device. Our prototype requires rooted mobile
devices and the monitored apps run in the user-space.
This requirement may reduce the usability for non-tech
savvy users.
• Encrypted traﬃc. For the encrypted network activi-
ties via HTTPS, one might adopt an authorized proxy to
decrypt and analyze the traﬃc. This method requires
the device to install a self-signed CA to encrypt the
network packets [29].

5. LABELING TRIGGERING RELATIONS

ON MOBILE DATA

A technical challenge in our work is the lack of readily
available labeled data, i.e., network events with labeled trig-
gering relations. We spend a substantial amount of eﬀort
designing methods to label the triggering relation of general
apps. Our approach is based on the timing perturbation. It
delays one request and see if others would be aﬀected. The
rationale behind this approach is that the delay of an indi-
vidual request will be propagated to the requests that are
triggered by it.

We elaborate the approach to label the triggering rela-
tions (data labeling operation in Figure 1). This process is
time-consuming and needs human eﬀorts. Therefore, the

time perturbation method is suitable for generating rules on
small-scale data, which ﬁts the needs of labeling and train-
ing purposes. In comparison, our learning-based approach
described in §3 can be used for analyzing and testing on
large-scale data.

Given two requests ri and rj, where ri triggers rj, the
time diﬀerence d(i,j) can be decomposed into several com-
ponents: (1) DNS query time to obtain the IP address, (2)
TCP connection time and one RTT (round-trip time), (3)
network delays (e.g. queuing delay), and (4) processing time
at both server and client sides. Any of the factors may im-
pact the discovery of causality for non-browser apps. We
use the time sequence of outbound packets to estimate the
triggering relations by creating temporal perturbations. To
distinguish the artiﬁcially-generated delays from other fac-
tors and random noise, we take advantage the statistical tests
to achieve high levels of conﬁdence. Our dependence analy-
sis and Rippler [42] operate at diﬀerent levels of granularity:
we detect dependencies at the level of outbound requests,
while Rippler does so at the level of network services.

Figure 3: We label training data using the timing pertur-
bation and incrementally build the TRG. Dashed lines rep-
resent the intermediate results and solid lines represent the
ﬁnalized triggering relationship.

Our dependency inference consists of three steps. In step
(i), we record R = {r1, r2, . . . , rn}, a series of network traﬃc
generated from one app. The list R serves as a baseline,
where the elements in R are ordered by their timestamps.
In step (ii), we re-act the scenario and delay t milliseconds
for each object in R. This step is based on the fact that rj
is a dependent of ri, if and only if rj is not loaded until ri
is fetched. For step (i) and each iteration in step (ii), we
perform m times to obtain m observations.

In step (iii), we apply statistical tests to the m observa-
tions for identifying the triggering relations. We deﬁne X(j,i)
as the variable for the outbound timestamp of j-th packet
when the i-th packet is delayed (i = 0, for the baseline in
Figure 3). We denote µ(j,i) and σ(j,i) as the mean and SD
of X(j,i). Because the delay injection is independent of each
iteration and the samples are from a normal distribution, we
adopt the two-sample pooled t-test to ﬁgure out whether the
j-th packet is aﬀected by the i-th packet [30]. The null hy-
pothesis is H0 : µ(j,0) = µ(j,i). We report the j-th packet is a
dependent of the i-th packet, once the statistical test rejects
H0. The dependency graph can be incrementally built based
on the ﬁnding of dependency from temporal perturbations,
as shown in Figure 3.

We assume the requests R are not changed during our
study. However, we do observe some requests from an app

BaselinetimeTask: to obtain network requestswith labeled triggering relations.Delay R1Delay R2Delay R3Delay R4timetimetimetimeR1       R2R3R4R5R1       R2R3R4R5R1        R4R5R2R3R1        R2R4R5R3R1       R2R3R4R5R2R3R4R5R1R2R4R3R5R1R2R4R5R3R152may slightly diﬀer between the iterations. Therefore, we
compute the edit distance and regard two URLs identical
if the similarity is less than a threshold. Furthermore, we
conﬁrm such a discrepancy rarely transforms the TRG to a
diﬀerent one.

6. EVALUATION AND RESULTS

We implement a prototype to analyze the real-world net-
work traﬃc on an Android device. Based on the collected
data, we conducted extensive tests on our proposed depen-
dence analysis solution. The questions we seek to answer
are:

tiﬁcations and malicious requests? (§6.5, §6.7)

• How accurate is the prediction for TRs? (§6.4, §6.6)
• How accurate is the classiﬁcation for detecting the no-
• Can it detect new malicious Android apps? (§6.7)
• Is our approach scalable for analyzing real-world data?
What is the performance of our approach? (§6.8)

6.1 Implementation

Our prototype implements all parts of the dependence dis-
covery and detection system. The data are processed on a
Linux machine with Intel i5-3320 and 16GB RAM.

Data collection and preprocessing. We root the de-
vice and install PythonForAndroid to run Python code. We
use tcpdump to sniﬀ HTTP requests whose headers contain
valid GET or POST request. To log the user’s events, we contin-
uously monitor the ﬁles from /data/input/. We use ps and
netstat commands to collect the process information and as-
sociate the network requests with a process. The collection
and preprocessing are implemented with 1,400 lines of code
in Python and Bash.

Pairing and classiﬁcation. We implemented the pairing,
TR analysis and feature extraction for detection in Java using
the Weka library with 1,200 lines of code. To build the TRG,
we wrote 800 lines code in Python.
6.2 Experimental Dataset

The experimental data are obtained from a Nexus 7 tablet.
The device, equipped with our data collecting code, is given
to a graduate student of a university for regular daily use.
The device comes with 28 bundled apps, and then we in-
stalled 36 popular apps via the oﬃcial Android store [3].
We ﬁrst collected the network and system logs for a contin-
uous 72-day period. These 64 apps are regarded as benign
and do not conduct malicious activities. The selected pop-
ular apps cover a wide range of categories, including news,
social network, shopping, games, entertainment, etc.

Thereafter, we installed 24 malicious apps and collected
data for 22 days. The malicious apps include repackaged
apps that covertly fetch advertisement requests, drive-by
download apps that install other apps when running, and
spy apps that keep sending out host and user’s sensitive in-
formation (e.g., bookkeeping). The total dataset is 14 GB,
including 1.4 million user input records, 1.8 million system
logs, and 565 thousand network events after consolidation.

Experiment setup. We conduct two experiments.
• Dataset I: We apply our approach on clean data only.
The purpose is to evaluate its ability to recognize the
auto-generated notiﬁcations and updates from other
benign requests.

• Dataset II: With the full dataset, we randomly insert
the malicious data (by day) into the benign data and
evaluate the eﬀectiveness of our approach to detect
malicious requests.

Dependency labeling of training data. We infer the
triggering relation of browser-generated HTTP requests, ac-
cording to a referrer-based method, which achieves nearly
100% accuracy [44]. To label the dependency of network
traﬃc that comes from non-browser apps, we ﬁrst installed
the apps in the Android emulator.3 We set up a proxy and
route the packets to a Linux host as a single point to control
the outbound traﬃc from the emulator. On the proxy, we
use netem to delay the traversing packets by adding the queu-
ing delays, according to the scheme mentioned in §5. We
run the timing perturbation approach and generate heuris-
tic rules to label all the triggering relations. Examples of
rules and learned patterns are listed in Table 4, for given
two requests ri and rj (ri proceeds rj), and a threshold τ .
With our hand-tuned rules, we discover the triggering re-
lation for 86.1% of HTTP requests. The rest of the requests
are labeled with the timing perturbation method.
6.3 Accuracy and Security Metrics

We apply three classiﬁers4, namely random forest (R-F),
C4.5 algorithm (C45), and logistic regression (LOG) in both
triggering relation modeling and detection phases. The op-
erations are evaluated using the following metrics.

• Pairing coverage (PC). This metric is to evaluate if
the (potential) triggering relation ri → rj (i < j) are
paired using the adaptive pairwise window (size = τ ).

|∀ri, rj ∈ R, s.t. ri → rj, d(i,j) ≤ τ|

P C =

|∀ri, rj ∈ R, s.t. ri → rj|

(3)
• TR accuracy (TRA): For each request r ∈ R, we
calculate the ratio of the number of requests correctly
identiﬁed its trigger to the total number of requests,
based on the ground truth. The metric evaluates the
eﬀectiveness of the classiﬁers.

|∀rj,∃ri ∈ G, s.t. ri → rj|
|∀rj,∃ri ∈ R, s.t. ri → rj|

T RA =

(4)
• Precision, Recall, F-score: False negatives (FN)
are the malicious requests being detected as benign
ones. False positives (FP) are the benign requests be-
ing detected as malicious ones. We use the conven-
tional metrics: i) false positive rate (FPR) and false
negative rate (FNR), ii) precision and recall, and iii)
F-score (the harmonic mean of precision and recall).

6.4 TR Analysis on Dataset I

Pairing. We calculate the adaptive pairing windows size
for dataset I using the 3-day moving average value and stan-
dard deviation (SD). The pairing window size for each sam-
pling day is shown in Figure 4. We observe that the average
of the time diﬀerence d(i,j) is a stable value, though the SD
ﬂuctuates a lot. We conﬁrm that the pairing coverage is
nearly 100% and only a few dependent pairs are missed due
to their extremely long delays. The high pairing coverage is
guaranteed as explained in §3.1.
3The emulator is used for labeling the triggering relations.
All other experiments are based on the real device.
4Classiﬁers are selected based on the 10-fold cross validation.

53No.

1

2

3

4

Rules (to decide that ri → rj , i.e., ri and rj have triggering relationship.)

Related Apps

ri.time - rj .time ≤ τ ∧ (ri.PID = rj .PID ∨ ri.program = rj .program) ∧ ri.host = rj .referrer. Traﬃc sent from browser apps.
ri.time - rj .time ≤ τ ∧ ri.program = rj .program ∧ rj .type = POST ∧ ri.host = rj .host
Traﬃc sent from news, social,
∧ rj .referrer = null.
or education apps.
ri.time - rj .time ≤ τ ∧ ri.program = rj .program ∧ func_sim(ri.host, rj .host) = true
Traﬃc sent from news, media,
∧ (ri.referrer = rj .referrer ∨ rj .referrer = null).
or education apps.
ri.time - rj .time ≤ τ ∧ ri.program = rj .program ∧ ri.destinationIP = rj .destinationIP
Traﬃc sent from news or
∧ func_sim(ri.host, rj .host) = true ∧ ri.program = rj .XRequestWith.
media apps (using AJAX).

Table 4: The examples of summarized triggering relation rules using the time injection method. The attributes of request are
extracted from packet headers and system events. func_sim is used to compare the similarity of two string ﬁelds.

comparison on shopping sites), similar requests are sent to
the same host from diﬀerent tabs. These requests are likely
to be predicted to wrong triggers, because users may switch
the tabs frequently and shortly. As this only happens on
browser apps, a ﬁner-granularity solution on tab-level events
can reduce the wrong predictions.
6.5 Recognition of Auto-generated Trafﬁc

The purpose of this experiment is to evaluate our ability to
recognize the auto-generated notiﬁcations and updates sent
from benign apps. This experiment helps us further reduce
false positive (false alerts). We evaluate the classiﬁcation
based on the results of running the C4.5 algorithm in §6.4.

Figure 6: The F-scores of classifying root-trigger requests
on dataset I.

In Figure 6, we plot the F-score of the three classiﬁers
on classifying the update requests. A peak is observed in all
classiﬁers, and the F-score reaches its highest value using 10-
day training size. This phenomenon is due to the constant
changes of app and user behaviors. Therefore, the size of
training windows is a trade-oﬀ between predict accuracy and
coverage. The prolonged training windows are not eﬀective
in characterizing the root-triggers from recently active apps.
Show in Figure 6, random forest classiﬁer outperforms the
other two classiﬁers and it maintains a relatively high and
steady accuracy.

Whitelist generation. We generate a list of HTTP en-
tries that belong to the auto-generated notiﬁcations and be-
nign update requests. The entries are presented as a 4-tuple
(host, destination IP, request type and app name6). We
label 28 distinct entries in the training data. These en-
tries belong to 6 apps. Random forest classiﬁer identiﬁes
41 new entries from the testing data (52-day data) that can
be added to the whitelist.
In the later detection, the re-

Figure 4: The adaptive pairing window size and its coverage
rate for each sampling day on dataset I.

We ﬁnd that parent-child, sibling, and same-tree rela-
tions account for 3%, 19% and 15% of the total number
of pairs.5 The rest of the pairs (62%) are labeled as no-
relation. We spot that the pairing window size is increased
from the 51st to 53rd days. This prolonged window is due
to a plural of AJAX calls from a browser app (Chrome).
Triggering Relation Modeling. We vary the sizes of
training and test data using i ∈ [1, 5, 10, 15, 20], i.e., we train
the data for i continuous day(s) and test the data in the
following i continuous day(s). Averaged results are reported
until the data on the last day of dataset I is tested. We show
the TR accuracy rates in Figure 5.

Figure 5: The TR accuracy of dataset I.

All classiﬁers predict better with the increasing of the
training sizes. The C4.5 algorithm outperforms the other
two in all settings and it achieves the best TR accuracy at
96.8%.

The wrongly predicted cases mainly come from the browser
apps. When users view multiple similar webpages (e.g. price

5The number of sibling relation is greater than that of same-
tree, which is due to the shallow tree structure formed by
the Android network requests.

6The request type speciﬁes the GET or POST requests. The
app name is identiﬁed using the PID and PPID (parent PID)
information obtained from netstat and ps commands.

1611162126313641465156616671Sampling day050100150200250300350400450500Window size, a.k.a., Pairingthreshold (seconds)0.900.910.920.930.940.950.960.970.980.991.00Pairing coverage3-Day Mov-Avg3-Day Mov-Avg+10*SDParing Coverage1-day5-day10-day15-day20-dayTraining size (days)91.0%91.5%92.0%92.5%93.0%93.5%94.0%94.5%95.0%95.5%96.0%96.5%97.0%TR AccuracyC4.5 AlgorithmRandom ForestLogistic Regression1-day5-day10-day15-day20-dayTraining size (days)0.880.900.920.940.960.98F-Score of classficationon update requestsC4.5 AlgorithmRandom ForestLogistic Regression54quests that match the whitelisted entires can be marked as
benign to reduce the false positives.

FP and FN analysis. For all classiﬁers, the false pos-
itive rate ranges from 1.4% to 2.9%. False positives are
mostly due to that the benign apps occasionally send out
statistical data for advertisement. For example, a social app
sends POST requests to www.google-analytics.com/collect.

The update requests sent to a popular third party host
may be classiﬁed as false negative. For example, a weather
app sometimes fetches metadata ﬁles (JSON or XML) from
s3.amazonaws.com, which counts as FNs.
6.6 TR Analysis on Dataset II

We run the TR Analysis on the dataset II. All TR accuracy
results demonstrate the same pattern as shown in Figure 5.
All TR accuracy results are above 96.0% for training data
that are greater than 5 days. The diﬀerences of TR ac-
curacy are less than 1.0% for three classiﬁers when using
15-day and 20-day training data. The results converge to a
high accuracy, which shows it is feasible to test real-world
(mixed benign and malicious) Android network traﬃc using
a comparable small training dataset.

Figure 7: The TR accuracy of dataset II.

6.7 Detection of Malicious Requests

The purpose of this experiment is to evaluate the eﬀec-
tiveness of constructed triggering relation graph in detect-
ing malware network activities. We ﬁrst remove the notiﬁ-
cations and update requests based on the whitelist in §6.5.
Thereafter, we run the binary classiﬁers on the root-triggers
of the constructed TRG (training size is 10-day). Shown in
Table 5, both tree-based classiﬁers (C45 and R-F) achieve the
better predict accuracy (F-scores) than the log regression.

Classiﬁer

FPR

FNR

Precision Recall

F-score

C45
R-F
LOG

5.2% 0.6%
0.7% 3.6%
8.1% 27.8%

0.991
0.999
0.980

0.994
0.964
0.722

0.993
0.981
0.832

Table 5: Detection accuracy rates on Dataset II.

We label 12 malicious apps, including adware, Trojan, and
drive-by download apps. We successfully detect 12 newly
installed malicious apps throughout the detection. A sum-
mary of the detected malicious apps is listed in Table 6. Our
solution provides a high accuracy in detecting both existing
malware families and new ones of all types. For the known
malware variants, e.g. wbfire.facts and com.crazyapps fami-
lies, our solution detects them as they send requests to fetch
advertisements or conduct bot activities. During the study,

our organization IDS (FireEye) only reported the malware
activities from the com.crazyapps.angry.birds and classiﬁed
it as Trojan.Android.Plankton. FireEye fails to detect other
malware in our experiment.

ID Package Name

Communication Server

Adware
1
2
3
4
5

com.allen.txtjjsz
com.chenyx.tiltmazs
com.zxcalendar.chapp
wbfire.facts family
com.ctinfotech.snake

gw.youmi.net
ade.wooboo.com.cn
api.is.apmob.cn
media.admob.com
media.admob.com

Trojan/Backdoor/Bots/Spyware
6
7†
8‡
9
10
11

com.crazyapps family
com.gfgfgg.dsdf
com.gucdxjdl.batterysaver
com.GoldDream.TingTing06i
com.qnuou.game
com.wing.qingshongxry

searchwebmobile.com
send.cxpts.com
send.cxpts.com
lebar.gicp.net
lebar.gicp.net
static2.ad.anzhi.com

Drive-by download/Update attack
7†
8‡
12
†, ‡: Both apps exhibit malicious behaviors in two categories.

com.gfgfgg.dsdf
com.gucdxjdl.batterysaver
com.Punda.Free

subscription.teebik.com
au.umeng.com
ad.leadboltapps.net

Table 6: The package name and the communication server of
malicious apps in our testing set of dataset II.

The malicious apps we detect cover a wide range of ad-
ware, Trojan, bots, spyware and drive-by download apps.
For example, a game app (com.Punda.Free) generates a short-
cut link icon to ad.leadboltapps.net/show_app_icon on the
Android desktop once it is installed. It issues GET and POST
requests to ad.leadboltapps.net. A bot app (com.gfgfgg.dsdf),
pretending to oﬀer optimization tools for Android, actively
sends out POST requests to send.cxpts.com every 5 minutes
in the background. We note that the Android bot is not
included in the training set, while we can detect it in the
testing. We identify the bot’s malicious activities, as their
requests are the lack of valid triggers. The detection does
not rely on its signatures or C&C servers information.

We regard these malicious apps as new, as none of them is
in the training set. Their traﬃc patterns may never be seen
before. Results show that our solution successfully identiﬁes
all malware by ﬂagging more than 99.1% their requests (out
of 33,000+ HTTP requests). Our solution can detect them
because the triggering relation tells the causality of requests
and the root-triggers reveal the requests’ legitimacy. Our
method does not have any assumptions on the type of apps
or what code obfuscation techniques used in malware.

Comparison with existing solutions. Existing solu-
tions for detecting malicious URLs and domains use tem-
poral, lexical and host-based features from the root-trigger
requests [5, 22]. These features are eﬀective in detecting
malware that communicates to the hosts containing abnor-
mal and meaningless URLs. However, their solutions do not
consider human inputs and event triggers.

In our experiments, we conﬁrm that the same classiﬁers,
solely using lexical and host-based features, fail to identify
3.3%-4.9% malicious requests and cannot detect 4 malicious
apps in Dataset II. By checking the network traﬃc of these
malicious apps, we found that their URLs of communication
hosts contain the common English vocabulary seen in benign
websites.

FP and FN analysis. Based on the detection using C4.5
algorithm classiﬁer, we ﬁnd that the FPs are mostly due to

1-day5-day10-day15-day20-dayTraining size (days)93.5%94.0%94.5%95.0%95.5%96.0%96.5%97.0%97.5%98.0%98.5%TR AccuracyC4.5 AlgorithmRandom ForestLogistic Regression55the redirected traﬃc to some well-known domain servers.
E.g., when a malicious app fetches a list of apps in its We-
bView for luring users to download, some requests are sent
to lh3.ggpht.com for retrieving images. These requests are
FPs, because the domain is benign. Other FPs may be due
to the uncommon domain names and unconventionally long
request strings. FNs are mainly sent by spyware or Trojan.

6.8 Performance

We evaluate the performance in terms of i) the pairing
eﬃciency for using our pairing algorithm and a baseline,
and ii) the running time of testing on pairwise comparisons
and detecting on root-triggers. We obtain the performance
results by averaging the running time over 5 rounds.

Pairing eﬃciency. The baseline pairing operation was
proposed in [44], which uses a ﬁxed time threshold as pairing
window and an eﬃcient pairing algorithm to pre-screen the
data before pairing.
It takes 185 and 121 seconds to use
the baseline algorithm and ours respectively, for pairing all
requests in our dataset. Our algorithm improves at least
30% pairing performance.

TR analysis and detection.

In Table 7, we report
the runtime of each operation in our prototype. The per-
formance of the classiﬁers is consistent for both datasets in
each operation. The training time is a dominant factor of
the total running time for all classiﬁers.

Operation Dataset

Runtime: training + test (seconds)

Pairing

TR

analysis

Detection

I
II

I
II
I
II

C45

0.13+1.9e-3
0.18+2.3e-3
0.05+8.2e-3
0.03+1.1e-3

0.064
0.087

R-F

0.16+2.3e-3
0.19+3.1e-3
0.07+9.7e-3
0.03+2.0e-3

LOG

1.65+6.8e-3
1.66+7.2e-3
0.21+1.2e-2
0.17+4.8e-3

Table 7: The performance of each operation is shown. The
results are calculated in seconds per 1000 records. The run-
ning time includes both training and testing in TR analysis
and detection operations.

In both TR analysis and detection operation, C4.5 algo-
rithm takes the least running time. The random forest clas-
siﬁcation is slightly slower than C4.5 algorithm, while lo-
gistic regression takes the longest time. Overall, the total
runtime of the detection operation is signiﬁcantly less than
that of the TR analysis, as the size of RT is much less than
that of P, i.e., |RT| (cid:28) |P|.

Summary. We highlight our ﬁndings below.
• The C4.5 algorithm and random forest classiﬁers give
high precision and recall on ﬁnding the triggering rela-
tions and identifying the malicious requests. The false
positive rate is 0.7% using the random forest classiﬁer.
Our approach to detecting malicious requests is scal-
able and does not require training on the entire data.
• Our evaluation involving 33,000+ Android app requests
achieves a high detection accuracy rate. Results show
that 99.1% of stealthy network activities with remote
hosts can be identiﬁed. We conﬁrm the detection capa-
bility of our approach by pinpointing the sparse anoma-
lies out of voluminous traﬃc data.

7. RELATED WORK

Existing work on protecting Android system and detecting
malicious apps is studied in the form of static analysis [21,
40], taint analysis [10, 38, 41], and privilege control [14, 32,
47]. Most static analysis solutions for detecting malicious
apps leverage the features from API calls [1, 35, 39], system
calls [20], function calls [15].

Dynamic analysis solutions (e.g., [6, 10, 41]) focus on ana-
lyzing the application behaviors at runtime. Crowdroid [6]
classiﬁes the benign and malicious apps based on the com-
monly seen system calls, which are obtained from real traces
via crowdsourcing. TaintDroid [10] monitors the sensitive
data ﬂow on the system-wide, but it cannot provide the
insights of how the data are triggered from the user’s per-
spective. AppIntent [41] considers the user-intended data
transmission on Android and builds the missing leak be-
tween the data leaking and user’s interactions, while it needs
tremendous eﬀorts in static taint analysis as it treats the
apps as whitebox. SmartDroid [48] proposes a hybrid anal-
ysis method to identify UI-based event trigger conditions
using the sensitive APIs. Our solution systematically diﬀers
from the aforementioned dynamic analysis, as ours provides
a new aspect by connecting the knowledge from user’s in-
teraction and network traﬃc for analysis the app’s behav-
ior, while treating the app as blackbox. App Guardian [46]
proposes to temporally pause suspicious background process
to prevent the potential data leaking. We do not solve the
side channel attacks as App Guardian does, but our solution
prevents the data exﬁltration by identifying the suspicious
outbound traﬃc. Recent work on policy analysis [31] uses
semi-supervised learning to model and analyze the policy
reﬁnement process. In our work, we applied the learning-
based approach on modeling and enforcing the dependency
of network requests.

Dependence analysis plays an important role in detec-
tion Android malware. In our work, we utilize the depen-
dence analysis on Android-generated network traﬃc to de-
tect malicious requests and apps. Our approach extends
the technologies on dependence analysis of network traf-
ﬁc [19,42,44,45] and outperforms rule-based methods [8,36].
Webprophet [19] and Rippler [42] present an approach to ac-
tively inject delays to infer the packet or service dependency.
Their solutions are designed for the purposes of performance
optimization and service dependency inference, as opposed
to malware detection. WebWitness [27] is proposed to trace
back the sequence of events (e.g., visited web pages) preced-
ing malware downloads. It leverages automatically labeled
malware download paths to better understand the attack
trends, especially how users reach attack pages on the web.
A most related work is introduced in [44]. Yet, our solution
diﬀers from theirs in three aspects.

1. Their work depends on a referrer-based heuristic, which
limits its applications. In comparison, we use the delay
injection approach for discovering triggering relations.
2. Their work is browser speciﬁc, as it requires a browser
extension to log users’ inputs. Our solution supports
all types of apps on Android and is more general.

3. Their solution requires a straightforward manually gen-
erated whitelist to ﬁlter out non-user triggered benign
requests. We designed a novel two-stage learning ap-
proach to automatically recognize benign requests.

56Android network traﬃc classiﬁcation has been studied in
the literature [7,9,33,37]. Authors in [37] proposed to distin-
guish the mobile traﬃc by investigating the traﬃc identify
from HTTP headers. NetworkProﬁler introduces a tech-
nique to generate network proﬁles for identifying Android
apps based on their HTTP traﬃc [9]. Recent studies [7, 23]
show that an attacker can recognize user’s actions by analyz-
ing the network traﬃc, even it is encrypted. In comparison,
our solution is focused on inferring the traﬃc dependence
for identifying the malicious requests, which is beyond the
traﬃc classiﬁcation and proﬁling problems.

Also related are studies that proposed classiﬁcation meth-
ods for identifying the suspicious/phishing URLs [5, 22, 34].
For example, Ma et al. proposed to examine the lexical
features of the URLs and features of the domain informa-
tion to identify malicious URLs [22]. Authors in [34] utilized
page content-related features to detect phishing pages. Most
of their features can be categorized into: i) time-based, ii)
lexical and iii) host-based features. However, they do not
include the dependency features as ours. Our study makes
an important step towards addressing the signiﬁcance of de-
pendency knowledge in the detection of malicious requests
on Android.

8. CONCLUSION AND FUTURE WORK

We described an Android malware detection technique
that analyzes the dependency of mobile network traﬃc. Our
analysis explores the request-level traﬃc dependence and
reasons about the root-triggers for all HTTP requests sent
from the device. We successfully demonstrated the use of
triggering relation discovery for enhancing the sensemaking
for security and identifying suspicious requests. Our evalua-
tion conﬁrms that our solution can identify 12 new malicious
Android apps that are not previously seen in the training
set. For the future work, we plan to deploy our tool to col-
lect Android network traﬃc and detect more malicious apps
via crowdsourcing. Also, we plan to extend our solution for
real-time triggering relation inference and online detection.

9. REFERENCES
[1] Y. Aafer, W. Du, and H. Yin. DroidAPIMiner: Mining

API-level features for robust malware detection in Android.
In SecureComm’13. Springer.

[2] H. M. Almohri, D. Yao, and D. Kafura. Process

authentication for high system assurance. IEEE TDSC,
11(2):168–180, 2014.

[3] Oﬃcial Android store. https://play.google.com/store/apps.
[4] D. Arp, M. Spreitzenbarth, M. Hubner, H. Gascon, and

K. Rieck. DREBIN: eﬀective and explainable detection of
Android malware in your pocket. In NDSS’14, 2014.

[5] L. Bilge, E. Kirda, C. Kruegel, and M. Balduzzi.

EXPOSURE: Finding malicious domains using passive
DNS analysis. In NDSS’11.

[6] I. Burguera, U. Zurutuza, and S. Nadjm-Tehrani.

Crowdroid: behavior-based malware detection system for
Android. In SPSM’11, pages 15–26.

[7] M. Conti, L. V. Mancini, R. Spolaor, and N. V. Verde.

Can’t you hear me knocking: Identiﬁcation of user actions
on Android apps via traﬃc analysis. In CODASPY’15,
pages 297–304, 2015.

[8] W. Cui, R. H. Katz, and W.-t. Tan. BINDER: an

extrusion-based break-in detector for personal computers.
In ACSAC’05, pages 361–370.

[9] S. Dai, A. Tongaonkar, X. Wang, A. Nucci, and D. Song.

NetworkProﬁler: Towards automatic ﬁngerprinting of
Android apps. In INFOCOM’13, pages 809–817, 2013.

[10] W. Enck, P. Gilbert, S. Han, V. Tendulkar, B.-G. Chun,

L. P. Cox, J. Jung, P. McDaniel, and A. N. Sheth.
TaintDroid: an information-ﬂow tracking system for
realtime privacy monitoring on smartphones. ACM TOCS,
32(2):5, 2014.

[11] W. Enck, D. Octeau, P. McDaniel, and S. Chaudhuri. A

study of Android application security. In USENIX’11, 2011.
[12] A. Endert, P. Fiaux, and C. North. Semantic interaction for

sensemaking: inferring analytical reasoning for model
steering. IEEE VCG, 18(12):2879–2888, 2012.

[13] A. Endert, P. Fiaux, and C. North. Semantic interaction for

visual text analytics. In CHI’12, pages 473–482, 2012.

[14] Y. Fratantonio, A. Bianchi, W. K. Robertson, M. Egele,
C. Kruegel, E. Kirda, and G. Vigna. On the security and
engineering implications of ﬁner-grained access controls for
android developers and users. In DIMVA’15, pages
282–303, 2015.

[15] H. Gascon, F. Yamaguchi, D. Arp, and K. Rieck. Structural
detection of Android malware using embedded call graphs.
In AISec’13, pages 45–54.

[16] L. Getoor and C. P. Diehl. Link mining: a survey. SIGKDD

Explor. Newsl., 7(2):3–12, December 2005.

[17] I. Kahanda and J. Neville. Using transactional information

to predict link strength in online social networks. In
ICWSM’09.

[18] Kidlogger. http://kidlogger.net/about.html.
[19] Z. Li, M. Zhang, Z. Zhu, Y. Chen, A. G. Greenberg, and

Y.-M. Wang. WebProphet: Automating performance
prediction for web services. In NSDI’10.

[20] Y.-D. Lin, Y.-C. Lai, C.-H. Chen, and H.-C. Tsai.

Identifying Android malicious repackaged applications by
thread-grained system call sequences. computers &
security, 39:340–350, 2013.

[21] L. Lu, Z. Li, Z. Wu, W. Lee, and G. Jiang. CHEX:

statically vetting Android apps for component hijacking
vulnerabilities. In CCS’12, pages 229–240, 2012.

[22] J. Ma, L. K. Saul, S. Savage, and G. M. Voelker. Beyond

blacklists: learning to detect malicious web sites from
suspicious URLs. In KDD’09, pages 1245–1254, 2009.

[23] B. Miller, L. Huang, A. D. Joseph, and J. D. Tygar. I know

why you went to the clinic: risks and realization of https
traﬃc analysis. In PETS’14, pages 143–163. Springer, 2014.

[24] D. Muthukumaran, A. Sawani, J. Schiﬀman, B. M. Jung,

and T. Jaeger. Measuring integrity on mobile phone
systems. In SACMAT’08, pages 155–164. ACM, 2008.

[25] C. Neasbitt, B. Li, R. Perdisci, L. Lu, K. Singh, and K. Li.
WebCapsule: Towards a lightweight forensic engine for web
browsers. In CCS ’15, pages 133–145, 2015.

[26] C. Neasbitt, R. Perdisci, K. Li, and T. Nelms. ClickMiner:

Towards forensic reconstruction of user-browser interactions
from network traces. In CCS’14, pages 1244–1255.

[27] T. Nelms, R. Perdisci, M. Antonakakis, and M. Ahamad.
WebWitness: investigating, categorizing, and mitigating
malware download paths. In USENIX Security’15, pages
1025–1040, 2015.

[28] F. Roesner, T. Kohno, A. Moshchuk, B. Parno, H. J.

Wang, and C. Cowan. User-driven access control:
Rethinking permission granting in modern operating
systems. In SP’12, pages 224–238. IEEE, 2012.

[29] D. Shackleford. Blind as a bat? supporting packet

decryption for security scanning (white paper). November
2012.

[30] G. Snedecor. Statistical methods. Iowa State University

Press, 1989.

[31] R. Wang, W. Enck, D. Reeves, X. Zhang, P. Ning, D. Xu,

W. Zhou, and A. M. Azab. EASEAndroid: Automatic
policy analysis and reﬁnement for security enhanced
Android via large-scale semi-supervised learning. In
USENIX Security’15, pages 351–366, 2015.

57[32] Y. Wang, S. Hariharan, C. Zhao, J. Liu, and W. Du.

Compac: Enforce component-level access control in
Android. In CODASPY’14, pages 25–36.

[33] X. Wei, L. Gomez, I. Neamtiu, and M. Faloutsos.

ProﬁleDroid: multi-layer proﬁling of Android applications.
In Mobicom’12, pages 137–148, 2012.

[34] C. Whittaker, B. Ryner, and M. Nazif. Large-scale

automatic classiﬁcation of phishing pages. In NDSS’10.

Android for privacy leakage detection. In CCS’13, pages
1043–1054.

[42] A. Zand, G. Vigna, R. Kemmerer, and C. Kruegel. Rippler:

Delay injection for service dependency detection. In
INFOCOM’14.

[43] H. Zhang, M. Sun, D. Yao, and C. North. Visualizing traﬃc

causality for analyzing network anomalies. In IWSPA’15,
pages 37–42, 2015.

[35] D.-J. Wu, C.-H. Mao, T.-E. Wei, H.-M. Lee, and K.-P. Wu.

[44] H. Zhang, D. Yao, and N. Ramakrishnan. Detection of

DroidMat: Android malware detection through manifest
and API calls tracing. In Asia JCIS’12, pages 62–69.

[36] G. Xie, M. Iliofotou, T. Karagiannis, M. Faloutsos, and

Y. Jin. ReSurf: Reconstructing web-surﬁng activity from
network traﬃc. In IFIP Networking Conference, 2013,
pages 1–9.

[37] Q. Xu, Y. Liao, S. Miskovic, M. Baldi, Z. M. Mao,

A. Nucci, and T. Andrews. Automatic generation of mobile
app signatures from traﬃc observations. In INFOCOM’15.
[38] L. Yan and H. Yin. DroidScope: Seamlessly reconstructing

the OS and dalvik semantic views for dynamic Android
malware analysis. In USENIX’12, pages 569–584, 2012.
[39] C. Yang, Z. Xu, G. Gu, V. Yegneswaran, and P. Porras.
DroidMiner: Automated mining and characterization of
ﬁne-grained malicious behaviors in Android applications. In
ESORICS’14, pages 163–182. Springer, 2014.

[40] W. Yang, X. Xiao, B. Andow, S. Li, T. Xie, and W. Enck.
AppContext: Diﬀerentiating malicious and benign mobile
app behaviors using context. In ICSE’15, 2015.

[41] Z. Yang, M. Yang, Y. Zhang, G. Gu, P. Ning, and X. S.

Wang. AppIntent: Analyzing sensitive data transmission in

stealthy malware activities with traﬃc causality and
scalable triggering relation discovery. In ASIACCS’14,
pages 39–50.

[45] H. Zhang, D. Yao, N. Ramakrishnan, and Z. Zhang.

Causality reasoning about network events for detecting
stealthy malware activities. Computers & Security,
58:180–198, 2016.

[46] N. Zhang, K. Yuan, M. Naveed, X. Zhou, and X. Wang.

Leave me alone: App-level protection against runtime
information gathering on Android. In SP’15, pages
915–930, 2015.

[47] X. Zhang, A. Ahlawat, and W. Du. AFrame: Isolating
advertisements from mobile applications in Android. In
ACSAC’13, pages 9–18.

[48] C. Zheng, S. Zhu, S. Dai, G. Gu, X. Gong, X. Han, and
W. Zou. SmartDroid: an automatic system for revealing
UI-based trigger conditions in Android applications. In
SPSM’12, pages 93–104. ACM, 2012.

[49] Y. Zhou and X. Jiang. Dissecting Android malware:

Characterization and evolution. In SP’12, pages 95–109.
IEEE, 2012.

58