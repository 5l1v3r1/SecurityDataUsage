Randomized Response Schemes,

Privacy and Usefulness∗

Francesco Aldà

Horst Görtz Institute for IT Security

and Faculty of Mathematics

Ruhr-Universität Bochum, Germany

francesco.alda@rub.de

Hans Ulrich Simon

Horst Görtz Institute for IT Security

and Faculty of Mathematics

Ruhr-Universität Bochum, Germany

hans.simon@rub.de

ABSTRACT
We deﬁne the notions of weak and strong usefulness and in-
vestigate the question whether a randomized response sche-
me can combine two conﬂicting goals, namely being weakly
(or even strongly) useful and, at the same time, providing
ε-diﬀerential privacy. We prove the following results. First,
if a class F cannot be weakly SQ-learned under the uni-
form distribution, then ε-diﬀerential privacy rules out weak
usefulness. This extends a result from [6] that was con-
cerned with the class of parities. Second, for a broad variety
of classes F that actually can be weakly SQ-learned under
the uniform distribution, we design a randomized response
scheme that provides ε-diﬀerential privacy and strong use-
fulness.

Categories and Subject Descriptors
F.2.m [Analysis of Algorithms and Problem Com-
plexity]: Miscellaneous; K.4.1 [Computers and Society]:
Public Policy Issues—Privacy

General Terms
Theory

Keywords
SQ-learning; Margin Complexity; Privacy

1.

INTRODUCTION

The perspectives of private data analysis and learning the-
ory are, in some sense, dual to each other, and both areas
are closely related. In learning theory, one has a sample (a
kind of database) and would like to infer information about
an unknown target concept. In private data analysis, one
has only indirect access to a database via queries. Counting

∗The research was supported by the DFG Research Training

Group GRK 1817/1

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
AISec’14, November 7, 2014, Scottsdale, Arizona, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2953-1/14/11 ...$15.00.
http://dx.doi.org/10.1145/2666652.2666654.

queries, in particular, are represented by predicates (like bi-
nary concepts in learning theory). An analyst of a database
D chooses a query f (roughly corresponding to a concept)
and wants to “learn” something about the unknown data in
D. Typically, she is not interested in the precise or approx-
imate reconstruction of D but wants to get some piece of
aggregated information only (like, for instance, the fraction
of instances in D satisfying the predicate f in case of a count-
ing query). With direct access to D, this would be child’s
play. However, if D contains sensitive information, the fol-
lowing obstacles will make the task of learning something
about the data more challenging:

• We would like the database access mechanism to pro-
vide “ε-diﬀerential privacy” [6] (a very well-established
notion by now).

• We would like to use very special database access mech-

anisms named “randomized response schemes” [6].
They have the advantage of not relying on trust in the
database manager.

This paper investigates to what extent randomized response
schemes M can combine ε-diﬀerential privacy with the con-
ﬂicting goal of providing useful answers to counting queries
taken from a (known) class F. In order to provide ε-diﬀeren-
tial privacy, the database D is transformed into a “noisy
database” ˆD = M(D). The crucial question is whether one
can still extract useful information from ˆD. This general
problem is addressed by a series of prior works (see for ex-
ample [4, 10]), which focus on a worst-case scenario, namely
asking that, for all possible choices of the database D, the
answers provided by the scheme are close (in probability) to
the “correct” ones. It turns out that the family of queries
for which this goal can be achieved is quite limited (see Sec-
tion 2.4 for further details).
In this paper, we relax the
requirement on the usefulness (thereby avoiding any trivi-
alization of the problem itself), by analyzing a “minimum”
goal which every private randomized response scheme should
accomplish. Speciﬁcally, we assume that “positive” instances
(satisfying the predicate underlying the counting query) en-
tering a database D are chosen uniformly at random from
the set of all positive instances in the universe. A similar
remark applies to “negative” instances. An extremely weak
notion of “usefulness” is obtained when we merely want to
be able to distinguish a database consisting exclusively of
negative instances from a database consisting exclusively of
positive ones (so that the correct answer to the query would
be either 0 or 1).
In [6], it is shown that, for the query
class of parities, even this (extremely) weak criterion cannot

15be satisﬁed by randomized response schemes that provide ε-
diﬀerential privacy (unless the database becomes exponen-
tially large). In this paper, we show the following results.
First, the (just mentioned) negative result from [6] can be
extended to any class that is not weakly SQ-learnable under
the uniform distribution (which includes the class of pari-
ties as a special case). Second, for a broad variety of classes
F that actually are weakly SQ-learnable under the uniform
distribution, we design a randomized response scheme that
provides ε-diﬀerential privacy and, at the same time, meets
a quite strong criterion of usefulness for every f ∈ F , namely
allowing to infer (in probability) from ˆD an approximation
ˆω of the true fraction ω of instances in D satisfying f .

We would like to stress that SQ-learnability is a quite in-
ﬂuential model with rich relations to other concepts in ma-
chine learning theory like, for instance, margin complexity
or evolvability [20, 7]. The results in this paper (and pre-
vious ones [14, 4, 10]) show that these concepts are of high
relevance in the ﬁeld of Diﬀerential Privacy too, so as to
establish a strong connection between these two ﬁelds.

2. DEFINITIONS, FACTS, AND MAIN RE-

SULTS

For a set S, 1S(x) ∈ {0, 1} denotes the characteristic
function with value 1 iﬀ x ∈ S. For a vector v with N
components, diag(v) denotes the (N × N )-matrix with the
components of v on the main diagonal and zeros elsewhere.
A probability vector p is a vector with non-negative compo-
nents that sum up to 1. The statistical diﬀerence between
two probability measures P, Q (sometimes called the total
variation distance) is given by
(cid:107)P − Q(cid:107)1 =

|P (x) − Q(x)| ,

(cid:88)

SD(P, Q) =

1
2

1
2

x

where, in the continuous case, the sum must be replaced by
an integral. The following is known:

Lemma 1

([17]). Given a sample point x drawn accord-
ing to P or Q with probability 1/2, respectively, the task of
deciding whether x was sampled according to P or Q has
Bayes error (1 − SD(P, Q))/2.
As usual, (cid:107)v(cid:107)2 denotes the Euclidean norm of the vector v.
For a matrix A, its spectral norm, denoted |||A|||2, and its
Frobenius norm, denoted (cid:107)A(cid:107)2, are given by

|||A|||2 = max
(cid:107)v(cid:107)2=1

(cid:107)Av(cid:107)2 and (cid:107)A(cid:107)2 =

A2
i,j

.

(1)

(cid:33)1/2

(cid:32)(cid:88)

i,j

It is well known that

|||A|||2 ≤ (cid:107)A(cid:107)2 and
|||A(cid:62)|||2

2 = |||A|||2

2 = max{v

A
2.1 Private Data Analysis

(cid:62)

(cid:62)

Av : (cid:107)v(cid:107)2 = 1} .

(2)

Let X be a ﬁnite universe which can be thought of as a
large, but ﬁnite, set of data records. A counting query over
X is given by a predicate f : X → {0, 1}. Every predicate
f splits X into Xf,0 and Xf,1 where, for b = 0, 1, Xf,b =
{x ∈ X| f (x) = b}. The predicate f is called balanced if
|Xf,0| = |Xf,1|. Let F be a ﬁnite class of counting queries
over X . It is called balanced if every f ∈ F is balanced.
A database over X is a tuple D ∈ X n for some n ∈ N. n

(cid:80)n

1
n

is called the size of D. Given a counting query f and a
database D = (d1, . . . , dn) ∈ X n, we may think of ¯f (D) :=

i=1 f (di) as the correct answer.

Definition 1

([6]). Let R be a (possibly inﬁnite) set.
A random map M : X ∗ → R (meaning that, for every
D ∈ X ∗, M(D) is an R-valued random variable) is said
to provide ε-diﬀerential privacy if, for every n ∈ N, for ev-
ery pair (D, D(cid:48)) ∈ X n × X n of databases diﬀering in one
entry only, and for every measurable S ⊆ R, the condition

Pr[M(D) ∈ S] ≤ eε Pr[M(D

(cid:48)

) ∈ S]

is valid.

Random maps, sometimes called database access mechanisms
in this context, are used for generating “noisy answers” to
queries. On one hand, the amount of noise should be large
enough for providing ε-diﬀerential privacy. On the other
hand, the noise should not torpedo the usefulness of the an-
swers. In this paper we focus on non-interactive database
access mechanisms meaning that the same mechanism is
used for a whole class F of counting queries. Useful non-
interactive database access mechanisms are harder to design
than their interactive counterparts1 but have the advantage
that the information M(D) can be computed once for the
whole class F and be made public. However, for obvious
reasons, the party that transforms D into M(D) must be
trusted.
In absence of a trusted party, one may need an
even more restricted access mechanism, which is deﬁned as
follows:

Definition 2

([6]). A randomized response scheme is

a database access mechanism M of the form

M(d1, . . . , dn) = (Z(d1), . . . , Z(dn))

(3)

for some random map Z : X → R.
Note that, in this case, M provides ε-diﬀerential privacy if
and only if Z does (so that the random map Z is forced
to blur almost all information in the individual data di). In
applications, a randomized response scheme enables users to
upload their data di in the noisy form Z(di) on the database
(without the need for a trusted party).

The following distribution has proved quite useful for pro-

viding ε-diﬀerential privacy:

Definition 3. For any λ > 0, the Laplace-distribution
Lap(λ) is the distribution over the reals that is given by the
density function

h(y) ∝ exp(−|y|/λ) .

The d-dimensional Laplace-distribution is the product of d
independent (1-dimensional) Laplace-distributions Lap(λi),
i = 1, . . . , d. It is denoted Lap((cid:126)λ) for (cid:126)λ = (λ1, . . . , λd). If,
for some λ > 0, λ = λ1 = . . . = λd, then we simply write
Lap(λ)d instead of Lap(λ, . . . , λ).
Adding Laplace-distributed noise to an Rd-valued function
f is eﬀective if f has a low “sensitivity” in the sense of the
following

1where the chosen random map may depend on the actual
query, or even on all queries that happened in the past

16Definition 4

X n → Rd is given by

([6]). The sensitivity of a function f :

S(f ) = sup
D,D(cid:48)

(cid:107)f (D) − f (D

(cid:48)

)(cid:107)1

where the supremum is taken over all D, D(cid:48) ∈ X n that diﬀer
in one entry only.

With these deﬁnitions, the following holds:

Lemma 2

([6]). Let f : X n → Rd be a function of ﬁnite
sensitivity, and let Y ∼ Lap(S(f )/ε)d. Then, the random
function F given by F (D) = f (D)+Y provides ε-diﬀerential
privacy.
2.2 Prerequisites from Learning Theory
In learning theory, a class F of counting queries is usually
called a concept class and the underlying universe X is called
the domain. For our purpose, it will be convenient to deal
with the sign matrix A = (A[x, f ]) ∈ {−1, 1}X×F such that
A[x, f ] = 2f (x)−1. As shown in [13], the (weak) learnability
of F in the so-called SQ-model of learning [15] is closely
related to linear arrangements A for the sign matrix A and
the margins achieved by A. In this section, we brieﬂy remind
the reader of these relations.
sign matrix A ∈ RM×N is given by

A d-dimensional (homogeneous linear) arrangement for a

A = (u1, . . . , uM ; v1, . . . , vN )

where ui, vj are vectors in Rd of unit Euclidean norm. With
an arrangement A for A, we associate the margin parameters
γi,j(A|A) = (cid:104)ui, vj(cid:105) · Ai,j and

M(cid:88)

i=1

γi,j(A|A) ,
¯γj(A|A) .

¯γj(A|A) =

1
M

¯γmin(A|A) = min

j=1,...,N

Clearly, the value of a margin parameter lies in the inter-
val [−1, 1]. The arrangement A for A is called error-free if
none of the margin parameters γi,j(A|A) is negative. The
parameter ¯γmin(A|A) gives the row-average margin that is
guaranteed by A for every column of A.
Intuitively, we
should think of an arrangement as being “good” if it induces
“large margin” parameters. For this reason, we deﬁne

¯γmin(A) = maxA ¯γmin(A|A) and
¯γmin(A|Aef ) ,
¯γef
min(A) = maxAef

where Aef ranges over error-free arrangements for A only.
Margin parameters exhibit nice relations to the so-called
Forster bound. For our purpose the following variant [13]
of the Forster bound [8] is needed:

FB(A) = max

q

FBq(A) for FBq(A) =

|||A · diag(q)1/2|||2

(4)
Here, q ranges over all N -dimensional probability vectors.
As shown in [3], the number of examples required to weakly
learn F in the SQ-model under the uniform distribution on
X is polynomially related to the SQ-dimension of F.2 We
2In this paper, we will not need a formal deﬁnition of the
SQ-model.

√
M

denote the latter as SQdim(A) (regarding the sign matrix
A as our representation of F). We are now prepared to de-
scribe the relations among the various parameters associated
with a sign matrix.

Lemma 3

([13]). Let A ∈ {−1, 1}M×N be a sign ma-

trix. Then the following holds:

wise polynomially related.

1. The parameters SQdim(A), FB(A), ¯γmin(A)−1 are pair-

(cid:80)N
j=1 qj ¯γj(A|A) where A ran-
ges over all arrangements for A and q ranges over all
probability vectors.

2. ¯γmin(A) = maxA minq

The reciprocal value of a margin parameter, like ¯γmin(A)−1
in Lemma 3, is referred to as “margin complexity” (“small
margin” = “high margin complexity”).
2.3 Outline of the Main Results

Although we will not make it explicit very often, we con-
sider the classes of counting queries as indexed by a “com-
plexity parameter” k, i.e., F = Fk (resp. X = Xk) is a mem-
ber of a family (Fk)k≥1 (resp. (Xk)k≥1). As for Boolean
predicates, k is typically the number of Boolean variables.
Furthermore, we always assume that log |Fk| and log |Xk| are
polynomially bounded in k. The sign matrix corresponding
to Fk is denoted Ak.
Given f ∈ ∪k≥1Fk, 0 ≤ ω ≤ 1, and n ≥ 1, we deﬁne an
(f, ω, n)-random database, denoted by X n
f,ω, as the outcome
of the following random experiment: draw ωn instances in-
dependently and uniformly at random from Xf,1, and draw
(1 − ω)n instances independently and uniformly at random
from Xf,0.
Informally, we consider a database access mechanism M
“useful” for ∪k≥1Fk if, for every counting query f , there
exists a function Qf such that (at least for random databases
D) the “noisy answer” Qf (M(D)) is (in probability) a “good
approximation” of the “correct answer” ¯f (D). Note that
¯f (D) = ω if D is an (f, ω, n)-random database. Moreover,
the required database size should be polynomially bounded
in k (and in some other relevant variables). This is captured
more formally in the following

Definition 5. We say that a database access mechanism
M is strongly useful for the family (Fk)k≥1 if there ex-
ists a function n = n(k, α, β) that is polynomially bounded
in k, 1/α, 1/β and if there exists a family (Qf )f∈∪k≥1Fk of
(possibly random) maps such that the following holds. For
every k ≥ 1, for every counting query f ∈ Fk, for all
0 ≤ ω ≤ 1 and all 0 < α, β ≤ 1/2, if n ≥ n(k, α, β),
f,ω and ˆω = Qf (M(D)), then the probability for
D = X n
|ˆω − ω| > α is bounded by β. Here the probability is taken
over the internal randomization in D, M and Qf .
We refer to n(k, α, β) as the sample size required by M.
We brieﬂy note that the main diﬀerence between our no-
tion of strong usefulness and the original notion of (α, β)-
usefulness in [4] is that we deal with an (f, ω, n)-random
database whereas, in [4], the criterion in terms of α, β has
to hold for all choices of D.

The deﬁnition of weak usefulness is similar except that ω
is set to either 0 or 1 and it suﬃces that, for each choice of
ω, β, ε, f , the probability for |ˆω − ω| < 1/2 is at least 1 − β
(so that the cases ω = 0 and ω = 1 can be distinguished
with a high probability of success).

17Definition 6. We say that a database access mechanism
M is weakly useful for the family (Fk)k≥1 if there exists a
function n = n(k, β) that is polynomially bounded in k, 1/β
and if there exists a family (Qf )f∈∪k≥1Fk of (possibly ran-
dom) maps such that the following holds. For every k ≥ 1,
for every counting query f ∈ Fk, for all ω ∈ {0, 1} and all
0 < β ≤ 1/2, if n ≥ n(k, β), D = X n
f,ω and ˆω = Qf (M(D)),
then the probability for |ˆω − ω| ≥ 1/2 is bounded by β.

The results in [6] imply that, for every ε > 0, there can
be no randomized response scheme for the class of parity
functions that provides ε-diﬀerential privacy and is weakly
useful. The main results in this paper are as follows:

Theorem 1. For every ε > 0, the following holds.

If
the margin complexity ¯γmin(Ak)−1 associated with a family
(Fk)k≥1 of concept classes is a super-polynomial function in
k, then (Fk)k≥1 has no randomized response scheme that
provides ε-diﬀerential privacy and is weakly useful.

The proof of this result is given in Sections 3 and 4. The
proof of the next one will be given in Section 5.

Theorem 2. For every ε > 0, there is a randomized re-
sponse scheme Mε for (Fk)k≥1 that provides ε-diﬀerential
privacy and is strongly useful if at least one of the following
conditions is valid:

• The classes Fk are balanced and ¯γmin(Ak)−1 is poly-

nomially bounded in k.
min(Ak)−1 is polynomially bounded in k.

• ¯γef

Moreover, there is a single polynomial3 in k, 1/α, 1/β, 1/ε
that bounds from above the sample size required by Mε.

It can be shown that Theorem 2 applies, for instance, to
Boolean Monomials, Boolean Clauses or, more generally, to
“Boolean Decision Lists with a bounded number of label-
changes”.
It applies furthermore to “Axis-Parallel Hyper-
Rectangles over a discrete domain”. This is explained in
more detail in Section 6.
Our deﬁnitions of weak and strong usefulness ignore eﬃ-
ciency issues. However, we will brieﬂy indicate in Section 7
under which conditions the randomized response scheme
from Theorem 2 can be implemented eﬃciently. This will
speciﬁcally be the case for the aforementioned function clas-
ses.
2.4 Comparison to Prior Work

A ﬁrst characterization of non-interactive private data
analysis in the “central” model, where the sensitive database
is managed by a trusted curator, is provided in [4]. Specif-
ically, the authors show that, ignoring computational con-
straints, it is possible to privately release sanitized databases
so as to provide (α, β)-usefulness for any concept class with
polynomial VC-dimension.

Private learning in the absence of a trusted curator is ad-
dressed by a series of prior works (see for example [14, 5]).
In particular, Kasiviswanathan et al. [14] prove that a con-
cept class is learnable in the statistical query model if and
only if it is learnable by what they call a local algorithm.
More speciﬁcally, they show that there is an eﬃcient mutual
simulation between the respective oracles for these models:

3as opposed to a family (pε)ε>0 of polynomials in k, 1/α, 1/β

the SQ-oracle and the so-called LR-oracle. A non-adaptive
local algorithm in the sense of [14] corresponds to an algo-
rithm which has access to the data via a randomized re-
sponse scheme in the sense of Deﬁnition 2. We would like
to stress the following diﬀerences between the results in [14]
and our main results:

• The SQ-learner (resp. the equivalent local algorithm)
gets statistical information about a database consisting
of data which are labeled according to an unknown tar-
get function f . In combination with a result from [3],
this oﬀers the possibility to design a non-adaptive SQ-
learner (resp. a non-adaptive local algorithm) which
weakly learns (Fk)k≥1 under the uniform distribution
provided that the SQ-dimension of Fk grows polyno-
mially with k only.

• In our setting, we have a known query function f in the
role of the unknown target function, but the random-
ized response scheme is applied to unlabeled data. It
should be noted that, even if the SQ-dimension of Fk is
polynomially bounded, there is no direct way to trans-
form a given non-adaptive SQ-learner into a successful
randomized response scheme of the form (3) in our set-
ting. The transformation from [14] cannot be used here
because it assumes a labeled database. The main prob-
lem in our setting (where D = (d1, . . . , dn) consists of
unlabeled data) is to ﬁnd a single random map Z such
that the tuple M(D) = (Z(d1), . . . , Z(dn)), despite of
having a low sensitivity, contains enough information
about the labels of all query functions f ∈ Fk. We will
see in Section 5 that using a linear arrangement for Fk
is the key for solving this problem.

In [10], Gupta et al. show that if a database access mech-
anism can only access the database by means of the SQ-
oracle (or, equivalently, by means of the LR-oracle), then the
concept class can be released so as to provide ε-diﬀerential
privacy and (α, β)-usefulness if and only if it is agnostically
SQ-learnable. The main diﬀerence between these results and
our contribution consists in the notion of usefulness that
the randomized response scheme must achieve. The work of
Gupta et al. requires the private mechanism to output val-
ues which are α-close to the true answers with probability
at least 1 − β for all possible choices of the input database
D, while we are satisﬁed if this property holds for (f, ω, n)-
random databases.
In other words, we have replaced the
worst-case analysis in [10] by a kind of average-case analysis.
While the worst-case analysis establishes the very high bar-
rier of “agnostic SQ-learnability”, our main results show that
the barrier in the average case, namely weak SQ-learnability
under the uniform distribution, is much lower.

3. PROOF OF THEOREM 1 FOR

BALAN-CED CLASSES

Here we present a proof under the additional assumption
that the classes Fk are balanced. This proof will be obtained
in a (more or less) straightforward manner from Theorem 3
below (being valid for all classes F), which generalizes a sim-
ilar theorem in [6] (being concerned with the class of parity
functions only). It turns out that the proof of Theorem 3
is quite similar to the proof of the less general result in [6].
The main new technical contribution is located in the proof
of Lemma 4 where we apply some algebraic manipulations

18that bring the Forster bound into play, which ﬁnally leads
us to the more general result.
Let F = {f1, . . . , fN} be a class of counting queries over
the universe X = {x1, . . . , xM}. A ∈ {−1, 1}X×F denotes
the corresponding sign matrix. In the sequel, we assume that
F is balanced. The general case is discussed in Section 4.
Let q = (q(f ))f∈F denote a probability vector. Drawing
f at random from F according to q is denoted by f ∼ F q.
Let X denote the random variable that is uniformly dis-
tributed on X . For every f ∈ F and every b = 0, 1, Xf,b
denotes the random variable that is uniformly distributed
on Xf,b. With each query function f ∈ F we associate the
f,1 ∈ X n, which (as ex-
two quite diverse databases X n
plained in Section 2.3) are the (f, ω, n)-random databases
for ω = 0, 1. An useful answer to a query instantiated by
f should be close to b if the underlying (random) database
is X n
f,b. Suppose that the answer to the query is derived
from an ε-diﬀerentially private randomized response scheme
M for some ε > 0. The following result indicates that the
usefulness of the answer is in this case severely limited (at
least for query classes whose Forster bound is large):

f,0, X n

SD(cid:0)M(X n

Theorem 3. If M is an ε-differentially private random-
ized response scheme for the balanced class F, then (given
the above notations) the following holds with a probability of
at least 7/8 (taken over f ∼ F q):

f,1)(cid:1) ≤ 4nU 1/3 for U =

f,0),M(X n

(eε − 1)2
FBq(A)2 .
(5)
Proof. Let Z : X → R be the random map such that (3)
holds. The following observations are made in [6] already:

1. The ε-diﬀerential privacy of M implies that

∀x, x

(cid:48) ∈ X ,∀z ∈ R : Pr[Z(x) = z] ≤ eε Pr[Z(x
(cid:48)

) = z] .
(6)

2. For each f ∈ F , the following holds:

(a) SD(cid:0)M(X n

f,1)(cid:1) ≤ n·SD (Z(Xf,0), Z(Xf,1)).

f,0),M(X n

2 ·
(b) SD (Z(Xf,0), Z(X)) = SD (Z(Xf,1), Z(X)) = 1

SD (Z(Xf,0), Z(Xf,1)).

(cid:18) U

(cid:19)1/3

It therefore suﬃces to show that the probability (taken over
f ∼ F q) for

SD (Z(Xf,1), Z(X)) ≤

(7)
is at least 1 − α for every choice of 0 < α < 1. This suﬃ-
cient condition actually holds as will become evident from
Lemmas 4 and 5 below. Setting α = 1/8, the theorem fol-
lows.

α

The proof of Theorem 3 made use of two lemmas that we

present now. For sake of brevity, we deﬁne

h(z|x) := Pr[Z(x) = z] ,
hf,b(z) := Pr[Z(Xf,b) = z] ,

h(z) = Pr[Z(X) = z] .

(8)

Let B ∈R {0, 1} denote the Bernoulli distribution with

parameter 1/2 such that b ∼ B is a perfect random bit.

Lemma 4. Suppose that Z is a random map satisfying
condition (6). Then, for all z in the range of Z and for U
as speciﬁed in (5), the following holds:

Ef∼F q ,b∼B[hf,b(z)] = h(z) and

Varf∼F q ,b∼B[hf,b(z)] ≤ U h(z)2

(9)

Proof. Let hmin(z) = minx∈X h(z|x), ¯h(z|x) = h(z|x)−
hmin(z), ¯hf,b(z) = hf,b(z) − hmin(z), and ¯h(z) = h(z) −
(cid:126)h(z) be the vector (¯h(z|x))x∈X .
hmin(z). Let furthermore
The following observations, partially made in [6] already,
hold for every z in the range of Z:

∀f ∈ F : Eb∼B[hf,b(z)] =

1
2

(hf,0(z) + hf,1(z))

= h(z)
Ef∼F q ,b∼B[hf,b(z)] = h(z)

Varf∼F q ,b∼B[hf,b(z)] = Varf∼F q ,b∼B

h(z|x) ≤ eεhmin(z)
(cid:107) (cid:126)h(z)(cid:107)2

2 ≤ M (eε − 1)2h(z)2

(14)
(10) holds because the functions f ∈ F are assumed as bal-
anced. (11) is an immediate consequence of (10). (12) is
obvious.
(14) follows
from (13) and the following calculation:

(13) holds because Z satisﬁes (6).

(cid:107) (cid:126)h(z)(cid:107)2

2 =

¯h(z|x)2 =

(h(z|x) − hmin(z))2

(cid:88)

(cid:88)

x

x

(13)≤ M (eε − 1)2hmin(z)2 ≤ M (eε − 1)2h(z)2

For sake of brevity, we set Aq = A · diag(q1/2) and proceed
as follows:

(10)

(cid:2)¯hf,b(z)(cid:3) (12)

(11)

(13)

Varf∼F q ,b∼B[hf,b(z)]

(12)
= Varf∼F q ,b∼B
= Ef∼F q ,b∼B

(11)

(cid:2)¯hf,b(z)(cid:3)
(cid:104)(cid:0)¯hf,b(z) − ¯h(z)(cid:1)2(cid:105)
(cid:33)2(cid:35)
(cid:88)
(cid:33)2(cid:35)

¯h(z|x)

x

(cid:88)
(cid:88)

x

x

2
M

(cid:34)(cid:32)
(cid:34)(cid:32)
(cid:88)
(cid:32)(cid:88)

1
M

x

1
M

x

(cid:34)(cid:32)
(cid:88)
(cid:88)

f

x,x(cid:48)

= Ef∼F q ,b∼B

¯h(z|x)1Xf,b (x) − 1
M

= Ef∼F q ,b∼B

¯h(z|x)(2 · 1Xf,b (x) − 1)

= Ef∼F q

¯h(z|x)A[x, f ]

(cid:33)2(cid:35)
(cid:33)2

1
M 2

1
M 2

1
M 2
1

=

=

=

(2)≤

q(f )

¯h(z|x)A[x, f ]

(cid:88)

f

¯h(z|x)¯h(z|x
(cid:48)

)

(cid:62)

(cid:126)h(z)

AqA

(cid:62)
q

(cid:126)h(z)

M 2 (cid:107) (cid:126)h(z)(cid:107)2

2|||Aq|||2

2

(14),(4)≤

(cid:48)
q(f )A[x, f ]A[x

, f ]

(cid:18) (eε − 1)h(z)

(cid:19)2

FBq(A)

The third equation in this calculation results from expand-
ing the deﬁnitions of ¯hf,b(z) and ¯h(z) and from making use
of the fact that f is balanced. The ﬁfth equation holds be-
cause 2· 1Xf,b (x)− 1 = (2b− 1)· A[x, f ]. Since the expression
containing the factor 2b−1 ∈ {±1} is squared, the parameter

19b vanishes at this stage of the computation. The remaining
steps in the calculation are rather obvious.

The following result (resp. a slightly less general version
of it) is shown in [6]. For sake of completeness, the proof is
given in Section A.

Lemma 5. Let h(z|x), hf,b(z|x) and h(z) be given by (8),
and let Z : X → R be a random map such that (9) holds
for every z in the range of Z. Let 0 < α < 1. Then the
probability for (7), taken over f ∼ F q, is at least 1 − α.

f,0 and X n

We are now ready to prove Theorem 1 under the assump-
tion that the classes Fk are balanced. Let M be a random-
ized response scheme for (Fk)k≥1 that provides ε-diﬀerential
privacy for some ε > 0. We will show that assuming M to
be weakly useful and ¯γmin(Ak)−1 to be super-polynomial in
k leads to a contradiction.
Suppose ﬁrst that M is weakly useful. Let β = 1/3. Then,
for all k and all f ∈ Fk, the criterion for weak usefulness
implies that the distributions X n
f,1 can be distin-
guished with a probability of at least 2/3 of success. Thus,
there will be the same small average error rate of at most
1/3, when we q-average over all f from Fk regardless of how
the distribution q on Fk is chosen.
Suppose second that ¯γmin(Ak)−1 is a super-polynomial func-
tion in k. Then, according to Lemma 3, FB(Ak) (as deﬁned
in (4)) is a super-polynomial function in k too. Thus, for ev-
ery k, there must exist a probability vector q = qk such that
FBqk (Ak) is a super-polynomial function in k. According to
Theorem 3 (applied to F = Fk), inequality (5) is valid with
a probability (taken over f ∼ F q) of at least 7/8. Since
the database size n may grow polynomially in k only, the
statistical diﬀerence between M(Xf,0) and M(Xf,1) will be
negligible (as k goes to inﬁnity) for a q-fraction 7/8 (or more)
of all f ∈ Fk. We apply Lemma 1 and conclude that, for the
“bad functions” f ∈ Fk, the Bayes error for distinguishing
between M(Xf,0) and M(Xf,1) will therefore be arbitrarily
close to 1/2 (as k goes to inﬁnity). Thus the Bayes error,
q-averaged over all f ∈ Fk cannot be signiﬁcantly smaller
than (7/8) · (1/2) + (1/8) · 0 = 7/16.
Since 7/16 > 1/3, we arrived at a contradiction.

4. PROOF OF THEOREM 1
In Section 3, we presented a proof of Theorem 1 under
the additional assumption that the classes Fk are balanced.
The proof was obtained in a (more or less) straightforward
manner from Theorem 3 which presents an upper bound on
SD(M(X n
f,1)). The main point in the proof was
that this upper bound is asymptotically smaller than 1/P (k)
for any polynomial P provided that the margin complex-
ity ¯γmin(Ak)−1 associated with (Fk)k≥1 is super-polynomial
in k. The main reasons why Theorem 1 holds even for un-
balanced classes are as follows:

f,0),M(X n

• Theorem 3 can be generalized to “almost balanced”
classes. See Theorem 4 below for the formal statement.
• If the margin complexity ¯γmin(Ak)−1 associated with
(Fk)k≥1 is super-polynomial in k, then there are al-
most balanced sub-classes of Fk whose margin com-
plexity is still super-polynomial in k. See Theorem 5
below for the formal statement.

We deﬁne the imbalance of a function f : X → {0, 1}
as ||Xf,1| − |Xf,0|| /|X|. We say that F has an imbalance
of a most ∆ if, for each f ∈ F, the imbalance of f is at
most ∆. Let X (cid:48) ⊇ X be an extended universe such that
|X (cid:48)| = |X|+∆|X|. Clearly, the functions f of a class with an
imbalance of at most ∆ can be extended to the larger domain
X (cid:48) so as to become (completely) balanced. This balanced
extension of f is denoted f(cid:48) so that F(cid:48) = {f(cid:48) : f ∈ F} is a
balanced extension of F. With these notations, the following
holds:

Lemma 6. For all f ∈ F , the following holds:

SD(Xf(cid:48),0, Xf,0) + SD(Xf(cid:48),1, Xf,1) ≤ 2∆ .

The (straightforward) proof is omitted.

Lemma 7. Suppose that F has an imbalance of at most
∆. Let F(cid:48) be its balanced extension, and let A and A(cid:48) be the
corresponding sign matrices, respectively. Let q be a proba-
bility vector for the functions in F. With these notations,
the following holds:

1

FBq(A(cid:48))2 ≤

1

FBq(A)2 +

√
2

∆

FBq(A)

+ ∆ .

Proof. For sake of brevity, we set Aq = A · diag(q)1/2.
We may think of A(cid:48) as having the sign matrix A (with |X|
rows) in its upper block and another sign matrix, say E,
with ∆|X| rows in its lower block. With these notations, the
spectral norm of A(cid:48)
q can be bounded from above as follows:
|||A(cid:48)
Thus, we obtain

q|||2 ≤ |||Aq|||2 +|||Eq|||2

(2)≤ |||Aq|||2 +(cid:107)Eq(cid:107)2

(1)≤ |||Aq|||2 +

∆M

√

1

FBq(A(cid:48))2

2

(4)
=

|||A(cid:48)

q|||2
(1 + ∆)M
≤ |||Aq|||2

+

2

2

≤ (|||Aq|||2 +
√
∆M|||Aq|||2

(1 + ∆)M

√
∆M )2

+

∆M
M

M
1

(4)
=

FBq(A)2 +

√
M
2

∆

FBq(A)

+ ∆ ,

which concludes the proof.

We now arrive at the following extension of Theorem 3:

Theorem 4. Suppose that F has an imbalance of at most
∆. Let F(cid:48) be its balanced extension, and let A and A(cid:48) be the
corresponding sign matrices, respectively. Let q be a proba-
bility vector for the functions in F, and let ε > 0. If M is
an ε-diﬀerentially private randomized response scheme, then
the following holds with a probability of at least 7/8 (taken
over f ∼ F q):

f,1)(cid:1) ≤ 2∆n + 4n(eε − 1)2 · B ,

SD(cid:0)M(X n

f,0),M(X n

where

B =

1

FBq(A)2 +

√

2

∆

FBq(A)

+ ∆ .

Proof. According to the triangle inequality,

SD(M(X n

f,0),M(X n

f,1))

is upper-bounded by

(cid:88)

SD(cid:0)M(X n

f,b),M(X n

f(cid:48),b)(cid:1)+SD(cid:0)M(X n

f(cid:48),1)(cid:1) .

f(cid:48),0),M(X n

b=0,1

20The ﬁrst sum is upper-bounded by

(cid:88)

SD(cid:0)X n

f,b, X n

f(cid:48),b

(cid:1) ≤ n · (cid:88)

b=0,1

b=0,1

SD (Xf,b, Xf(cid:48),b)

5. PROOF OF THEOREM 2

The analysis of the randomized response scheme that we
are going to design requires Hoeﬀding’s inequality [11, 18],
and, in addition, the following tail bound:

and therefore, by virtue of Lemma 6, by 2∆n. According to
Theorem 3, the following holds with a probability of at least
7/8 (taken over f ∼ F q):

SD(cid:0)M(X n

f(cid:48),0),M(X n

f(cid:48),1)(cid:1) ≤ 4n(eε − 1)2

FBq(A(cid:48))2

Plugging in the upper bound on 1/FBq(A(cid:48)) from Lemma 7
and putting all pieces together, the theorem follows.

Pr

Here comes the ﬁnal piece of the puzzle in our proof of The-
orem 1:

Theorem 5. If the margin complexity ¯γmin(Ak)−1 asso-
ciated with a family (Fk)k≥1 of concept classes is a super-
polynomial function in k, then, for each polynomial P (k) ≥
1, there exists a family of sub-classes F P
k ⊆ Fk such that the
following holds:

1. For each k ≥ 1, the imbalance of F P

k is bounded by

1/P (k).

family (F P

2. The margin complexity ¯γmin(AP

Proof. We simply choose F P

k )−1 associated with the
k )k≥1 is a super-polynomial function in k.
k as the set of all f ∈ Fk
whose imbalance is bounded by 1/P (k). Let k be arbi-
trary but ﬁxed. Let F = Fk and A = Ak. Each, say d-
dimensional, arrangement A for the sign matrix A can be
extended as follows:

• Add an extra dimension d + 1.
• If f ∈ F P , then the coordinate d + 1 of the vector vf

is set to 0.

• If f ∈ F\F P is biased towards positive (resp. negative)
examples, then the coordinate d + 1 of vf gets value 1
(resp. −1), and the remaining coordinates are set to 0.

• The coordinate d + 1 of each vector ux is set to 1.
• For each x ∈ X , the vector ux is normalized so as
to have unit Euclidean norm (by applying the scaling
factor 1/

√

2).

The eﬀect is as follows:

√
achieved by the extension of A is at least (
√

• For each function f ∈ F \ F P , the average margin
2P (k))−1.
• For each function f ∈ F P , the average margin achieved
by the extension of A coincides with 1/
2 times the
average margin achieved by the original arrangement
A.

Since ¯γmin(Ak)−1 is super-polynomial in k but the func-
tions from F \ F P cannot be blamed for it, it follows that
¯γmin(AP

k )−1 is super-polynomial in k.

The proof of Theorem 4 can now be completed in a similar
fashion as it was done at the end of Section 3 for the special
case of balanced classes. We omit the details.

Theorem 6. For i = 1, . . . , n and j = 1, . . . , d, let Yi,j ∼
Lap(λ) be i.i.d. random variables. For each i = 1, . . . , n, let
vi = (vi,j)j=1,...,d be a vector of unit norm, i.e., (cid:107)vi(cid:107)2 = 1.
Then, for each 0 < δ ≤ √

8λn, the following holds:

(cid:34) n(cid:88)

d(cid:88)

i=1

j=1

(cid:35)

(cid:18)

(cid:19)

vi,jYi,j ≥ δ

≤ exp

− δ2
8nλ2

Since we were not able to ﬁnd a proper reference, we will
give the proof of Theorem 6 (plus some additional remarks)
8λn in The-
8λn, we

in Section B. Note that the assumption δ ≤ √
orem 6 is not very restrictive because, for δ ≥ √
(cid:34) n(cid:88)

(cid:34) n(cid:88)

d(cid:88)

d(cid:88)

(cid:35)

(cid:35)

√

get

vi,jYi,j ≥ δ

≤ Pr

vi,jYi,j ≥

8λn

Pr

i=1

j=1

j=1

i=1

−n .

≤ e

We are now ready for the central part of this section. As
usual, let F denote a class of counting queries over the uni-
verse X . A ∈ {−1, 1}X×F denotes the corresponding sign
matrix. Let A = ((ux)x∈X , (vf )f∈F ) be a d-dimensional
arrangement for A. We aim at designing a randomized re-
sponse scheme that is strongly useful and provides ε-diffe-
rential privacy. To this end, we deﬁne the random map
Z : X → Rd according to

Z(x) = ux + Y for Y ∼ Lap

.

(15)

(cid:33)d

(cid:32)

√
d

2

ε

The random vectors Y are chosen independently for diﬀerent
choices of x.

Lemma 8. Let ε > 0. Choose the random map Z ac-
cording to (15) and M according to (3). Then, M provides
ε-diﬀerential privacy.

Proof. Let D(cid:48) ∈ X n be a second database diﬀering from
k (cid:54)= dk. Consider the map
D in one entry only, say in entry d(cid:48)
h(D) = (ud1 , . . . , udn ) ∈ (Rd)n. Then
√
(cid:107)h(D)−h(D
d(cid:107)udk −ud(cid:48)

(cid:107)2 ≤ 2
√
It follows that the sensitivity of h is bounded by 2
d. Ac-
cording to Lemma 2 (applied with nd in place of d), the
mechanism M provides ε-differentially privacy.

)(cid:107)1 = (cid:107)udk −ud(cid:48)

(cid:107)1 ≤

√

d .

(cid:48)

k

k

For sake of simplicity, we will use the abbreviations ¯γ(f ) =
¯γf (A|A) and ¯γmin = ¯γmin(A|A) in what follows. As in
f,ω ∈ X n denotes an
the deﬁnition of strong usefulness, X n
(f, ω, n)-random database. Let f ∈ F be a counting query
and v = vf its representation in the arrangement A. The
following margin parameters, associated with f (resp. v) and
b = 0, 1, will play a central role:

(cid:88)

x∈Xf,b

¯γb(f ) =

1

|Xf,b|

(2b − 1)(cid:104)ux, v(cid:105) ,

˜γ(f ) =

1
2

(¯γ0(f ) + ¯γ1(f ))

(16)

(17)

21(cid:32)

(cid:33)d

(15)
= uXi + Yi
√
2

d

.

ε

n(cid:88)

ˆD = ( ˆX1, . . . , ˆXn) for ˆXi = Z(Xi)

and Yi ∼ Lap

Consider the random variable S = S1 + S2 for

S1 =

1
n

(cid:104)uXi , v(cid:105) and S2 =

1
n

(cid:104)Yi, v(cid:105) .

(18)

n(cid:88)

i=1

Now, we obtain

E[S] = E[S1] = ω¯γ1(f ) − (1 − ω)¯γ0(f )

= ω(¯γ0(f ) + ¯γ1(f )) − ¯γ0(f ) .

Solving for ω yields

ω =

E[S] + ¯γ0(f )
¯γ0(f ) + ¯γ1(f )

.

(19)

Treating S as a (hopefully good) approximation of E[S], this
motivates the following deﬁnition of ˆω:

(cid:80)n

(cid:68) ˆXi, v

(cid:69)

.

i=1

It is assumed in Theorem 2 that there are arrangements

Ak for the classes Fk such that the margin complexity

If the random database D = X n
f,ω contains the instances
X1, . . . , Xn, then its noisy version ˆD = M(D) has the fol-
lowing form:

1. Given ˆD, compute S = S1 + S2 = 1
n

i=1

2. Compute ˆω = Qf ( ˆD) according to the right hand-side
of (19) except that S is substituted for (the unknown)
E[S].

View now F = Fk as a member of the parametrized class
(Fk)k≥1. Remember that we assume log |Xk| and log |Fk|
to be polynomially bounded in k. Note that ¯γ(f ) = ˜γ(f )
if f is balanced, and ¯γ(f ) ≤ 2˜γ(f ) if the arrangement A
is error-free. Therefore, Theorem 2 will be an immediate
consequence of the following two results.

Theorem 7. Let ε > 0. The randomized response scheme,
given by Z and (Qf )f∈F as deﬁned above, provides ε-diffe-
rential privacy. Moreover, it is strongly useful provided that
the size n = n(d, α, β, ε, ˜γmin) of the (random) database sat-
isﬁes

(cid:18) 2

(cid:19)

β

n ≥ 32 · d

α2ε2˜γ2

min

ln

(20)

where ˜γmin = minf∈F ˜γ(f ).

Proof. ε-diﬀerential privacy is granted by Lemma 8. We
have to show that, with probability at least 1− β, |ˆω − ω| ≤
α where ˆω is calculated according to the right hand-side
of (19) except that S is substituted for E[S]. An inspection
of (19) shows that the condition |ˆω − ω| ≤ α is equivalent to
|S−E[S]| ≤ α(¯γ0(f )+¯γ1(f )). Since S = S1+S2 and 2˜γmin ≤
2˜γ(f ) = ¯γ0(f ) + ¯γ1(f ), the latter condition is guaranteed if
the following holds:

|S1 − E[S1]| ≤ α˜γmin and |S2 − E[S2]| ≤ α˜γmin

(21)

According to (18), S1 is the average over the independent
random variables (cid:104)uXi , v(cid:105) ∈ [−1, 1]. An application of Ho-
eﬀding’s inequality shows that the probability for the ﬁrst
condition in (21) to be violated is bounded by

2 exp(−α2˜γ2

minn/2).

According to Theorem 6, the probability for the second con-
dition in (21) to be violated is bounded by
minε2n/(32d)),

2 exp(−α2˜γ2

a bound which is more restrictive than the preceding one.4
Setting

(cid:18)−α2˜γ2

(cid:19)

2 exp

minε2n
32d

≤ β

and solving for n, we see that the database size n, as speciﬁed
in (20), is suﬃciently large.

¯γmin(Ak|Ak)

−1

is polynomially bounded in k. As already noted above, ¯γmin
and ˜γmin are equal up to a factor of at most 2 under the
assumption (made in Theorem 2) that the classes Fk are
balanced or that the arrangements Ak are error-free. An
inspection of (20) shows that the only remaining parame-
ter with a possibly super-polynomial growth in k (or in the
other relevant parameters) is the dimension d = d(Fk) of the
arrangement. However, d can be assumed as small because
of the following result:

Theorem 8. Given a (possibly high-dimensional) arrange-
ment A for F such that γ := ˜γmin(A|A) > 0, there exists a
d-dimensional arrangement A(cid:48) for F such that ˜γmin(A|A(cid:48)) ≥
γ/8 provided that
d ≥ 8 ln

(cid:18) 4(M + N )

(cid:18) 8N

(cid:19)

(cid:19)

(22)

+

128
γ2 ln

β

β

The proof, which is based on random projection techni-
ques [12, 1, 2], is found in Section C.

6. ARRANGEMENTS WITH A NON-NEG-

LIGIBLE MINIMUM MARGIN

In this section, we start with a (more or less trivial) ob-
servation, Lemma 9 below, which is applied afterward to a
couple of Boolean concept classes.
Let A ∈ {−1, 1}M×N be a sign matrix and let A be an
arrangement for A. The margin parameter ¯γmin(A|A) that
is used in Theorems 1 and 2 is clearly bounded from below
by
γmin(A|A) = min{γi,j(A|A) : i = 1, . . . , M , j = 1, . . . , N} ,
which is the minimum among all M N individual margin
parameters.
Let (Fk)k≥1 be a family of concept classes, and let Ak ∈
{−1, 1}Xk×Fk be the sign matrix associated with Fk. We
say that (Fk)k≥1 has a non-negligible minimum margin if
there exists a polynomial P (k) ∈ N, and, for all k ≥ 1,
an arrangement Ak such that γmin(Ak|Ak) ≥ 1/P (k). This
clearly implies that each arrangement Ak for Ak is error-free
min(Ak|Ak). It follows that the
and that γmin(Ak|Ak) ≤ ¯γef
conclusion of Theorem 2 holds, in particular, for all classes
with a non-negligible minimum margin.
the assumption made there, namely δ ≤ √
4Note that the application of Theorem 6 is justiﬁed because
√
8λn, is granted in
our application where δ = α˜γmin ≤ α ≤ 1 and λ = 2
d/ε ≥
2.

22For each k ≥ 1, let Fk be a class of concepts over the
Boolean domain Xk = {0, 1}k or Xk = {−1, 1}k. We say
that (Fk)k≥1 is linearly separable with polynomially bounded
weights if there exists a polynomial P (k) ∈ N, and, for all
k ≥ 1 and all f ∈ Fk, a weight vector

vf ∈ {−P (k), . . . , 0, . . . , P (k)}k

together with a threshold t ∈ R such that, for all x ∈ Xk,

f (x) = 1 ⇔ (cid:104)vf , x(cid:105) ≥ t .

(23)

(cid:48)
x

=

Assume now that this is the case. Clearly, the threshold t
can w.l.o.g. be chosen from the interval [−kP (k), kP (k)]. It
follows that (Fk)k≥1 has a non-negligible minimum margin.
We could, for example, represent x and f by the (k + 1)-
dimensional vectors

(cid:17)
(cid:16)(cid:112)P (k) · x,(cid:112)kP (k)
(cid:32)
1(cid:112)P (k)
t(cid:112)kP (k)
respectively, so that (cid:10)v(cid:48)
f , x(cid:48)(cid:11) = (cid:104)vf , x(cid:105) − t. Moreover, t
f(cid:107)2,(cid:107)x(cid:48)(cid:107)2 ≤ (cid:112)2kP (k). After

can be chosen in the middle between two consecutive non-
negative integers. Denote the arrangement obtained by this
setting as Ak. Note that (cid:107)v(cid:48)
normalizing all vectors to unit Euclidean norm, we obtain
γmin(Ak|Ak) ≥ 1/(4kP (k)). We arrive at the following re-
sult:

· vf ,−

(cid:33)

(cid:48)
f =

(24)

and

v

,

Lemma 9. Let P (k) be a polynomial.

If (Fk)k≥1 is a
family of Boolean concept classes that is linearly separable
with integer weights of absolute value at most P (k), then
there exist arrangements (Ak)k≥1 such that γmin(Ak|Ak) ≥
1/(4kP (k)).

We now discuss some applications.

Boolean Monomials.
A Boolean monomial is linearly separable with weights
from {−1, 0, 1} (in the obvious fashion). Thus, Lemma 9
applies with P (k) = 1 for all k ≥ 1. We can conclude
that, for monomials over k Boolean variables, the minimum
margin is at least 1/(4k), resp. at least 1/(2k) as it had been
observed in [9].

Discrete Axis-Parallel Hyper-Rectangles.
Let J(k) ∈ N be polynomially bounded in k, and let Xk =
{1, . . . , J(k)}k. An axis-parallel hyper-rectangle over Xk,
also called “k-dimensional box”, is a subset of the form

{a1, . . . , b1} × . . . × {ak, . . . , bk}

where ai, bi ∈ {1, . . . , J(k)}. Using the predicates xi ≥ a
and xi ≤ b for a, b ∈ {1, . . . , J(k)}, it is easy to cast a
k-dimensional box as a monomial over 2kJ(k) Boolean vari-
ables. It follows that, for boxes of this form, the minimum
margin is at least 1/(4k2J(k)).

Boolean Decision Lists.

For a Boolean variable z, let z1 = z and z0 = ¯z. We
consider decision lists5 over the Boolean variables z1, . . . , zk
of the form

[(zε1
5introduced in [19]

i(1), b1), . . . , (zεs

i(s), bs), bs+1]

(25)

where 1 ≤ s ≤ k, i(1), . . . , i(s) ∈ {1, . . . , k}, and ε1, . . . , εs,
b1, . . . , bs ∈ {0, 1}. A list of this form represents a Boolean
function that assigns a bit to each vector x ∈ Xk = {0, 1}k
as follows:

• If x does not satisfy any literal zεj

i(j) in the list, then

return the bit bs+1.

• Otherwise, let j be smallest index such that x satisﬁes

the literal zεj

i(j). Return bj.

It is well known that Boolean decision lists with a bounded
number of label changes (from 0 to 1 or vice versa) within
the sequence b1, . . . , bs, bs+1 are linearly separable with poly-
nomially bounded weights. Here, we determine a relatively
precise polynomial bound Pc(k) where c denotes an upper
bound on the number of label changes. We view c as a
constant and k as a variable that may grow to inﬁnity. Con-
sider a decision list over the domain Xk = {0, 1}k as given
in (25). By adding redundant items if necessary, we may
assume that the list contains every Boolean variable exactly
once. For sake of simplicity, we apply a renumbering of in-
dices so that the decision list looks like

[(zεk

k , bk), . . . , (zε1

1 , b1), b0] .

(26)

Let us ﬁrst assume that ε1 = . . . = εk = 1, i.e., all literals
in the list are unnegated. We may clearly assume that b1 (cid:54)=
b0 so that the ﬁrst label change (from right to left) occurs
between b0 and b1. Let c(cid:48) ≤ c denote the total number of
label changes. Let f be the Boolean function represented
by (26). We will design a weight vector vf = (w1, . . . , wk)
and a threshold t such that (23) holds. To this end, we
decompose b1, . . . , bk (in this order) into c(cid:48) ≤ c maximal
segments without label change. Let I1, . . . , Ic(cid:48) (in this order)
denote the corresponding partition of {1, . . . , k} (so that, for
instance, 1 ∈ I1 and k ∈ Ic(cid:48) ). Now we choose the weights
w1, . . . , wk and the threshold t for the given decision list in
accordance with the following policy:

• For all j = 1, . . . , c(cid:48), all weights in {wi : i ∈ Ij} get the
same value, say ±Wj where Wj > 0. More precisely,
the value is Wj (resp. −Wj) if the label associated with
the segment Ij is 1 (resp. 0).

• Let W1 = 1 and, for j = 1, . . . , c(cid:48), let kj = |Ij|. The
values (Wj)j=2,...,c(cid:48) are chosen so that the following
holds:

(cid:26) k1W1

Wj > kj−1Wj−1+kj−3Wj−3+. . .+

if j is even
if j is odd
• The threshold t is set to 1/2 (resp. −1/2) if b0 = 0

k2W2

(resp. b0 = 1).

It is easy to verify that any weight vector and threshold
chosen in accordance with this policy leads to a representa-
tion vf of the function f associated with (26) such that (23)
holds (provided that ε1 = . . . = εk = 1). The following is an
easy-to-solve recursion for Wj leading to weights w1, . . . , wk
that actually do respect the policy:

1. k0 = W0 = 1. For j = 1, . . . , c(cid:48) : kj = |Ij|.

2. For j = 0, . . . , c(cid:48) − 1: Wj+1 =(cid:80)j

i=0 kiWi.

23(cid:88)

(cid:89)

I⊆{1,...,j−1}

i∈I

Suppose that the weights Wj are chosen according to this
recursion. We claim that, for all j = 0, 1, . . . , c(cid:48),

Wj =

ki .

(27)

collapses to(cid:81)

For j = 0, this is correct because the right hand-side in (27)
i∈∅ ki = 1 = W0. Assume inductively that the

claim is correct for Wj. Then,

j−1(cid:88)

i=0

Wj+1 = kjWj +

= (kj + 1)

kiWi = (kj + 1)Wj

(cid:88)

(cid:89)

(cid:88)

(cid:89)

I⊆{1,...,j−1}

i∈I

I⊆{1,...,j}

i∈I

ki =

ki ,

in k1, . . . , kc(cid:48) subject to c(cid:48) ≤ c and (cid:80)c(cid:48)

which proves the claim. The method of Lagrangian multipli-
ers yields that the largest weight Wc(cid:48) , viewed as a function
j=1 kj = k, is maxi-
mized for kc(cid:48) = 1 and k1 = . . . = kc(cid:48)−1 = (k − 1)/(c(cid:48) − 1).
Thus, we obtain the following upper bound on the weights
used for the representation of the decision list:

(cid:88)

(cid:89)

I⊆{1,...,c(cid:48)−1}

i∈I

ki =

(cid:32)

c(cid:48)−1(cid:88)
(cid:18)

j=0

=

1 +

c(cid:48) − 1

j

k − 1
c(cid:48) − 1

(cid:19)j

(cid:33)(cid:18) k − 1
(cid:19)c(cid:48)−1

c(cid:48) − 1

Calculus yields that the worst-case is obtained for c(cid:48) = c.
We ﬁnally remove the artiﬁcial assumption ε1 = . . . = εk =
1. Suppose that the parameters εi ∈ {0, 1} are not necessar-
ily set to 1. If εi = 0, we should think of xi in (23) as being
replaced by 1 − xi. Multiplying out, we see that this trans-
formation has an eﬀect on the threshold t only, but not on
the weight vector vf . Thus, the upper bounds obtained for
the absolute values of the weights remain valid. The whole
discussion can be summarized as follows:

Theorem 9. Let c ≥ 1 be a constant. Then, for all r ≥ 0,
the following holds. A decision list with at most k Boolean
variables and with at most c label changes is linearly sepa-
rable with weights whose absolute values are bounded by

(cid:18)

(cid:19)c−1

.

k − 1
c − 1
7. EFFICIENCY ISSUES

Pc(k) =

1 +

Theorem 2 ignores eﬃciency issues. However, an inspec-
tion of its proof in Section 5 shows that eﬃciency is actually
obtained under the following conditions:

1. Given x ∈ Xk (resp. f ∈ Fk), the vector ux (resp. vf )
of the corresponding (low-dimensional) linear arrange-
ment Ak can be computed eﬃciently.

2. Given f ∈ Fk, a random instance in Xf,0 (resp. Xf,1)

can be eﬃciently generated.

The only little change in the computation of ˆω (in compari-
son to the computation described in Section 5) is as follows.
We replace the right hand-side in (16) by the correspond-
ing empirical average taken over a sample drawn uniformly
at random from Xf,b. An easy application of Hoeﬀding’s
bound shows that (under the assumptions made in Theo-
rem 2) the size of this sample can be polynomially bounded

in terms of the relevant parameters. It is furthermore not
hard to show that the above two conditions are satisﬁed by
all classes discussed in Section 6.

8. CONCLUSIONS

This paper investigated to what extent randomized re-
sponse schemes can combine ε-diﬀerential privacy with the
conﬂicting goal of providing useful answers to counting que-
ries taken from a known class F. We introduced the notions
of weak and strong usefulness and proved the following re-
sults. First, if F cannot be weakly SQ-learned under the uni-
form distribution, then ε-diﬀerential privacy rules out weak
usefulness. Second, for a broad variety of classes F that
actually can be weakly SQ-learned under the uniform dis-
tribution, we designed a randomized response scheme that
provides ε-diﬀerential privacy and strong usefulness. Fi-
nally, we presented some applications.
In particular, we
showed that our ε-diﬀerentially private randomized response
scheme is strong useful for the class of Boolean Monomials,
Boolean Clauses and, more generally, for “Boolean Decision
Lists with a bounded number of label-changes”. The same
result also applies to “Axis-Parallel Hyper-Rectangles over
a discrete domain”.

9. REFERENCES
[1] R. I. Arriaga and S. Vempala. An algorithmic theory
of learning: Robust concepts and random projection.
In Proceedings of the 40’th Annual Symposium on the
Foundations of Computer Science, pages 616–623,
1999.

[2] S. Ben-David, N. Eiron, and H. U. Simon. Limitations

of learning via embeddings in euclidean half-spaces.
Journal of Machine Learning Research, 3:441–461,
2002.

[3] A. Blum, M. Furst, J. Jackson, M. Kearns,

Y. Mansour, and S. Rudich. Weakly learning DNF and
characterizing statistical query learning using Fourier
analysis. In Proceedings of the 26th Annual Symposium
on Theory of Computing, pages 253–263, 1994.

[4] A. Blum, K. Ligett, and A. Roth. A learning theory

approach to non-interactive database privacy. Journal
of the Association on Computing Machinery, 60(2):12,
2013.

[5] J. C. Duchi, M. I. Jordan, and M. J. Wainwright.

Privacy aware learning. In Advances in Neural
Information Processing Systems 25, pages 1430–1438,
2012.

[6] C. Dwork, F. McSherry, K. Nissim, and A. Smith.

Calibrating noise to sensitivity in private data
analysis. In Proceedings of the 3rd Theory of
Cryptography Conference, pages 265–284, 2006.

[7] V. Feldman. A complete characterization of statistical

query learning with applications to evolvability. In
Proceedings of the 50’th Annual Symposium on the
Foundations of Computer Science, pages 375–384,
2009.

[8] J. Forster. A linear lower bound on the unbounded

error communication complexity. Journal of Computer
and System Sciences, 65(4):612–625, 2002.

[9] J. Forster, N. Schmitt, H. U. Simon, and T. Suttorp.

Estimating the optimal margins of embeddings in

24euclidean half spaces. Machine Learning,
51(3):263–281, 2003.

[10] A. Gupta, M. Hardt, A. Roth, and J. Ullman.

Privately releasing conjunctions and the statistical
query barrier. SIAM Journal on Computing,
42(4):1494–1520, 2013.

[11] W. Hoeﬀding. Probability inequalities for sums of

bounded random variables. Journal of the American
Statistical Association, 58:13–30, 1963.

[12] W. B. Johnson and J. Lindenstrauss. Extensions of

Lipschitz mapping into Hilbert spaces. Contemp.
Math., 26:189–206, 1984.

[13] M. Kallweit and H. U. Simon. A close look to margin

complexity and related parameters. In JMLR
Workshop and Conference Proceedings, Volume 19:
COLT 2011, pages 209–223, 2011.

[14] S. P. Kasiviswanathan, H. K. Lee, K. Nissim,

S. Raskhodnikova, and A. Smith. What can we learn
privately. SIAM Journal on Computing,
40(3):793–826, 2011.

[15] M. Kearns. Eﬃcient noise-tolerant learning from
statistical queries. Journal of the Association on
Computing Machinery, 45(6):983–1006, 1998.

[16] S. Kotz, T. Kozubowski, and K. Podgorski. The

Laplace Distribution and Generalizations. Birkh¨auser,
2001.

[17] E. L. Lehmann and J. P. Romano. Testing Statistical

Hypotheses. Springer, 2006.

[18] D. Pollard. Convergence of Stochastic Processes.

Springer, 1984.

[19] R. Rivest. Learning decision lists. Machine Learning,

2(3):229–246, 1987.

[20] L. G. Valiant. Evolvability. Journal of the Association

on Computing Machinery, 56(1):3:1–3:21, 2009.

APPENDIX
A. PROOF OF LEMMA 5

We say that z is “δ-bad” for (f, b) if |hf,b(z) − h(z)| >
δh(z). Note that z is δ-bad for (f, 1) iﬀ z is δ-bad for (f, 0)
because of (10). By the Chebychef bound,

Pr
f∼F q

[z is δ-bad for (f, 1)] =

Pr

f∼F q ,b∼B

[z is δ-bad for (f, b)]

≤ Varf∼F q ,b∼B[hf,b(z)]

δ2h(z)2

(9)≤ U
δ2

holds for every z. Therefore,

Pr

f∼F q ,z∼Z(X)

[z is δ-bad for (f, 1)] ≤ U

δ2 .

(28)

Let 0 ≤ β ≤ 1 be maximal such that there exists F(cid:48) ⊆ F
with the following properties:

1. (cid:80)

f∈F(cid:48) q(f ) ≥ α.

2. For every f ∈ F(cid:48), Prz∼Z(X)[z is δ-bad for (f, 1)] > β.
The deﬁnition of β implies that

Pr

f∼F q ,z∼Z(X)

[z is δ-bad for (f, 1)] > αβ .

In view of (28), we may conclude that β < U/(αδ2). Set
now β0 = U/(αδ2).
It follows that the probability, taken
over f ∼ F q, for

Pr

z∼Z(X)

[z is not δ-bad for (f, 1)] ≥ 1 − β0

(29)
is at least 1−α. Consider now a ﬁxed pair (f, 1) for which (29)
actually holds. Let zbad range over the values z that are δ-
bad for (f, 1), and let zgood range over the remaining values.
We are now ready for bounding the statistical diﬀerence:

SD(Z(Xf,1), Z(X)) =

|hf,1(z) − h(z)|

|hf,1(z) − h(z)| +

|hf,1(z) − h(z)|



1
2

(cid:88)
(cid:88)

z

zbad

(cid:88)
(cid:18)

zgood

δ +

(cid:19)

U
αδ2

=

1
2

≤ 1
2

1

Setting δ = (U/α)1/3, we obtain the upper bound (U/α)1/3
on SD(Z(Xf,1), Z(X)), as desired.

i=1

etδ

(cid:104)

we get

(cid:80)d

Pr[S > δ] = Pr

j=1 vi,jYi,j. By the Markov inequality,

B. PROOF OF THEOREM 6

Let S = (cid:80)n
etS > etδ(cid:105) ≤ E(cid:2)etS(cid:3)
for every t > 0. Note that mS(t) = E(cid:2)etS(cid:3) is the moment
generating function of S and mY (t) = E(cid:2)etY(cid:3) =
1 − (λt)2 > exp(cid:0)−2(λt)2(cid:1). Making use of the assumption

1−(λt)2 ,
deﬁned for t < 1/λ,
is the moment generating function
of Y ∼ Lap(λ) [16]. Suppose that (λt)2 ≤ 1/2 so that
(cid:107)vi(cid:107)2 = 1, it follows that

1
1 − v2
i,j(λt)2 <

1
exp(−2v2
i,j(λt)2)

n(cid:89)
= exp(cid:0)2(λt)2n(cid:1) .
(cid:18)
etδ < exp(cid:0)2λ2t2n − tδ(cid:1) = exp
Setting t = δ/(4λ2n), we may conclude that
Pr[S > δ] ≤ mS(t)
as desired. Note that the assumption δ ≤ √
8λn makes sure
that the constraint λ2t2 ≤ 1/2, a constraint that we needed
in the course of our proof, actually holds for our ﬁnal choice
t = δ/(4λ2n).

− δ2
8nλ2

n(cid:89)

d(cid:89)

d(cid:89)

mS(t) =

(cid:19)

i=1

j=1

i=1

j=1

,

Setting vi = (1, 0, . . . , 0) for i = 1, . . . , n, Theorem 6 col-

lapses to the following result:

Corollary 1. For i = 1, . . . , n, Yi ∼ Lap(λ) be i.i.d. ran-
dom variables. Then, for each δ > 0 such that δ ≤ √
8λn,

the following holds:

(cid:34) n(cid:88)

(cid:35)
Yi ≥ δ
Again, the assumption δ ≤ √

Pr

i=1

(cid:18)

(cid:19)

≤ exp

− δ2
8nλ2

8λn is not very restrictive.

We would like to note that a result similar to Corollary 1 is
found in [14]. The proof there, however, is (slightly) ﬂawed
(although the ﬂaw can easily be ﬁxed at the expense of a

(30)

25slightly weaker statement). Our proof of Theorem 6 actu-
ally builds on the proof given in [14] (except for the slightly
ﬂawed part).

C. PROOF OF THEOREM 8

We will make use of the following two results:

Pr(cid:2)(cid:12)(cid:12)(cid:107)u

Lemma 10

([1]). Let u ∈ Rr be arbitrary but ﬁxed. Let
R = (Ri,j) be a random (d × r)-matrix such that the entries
Ri,j are i.i.d. according to the normal distribution N (0, 1).
(Ru) ∈ Rd. Then
Consider the random projection u(cid:48) := 1√
the following holds for every constant γ > 0:

d

2

−γ2d/8

Lemma 11

(cid:48)(cid:107)2
2 − (cid:107)u(cid:107)2
([2]). Let v, x ∈ Rr be arbitrary but ﬁxed.
Let R be the same random (d × r)-matrix and let u (cid:55)→ u(cid:48)
be the same random projection as in Lemma 10. Then the
following holds for every constant γ > 0:
2 + (cid:107)x(cid:107)2
2)

(cid:48)(cid:11) − (cid:104)v, x(cid:105)(cid:12)(cid:12) ≥ γ

(cid:105) ≤ 4e

(cid:104)(cid:12)(cid:12)(cid:10)v

((cid:107)v(cid:107)2

−γ2d/8

Pr

, x

(cid:48)

(cid:12)(cid:12) ≥ γ(cid:107)u(cid:107)2

2

(cid:3) ≤ 2e

2

We are now prepared for the proof of Theorem 8. Let
M := |X| be the number of instances in the universe, and let
N := |F| be the number of counting queries in F. Suppose
that A is an r-dimensional arrangement for F (resp. for the
corresponding sign matrix A) such that γ := ˜γmin(A|A) > 0.
Let A(cid:48) be the d-dimensional arrangement for F that results
from A by randomly projecting every vector u ∈ {ux : x ∈
X} (resp. every vector v ∈ {vf : f ∈ F}) to u(cid:48) (resp. to v(cid:48)).
Recall that the vectors ux, vf have unit Euclidean length.
Let p1 denote the probability for

∃w ∈ {ux : x ∈ X} ∪ {vf : f ∈ F} : (cid:107)w

(cid:48)(cid:107)2 ≥ 2 .

(31)

According to Lemma 10, applied with 1 in the role of γ and
combined with the union bound, we get p1 ≤ 2(M +N )e−d/8.
For every f ∈ F , consider the vector

xf =

 .

1
2

b=0,1

Ax,f ux

x∈Xf,b

|Xf,b|

 1
· (cid:88)
(cid:88)
∃f ∈ F :(cid:12)(cid:12)(cid:10)v
(cid:11) − (cid:104)vf , xf(cid:105)(cid:12)(cid:12) ≥ γ
(cid:1) · γ

≥(cid:0)(cid:107)vf(cid:107)2

2 + (cid:107)xf(cid:107)2

(cid:48)
f , x

(cid:48)
f

2

γ
2

= 2 · γ
4

.

4

Note that (cid:107)xf(cid:107)2 ≤ 1. Let p2 denote the probability for

(32)
Note that (cid:104)vf , xf(cid:105) = ˜γf (A|A) ≥ γ for all f ∈ F . Moreover,

2

.

According to Lemma 11, applied with γ/4 in the role of γ
and combined with the union bound, we get

p2 ≤ 4N e

−γ2d/128.

As an easy calculation shows, condition (22) implies that,
with a probability of at least 1 − β, none of the events (31),
(32) occurs. If none of these events occurs, then

˜γmin(A|A(cid:48)

) = min

f∈F ˜γf (A|A(cid:48)

) ≥ (1/4)(γ/2) = γ/8 ,

where the scaling factor 1/4 takes into account that the vec-
tors in the arrangement A(cid:48) (with an Euclidean length of at
most 2) must still be normalized so as to have unit Euclidean
length.

26