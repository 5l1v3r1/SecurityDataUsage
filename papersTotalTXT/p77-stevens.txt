On the Hardness of Evading

Combinations of Linear Classiﬁers

Dept. of Computer and Information Science

Dept. of Computer and Information Science

Daniel Lowd

University of Oregon
Eugene, OR 97403

lowd@cs.uoregon.edu

David Stevens

University of Oregon
Eugene, OR 97403

dstevens@cs.uoregon.edu

ABSTRACT
An increasing number of machine learning applications in-
volve detecting the malicious behavior of an attacker who
wishes to avoid detection. In such domains, attackers mod-
ify their behavior to evade the classiﬁer while accomplishing
their goals as eﬃciently as possible. The attackers typically
do not know the exact classiﬁer parameters, but they may
be able to evade it by observing the classiﬁer’s behavior on
test instances that they construct. For example, spammers
may learn the most eﬀective ways to modify their spams
by sending test emails to accounts they control. This prob-
lem setting has been formally analyzed for linear classiﬁers
with discrete features and convex-inducing classiﬁers with
continuous features, but never for non-linear classiﬁers with
discrete features. In this paper, we extend previous ACRE
learning results to convex polytopes representing unions or
intersections of linear classiﬁers. We prove that exponen-
tially many queries are required in the worst case, but that
when the features used by the component classiﬁers are dis-
joint, previous attacks on linear classiﬁers can be adapted
to eﬃciently attack them. In experiments, we further ana-
lyze the cost and number of queries required to attack dif-
ferent types of classiﬁers. These results move us closer to
a comprehensive understanding of the relative vulnerability
of diﬀerent types of classiﬁers to malicious adversaries.

Categories and Subject Descriptors
I.2.6 [Artiﬁcial Intelligence]: Learning—Concept Learn-
ing; F.2 [Analysis of Algorithms and Problem Com-
plexity]: Miscellaneous

Keywords
Adversarial machine learning; spam

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
AISec’13, November 4, 2013, Berlin, Germany.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2488-5/13/11 ...$15.00.
http://dx.doi.org/10.1145/2517312.2517318

1.

INTRODUCTION

In a growing number of adversarial domains, including
many kinds of spam and fraud, machine learning is being
used to detect malicious behavior [5, 6, 8, 18, 17, 14, 13, 3].
In such domains, criminals have a strong incentive to mod-
ify their behavior to evade detection. As criminals adapt,
system designers respond by updating their classiﬁers, lead-
ing to a never-ending arms race. In order to get the upper
hand, we need a better understanding of these adversarial
dynamics so that we can understand the vulnerabilities of
current approaches and design more robust methods in the
future.

In this paper, we look at the theoretical and practical dif-
ﬁculty of evading certain types of non-linear classiﬁers with
discrete feature spaces. To successfully evade a classiﬁer,
the attacker must ﬁnd an instance that is classiﬁed as neg-
ative (innocent) and accomplishes the attacker’s goal, such
as an eﬀective spam email that gets past a spam ﬁlter. In
many cases, the attacker does not know the exact classiﬁer,
but can receive some feedback through interaction. For ex-
ample, spammers may send “test emails” to accounts they
control to see if their candidate spams would be blocked.
Similarly, creators of comment spam can see if their post-
ings are ﬂagged or deleted, criminals on Twitter can see if
their accounts are banned, and web spammers can query
search engines searches to see the rankings of their own web
pages. However, our focus is not on domain-speciﬁc heuris-
tics but on general-purpose attacks which can be applied to
a wide variety of domains.

We perform our analysis within the framework of adver-
sarial classiﬁer reverse engineering (ACRE) [10]. We as-
sume that the adversary’s preferences can be described by
a cost function, where lower costs represent instances that
are more desirable, such as more eﬀective spam emails. A
concept class is said to be ACRE learnable for a given cost
function if an attacker can ﬁnd the lowest-cost negatively-
classiﬁed instance using only a polynomial number of mem-
bership queries. If an attacker can ﬁnd an instance within
a factor k of optimal with a polynomial number of queries,
then the concept class is said to be ACRE k-learnable. Pre-
vious work has shown that convex-inducing classiﬁers are
ACRE learnable (to arbitrary precision) in continuous fea-
ture spaces when the cost function is an L1 distance [12], and
linear classiﬁers are ACRE 2-learnable with binary-valued
features when the cost function is Hamming distance rela-
tive to some “ideal” instance [10]. However, there have been
no results on learning non-linear classiﬁers in discrete feature
spaces, except for very limited Boolean formulae [10].

77The discrete case is much harder than the continuous case
because the adversary can only modify each dimension by
discrete steps, and cannot perform arbitrary-precision line
searches. This reduces the set of points that can be queried
from all of Rn to merely the points on an n-dimension hy-
percube, assuming n binary-valued features. The discrete
case is also arguably more realistic, because many classiﬁers
use discrete attributes such as the presence or absence of a
word or the number of times it appears, and these quanti-
ties cannot take on fractional values. Even truly continuous
features, such as network latency, are often discretized by
rounding to some precision, such as milliseconds.

In this initial work, we focus on non-linear classiﬁers where
either the positive or negative class is given by a convex poly-
tope. Since a convex polytope can be deﬁned as an intersec-
tion of half-spaces, this is equivalent to an ensemble of linear
classiﬁers where the aggregation function is AND (for a con-
vex positive class) or OR (for a convex negative class). For
example, consider a spam ﬁlter that has a sequence of indi-
vidual linear classiﬁers that recognize diﬀerent types of spam
or recognize spam based on diﬀerent types of features. Here,
the negative class is a convex polytope, since such instances
must be labeled as negative by every one of the component
linear classiﬁers. In the limit as the number of component
classiﬁers grows, these polytopes can approximate any con-
vex set, making this a very general and powerful concept
class. This class also has practical, real-world relevance: for
example, Google uses a similar ensemble to label malicious
advertisements [16].

We begin by showing hardness results that demonstrate
that this set cannot be attacked eﬃciently in the worst case.
We then consider the restriction where the linear classiﬁers
operate on disjoint sets of features. Here we are able to
demonstrate attacks that are as eﬃcient as attacking a sin-
gle linear classiﬁer when the positive class is convex, and less
eﬃcient but still polynomial time when the negative class is
convex. After proving these theoretical results, we demon-
strate the eﬃciency of our methods on non-linear classiﬁers
trained from spam data.

2. ACRE LEARNING

We begin by brieﬂy reviewing the deﬁnitions of ACRE
learning, deﬁning the concept classes of interest, and pre-
senting previous results on linear classiﬁers with binary-
valued features. This will serve as the foundation for our
new results on non-linear classiﬁers with binary-valued fea-
tures. For further details on many of these topics, see Lowd
and Meek [10].
The instance space, X , is the set of all possible inputs to
the classiﬁer. For this paper, we assume that the instance
space consists of n-dimensional binary-valued feature vec-
tors, x = (x1, x2, . . . , xn). For example, in spam ﬁltering,
the ith feature could represent the presence (xi = 1) or ab-
sence (xi = 0) of the ith word used by the ﬁlter.
For convenience, we use the symbol ⊕ as a feature-wise

XOR operator with the following notation:

(x ⊕ i) = (x1, x2, . . . ,¬xi, . . . , xn)

In other words, the ith feature value in the resulting instance
has been “toggled” from 0 to 1 or 1 to 0, and all other values
remain identical to those in x. This operation can also be
used with a set of feature indices, e.g., (x ⊕ {i, j, k}), which
toggles all features in the set.

A classiﬁer, c(x), is a function from the instance space to
the labels ‘−’ and ‘+’, indicating the negative/innocent and
positive/malicious classes, respectively. A concept class is a
set of classiﬁers C. For this paper, one important concept
class is linear classiﬁers, in which the label is determined by
a weighted sum of the features:

(cid:40)

c(x) =

+ if w · x ≥ T
− otherwise

where w is a vector of real-valued weights and T is a real-
valued threshold.
An adversarial cost function, a(x), is a function from the
instance space X to the non-negative reals R+ which deﬁnes
the adversary’s relative preferences over the instance space.
While many diﬀerent cost functions are possible, for this
paper we assume that it is the Hamming distance relative
to the adversary’s “ideal” instance, xa:
|xi − xa
i |

n(cid:88)

a(x) =

i=1

For a given instance x, we refer to each feature index i for
which xi (cid:54)= xA
i as a “change.” Assuming the cost function is
clear from context, we use Cx to refer to the set of all such
changes:

Cx = {i | xi (cid:54)= xa
i }

Thus, we can equivalently deﬁne a Hamming distance cost
function as the number of changes: a(x) = |Cx|.

As a concrete example, suppose a spammer wishes to send
the most eﬀective spam email that will get past a particular
ﬁlter. The spammer’s ideal spam message is xa, which has a
cost of zero. Other messages have a cost equal to the number
of words added or removed from xa, since emails that are
more corrupted are likely to be less eﬀective.

For a given classiﬁer c(x), the minimal adversarial cost
(MAC) is the smallest cost of any negatively classiﬁed in-
stance, and an instance of minimal adversarial cost (IMAC)
is any negative instance with this cost. A k-IMAC is a neg-
ative instance with a cost of at most k times the MAC. In a
membership query, the attacker requests the label c(x) for a
chosen instance x. We assume that x may be any instance
in X without restriction. The attacker may use this infor-
mation to plan future queries and eventually ﬁnd an IMAC
or k-IMAC.
A concept class C is ACRE learnable for a set of cost func-
tions if, for any c ∈ C, an adversary can always ﬁnd an IMAC
given one negative instance x− and a number of membership
queries that is polynomial in the size of the problem repre-
sentation. The class is ACRE k-learnable if the adversary
can ﬁnd a k-IMAC with a polynomial number of queries.

Lowd and Meek [10] show that linear classiﬁers with dis-
crete features are ACRE 2-learnable with a Hamming dis-
tance cost function. They do this constructively by introduc-
ing an algorithm, FindBooleanIMAC, and proving its cor-
rectness. See Algorithm 1 for pseudocode. FindBoolean-
IMAC works by iteratively reducing the cost of a negative
instance. It begins with the provided negative example, x−,
and uses two operations to reduce its cost: removing one
change in the current instance and replacing two changes in
the current instance with one not in the current instance.
Recall that a “change” is simply a feature whose value is dif-
ferent in xa. Each operation reduces the number of changes,

78Algorithm 1 FindBooleanIMAC(cid:0)xa, x−(cid:1)

(adapted from [10])

y ← x−
repeat

yprev ← y
for all f ∈ Cy do

if c(y ⊕ f ) = − then

toggle f in y

end if
end for
for all f1 ∈ Cy; f2 ∈ Cy; f3 (cid:54)∈ Cy do

if c(y ⊕ {f1, f2, f3}) = − then

toggle f1, f2, and f3 in y

end if
end for

until yprev = y
return y

and thus the cost, by one. When neither operation can
be performed without resulting in a positively-classiﬁed in-
stance, the algorithm terminates. Lowd and Meek prove
that this instance must have at most twice the optimal cost:
if an instance with less than half the cost existed, then it
would be possible to replace two changes in the suboptimal
instance with one of the changes from the optimal instance.

3. EVADING CONVEX POLYTOPES

While previous results have demonstrated that linear clas-
siﬁers are easy to evade, little is known about attacking non-
linear classiﬁers in discrete feature spaces. Nelson et al. [12]
prove that convex-inducing classiﬁers in a continuous fea-
ture space are ACRE learnable with an L1 cost function.
However, their results do not generalize to discrete feature
spaces. Continuous spaces are easier than discrete spaces
because the features can be adjusted by arbitrarily large or
small amounts in order to explore the decision boundary of
the classiﬁer.
In discrete feature spaces, it is NP-hard to
even determine the sign of a feature weight [10].

We now deﬁne an interesting non-linear concept class and
discuss the worst-case hardness of evading these classiﬁers
in discrete feature spaces. Like Nelson et al. [12], we focus
on classiﬁers where the set of positive or negative instances
deﬁnes a convex set. However, we further restrict this set to
be a convex polytope, which is equivalent to a conjunction
or disjunction of linear classiﬁers when the positive label is
interpreted as True. The following deﬁnitions specify this
more precisely.

Deﬁnition A disjunction (or union) of linear classiﬁers is
an ensemble of linear classiﬁers in which instances are la-
beled as positive if any component classiﬁer labels it as pos-
itive:

(cid:40)

c(x) =

+ if ci(x) = + for any i ∈ {1, . . . , C}
− otherwise

where C is the number of component classiﬁers.

Deﬁnition A conjunction (or intersection) of linear clas-
siﬁer is an ensemble of linear classiﬁers in which instances
are labeled as positive if all component classiﬁers label it as

(cid:40)

positive:

c(x) =

+ if ci(x) = + for all i ∈ {1, . . . , C}
− otherwise

where C is the number of component classiﬁers.

To demonstrate the connection between these ensembles
and convex polytopes, note that each component linear clas-
siﬁer deﬁnes two half-spaces, one for each class. In a disjunc-
tion of linear classiﬁers, the positive class is the union of the
positive half-spaces from each component, and the negative
class is the intersection of the negative half-spaces from each
component. Thus, in a disjunction of linear classiﬁers, the
negative instances are deﬁned by a convex polytope, and in
a conjunction of linear classiﬁers, the positive instances are
deﬁned by a convex polytope. Since these deﬁnitions are
equivalent, we use them interchangeably.

Convex polytopes, or unions and intersections of half-
spaces, are interesting for several reasons. In the limit, con-
vex polytopes can approximate any convex set arbitrarily
well. Thus, methods that work on convex polytopes may
work on other convex-inducing classiﬁers, such as convex
quadratic classiﬁers. Unions and intersections of half-spaces
are also a practical way to build real-world classiﬁcation sys-
tems, as demonstrated by Google’s use of a combination
of linear classiﬁers to detect malicious advertisements [16].
These types of classiﬁers are also easier to analyze than ar-
bitrary convex-inducing classiﬁers.

Biggio et al. [2] have also analyzed the diﬃculty of evad-
ing combinations of linear classiﬁers, although they use a
diﬀerent measure of evasion diﬃculty. Their results suggest
that combinations of classiﬁers are harder to attack, which is
consistent with some of our theoretical results below. They
also ﬁnd that classiﬁer combinations often have lower accu-
racy than a single linear classiﬁer, which is consistent with
our later experimental results.
3.1 Hardness Results for Convex Polytopes

Lowd and Meek [10] prove that even recovering the signs
of features in a linear classiﬁer is NP-Complete when the
features are binary-valued. Since a linear classiﬁer is a spe-
cial case of a convex polytope, this results applies to these
more complicated classiﬁers as well. However, while they
show that a 2-approximation can be eﬃciently found in a
linear classiﬁer, we will prove that ﬁnding a constant factor
approximation for convex polytopes requires an exponential
number of queries in the worst case. We consider the cases
of a convex negative class (disjunction of linear classiﬁers)
and convex positive class (conjunction) separately.

Theorem 1. With binary-valued features and Hamming
distance costs, disjunctions of linear classiﬁers are not ACRE
k-learnable for any constant k.

Proof. We prove this constructively by deﬁning a set of
classiﬁers for which ﬁnding a k-IMAC requires exponentially
many queries.

2k , and H of size n

2 − n

2k − 1. We construct

Partition the set of n features into three sets: F of size

2 + 1, G of size n
n
a set of n
all features in F is − 1
in G is − n

2k classiﬁers.

(cid:0) n
k − 1(cid:1) /(cid:0) n

n

In the ith classiﬁer, the weight for
2 +1 , the weight for the ith feature
k , the weight for all other features in G or H is

2k − 1(cid:1), and the threshold is −1 + 1

n .

79Deﬁne xa as xa

and x(cid:48) as x(cid:48)

i = 1 for i ∈ G. Thus, c(x−) = −, as − 1

i = 1 for i ∈ F ,
−
i = 0 for all i, x− as x
2 + 1(cid:1) = −1, which is less than the threshold of −1 + 1
(cid:0) n
2 +1 ·
for each of the component classiﬁers. To show that x(cid:48) is
elements of G each contribute a weight of(cid:0) n
2k − 1(cid:1),
also negative, consider the ith component classiﬁer. The ith
element of G contributes a weight of − n
2k −1
k − 1 = −1, which is less than
for a total weight of − n
2k , x− is
the threshold. Since a(x−) = n
not a k-IMAC.

k − 1(cid:1) /(cid:0) n

2 + 1 and a(x(cid:48)) = n

k and the other n

k + n

n

n

We can describe the negative instances more generally as
follows: Any instance that has at least one feature from
G ∪ H is positive, unless it includes every feature from G
and none from H. This is because every feature in G or H
has a large positive weight in every or almost every com-
ponent classiﬁer. This positive weight more than outweighs
the total contribution of all features in F . Thus, once such
a feature has been added, the only way to satisfy each of
the classiﬁers is to include its “special” feature from G. If
any such feature is omitted, then at least one of the classi-
ﬁers will remain positive, leading to a positive class label. If
any extra features from H are added, then there is no way
to counteract this positive weight in all classiﬁers, so the
instance will always be positive.

To ﬁnd a negative instance with low cost, the attacker
must determine which features are in G. However, each
membership query provides very little information about G
since only one non-empty subset of G ∪ H can ever result
in a negative instance. Thus, G can only be determined by

brute-force, which requires (cid:0)n/2−1
(cid:0)n/2−1

(cid:1) queries. Basic proper-
(cid:1)n/2k, which is exponential in n.

ties and lower bounds of combinations allow us to conclude:

(cid:1) >(cid:0) n/3

(cid:1) ≥(cid:0) 2k

n/2k

n/2k

n/2k

3

Our proof relies on creating a classiﬁer with n/2k compo-
nents where a feature that has a positive sign in one compo-
nent may have a negative sign in another. Thus, an inter-
esting question is: What hardness results can be achieved
with only a small number of classiﬁers and where features
have consistent signs? While this question remains open for
convex negative instances, we can show such a result when
the positive class is convex.

Theorem 2. With binary-valued features and Hamming
distance costs, conjunctions of linear classiﬁers are not ACRE
k-learnable for any constant k.

2 − n

4k features is −4k, the weight of the last n

Proof. When the positive set is a conjunction, then the
adversary needs to ﬁnd an instance that is classiﬁed as neg-
ative side by just one of the component linear classiﬁers.
Suppose that there are two components, c1 and c2. In c1,
2 features is −2, the weight of the
the weight of the ﬁrst n
next n
4k features
is +2n, and the threshold is −n + 1.1 Thus, c1 will classify
an instance as negative if it has all of the n
2 low-weight fea-
tures, all of the n
4k higher-weight features, or an appropriate
combination of the two sets. In c2, the weight of the ﬁrst
2 features is −4, the weight of all other features is 0, and
n
the threshold is −n− 1. Thus, c2 will classify an instance as
negative if it has at least n
2 low-weight features.
Let xa
i = 0 for all i. Thus, the optimal evasion cost for c1
1If necessary, we can reorder all of the feature indices ran-
domly to ensure that the adversary does not exploit any
ordering properties of these feature indices.

4 + 1 of the n

4k but the optimal evasion cost for the c2 is n

is n
−
choosing x− so that x
i = 1 for i = 1 . . . n
negative by both components.

4 + 1. By
2 , x− is marked as

To ﬁnd an instance with a cost lower than n

To be positive in c2, such a y must have at most n

4 + 1, the at-
tacker must ﬁnd the n
4k features that have the higher magni-
tude weights in c1. To determine that one of these features
has a large weight in the c1, the attacker must ﬁnd at least
one instance y such that c1(y) = − and c2(y) = +. If all
the instances queried are classiﬁed as negative by c2, then
the label from c1 has no eﬀect on the overall conjunction.
If all instances queried are classiﬁed as positive by c1, then
there is no information to diﬀerentiate any of the features
used by the c1.
4 of the
low-weight features. In c1, the total weight of these is − n
2
or more. Therefore, to be classiﬁed as negative y must also
have at least n
8k of the higher-weight features and none of
the positively-weighted features. Since the adversary does
not know which are which, it must guess a set of at least n
8k
“good” features that includes none of the bad ones. There
is no advantage to choosing a set of more than n
8k features,
since a larger fraction of these will contain at least one bad

feature, leading to a positive instance. There are(cid:0) n/2
8k features, and(cid:0)n/4k

to choose a candidate set of n
possible sets contain entirely “good” features. Using upper
and lower bounds on numbers of combinations, the number
e )n/8k,
of possible sets for every “good” set is at least ( 2k
which is exponential in n. Thus, the adversary must issue
an exponential number of queries before it can be sure to
ﬁnd the features needed to construct a low-cost negative
instance.

(cid:1) ways
(cid:1) of those

n/8k

n/8k

4. DISJOINT FEATURES

Since convex polytope classiﬁers are hard to evade in the
worst-case, we now consider a restricted subclass of con-
vex polytopes and prove that this concept class is ACRE
2-learnable under Hamming distance cost functions.

Deﬁnition A disjoint conjunction (or disjunction) of linear
classiﬁers is a conjunction (or disjunction) of linear classi-
ﬁers in which the weight for the ith feature is non-zero in at
most one of the component linear classiﬁers.
We use Fi to denote the set of all features that have non-

zero weight in the ith component linear classiﬁer.

This is a signiﬁcant restriction, but not an implausible
one. An email system may have one component classi-
ﬁer trained on plain-text subject and body content, one on
HTML content, and perhaps another that looks at images.
An email may have to pass one or all of these ﬁlters to
avoid be labeled as spam. Another example is multi-factor
biometric authentication, in which separate classiﬁers could
analyze a person’s ﬁngerprint, face, and voice. In general,
when combining information from classiﬁers trained to look
at very diﬀerent aspects of an object, these classiﬁers may
often use entirely separate sets of features.

We now prove that these classes are vulnerable by extend-
ing FindBooleanIMAC (Algorithm 1) to work both for a
convex negative class and a convex positive class.
4.1 Evading Disjoint Disjunctions

In this case, the class label is a disjunction of the results
of each linear classiﬁer. Thus, to be labeled as negative, the

80adversary must create an instance that is labeled as negative
by each linear classiﬁer. We prove that this concept class
is ACRE 2-learnable by showing that FindBooleanIMAC
always ﬁnds a 2-IMAC without requiring any modiﬁcations.

Theorem 3. With binary-valued features and Hamming
distance costs, disjoint disjunctions of linear classiﬁers are
ACRE 2-learnable.

Proof. For a disjunction, the negative instances lie in
the intersection of all negative half-spaces. Thus, the at-
tacker must evade every component linear classiﬁer simul-
taneously. Since their features are disjoint, no change used
to evade one component has any aﬀect on the other compo-
nents. Therefore, the evasion subproblems are independent,
and the minimum cost to evade the disjunction is the sum
of the minimum cost to evade each component.

Let y be an instance returned from running FindBoolean-
IMAC on a disjoint disjunction of linear classiﬁers. It fol-
lows from the termination conditions of FindBooleanI-
MAC that no individual change can be removed from Cy
and no pair of changes can be replaced with one not in Cy
without resulting in a positive instance. Since Fi is a sub-
set of the feature space, it also follows that no individual
change in Cy ∩ Fi can be removed and no pair of changes
in Cy ∩ Fi can be replaced by one in Fi − Cy without re-
sulting in a positive instance according to the ith classiﬁer
ci (which is the only classiﬁer aﬀected by these changes).
Thus, the restriction of y to the features in Fi satisﬁes the
termination conditions of running FindBooleanIMAC on
ci. Since FindBooleanIMAC is guaranteed to ﬁnd a 2-
IMAC for linear classiﬁers, |Cy ∩ Fi| must be at most twice
the minimal evasion cost for ci. Since each feature subset of
y evades the corresponding component with at most twice
the minimum cost, the sum of these costs must be at most
twice the total minimum cost.
4.2 Evading Disjoint Conjunctions

In this case, the class label is a conjunction of the re-
sults of each linear classiﬁer.
In contrast to the previous
case, this means the adversary need only create an instance
that is labeled as negative by one component linear classi-
ﬁer. Therefore, the minimum cost to evade a conjunction is
the minimum cost to evade any one of its component clas-
siﬁers. However, in this case it is not enough to just ﬁnd a
negative instance and run FindBooleanIMAC. The reason
for this is twofold. Firstly, a negative instance is only guar-
anteed to be negative in one of the linear classiﬁers. Thus,
FindBooleanIMAC may not ﬁnd the boundary of any of
the classiﬁers that do not initially classify the instance as
negative. Second, while FindBooleanIMAC will provide
a 2-IMAC for some component, this component may have
a much higher evasion cost than the lowest one. Therefore
a more careful exploration is required and it is necessary to
have enough negative instances so that each linear classiﬁer
classiﬁes at least one instance as negative. The following
algorithm takes an adversarial instance xa and this mini-
mal set of innocent examples X− and, using a variant of
FindBooleanIMAC as a subroutine, ﬁnds a collection of
instances that evade every component classiﬁer with at most
twice the optimal cost.

Our method for eﬃciently evading disjoint conjunctions of
linear classiﬁers is shown in Algorithm 2. FindBoolean-
ConjunctionIMAC repeatedly calls a modiﬁed version of

Algorithm 2 FindBooleanConjunctionIMAC(cid:0)xa, X−(cid:1)

R ← ∅; S ← ∅
for all x− ∈ X− do
toggle f in x−

for all f ∈ Cx− ∩ R do

end for
while c(x−) = − do

y ← FindBooleanIMAC-R(cid:0)xa, x−, R(cid:1)

add y to the solution set S
add Cy to the removed features set R
for all f ∈ Cx− ∩ Cy do

toggle f in x−

end for
end while

end for
return the lowest-cost instance in S

FindBooleanIMAC to ﬁnd negative instances that evade
some component classiﬁer with at most twice the optimal
cost. The modiﬁed subroutine, FindBooleanIMAC-R, ac-
cepts a set of features R that must not be modiﬁed. As each
negative instance is found, its features are added to R and
removed from any initial negative instances in order to force
the algorithm to ﬁnd at least one instance that evades each
component classiﬁer. By taking the minimum cost of these,
we obtain a cost that is at most twice optimal. We formalize
these arguments in the proof of the following theorem.

Theorem 4. With binary-valued features and Hamming
distance costs, disjoint conjunctions of linear classiﬁers are
ACRE 2-learnable when given a negative instance for each
component.

Proof. The minimum cost of evading the conjunction is
the minimum cost of evading any one of its component clas-
siﬁers. Thus, it suﬃces for the attacker to ﬁnd a negative
instance for each component with at most twice the mini-
mum cost. The instance with smallest cost in this set must
have at most twice the minimum overall cost.

We now prove that the set S constructed by FindBoolean-
ConjunctionIMAC contains a 2-IMAC for each of the com-
ponent classiﬁers. Consider the instance y returned from
FindBooleanIMAC-R in one iteration of the while loop.
Since the subroutine only returns negative instances, y must
be negative in at least one classiﬁer, ci. Any changes in Cy
that do not pertain to the ith classiﬁer can be removed with-
out aﬀecting the label of y. Since the subroutine removes
all such changes before terminating, Cy ⊂ Fi.

The only way to add features to R is for them to ﬁrst
be in some Cy. Since the features are disjoint, R will only
contain features from Fi after ﬁnding some instance y that
is negative in ci and adding it to S.
If ci(y) = − and R ∩ Fi = ∅, then FindBooleanIMAC-
R considered all possible modiﬁcations relevant to ci so y
If R ∩ Fi is non-
is a 2-IMAC for the linear classiﬁer ci.
empty, then at least one instance that is negative in ci must
already be in S. The ﬁrst of these instances is a 2-IMAC,
since R ∩ Fi was empty when it was originally found.
It remains to be shown that we will ﬁnd a 2-IMAC for ev-
ery component. Since x− has no changes in R and the sub-
routine is not allowed to add any change in R, Cy must have
no change in R. Cy is non-empty and disjoint from R, so R

81grows in each iteration. As long as some other classiﬁer cj re-
mains for which there is no 2-IMAC in S, there must be some
remaining x− ∈ X− such that cj(x−) = − and R ∩ Fj re-
mains empty. As R grows, the number of remaining features
for other classiﬁers decreases until FindBooleanIMAC-R
can no longer ﬁnd negative instances for any other classiﬁer
and must therefore eventually ﬁnd a 2-IMAC for cj.

Therefore, FindBooleanConjunctionIMAC ﬁnds a 2-
IMAC for every component, and thus returns a 2-IMAC for
the overall conjunction.

5. EXPERIMENTAL RESULTS

While we have shown that optimally evading disjoint con-
junctions and disjunctions of classiﬁers can be approximated
with a polynomial number of queries, we are also interested
in how expensive this is in practice. Continuing the theme
of email spam, we created classiﬁers trained on email data
and had an adversary attempt to deceive these classiﬁers
with minimal changes to their spam instances. The adver-
sary may only interact with a classiﬁer by querying it with
an email instance to determine if it is classiﬁed as spam or
non-spam. This could be done by creating two email ac-
counts, sending messages from one to the other, and seeing
how the message is classiﬁed when it is received. Rather
than assuming that the adversary knows the entire feature
space, we restrict the classiﬁer to a plausibly guessable sub-
set. We examine attacking both disjunction and conjunction
classiﬁers, as well as baseline linear classiﬁers.
5.1 Classiﬁer Conﬁgurations

All of our experiments were written in Python, using the
Scikit-learn learning framework [15] to create and train clas-
siﬁers. We trained Logistic Regression, Linear Support Vec-
tor Machine (SVM) and Na¨ıve Bayes classiﬁers on the 2005
TREC public spam corpus [4], which is comprised of 92,189
labeled email messages. For simplicity, we restrict features
considered to words that appear in at least ten documents
and ignore common English stop words. To create disjoint
linear classiﬁers to compose into our disjunction and con-
junction classiﬁers, we split the feature space ﬁrst three ways
and then ﬁve ways to have classiﬁers composed of three or
ﬁve linear classiﬁers. We achieve this split ﬁrst by random
partition and again by training a linear SVM classiﬁer on
all of the features and then using the value of the weights in
the classiﬁer’s decision function to partition the features into
sets ranging from the most “spammy” to most “hammy” fea-
tures. This was partially inspired by Jorgensen et al. [9], who
suggest using separate classiﬁers on the spammy, hammy,
and neutral features in order to avoid “good word” attacks.
These many combinations gave us a wide variety of classi-
ﬁers for our experiments. For comparison purposes, we also
train a linear classiﬁer of each type using the entire feature
set. Our methodology for training classiﬁers is intentionally
simple, since our goal is to analyze the attack algorithms,
not to develop the best spam ﬁlters.

To make the classiﬁers roughly comparable, we increase
or decrease the bias of each component linear classiﬁer by
a constant value so that the false negative rate (fraction of
spam classiﬁed as innocent) is 10%. This makes all of the
classiﬁers equally eﬀective at blocking spam, although their
false positive rates (fraction of innocent email classiﬁed as
spam) are diﬀerent. Other previous work has used a ﬁxed
false positive rate [11, 10, 1]. We chose to ﬁx the false neg-

Best FP rate (%) Mean FP rate (%)
NB SVM
LR NB SVM LR
0.04
0.04
0.18
0.07
9.97
6.87
5.21
1.20
2.90
4.85
12.90
6.75

0.07
2.19
2.84

0.18
3.93
5.41

Linear
Conj.
Disj.

Table 1: Best and geometric mean false positive rate for
each base classiﬁer type with each ensemble method.

ative rate rather than the false positive rate because we are
interested in how attack diﬃculty depends on classiﬁer type,
not how it depends on the base eﬀectiveness of each classi-
ﬁer.

See Table 1 for a basic comparison of the false positive
rates of the diﬀerent types of classiﬁers. Both of the ensem-
ble methods performed worse than the simple linear classi-
ﬁer with all three classiﬁer types, which suggests that these
classiﬁers are not especially accurate on this domain. This
is consistent with the results of Biggio et al. [2], who found
that multiple classiﬁer systems were usually less accurate
than single classiﬁers. In our experiments, combinations of
3 classiﬁers were more accurate than 5, and partitioning fea-
tures by their relative spamminess or hamminess was usually
more accurate than the random partition.
5.2 Attack Conﬁguration

Once our classiﬁers have been trained, we then run our ad-
versarial algorithms as described above, as well as the Find-
BooleanIMAC algorithm. We perform the same optimiza-
tions used in [10] when performing the FindBooleanIMAC
subroutine, except that we considered swapping all pairs of
changes rather than only adjacent pairs. This is necessary
with multiple classiﬁers because two consecutive changes in a
list might pertain to diﬀerent components. As an additional
optimization for the conjunction attack, when we ﬁnd an
instance y that evades one of the component classiﬁers, we
search for other features from the same component to add
to R, which accelerates convergence.

Our adversarial and negative instance(s) are chosen ran-
domly from the corpus. Because it is not realistic for an
adversary to have complete knowledge of the classiﬁer’s fea-
ture space, we instead restrict the set of changeable features
to one of three sets. The ﬁrst is the set of all words in any
of the ispell dictionaries (21,515 words). The other two are
much smaller: the 1,000 most common words in the training
data (excluding stop words) and 1,000 words selected ran-
domly from the training data. While these technically re-
quire additional knowledge of the classiﬁer, we believe these
sets would be very easy to approximate using public email
data or other text corpora. In addition to the features in the
given set, the attacker is also allowed to include any feature
present in the adversarial or negative instance(s).

In preliminary experiments, we found that the adversary’s
task was often much too easy: many spams could be dis-
guised with a single change, and many others could be dis-
guised with only two or three changes. When the opti-
mal number of changes is some small constant k, then an
optimal attack can be found by brute-force search in just
O(nk) queries. It is only as the required number of changes
grows larger that our polynomial-time algorithms are neces-
sary. Therefore, to better analyze our algorithms, we artiﬁ-
cially increase the “spamminess” of each email. We do this

82by changing 100 randomly-selected features, each of which
makes the email look more like spam in one of the compo-
nent classiﬁers.

We run the attack 100 times for each conﬁguration of clas-

siﬁers and features, resulting in around 9,200 runs overall.
5.3 Results

In this evaluation, we are primarily interested in three
things: the vulnerability of each classiﬁer, the cost of each
attack attack relative to the optimal attack, and the overall
complexity of the attacks.2
Vulnerability
Before discussing the performance of our attacks, we ﬁrst ex-
amine how vulnerable these classiﬁcation systems actually
are. Figure 1 shows the optimal costs for defeating each clas-
siﬁer type. For example, Figure 1a shows the distribution
of the optimal costs for defeating the classiﬁers composed of
logistic regression classiﬁers, as well as the costs for defeat-
ing a single logistic regression classiﬁer. As we may expect
due to the similarity in the attack algorithm, the disjunction
and linear attack tend to be similar in the size of the optimal
disguise, with a very similar distribution in the Na¨ıve Bayes
case, and the linear classiﬁer being easier to avoid in the
other two cases. The actual distribution varies based on the
type of linear classiﬁer used, with Logistic Regression often
not needing more than ten changes and Na¨ıve Bayes needing
40 or more (and sometimes as many as 100). The SVM clas-
siﬁers see the largest variance, and lie somewhere between
the other two. These values are not surprising, as Logistic
Regression and SVM tend to assign large weights to a few
features, which the adversary can modify to more eﬃciently
evade the classiﬁers. In sharp contrast to the Disjunction
and Linear classiﬁers, the Conjunction classiﬁer was defeat-
able with less than ten changes at least 90% of the time in
every case. These results show while the disjunctive clas-
siﬁers may sometimes be harder to defeat than the linear
classiﬁers, the conjunctive ones tend to be much easier. In-
tuitively, this makes sense, as in the former case, the adver-
sary must trick every linear classiﬁer, while in the latter, a
single defeat is suﬃcient. One of the variables under consid-
eration was the way in which the features were partitioned
between classiﬁers. However, both partitions we considered
resulted in nearly identical distributions.
Approximation Quality
Our algorithms for attacking each classiﬁer type is guaran-
teed to be within a factor of two of optimal. However, it is
important to note that this is only if the exact feature space
is known. If working with an approximated feature space, as
we do in our experiments, the concept of “optimal” must be
restricted to only consider the features available to the at-
tacker. Figure 2 illustrates the relative costs of our attacks,
both compared to the restricted vocabulary and the true op-
timal cost. As expected, we never go over a factor of two
in the restricted case. More interestingly, over 90% of the
time, the algorithms are able to get within a factor of two
of the true optimal, despite only having an approximation
of the feature space. It is sometimes as bad as four times
optimal, but the frequency of costs decrease exponentially
as they get higher.
2All ﬁgures in this section were created with the Matplotlib
Python package [7].

To evaluate the dependence of the algorithm on the vocab-
ulary used, we now split Figure 2b by vocabulary, as shown
in in Figure 2c. As expected, the much larger dictionary
set performs the best – almost always within a factor of 1.5.
However, it is interesting to note that the random set is not
much worse than the set of the top features, with all of the
sets rarely going over a ratio of 2. This shows that a much
smaller feature set can be selected with very little assumed
information of the classiﬁer without too much of a hit in
approximation quality.

Complexity
The last major issue to evaluate is that of complexity. The
bottleneck operation in most adversarial scenarios will be
query time (e.g., the time it takes to send and email and
have the mail server label it). As we just showed, a better
approximation can be achieved by increasing the size of the
feature set the adversary may consider. However, increasing
this size will also lead to a larger number of queries. There-
fore, we wish to evaluate the complexity of the attack by
looking at the average number of queries as a function of
the size of the considered feature set.

Figure 3 shows the number of queries as a function of the
optimal number of changes (restricted to the given vocab-
ulary). Again, we see that the conjunction classiﬁer is eas-
ier to defeat than the others, while the disjunction behaves
much like the linear classiﬁer. Since these log-log plots dis-
play a nearly constant slope, this suggests that the number
of queries is a polynomial function of the number of required
changes. The dependence appears to be approximately lin-
ear for the conjunction and quadratic for the disjunction.
Additionally, the full dictionary results only diﬀer from that
of the much smaller vocabularies by a constant factor of
approximately 20. This suggests that the complexity only
increases linearly with the size of the vocabulary used. Most
of the time, using an entire dictionary will not be possible.
However, in the previous section we showed that it is easy to
guess a much smaller set that leads to very eﬀective results.
This linear relationship means that the attacker can easily
ﬁnd an attack that achieves the approximation quality they
desire within an acceptable number of queries.

These relationships make sense based on the structure
of the algorithm. Before converging, FindBooleanIMAC
must verify that no possible change could replace any two
existing changes in the current instance. The number of pos-
sible changes is approximately the number of features, and
the number of existing changes is approximately the opti-
mal cost, leading to a complexity of O(nm2) where m is the
optimal cost.

The number of queries used might render these speciﬁc
tasks impractical for a real-world adversary. However, our
experimental procedure of adding extra spammy features to
every email makes the problem artiﬁcially harder. On un-
modiﬁed spam, the optimal cost is lower and so the number
of queries is much smaller. Furthermore, these algorithms
were designed to guarantee a nearly optimal attack against
any classiﬁer, which is a much more general problem than
most adversaries face. In practice, an attacker is free to stop
these algorithms early and use the best attack found so far.
Many queries are spent in these algorithms proving that an
attack is nearly optimal. Additionally, many queries might
be spent with only slight improvement. Because of this, the
number of queries can be signiﬁcantly reduced if the algo-

83(a) Logistic Regression

(b) Na¨ıve Bayes

(c) SVM

Figure 1: Optimal cost of defeating each classiﬁer type

(a) Optimal on Restricted Vocabulary

(b) True Optimal

(c) True Optimal, by Attack Vocabulary

Figure 2: Ratio of number changes found to optimal disguise.

rithm terminates as soon as an attack with an acceptable
cost is found.

In Figure 4, we show the number of queries needed if the
algorithm terminates as soon as an attack is found that is
within a factor of two of the optimal achievable attack. In-
terestingly, the number of queries no longer exhibits a strong
dependence on the number of optimal changes. Further-
more, the average number of queries is lowered by at least
a factor of 100 in each case. This shows that the number of
queries required to ﬁnd a nearly optimal instance is just a
small fraction of the queries required to guarantee a nearly
optimal instance. Therefore, our algorithms are nearly as
eﬀective and vastly more eﬃcient if stopped early, making
these attacks much more practical.

Finally, if the attacker has background knowledge about
which features are likely to be spammy or innocent, then
the order of the queries can be changed in order to ﬁnd
a good attack much faster. This background knowledge
could come from educated guesses or from learning a simi-
lar model on publicly available data, as done by Biggio et
al. [1]. Knowledge can also be gained by running our algo-
rithms and observing which features are helpful or harmful;
this could then be reused on future attacks against the same
or similar classiﬁers. Lowd and Meek discuss even more eﬃ-
cient heuristics against linear classiﬁers [11], many of which
could be adapted to combinations of linear classiﬁers. Any
of these techniques could be used in conjunction with the
early stopping method described above to construct very
practical evasion attacks.

6. CONCLUSION

Understanding the theoretical hardness of attacking diﬀer-
ent types of classiﬁers is important for designing more robust
systems in adversarial domains. In addition to measuring ac-
curacy and eﬃciency, the hardness of evading a classiﬁer can
constitute another important dimension to consider. Previ-
ous results had shown that an attacker could ﬁnd a negative
instance to evade any linear classiﬁer with at most twice
the minimum Hamming distance using only a polynomial
number of membership queries.

In this paper, we presented the ﬁrst results for evading
non-linear classiﬁers in discrete feature spaces. We found
that arbitrary conjunctions and disjunctions of linear classi-
ﬁers require exponentially many queries to even approximate
minimum Hamming distance. This suggests that they may
be more robust than individual linear classiﬁers. However,
when the component classiﬁers operate on disjoint sets of
features, we proved that these ensembles of linear classiﬁers
can still be attacked relatively eﬃciently.

We also showed the relative practicality of these attacks
using combinations of classiﬁers trained on email spam. The
number of queries required depends linearly on the number
of features considered and quadratically on the optimal cost.
We found smaller vocabularies to be nearly as eﬀective as
larger vocabularies while requiring fewer queries. Addition-
ally, simply stopping the algorithms early can ﬁnd nearly
optimal attacks while reducing the number of queries by
two orders of magnitude.

01020304050# Optimal Changes0.00.20.40.60.8FrequencyConjunctionDisjunctionLinear020406080100# Optimal Changes0.00.20.40.60.8FrequencyConjunctionDisjunctionLinear05101520253035# Optimal Changes0.00.10.20.30.40.50.6FrequencyConjunctionDisjunctionLinear1.01.21.41.61.82.0# Changes Found / Restricted Optimal # of Changes0.000.050.100.150.200.250.300.35Frequency1.01.52.02.53.03.54.0# Changes Found / Optimal # of Changes0.000.050.100.150.200.250.300.35Frequency1.01.52.02.53.03.5# Changes found / # Optimal changes0.00.10.20.30.40.50.60.70.8FrequencyRandom 1000Top 1000Dictionary84(a) Conjunction

(b) Disjunction

(c) Linear

Figure 3: Number of queries as a function of the restricted optimal number of changes (log-log plot)

(a) Conjunction

(b) Disjunction

(c) Linear

Figure 4: Number of queries needed to be within a factor of 2 of the restricted optimal number of changes

It is important to remember that our results are only
upper bounds on the diﬃculty of real-world classiﬁers. In
practice, any given classiﬁer may have properties that make
it much easier to attack than the hardest classiﬁer in the
concept class. Furthermore, if the adversary has additional
background knowledge or does not need to guarantee ap-
proximate optimality, much more eﬃcient attacks may be
possible. Thus, these results provide insight and guidance
but do not constitute a complete evaluation of a classiﬁer’s
security.

In ongoing work, we are investigating other criteria that
may make conjunctions and disjunctions of linear classiﬁers
easy to attack. For example, bounds on the number of com-
ponents or the relative weights of the features may make
eﬃcient attacks possible even when the features overlap.

7. ACKNOWLEDGMENTS

This research was partly funded by ARO grant W911NF-
08-1-0242. The views and conclusions contained in this doc-
ument are those of the author and should not be interpreted
as necessarily representing the oﬃcial policies, either ex-
pressed or implied, of ARO, NSF, or the U.S. Government.

8. REFERENCES
[1] B. Biggio, I. Corona, D. Maiorca, B. Nelson,

N. Srndic, P. Laskov, G. Giacinto, and F. Roli.
Evasion attacks against machine learning at test time.
In European Conference on Machine Learning and

Principles and Practice of Knowledge Discovery in
Databases (ECML PKDD). Springer, 2013.

[2] B. Biggio, G. Fumera, and F. Roli. Multiple classiﬁer

systems for adversarial classiﬁcation tasks. In
Proceedings of the 8th International Workshop on
Multiple Classiﬁer Systems, pages 132–141. Springer,
2009.

[3] D. Chau, S. Pandit, and C. Faloutsos. Detecting

fraudulent personalities in networks of online
auctioneers. Knowledge Discovery in Databases:
PKDD 2006, pages 103–114, 2006.

[4] G. Cormack and T. Lynam. Spam corpus creation for

trec. In Stanford University, 2005.

[5] L. F. Cranor and B. A. LaMacchia. Spam!

Communications of the ACM, 41(8):74–83, August
1998.

[6] I. Drost and T. Scheﬀer. Thwarting the nigritude

ultramarine: Learning to identify link spam. In
Proceedings of the Sixteenth European Conference on
Machine Learning, pages 96–107. Springer, 2005.

[7] J. D. Hunter. Matplotlib: A 2d graphics environment.

Computing In Science & Engineering, 9(3):90–95,
2007.

[8] N. Jindal and B. Liu. Opinion spam and analysis. In
Proceedings of the International Conference on Web
Search and Web Data Mining, pages 219–230. ACM,
2008.

[9] Z. Jorgensen, Y. Zhou, and M. Inge. A multiple

instance learning strategy for combating good word

100101102Restricted Optimal # of Changes100101102103104105106107# of Queries1000 WordsDictionary (23K)100101102103Restricted Optimal # of Changes103104105106107108109# of Queries1000 WordsDictionary (23K)100101102103Restricted Optimal # of Changes103104105106107108109# of Queries1000 WordsDictionary (23K)100101102103Restricted Optimal # of Changes101102103104105106107# of Queries1000 WordsDictionary (23K)100101102103Restricted Optimal # of Changes102103104105106107108# of Queries1000 WordsDictionary (23K)100101102103Restricted Optimal # of Changes103104105106107108# of Queries1000 WordsDictionary (23K)85attacks on spam ﬁlters. J. Mach. Learn. Res.,
9:1115–1146, June 2008.

[10] D. Lowd and C. Meek. Adversarial learning. In

Proceedings of the eleventh ACM SIGKDD
international conference on Knowledge discovery in
data mining, KDD ’05, pages 641–647, New York, NY,
USA, 2005. ACM.

[11] D. Lowd and C. Meek. Good word attacks on

statistical spam ﬁlters. In Proceedings of the Second
Conference on Email and Anti-Spam (CEAS), pages
125–132, 2005.

[12] B. Nelson, B. I. P. Rubinstein, L. Huang, A. D.

Joseph, S. J. Lee, S. Rao, and J. D. Tygar. Query
strategies for evading convex-inducing classiﬁers.
CoRR, abs/1007.0484, 2010.

[13] J. Neville, O. ¸Sim¸sek, D. Jensen, J. Komoroske,

K. Palmer, and H. Goldberg. Using relational
knowledge discovery to prevent securities fraud. In
Proceedings of the Eleventh ACM SIGKDD
International Conference on Knowledge Discovery in
Data Mining, pages 449–458. ACM, 2005.

[14] D. O’Callaghan, M. Harrigan, J. Carthy, and

P. Cunningham. Network analysis of recurring
YouTube spam campaigns. In Proceedings of the Sixth
International AAAI Conference on Weblogs and Social
Media. AAAI Press, 2012.

[15] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,

B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research,
12:2825–2830, 2011.

[16] D. Sculley, M. E. Otey, M. Pohl, B. Spitznagel,

J. Hainsworth, and Y. Zhou. Detecting adversarial
advertisements in the wild. In Proceedings of the 17th
ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages
274–282. ACM, 2011.

[17] G. Stringhini, C. Kruegel, and G. Vigna. Detecting
spammers on social networks. In Proceedings of the
26th Annual Computer Security Applications
Conference, pages 1–9. ACM, 2010.

[18] S. Yardi, D. Romero, G. Grant, and d. boyd. Detecting
spam in a Twitter network. First Monday, 15(1), 2010.

86