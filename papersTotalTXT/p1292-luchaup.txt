Formatted Encryption Beyond Regular Languages

Daniel Luchaup

Madison, WI, USA

University of Wisconsin
luchaup@cs.wisc.edu
Thomas Ristenpart
University of Wisconsin
Madison, WI, USA
rist@cs.wisc.edu

Thomas Shrimpton
Portland State University
teshrim@cs.pdx.edu

Portland, OR, USA

Somesh Jha

University of Wisconsin
Madison, WI, USA
jha@cs.wisc.edu

ABSTRACT
Format-preserving and format-transforming encryption (FPE
and FTE, respectively) are relatively new cryptographic prim-
itives, yet are already being used in a broad range of real-
world applications. The most ﬂexible existing FPE and FTE
implementations use regular expressions to specify plaintext
and/or ciphertext formats. These constructions rely on the
ability to eﬃciently map strings accepted by a regular ex-
pression to integers and back, called ranking and unranking,
respectively.

In this paper, we provide new algorithms that allow en-
cryption with formats speciﬁed by context-free grammars
(CFGs). Our work allows for CFGs as they appear in prac-
tice, partly a pure grammar for describing syntax, and partly
a set of lexical rules for handling tokens. We describe a
new relaxed ranking method, structural ranking, that nat-
urally accommodates practical CFGs, thereby empowering
new FPE and FTE designs. We provide a new code library
for implementing structural ranking, and a tool that turns
a simple YACC/LEX-style grammar speciﬁcation into rank-
ing code. Our experimental analysis of the code shows that
the new CFG ranking algorithm is eﬃcient in interesting
settings, even when the grammars are ambiguous. For ex-
ample, we show that one can eﬃciently rank C programs of
size thousands of kilobytes in milliseconds.

Keywords
Format-preserving encryption; format-transforming encryp-
tion; ranking

1.

INTRODUCTION

Format-preserving encryption (FPE) [2–4] and Format-
transforming encryption (FTE) [7], have recently emerged
as practically important versions of formatted encryption.
Loosely speaking, formatted encryption is like conventional
encryption in terms of the security goals, but with the addi-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660351 .

tional functionality of transforming plaintexts conforming to
one format — say, described by a regular expression (regex)
— into ciphertexts conforming to another.

When FPE is used, the plaintext and ciphertext formats
are the same. This admits, for example, in-place encryption
of database entries that must abide by some strict format-
ting requirements, such as being a valid credit-card number,
social security number, or postal address. FPE is already
widely used in the payment industry.

FTE, on the other hand, encrypts a plaintext of a given
format to ciphertext of a diﬀerent format. FTE is a gen-
eralization of FPE. Dyer et al. [7] recently introduced FTE
and showed its eﬃcacy as a tool for circumventing network
monitors that use deep-packet inspection to block protocols
such as Tor [6].

Ranking schemes and formatted encryption. At the
core of all known instantiations of FPE and FTE schemes are
eﬃcient algorithms for ranking languages, i.e., for mapping
language elements to integers in an invertible fashion. Gold-
berg and Sipser (GS) [8] introduced the notion of ranking as
a technique for optimal language compression. In particu-
lar, the GS rank of a word w in a language L is the position
of w in the lexicographical ordering of L, given as a non-
negative integer. Thus, ranking deﬁnes a bijective function
(rank), and unranking deﬁnes the inverse function (unrank)
that maps n ∈ {0, 1, . . . ,|L| − 1} to the word w ∈ L whose
rank is n. A pair of ranking and unranking algorithms for a
language L constitute a ranking scheme.

GS [8] gave a polynomial-time ranking scheme for lan-
guages represented by unambiguous context-free grammars
(CFGs). They also suggest a method for ranking a regu-
lar language, given a DFA for it, but it was Bellare et al. [2]
who provided the ﬁrst detailed algorithm for such a method.
(Since they credit the method to GS, we will continue to call
it GS ranking.)

Moreover, Bellare et al. [2] show how to instantiate eﬃ-
cient FPE when the format is the regular language L speci-
ﬁed by a DFA. Given a blockcipher E that operates over the
integers {0, 1, . . .|L|−1 } (an integer cipher ), the FPE cipher-
text of plaintext x ∈ L is deﬁned by y = unrank(EK (rank(x)).
For obvious reasons, this is known as rank-encipher-unrank
FPE.

This scheme is deterministic, as no per-plaintext random-
ness or internal state is used. On the other hand, the FTE
scheme of Dyer et al. [7] uses conventional (randomized or
stateful) authenticated encryption (AE) as its underlying

1292encryption primitive in their scheme. To encrypt a plain-
text x ∈ L1, one represents x as a bit string, encrypts it
using the AE scheme, treats the resulting AE ciphertext as
the base-2 representation of an integer, and unranks this
to the appropriate string y ∈ L2. The Dyer et al. scheme
supports ciphertext formats speciﬁed by regular expressions
(regexes), by way of ﬁrst converting the regex to an NFA
and then to a (minimal) DFA using standard algorithms.
The latter NFA-to-DFA step can result in an exponential
increase in the number of states of the resulting DFA au-
tomaton.

Luchaup et al. [14] provide a uniﬁed framework for for-
matted encryption that captures all of the FPE and FTE
schemes so far described, and they go on to give regex-
speciﬁed FTE and FPE schemes that avoid NFA-to-DFA
conversion. To do so, they introduced the notion of relaxed
ranking, which maps strings in a language L to integers in
ZN for N ≥ |L|. The relaxed ranking must also be eﬃ-
ciently invertible on the image of L. They show how to
build FTE/FPE schemes using relaxed ranking, and pro-
vide an eﬃcient relaxed ranking scheme starting only from
the NFA representation of a language.

Context-free-language formats. All the work so far
shows how to eﬃciently support formats deﬁned by regular
languages. But there are important settings in which allow-
ing formats to be speciﬁed by CFGs would be more useful.
For example, network protocol messages, ﬁle formats, and
(programming) language speciﬁcations are often described
by CFGs, not regexes. To overcome the restriction to regu-
lar languages, one needs an algorithm for eﬃciently ranking
strings in a format described by a CFG. To the best of our
knowledge, the only known ranking method using CFGs,
that of GS [8], works only for unambiguous CFGs, is far
too slow in practice, and lacks a corresponding unranking
method.

What’s more, while CFGs are more expressive then regexes,
they are typically more diﬃcult to specify and use, hence the
widespread reliance on tools such as YACC and LEX [13]. In
practice, almost no context-free language (CFL) is speciﬁed
solely based on a CFG. A CFL is typically speciﬁed at two
levels: a syntax level speciﬁed using a CFG; and a lexical
level speciﬁed using regular expressions. The lexical level
deﬁnes the symbols, also called tokens, used by the CFG. A
grammar that uses regexes for tokens can be converted to
a pure CFG, by including token deﬁnition in the CFG de-
scription, but this comes at the cost of programming eﬀort,
ﬂexibility, explosion of the number of symbols, and, most
importantly, runtime performance. Formatted encryption
supporting CFLs should likewise handle this separation.

To sum up, the potential of formatted encryption is lim-
ited by the lack of (relaxed-)ranking schemes that eﬃciently
handle context-free languages in practical settings. The
main eﬀort of this work is to move beyond this limitations.

Our contributions. We provide the ﬁrst eﬃcient approach
for performing relaxed ranking from a CFG (either pure,
or using lexical tokens). Our approach for CFLs follows a
general two-step framework ﬁrst introduced by Luchaup et
al. [14], here we ﬁrst map strings in the language to one of
their parse trees (chosen deterministically), and then we per-
form a (strict) ranking of parse trees. Thus if the grammar
is not ambiguous (each string has only one parse tree), then
our relaxed-ranking approach becomes strict ranking. For

the second step, we give a ranking method for parse trees
which is “structural” in the sense that it allows the use of
distinct ranking algorithms for trees derived from distinct
grammar symbols. In particular, it allows for distinct rank-
ing algorithms for the tokens of the language. As a result
our method also works with languages speciﬁed using the
two levels, syntax and lexical. This is an essential feature
for eﬃciency and for using existing grammar speciﬁcations.
In fact, our method allows relaxed ranking of a slightly
more general class of languages that we call cfg-parseable
languages. An example is a format that starts with a byte
counter that speciﬁes how long the rest of the data is, and
that ends with a checksum. This format is not context-free,
or even context-sensitive.

Since ranking has applications beyond formatted encryp-
tion, such as compression and random language member
generation [8, 11, 16], we also provide the ﬁrst analysis of
the issues involved when we replace ranking with relaxed
ranking in those applications. One central matter in this
analysis is the ambiguity of the grammar. We quantify am-
biguity, and then show how it aﬀects the quality of ranking
applications, including formatted encryption.

We also provide a library that implements our relaxed
ranking for arbitrary CFGs, and we deliver a tool that turns
a simple YACC/LEX-like grammar speciﬁcation into code
that performs ranking for that grammar. This is then used
as a pluggable component in the FPE/FTE framework from
Luchaup et al. [14], yielding formatted encryption of context-
free languages. We report on performance of our relaxed-
ranking schemes, and show that we can rank C programs up
to 5,000 bytes in under one second, or even under 12 ms if
we bound the length of lexical tokens.

2. BACKGROUND

In this section we describe the state of the art in Format-
ted Encryption. We start with a formal deﬁnition of ranking
and unranking, followed by a description of FTE and FPE
(by extension). After that, we introduce relaxed ranking,
and explain how FTE and FPE are adapted to use it. We
conclude with the limitations of the current work.

Basic notions. A format is simply a language L, a set of
strings over some alphabet. We will use the terms language
and format interchangeably, and for simplicity we assume
languages that are ﬁnite. In practice one may use inﬁnite
languages, but then take as format a slice of the language:
= {w ∈ L : |w| = n}
if L is a language and n ∈ N, then L
is the slice of L that contains all its strings of length n.

(n)

A format speciﬁcation describes a format L. A trivial
speciﬁcation is simply to list all elements of L, but for large
languages this won’t be eﬃcient, so we do not consider it
further. Rather, we seek compact and developer-friendly
speciﬁcations. When L is regular, then options for spec-
iﬁcation include using a regular expression (regex), a non-
deterministic ﬁnite automaton (NFA), or deterministic ﬁnite
automaton (DFA). When L is context-free (but not regular),
then a CFG becomes a natural option. Supporting CFG
speciﬁcations is the goal of our paper.
Following Luchaup et al. [14], a format-transforming en-
cryption (FTE) scheme is a pair of algorithms (E,D). The
encryption algorithm E may or may not be randomized. It
takes as input a pair of format speciﬁcations that specify
a plaintext format Lp and a ciphertext format Lc, as well

1293as a message M ∈ Lp, and a secret key. It produces a ci-
phertext C ∈ Lc or a special error symbol ⊥. Decryption D
reverses the operation. To indicate the type of speciﬁcations
supported by a scheme, we will often refer to XXX-speciﬁed
FTE, where XXX ∈ {DFA, NFA, regex, CFG}.

An FPE scheme is an FTE scheme for which Lp = Lc,
meaning that ciphertexts and plaintexts must share the same
format. We will use the term FTE to refer to schemes that
may or may not be format-preserving.

DFA-speciﬁed FTE. Bellare, Ristenpart, Rogaway, and
Stegers (BRRS) [2] ﬁrst formalized the notion of FPE, build-
ing oﬀ prior work on de novo constructions [18], arbitrary-
set enciphering schemes [3], and industry demand for the
primitive. They also introduced an FPE scheme that works
for any regular language.
In BRRS, formats are speciﬁed
using a deterministic ﬁnite automaton (DFA). Their con-
struction makes use of ranking, which was ﬁrst introduced
in the context of language compression by Goldberg and
Sipser (GS) [8]. Ranking found subsequent use in appli-
cations such as random language member generation [16],
biology (c.f. [11]), and now in formatted encryption.
Let L be a language with |L| = N . A ranking scheme
for L is a bijection rankL : L → ZN together with its inverse
unrankL : ZN → L. It is common to deﬁne the ranking func-
tion with respect to some total order ≺ on L; for example,
the GS ranking takes ≺ to be the lexicographical ordering.
Given a total order, the ranking function (relative to ≺) is
deﬁned by rankL(x) = |{y ∈ L : y ≺ x}|.

BRRS give an FPE scheme for arbitrary regular languages
using DFA-based ranking. A simple generalization of their
FPE scheme to an FTE scheme is the following. Consider
two languages X and Y , such that rankX and unrankX form
a ranking scheme for X, and rankY and unrankY form a
ranking scheme for Y . Assume that |X| = |Y |, and a cipher
E : {0, 1}k× Z|Y | → Z|Y | for some key length k. (Recall that
a cipher is a family of functions such that for any K ∈ {0, 1}k
it is the case that EK (·) = E(K,·) deﬁnes a permutation on
−1
K . Then a plain-
its domain.) Let the inverse of EK be E
text M ∈ X can be encrypted to ciphertext C ∈ Y using
the rank-encipher-unrank construction deﬁned as shown in
Figure 1.
EK (M ) :
If M /∈ X then Return ⊥
r ← rankX (M )
c ← EK (r)
Return unrankY (c)

DK (C) :
If C /∈ Y then Return ⊥
c ← rankY (C)
r ← DK (c)
Return unrankX (r)

Figure 1: Encryption and decryption using the rank-
encipher-unrank construction.

When X = Y , this matches the BRRS FPE scheme.
We note that the BRRS scheme uses the GS algorithm for
(un)ranking, and this requires X and Y to be speciﬁed by
DFAs. BRRS also argue that the DFA representation is ef-
fectively necessary for regular X, Y , because ranking from
given either the regex or NFA representations is PSPACE-
complete.

Regex-speciﬁed FTE. FTE was ﬁrst introduced by Dyer,
Coull, Ristenpart, and Shrimpton (DCRS) [7]. Their mo-
tivating application was avoidance of network censors that
identify anti-censorship protocols via regex-based deep-packet

inspection. Hence, they targeted FTE with formats speciﬁed
by regular expressions. They also sought to support arbi-
trary unformatted data as plaintexts. Like BRRS, they used
the DFA-based GS ranking scheme. To accommodate regex
speciﬁcations, DCRS employed the classic regex-to-NFA-to-
DFA conversion process (c.f., [19]). For some regexes, this
process leads to poor performance (or even failure) because
the NFA-to-DFA conversion results in an exponential in-
crease in automaton size. However, they show experimen-
tally that this behavior is not typical in their use cases.

FTE was revisited by Luchaup, Dyer, Jha, Ristenpart,
and Shrimpton (LDJRS) [14], who generalized the DCRS
treatment in various ways. LDJRS also provided a new
method for regex-speciﬁed FTE that completely avoids the
NFA-to-DFA conversion. The basis of their new technique is
called relaxed ranking. Consider a language L and the inte-
gers ZN for some N ≥ |L|. A relaxed-ranking scheme for L is
a pair of functions RankL : L → ZN and UnrankL : ZN → L
such that for all x ∈ L it holds that UnrankL(RankL(x)) = x.
See Figure 2. (We capitalize Rank and Unrank to distinguish
them from strict ranking with functions rank and unrank.)
It follows that Rank is always injective, while Unrank may
be surjective in the case that |L| < N .

LDJRS gave a recipe for building a relaxed-ranking scheme
for a language L, requiring three ingredients (see the mid-
dle diagram in Figure 2). First, one speciﬁes an “inter-
mediate” set I for which one has eﬃcient algorithms for
rankI : I → ZN and unrankI : ZN → I where N = |I|. Sec-
ond, a function map : L → I. Finally, a function gen : I → L
such that ∀x ∈ L, gen(map(x)) = x. (It follows that map
is injective and gen is surjective). From these, they deﬁne
RankL : L → ZN by RankL(x) = rankI (map(x)), and they
deﬁne UnrankL : ZN → L by UnrankL(a) = gen(unrankI (a)).
To accommodate relaxed ranking, LDJRS designed an
FTE scheme that uses a technique known as cycle waling.
We give their cycle-walking FTE scheme in the rightmost
box of Figure 2, and use Img(L) to denote the image of
RankL for L ∈ {X, Y }. Encryption with cycle walking seeks
a point inImg (Y ) ⊆ ZN which will map to a valid ciphertext
under the relaxed ranking. Note that the scheme as written
assumes plaintext format X and ciphertext format Y are
such that |X| = |Y |, but LDJRS give variants that work for
more general pairs of formats. We refer the reader to [14]
for the details.

LDJRS give a relaxed ranking scheme for NFAs, which en-
ables eﬃcient regex-speciﬁed FTE by ﬁrst converting a regex
to an NFA. They show that NFA-based relaxed ranking of-
ten leads to schemes almost as fast as DFA-based ranking
schemes without the blow-up in automaton size.

CFG-speciﬁed FTE. While regex-speciﬁed FTE may be
suﬃcient in some contexts, there are settings in which we
would prefer to use context-free grammars (CFGs). CFGs
allow expression of richer languages, and are often used in
practice to describe the structure of ﬁles, web pages, protocol
messages, and more. However, none of the BRRS, DCRS or
LDJRS schemes provide CFG-speciﬁed FTE.

GS sketched a (high-degree) polynomial-time ranking for
context-free languages. While theoretically interesting, their
ranking only works with unambiguous CFGs and has no
matching unranking algorithm. Recall that an ambiguous
CFG is one which has multiple derivations for some string
in the associated language; moreover, determining whether
or not a grammar is ambiguous is undecidable. Thus, the

1294Rank L

L

Img(L)

map

rankI

L

map(L)

Z

N

L

Unrank

ZN
RankL : L → ZN injective
UnrankL : ZN → L surjective
∀x : UnrankL(RankL(x)) = x

I

unrank
I
|I| = N

gen
map : L → I injective
gen : I → L surjective
∀x : gen(map(x)) = x

EK (M ) :
If M /∈ X then
Return ⊥
r ← RankX (M )
Do
r ← EK (r)
Until r ∈ Img(Y )
Return UnrankY (r)

DK (C) :
If C /∈ Y then
Ret ⊥
r ← RankY (C)
Do
r ← DK (r)
Until r ∈ Img(X)
Return UnrankX (r)

Figure 2: (Left) Diagram of relaxed ranking. (Middle) Using strict ranking on an intermediate set I to obtain relaxed ranking
on L. (Right) The cycle-walking construction of FTE using relaxed ranking from [14] for formats X, Y with |X| = |Y |.

GS ranking based on CFGs is not well suited to practice.
M¨akinen [15] later presented an algorithm for ranking and
unranking of Slizard languages (not general CFL). How-
ever, the presentation leaves room for interpretation and it
appears to contain ﬂaws in some parts of the algorithm.

In summary, there currently exists no eﬃcient mechanism
for ranking (relaxed or otherwise) given a CFG representa-
tion of a CFL. However, the diﬃculty in handling CFLs goes
beyond the lack of eﬃcient CFG-based ranking methods.
Using a monolithic or “pure” CFG for the language would
be impractical, even if eﬃcient CFG-based ranking existed.
This is because in practice people do not use “pure” CFGs to
describe a language. In practice, a language is described at
two levels: a lexical level that deﬁnes parsing tokens using
regexes, and a second level that deﬁnes the syntax using a
pure CFG that has the tokens as terminals. Although every
such two-level grammar can be converted to a pure CFG, by
absorbing the tokens in the CFG description, this comes at
the cost of programming eﬀort, ﬂexibility, explosion of the
number of symbols, and runtime performance.

Another diﬃculty is that a CFG speciﬁcation is often more
complex than a regex speciﬁcation, and we need a tool to
relieve the programmer from the tedious and error prone
process of CFG speciﬁcation.

Finally, some formats used in practice are not context free.

For instance, counter based formats of the form:

<Item> := <# bytes in Item> BYTE* <Checksum>

We are not aware of any ranking method that can handle
such formats in a principled way.

In the rest of this paper we show how we address these
limitations. First we provide an eﬃcient CFG-based relaxed
ranking. Then we show how we adapt it to handle two-level
lexer/parser language speciﬁcations, including ones that in-
clude counters and similar non-context-free embellishments.

3. RELAXED RANKING FOR CFLS

We present an algorithm for relaxed ranking of a context-
free language. It will use a two-stage approach, with inter-
mediate objects being parse trees. First we present some
background on CFGs.
3.1 Background and Deﬁnitions
A context-free grammar (CFG) is a 4-tuple G = (N , Σ,
R, S), where N is a ﬁnite set of non-terminals; Σ is a ﬁnite
set of terminals, such that N ∩ Σ =∅ ; R is a ﬁnite relation
R ⊆ N × (N ∪ Σ)
If
(A, w) ∈ R, we write A → w, and we say that w is derived
from A in one step. We extend → to a derivation relation

; and S ∈ N is the start symbol.

∗

∗

∗

w1Aw2.

to words in (N ∪ Σ)

from words in (N ∪ Σ)
as follows.
We say that s2 is derivable from s1 in one step, and we
write s1 → s2 iﬀ ∃X ∈ N ∃w1, w2, w3 ∈ (N ∪ Σ)
∗
: s1 =
w1Xw3 ∧ X → w2 ∧ s2 = w1w2w3. In the later case, we say
that s2 = w1w2w3 can be derived from s1 = w1Xw3 using
the rule ρ = X → w2, and we write s1 →ρ
s2; if w1 ∈ Σ
∗
we say that w1Xw2 → w1w2w3 is the leftmost derivation
(by expanding the leftmost non-terminal). The transitive
closure of → is →∗
. The language accepted by G is the set
: S →∗
w}. A language is a context-free
L(G) = {w ∈ Σ
∗
language (CFL) if it is accepted by a CFG. A grammarG
is unambiguous iﬀ for all w ∈ L(G) there is exactly one
sequence of leftmost derivations ρ1, ρ2, ..., ρk that lead from
S to w, i.e., S →ρ1 s1 →ρ2 s2 →ρ3 ...sk−1 →ρk sk = w.
A symbolX in a CFG is called useless if it does not occur
in any derivation of a word from L(G). A symbol A ∈ N
is recursive if there is a non empty chain of rules such that
A →∗
For every non-terminal A ∈ N , we as-
Rule ordering.
sume that the ordered set (enumeration) of rules that have
A on the left hand side is RA = {ρ1, ..., ρ|RA||ρi : A → wi ∈
R}. This enumeration deﬁnes an arbitrary but ﬁxed order
ρj ⇐⇒ k < j (i.e. ρk pre-
on A’s rules, where ρk <
cedes ρj in the ordering RA). The k
rule in RA is denoted
by RA[k]. This assumption will be used in the upcoming
relaxed ranking algorithm.
Labeled trees and parse trees.
Informally, a parse tree
(or a derivation tree) for a CFG grammar G = (N , Σ,R, S)
is a tree in which every node is labeled with a rule ρ ∈ R.
The rule for a node determines how that node is “expanded”
by G, i.e. the number of the node’s children and their labels,
which must correspond to the non-terminals on the right side
of RA[k].
To make things precise, we deﬁne the following. A labeled
tree T =(V, E, r0, λ) over a set Λ is a directed tree where each
vertex has a label in Λ. Here (V, E, r0) is a tree where V is
the set of vertexes, E ⊆ V × V is the set of directed edges,
r0 ∈ V is the root; Λ is the set of labels; and λ : V → Λ
associates a label to each vertex. For a vertex v ∈ V , v[k]
child of v. The tree is ﬁnite, unless otherwise
is the k
speciﬁed. A parse-tree for a CFG grammar G = (N , Σ,R, S)
is a labeled tree T =(V, E, r0, λ) overR , where:

R

th

th

1. λ : V → R labels each node v with a rule. If λ(v) = ρ :
A → ... ∈ R, we say that v’s type is A and v’s rule is
ρ. For convenience, we use the notation v.s to denote
A, andv.ρ to denote ρ.

12952. For every node v,

if v.ρ is A → s0A1s1A2...Ansn,
, and Ai ∈ N , then v must
where n ≥ 0, si ∈ Σ
have n children v1, ..., vn, and each child vi must have
vi.s = Ai.

∗

Deﬁne P T( G) to be the set of all parse trees generated by
a grammarG , and P T( G, A) ⊆ P T( G) to be the subset of
parse trees with type of the root as A.

∗

If v1, ..., vn are v’s children, n ≥ 0, si ∈ Σ
∗

The yield of a parse tree.
Next, we deﬁne a func-
tion that takes a parse tree as input, and outputs the string
which is the yield for the tree. Let T be a parse tree with
root r0, and let v be a parse-tree node. Let W(v) ∈ Σ
∗
be the string recognized by v, deﬁned inductively as fol-
, and
lows.
Ai ∈ N , and if v.ρ is A → s0A1s1A2...Ansn, then W(v) =
s0W(v1)s1W(v2)...W(vn)sn. Overloading notation, deﬁne
W(T ) =W (r0), i.e. the string recognized by T (the yield
of T ). Deﬁne (cid:17)T(cid:17) to be |W(T )|, the length of the yield
W(T ). Thus, we can deﬁne a function W : P T( G) → Σ
∗
.
Now, for every A ∈ N , let WA : P T( G, A) → Σ
be de-
ﬁned by WA(T ) =W (T ). Note that when A = S, we have
WS : P T( G, S) → L(G).
Minimal parse trees. We will propose a ranking scheme
that counts the number of parse-trees that yield strings of
a given size. To avoid the problem that there may be an
inﬁnite number of such trees, we introduce the concept of
minimal parse-trees, and we will count only those.
Let G = (N , Σ,R, S) be a CFG. We say that T1 ∈
P T( G, A) is reducible iﬀ T1 has a subtree T2 ∈ P T( G, A)
with (cid:17)T1(cid:17) = (cid:17)T2(cid:17) (equivalently, with W(T1) =W (T2)).
Furthermore, we say that T is minimal iﬀ T has no reducible
subtrees. We deﬁne M P T (G) to be the set of minimal
parse trees generated by a grammar G, and M P T (G, A) ⊆
P T( G, A) is the set of minimal parse trees with root A. By
extension, we deﬁne the set of such parse trees with yield-
length k to be M P T (G, A, k) = {Y ∈M P T (G, A) : (cid:17)Y (cid:17)=k}.
We these deﬁnitions in place, we state a simple lemma,
whose proof we defer to the full version of this paper. In
Section 3.2, this lemma will provide assurance that we are
counting ﬁnite sets.

Lemma 1. For all k ≥ 0, M P T (G, A, k) is ﬁnite.

3.2 Relaxed ranking

Having built up this machinery, we proceed to describe
our two-step relaxed ranking approach. Let G be a CFG
corresponding to a CFL L0. We pick a slice size z ∈ N and
(z)
describe the relaxed ranking of the language slice L = L
0 .
We deﬁne the intermediate set I = M P T (G, S, z), the set of
minimal parse trees derived from S, the start symbol for G,
which yield a string of length z. Given this I and L, it
remains to deﬁne the functions gen and map. For simplicity,
in the rest of this section, when we use the term parse tree,
we mean minimal parse tree, unless otherwise speciﬁed.
First, let gen = WS, i.e. for all T ∈ I = M P T (G, S, z),
gen(T ) = WS(T ). To deﬁne map, we assume that we have a
deterministic parser which produces minimal parse trees (we
will address this assumption later). Then, for all w ∈ L, we
deﬁne map(w) to be a deterministically picked parse tree for
w in M P T (G, S). In other words map is the deterministic
parser. So, to ﬁnish our two-step relaxed ranking, it remains
to deﬁne the strict ranking and unranking algorithms, rankI
and unrankI , from parse trees to integers.

Just before doing exactly that, we note that ranking a
word or string w ∈ L clearly requires parsing to generate
a tree in M P T (G, S). The common parsing methods for
3|G|) time in the worst
an arbitrary grammar can take O(l
case, where l is the length of the word being parsed. (Ear-
ley’s method is said to perform much better in practice.)
The worst case cubic time of parsing a general CFG cannot
be improved much, and [12] shows why this is the case by
providing an eﬃcient reduction of Boolean Matrix Multipli-
cation to CFG parsing.

Strict ranking/unranking of minimal parse trees. We
show how to perform ranking by comparing two parse trees
whose roots have the same type. While it is possible to de-
ﬁne a ordering on all parse trees, this is not necessary for
our purpose. Our ﬁnal goal is ordering of strings in L(G),
hence strings derived from S, whose parse trees have some
rule of the form S → w at the root. For this, it suﬃces to
deﬁne an ordering only among trees of the same type, but
we must take a few precautions to prevent the case where a
string may have an inﬁnite number of parse-trees.
Assume that ≺A is a total order on M P T (G, A) for non-
terminal A (≺A is induced by the ordering on the rules
and described in detail later in the section). For all X ∈
M P T (G, A), let rankA(X) be deﬁned as:

rankA(X)=|{Y∈M P T (G, A,(cid:17)X(cid:17)) : Y≺AX}|

(1)
The deﬁnition is well founded because for any G the set
M P T (G, A,(cid:17)X(cid:17)) is ﬁnite (Lemma 1); which is why we re-
quire parsers that produce minimal parse-trees. This is a re-
alistic requirement satisﬁed by all eﬃcient parsing methods
that we know. For a large class of grammars, such as all un-
ambiguous grammars, or grammars without -productions,
all parse trees are minimal, i.e. M P T = P T. For other
grammars we can easily remove -productions so that they
have only minimal parse trees, but we do not require such
a conversion as long as there is a parser that can produce
minimal parse trees.
Observe that rankA(X) is the position of X among the
trees of same yield length in M P T (G, A,(cid:17)X(cid:17)), rather than
all trees in M P T (G, A). This is convenient for language
slices of the form L(G)

(n)

.

In the following we show how to obtain a total order for
the parse-trees of a grammar. The ordering is based on the
length of the yield of the trees, on an ordering of the gram-
mar rules, and on the structure of the trees. In particular:
1. Trees with shorter yields precede trees with longer

yields.

2. For the same length, the trees derived from an earlier
grammar rule precede the trees derived from a later
rule.

3. At the same yield length and grammar rule, the trees
are compared based on their ordered children, in a
manner similar to lexicographical ordering: we ﬁrst
compare the children on the ﬁrst position, and if equal
then we compare the children on the second position,
etc.

For simplicity of presentation, but without lack of gener-
ality, it is useful to assume that the grammar has a simpler
form, which we call a weak normal form (WNF). Formally,
a grammarG = (N , Σ,R, S) is in weak normal form (WNF)
if G has no useless symbols and G’s rules can only have one
of the following forms:

1. A → A1A2, with A, A1, A2 ∈ N

12962
3
4
5
6
7
8

9
10

11
12

1
2
3
4
5
6

7
8
9
10
11

12
13

14
15

R

2. A → α, with A ∈ N , α ∈ Σ
3. A → A1, with A, A1 ∈ N
4. A → , with A ∈ N

We note that WNF is similar to Chomsky normal form
(CNF), but less restrictive, because WNF allows two ad-
ditional rule forms. Any CFG can easily be converted to to
WNF, similar to the conversion to CNF, and CNF implies
WNF.
Let G = (N , Σ,R, S)
Typed Parse-Tree Ordering.
be an arbitrary CFG in WNF, and assume that there is an
on R that orders the rules in RA, for
ordering relation <
every A ∈ N . The following deﬁnition speciﬁes an ordering
relation on the set M P T (G, A), and from this we will deﬁne
how to rank parse trees.

R

Definition 1. For every non-terminal A ∈ N , we deﬁne
an ordering relation ≺A on M P T (G, A) as follows: ∀X, Y ∈
M P T (G, A) : Y ≺A X iﬀ (cid:17)Y (cid:17) < (cid:17)X(cid:17) or (cid:17)Y (cid:17) = (cid:17)X(cid:17) and
one of the following conditions is true:

X.ρ

1. Y.ρ <
∧ Y [1] ≺A1 X[1]
2. Y.ρ = X.ρ = A → A1
3. Y.ρ = X.ρ = A → A1A2 ∧ Y [1] ≺A1 X[1]
4. Y.ρ = X.ρ = A → A1A2 ∧ Y [1] = X[1] ∧ Y [2] ≺A2

X[2]

Although these equations can lead to recursion (if A is re-
cursive), the deﬁnition is well founded because X and Y are
ﬁnite trees, and their children have fewer nodes.

The following lemma, whose proof we defer to the full ver-
sion, provides assurance that our upcoming ranking scheme
works.

Lemma 2. For every CFG G and non-terminal A in G

the order ≺A is a strict total order on M P T (G, A).

If ρ ∈ RA, the conditions described in Deﬁnition 1 are
mutually exclusive and we count how many Y ’s satisfy each
of them using the following auxiliary values:

1. Nρ(l) = number of parse trees that yield a string of

length l derived from rule ρ.
Nρ(l) = |{Y ∈ M P T (G) : (cid:17)Y (cid:17) = l ∧ Y.ρ = ρ}|

2. NA(l) = number of parse trees that yield a string of

length l derived from A.
NA(l) = |M P T (G, A, l)|

(cid:2)

(cid:2){Y ∈ M P T (G, A, l) : Y.ρ <
R

3. Bρ(l) = number of parse trees that yield a string of
length l derived from a rule of A preceding ρ. This
corresponds to condition 1 in Deﬁnition 1.
ρ}(cid:2)
(cid:2)
Bρ(l) =
4. Bρ(l1, l) = number of parse trees that yield a string of
length l derived from rule ρ : A → A1A2, and whose
ﬁrst child yields a string shorter than l1.
Bρ(l1, l) = |{Y ∈ M P T (G, A, l) : (cid:17)Y [1](cid:17) < l1}|

We use these values to compute rankA(X) based on Equa-
tion (1), by observing that the conditions in Deﬁnition 1
are mutually exclusive. This justiﬁes the correctness of the
ranking function shown in Algorithm 1. Observe that (cid:17)X(cid:17) (cid:18)=
(cid:17)Y (cid:17) =⇒ rankA(X) (cid:18)= rankA(Y ). Therefore the restriction
of rankA to I = M P T (G, S, z) is bijective; its inverse is given
by unrankA( , z) in Algorithm 2.

The correctness of unranking in Algorithm 2 uses the fol-
, X.ρ =
lowing observation: Assume that ρ, ρ
ρ and (cid:17)X(cid:17) = l. Then (1) Bρ(l) ≤ rankA(X) < Bρ(cid:2) (l). Using

(cid:3)∈RA, ρ <

R

ρ

(cid:3)

1 rankA(X ∈ M P T (G, A)) :

ρ ← X.ρ;
l ← (cid:4)X(cid:4);
switch (ρ) do // check all WNF rule forms
return Bρ(l) +rank A1

case A → A1 :
case A → A1A2:
l1 ← (cid:4)X[1](cid:4);
r1 ← rankA1
(X[1]); // rank of 1st
r2 ← rankA2
(X[2]); // rank of 2nd
return Bρ(l) +B ρ(l1, l) +r 1 ∗ NA2

case A →  :
case A → α :

return Bρ(l);
return Bρ(l);

(X[1]);

child
child

(l − l1) +r 2;

Algorithm 1: Ranking of parse trees

unrankA(rank r, length l) :

ρ ← max<R{ρ ∈ RA : Bρ(l) ≤ r};
r(cid:3) ← r − Bρ(l);
switch ρ do // check all WNF rule forms

case A → A1: return Tree(ρ, unrankA(r(cid:3)
case A → A1A2: //child yields?
and 2nd
// if r1, r2 = rank of 1st

, l));

child,
and l1, l2 = length of children yields,
then (1) l1 + l2 = l, and (2)l 1 satisfies
r(cid:3) = Bρ(l1, l) +r 1 ∗ NA2 (l − l1) +r 2

(r1, l1);// 1st

l1 ← max{l1 ∈ [0..l] :B ρ(l1, l) ≤ r(cid:3)};
r(cid:3)(cid:3) ← r(cid:3) − Bρ(l1, l);
/ NA2 (l − l1);
r1 ← r(cid:3)(cid:3)
X[1] ← unrankA1
r2 ← r(cid:3)(cid:3) % NA2 (l − l1);
X[2] ← unrankA2
return Tree(ρ, X[1], X[2]);
case A → : return Tree(ρ);
case A → α: return Tree(ρ);
Algorithm 2: Unranking of parse trees

(r2, l2);// 2nd

child

child

this observation it is easy to see that the value max<R{ρ ∈
RA : Bρ(l) ≤ r} uniquely and correctly identiﬁes the desired
rule of A. The only non-trivial case is when this rule is of
the form A → A1A2. In this case, we must determine l1,
which is the size of the corresponding word derived from A1.
= r−Bρ(l)
Using the formula in the ranking function and r
we get:

(cid:3)

(cid:3)

r

= Bρ(l1, l) +rank A1 (X[1])NA2 (l−l1) +rank A2 (X[2])
< Bρ(l1, l) +rank A1 (X[1])NA2 (l−l1) +N A2 (l−l1)
= Bρ(l1, l) + (rankA1 (X[1]) + 1)NA2 (l−l1)
≤ Bρ(l1, l) +N A1 (l1)NA2 (l−l1)
= Bρ(l1 + 1, l)

This guarantees that the selection of l1 such that Bρ(l1, l) ≤
(cid:3)
< Bρ(l1 + 1, l) succeeds and is correct. The correctness
r
for the rest of the unranking algorithm is straightforward.

Algorithm 3 computes the values Nρ(l), NA(l), Bρ(l) and
Bρ(l1, l), using memoization only for NA and Nρ. All mem-
oization tables entries are initialized to ⊥. Although the
grammar can be recursive, the functions in Algorithm 3 do
not lead to inﬁnite recursions, because of the check and ini-
tialization code placed at lines 2–3 in Nρ and at lines 13–14
in NA. If, during the ﬁrst execution of NA(l), a recursive
call to NA is made using the same value for l, then the sec-
ond call returns 0. This is correct, because NA counts the
number of minimal parse trees rooted in A, and a minimal
parse tree in M P T (G, A, l) should not have another subtree
in M P T (G, A, l).

12971 Nρ(l ∈ N) :

// use table Nρ initialized with ⊥

if ( Nρ[l] (cid:9)= ⊥) then return Nρ[l];
Nρ[l] ← 0;
switch (ρ) do // check all WNF rule forms

case A → A1 :
case A → A1A2: r ← Σl
case A →  :
case A → α :

r ← NA1
r ← (l = 0)?1 : 0;
r ← (l = 1)?1 : 0;

i=0NA1

(l);

(i)NA2

(l − i);

// must be 0, in case of recursion

Nρ[l] ← r;
return Nρ[l];

12 NA(l ∈ N) :

// use table NA initialized with ⊥

if ( NA[l] (cid:9)= ⊥) then return NA[l];
NA[l] ← 0;
NA[l] ← (cid:2)
return NA[l];

Nρ(l);

ρ∈RA

// must be 0, in case of recursion

2
3
4
5

6
7
8

9
10
11

13
14
15

16
17

18 Bρ(l ∈ N) :
return

19

(cid:2)

ρ(cid:2)<Rρ Nρ(cid:2) (l);

20
21 BA→A1A2
22

(l1, l ∈ N) :
i=0 NA1

return Σl1−1

(i)NA2

(l − i);

Algorithm 3: Computing Nρ, NA, Bρ and Bρ. Nρ and NA
use memoization tables Nρ and NA, respectively.

(cid:2)

(cid:2)
l
(cid:2)Σ

Complexity of the algorithms. Assume that (cid:17)X(cid:17) =
l. The ranking and unranking functions visit each of the
O(l|R|) parse-tree nodes of X exactly once. The complexity
of the computation at each node depends on the amount of
memoized information for Nρ(l), NA(l), Bρ(l) and Bρ(l1, l).
(cid:2), we let b(l) = O(l)
Considering numbers as large as
be the space needed to store such a number, a(l) = O(l)
be the time needed to add two such numbers, m(l) be the
time needed to multiply two such numbers, and d(l) be the
time needed to divide two such numbers. We assume a(l) ≤
m(l) ≤ d(l).
At one end, all these values are pre-computed for up to
a maximum value of (cid:17)X(cid:17) = l.
In this case, the tables
2|R|) entries, where each entry holds an integer
have O(l
whose representation may take O(l) bits, therefore the total
3|R|), where |R| is the number of rules
amount of space is O(l
m(l)|R|)
in the grammar. Filling in these tables takes O(l
time, where m(l) is the complexity of multiplying two num-
bers of O(l) bits.
If all tables are available, then ranking spends O(m(l))
time at each node, for a total of O(lm(l)|R|) time. Un-
ranking spends O(d(l)) time at each node, for a total of
O(ld(l)|R|), where d(l) is the complexity of dividing two
numbers of O(l) bits.

3

3.3 Ambiguity

Ambiguity is the key factor that determines the quality
and usefulness of relaxed ranking, and yet it was not fully
explored so far, although it was implicitly used in [14]. Am-
biguity is not speciﬁc to CFG ranking, but it is particularly
relevant, since it is impossible to decide whether a grammar
is ambiguous or not. A related line of work [9, 10, 17] re-
lates ambiguity to complexity of languages. In this section,
however, we deﬁne and analyze ambiguity in the context of
relaxed ranking, and explain why relaxed ranking is useful,
despite potential ineﬃciencies caused by ambiguous repre-
sentations.

The ambiguity-factor of a relaxed ranking using function
UnrankL : ZN → L is deﬁned to be the ratio β = N/|L|. We
note that if the relaxed ranking scheme is obtained with an
intermediate set I, then β = |I|/|L|. For instance, an am-
biguous grammar has multiple parse trees for some strings
and the ambiguity-factor measures how many more trees are
than strings (of a given size).

For most of ranking applications, it is easy to see that
relaxed ranking can be used as a swap-in replacement for
strict ranking, though possibly with slight degradation in
performance. The ambiguity-factor quantiﬁes this degrada-
tion.
For the following three application areas, consider a ﬁnite
language slice L, and a relaxed-ranking scheme RankL : L →
Z|I| based on an intermediate set I.
Compression. Relaxed ranking RankL(x) compresses a
word x ∈ L using γ = (cid:20)log2 |I|(cid:21) bits. Decompression fol-
lows from UnrankL(RankL(x)) = x. Compared to the ideal
case of ranking using α = (cid:20)log2 |L|(cid:21) bits, relaxed ranking has
an overhead of γ − α bits, or approximately (cid:20)log β(cid:21) where
β = |I|/|L| is the ambiguity-factor of the relaxed ranking
scheme. Thus the lower the ambiguity, the better the com-
pression.

Random member generation. To randomly and uni-
formly generate a string in a language L, one can pick a ran-
dom number n ∈ Z|L| and then compute x = unrank(n) us-
ing a strict ranking scheme. When we seek to replace unrank
with relaxed unranking, we need to avoid distribution biases
towards those elements x ∈ L for which x = UnrankL(n)
holds for multiple n ∈ Z|I|. This prevents highly ambiguous
elements from being generated more often than the others.
One can solve this using rejection sampling: repeatedly pick
n ∈ Z|I| until Rank(Unrank(n)) = n. The expected number
of trials is exactly the ambiguity-factor β = |I|/|L|.
Formatted encryption. Replacing ranking with relaxed
ranking does not work directly for formatted encryption.
Therefore, Listing 2 uses the technique of cycle walking. It
easy to see that the expected number of steps in the cycle
walk is upper bounded by the ambiguity-factor β = |I|/|L|,
because |Img(L)| = |L|. The other algorithmic adaptations
for relaxed ranking in [14] are similarly inﬂuenced by the
ambiguity-factor. Observe that there is no need to decide
whether the grammar is ambiguous, the algorithms are pro-
tected against this possibility.
If the grammar is not am-
biguous, then relaxed ranking is in fact strict ranking, and
the algorithmic overhead is minimal. For instance the cycle-
walk in Listing 2 stops after one step.

Our thesis (backed by experimental data) is that for com-
mon applications, the ambiguity-factor is low and relaxed
ranking is a good replacement for strict ranking.
3.4 Type-Based Customization
Recall that our deﬁnition 1 is structural. In other words,
the ordering deﬁnes ≺A based on ≺A1 and ≺A2 , which im-
plicitly have similar (potentially recursive) deﬁnitions. The
result is a family of ordering relations, one for each non-
terminal A. Algorithm 1 provides a template for the cor-
responding ranking functions. However, the soundness of
Deﬁnition 1 requires only that ≺A1 is a correct ordering
for M P T (G, A1), it does not require that ≺A1 be deﬁned
precisely as we do. This means that, if some nontermi-
nal B generates trees that are more eﬃcient to compare

1298and rank using a diﬀerent method, then we can use that
method instead to deﬁne ≺B and respectively to implement
rankB. As long as rankB provides a correct ordering for
M P T (G, B), then the overall ranking of M P T (G, S) is still
correct. Speciﬁcally, if the language derived from B is reg-
ular, then ranking for regular languages can be used. Our
library and code generation tool (presented in Sec. 4) uses
this important optimization to allow ranking of grammars
whose tokens are speciﬁed as regular expressions.
3.5 Beyond CFGs

This section shows how some language features that are
not context-free can be potentially handled by our method-
ology. If a format uses arbitrary speciﬁed delimiters, length
speciﬁers, or checksums, then the language it describes is
not context free. Nevertheless, in many cases we can still
use our approach to obtain a relaxed ranking scheme for the
language, as long as the intermediate set can be ranked, and
the user provides eﬃcient implementations for map and gen.
We say that a language L is cfg-parseable if there is a CFG
G with set I=M P T (G, S), and two functions map : L → I
and gen : I → L such that ∀x ∈ L : gen(map(x)) = x.
Let’s consider an example. Assume that the data consists
of a sequence of items. Each item starts with a counter
which represents the size of the item in bytes, followed by a
sequence of bytes, and it ends with a checksum byte.

<Data> := <Item> | <Item> <Data>
<Item> := <Count> BYTE* CHECKSUM
<Count> := [0-9]+

The data described by this format is not a context free lan-
guage, because of the counter and the checksum. However,
the counter is used only for parsing, and both the counter
and the checksum are completely deﬁned by the data.
If
properly parsed, the content of the data is given by the fol-
lowing context free grammar G

<Data> := <Item> | <Item> <Data>
<Item> := BYTE | BYTE <Item>

Then, as in the case of ranking a CFG, we take the interme-
diate set I = M P T (G, Data). We assume that the user has
a deterministic parser, that maps each element in the initial
language L to a tree in I. The reverse transformation gen
maps trees from I back to L by inserting the appropriate
checksums and the counters.

Note that, for technical reasons, when we build the mem-
oization tables we may want to account for the length of the
counter and checksum. In that case we can use the grammar
<Data> := <CItem> | <CItem> <Data>
<CItem> := CT <Item> CS
<Item> := BYTE | BYTE <Item>

Here CT and CS are just properly sized constant terminals,
which are used as placeholders to be replaced with with the
proper data by the function gen.

There is a large class of languages L that are not CFG,
but which can be eﬃciently parsed to minimal parse-trees
for some CFG G. Parsing discards or replaces those ex-
tra elements in the original language (such as counters or
checksums) that prevent it from being context free, if such
elements can be inferred from the rest of the tree (otherwise
our scheme does not apply). The reverse transformation
takes a minimal parse-tree and provides its yield annotated
with exactly that additional information that is discarded

by parsing (such as counters or checksums) and that can be
inferred from the tree.

4.

IMPLEMENTATION

We implemented a library in C++ and a tool for relaxed
ranking with CFGs. The library oﬀers a low-level interface
to describe any CFG, and it oﬀers relaxed ranking based
on parse trees. To aid developers, our tool supports con-
verting from more user-friendly descriptions of CFGs to the
speciﬁcations handled by the library. It also supports LALR
grammars. See Figure 3 for diagrams depicting the library
and tool.

We note that, up to this point, we have for simplicity
explained our algorithms assuming CFGs are in weak normal
form (WNF), but in fact we have implemented algorithms
that work for any CFG.
4.1 Library for ranking CFG parse-trees

Our library oﬀers an API to build an internal represen-
tation of any grammar G. The API is low-level, meaning
that the user must deﬁne a C++ ranker class, using our
library, where all grammar components are individually en-
coded: terminals, non-terminals, and rules. Objects of the
resulting class can rank/unrank minimal parse-trees of G, as
shown in Figure 3. We used the GNU Multiple Precision
Arithmetic Library (GMP [1]) to represent large integers.

As explained in Section 3.4, there may be parts of the
grammar for which alternative ranking methods work bet-
ter. Our library is object-oriented and oﬀers a default im-
plementation of Algorithm 1, which the user can change as
they see ﬁt.

One such particular case is so important that we intro-
duced an alternative ranking for it. This is the case of lexi-
cal tokens. Consider the grammar GD in Figure 4. GD does
not have a pure context free speciﬁcation because the token
ID is speciﬁed using regexes. The grammar can be turned
to a pure CFG by replacing the deﬁnition of ID with

ID := ’a’|’b’|...|’Z’|’a’ID | ...|’Z’ID

Such a change is undesirable because: (1) people prefer using
regexes for lexical tokens, and (2) ranking a regular language
is usually much faster using a DFA or NFA method [2, 14]
than using the CFG method.

As shown in Figure 3, the user is responsible for producing
a deterministic parser. If the user provides a parser, then
the ranker object (built using our library) performs relaxed
ranking of words in L(G).
4.2 Code Generation Tool

We provide a tool that takes a simple grammar speciﬁ-
cation and produces: (1) source code for a ranker object
based on our API, and (2) a LEX/YACC-based parser for
the grammar (provided that the grammar is LALR). The
design goal of our tool was to specify a grammar in an in-
tuitive way (such as Backus Normal Form, or LEX/YACC
syntax as in Figure 4) and to eﬀortlessly generate from it
library code. Figure 3 gives a diagrammatic description of
this process. The lower box in Figure 4 shows how to spec-
ify the grammar GD discussed previously for input to our
tool. The tool uses a LEX/YACC syntax for its rules. We
made this decision because users are already familiar with
LEX/YACC, and many grammars already have a LEX/Y-

129913005. EXPERIMENTAL RESULTS

5.2 Performance

Our experiments were designed to answer the following

questions about our library and tool:

1. How easy is it to produce the (un)ranking code?
2. How eﬃcient is our method?

To answer the ﬁrst question we report on our experience
using the library and tool. While this is a limited experience,
it oﬀers a high-level comparison of diﬀerent use cases of the
library and the tool.

To answer the second question, consider the rank-and-
encipher method in Section 2. This method is modular,
and its overall performance is determined by: (1) the size of
the intermediate set; (2) the performance of relaxed ranking
and unranking; (3) the number of repetitions of the loop
(i,e, the length of the cycle walk); and (4) the performance
of the intermediate integer encryption step. The quality of
relaxed ranking only inﬂuences the ﬁrst three items, and so
we focus our evaluation on microbenchmarking these items.
Because eﬃcient relaxed ranking requires memoization, we
also report the memoization time and approximate space.

5.1 Our experience using the tool

We evaluated three usage scenarios: (1) the user writes a
new grammar speciﬁcation using the low level API; (2) the
user writes a new grammar speciﬁcation using our tool; and
(3) the user converts an existing LEX/YACC parser to use
our tool.

API – New Speciﬁcation. We used our library’s low-level
API to manually specify the GD grammar from Section 4.1.
We also wrote a parser to be used by the ranker code. In to-
tal, it took about one author one hour; we used LEX/YACC
for the parser. This time quickly increased with grammar
size. Because of this, we did not attempt manual encoding
of much larger grammars.

Tool – New Speciﬁcation. Using our tool, the speciﬁ-
cation for the GD grammar took about 5 minutes and the
code was generated in under 1 second. For a new grammar,
using our tool is simpler than specifying the grammar us-
ing LEX/YACC, because only the rules need to be deﬁned
(without the supporting code and data structures involved
in real parsers).

Tool – Existing Speciﬁcation. We downloaded a spec-
iﬁcation for the C99 language from [5]. We chose C99 be-
cause it is a well known example of a complex CFL. The
YACC rules did not need any modiﬁcation. The LEX rules
contained embedded code, and it took about 10 minutes to
remove it. In the end, we obtained a 350 line speciﬁcation
that contained 68 non-terminals, 63 lexical tokens (deﬁned
using regular expressions), 24 constant tokens and 236 rules.
Our tool turned this 350 line speciﬁcation (no empty lines or
comments) into a speciﬁcation consisting of 2,769 lines of C
code. This took about 18 seconds. The majority of time was
spent checking for lexical token clashes. After the initial run,
it took about 30 minutes to debug the initial failures caused
by the fact that the regular expressions understood by our
library use a 256-byte character set, and string literals and
comments expanded to strings that caused scanning prob-
lems. After those errors were removed, ranking/unranking
and parsing worked correctly.

We evaluate relaxed ranking with CFGs by measuring four
values: (1) memoization time, (2) memoization space, (3)
ranking time, and (4) unranking time. We also report on
grammar ambiguity and language size.
5.2.1 Methodology
We parameterize our results along two dimensions: gram-
mar and language slice size. If L is a language and n ∈ N,
then L’s slice of size n is the set of strings of length n in L,
= {w ∈ L : |w| = n}. Slice size determines (among
i.e., L
other things) the dimension of the memoization tables.

(n)

Grammars. We explore three grammars:

• C99 is the C grammar mentioned in Section 5.1.
• C992 is a grammar with the same syntax as C99, but
C992 has identiﬁer and constant tokens of at most two
characters long.

• GD is the grammar deﬁned in Figure 4, Section4.1 .
To explain why we investigated C992, note that in practice
C programs contain reasonably sized identiﬁers. But this
does not necessarily hold for a random program obtained by
unranking an arbitrary number. For instance, when we in-
spected the output of unranking random value generated for
a 4000-byte slice, we observed identiﬁers as long as 68 char-
acters. We used C992 to provide experience when avoiding
such large tokens.

We used slices between 1000 and 4000 bytes (no comments
or white spaces counted, in the case of C99 and C992), at
100 byte intervals.

(n)

Tests. For each grammar G (with start symbol S) and
slice size n we run the following experiment. We performed
using the intermediate set I =
relaxed-ranking of L(G)
M P T (G, S, n), the set of minimal parse trees T with (cid:17)T(cid:17) =
n.
In the ﬁrst stage, we evaluate the memoization and com-
pute the size of the intermediate set using N = |I| = NS(n);
NS is given by Algorithm 3 (when A = S). When NS(n) is
called for the ﬁrst time, it has the side eﬀect of ﬁlling the
memoization tables. We report |I|, as well as the total time
and space required by memoization. The space is estimated
by reading the data size from the /proc interface, before
and after memoization, because it is hard to measure the
exact space used by the large integers in the GMP library.
In the second stage, we evaluate ranking and unranking,
and we estimate the ambiguity-factor. We repeat ranking
and unranking C = 100 times. For each 1 ≤ k ≤ C we let
r = (N − 1)×(cid:22)k/C(cid:23) and compute w = Unrank(r) which is a
= Rank(w). If r (cid:18)= r
(cid:3)
string in L(G)
then r (cid:18)∈ Img(L(G)
) and we say that r is an outsider, and
w is an ambiguous string. We report the average time for a
single ranking and unranking operation, and the number U
of outsiders.

. Then we compute r

(cid:3)

(n)

(n)

It is easy to see that for a uniform distribution as C grows
towards N , we can approximate the ambiguity-factor β =
N/|L| as β (cid:24) C/(C − U ).
5.2.2 Microbenchmarking Results
We ran all the experiments on a laptop with 32GB RAM
and a i7-2720QM CPU with 4 cores running at 2.2GHz. Our
implementation is single-threaded, so the number of cores
does not aﬀect the results.

13016,000

4,000

2,000

)
s
(

e
m
T

i

3,000

2,000

1,000

)
B
M

(

y
r
o
m
e
M

Rank
Unrank

600

400

200

)
s

m

(

e
m
T

i

0
1,000

1,500

2,000

2,500

3,000

3,500

4,000

0
1,000

1,500

2,000

2,500

3,000

3,500

4,000

0
1,000

1,500

2,000

2,500

3,000

3,500

4,000

)
s
(

e
m
T

i

5,000

4,000

3,000

2,000

1,000

3,000

2,000

1,000

)
B
M

(

y
r
o
m
e
M

Rank
Unrank

1.2

1

0.8

0.6

0.4

0.2

)
s

m

(

e
m
T

i

0
1,000

1,500

2,000

2,500

3,000

3,500

4,000

0
1,000

1,500

2,000

2,500

3,000

3,500

4,000

0
1,000

1,500

2,000

2,500

3,000

3,500

4,000

25

20

15

10

5

)
s
(

e
m
T

i

60

40

20

)
B
M

(

y
r
o
m
e
M

Rank
Unrank

)
s

m

(

e
m
T

i

25

20

15

10

5

0
1,000

1,500

2,000

2,500

3,000

3,500

4,000

0
1,000

1,500

2,000

2,500

3,000

3,500

4,000

0
1,000

1,500

2,000

2,500

3,000

3,500

4,000

Slice size

Slice size

Slice size

Figure 5: Results for the (top row) C99 grammar, (middle row) C992 grammar, and (bottom row) GD grammar
using slices n = 1000, 1100, . . . ,4000. (Left column) The memoization time is the time to execute NS(n) (using NS from
Algorithm 3, when A = S) for the ﬁrst time. (Middle column) The memoization space is estimated using the /proc
interface. (Right column) Ranking and unranking was performed as in Algorithm 1 and Algorithm 2.

The results are shown in Figure 5, where we report on
initialization time (ﬁrst column), memoization space uti-
lization (second), and ranking and unranking times (last
column). Note that the ranking/unranking times are mea-
sured in milliseconds, and these operations are fast. We also
computed the size of the intermediate sets exactly, but we
noticed that they can be approximated as |I| (cid:24)10
1.87×n
for C99, |I| (cid:24)10
for
GD. Thus in all cases the size of the intermediate set I was
exponential in the slice size n.

for C992, and |I| (cid:24)10

1.67×n

0.83×n

5.2.3 Ambiguity
Although C99 is ambiguous, our experiments detected no
outsiders even after we increased the repeat count C from
100 to 10000 trials per slice. The reason is that the density
of ambiguous strings is very low.

To better understand this, consider the smaller, but still
ambiguous grammar GA in Figure 6. The lexical token ID
represents an identiﬁer, and the token NUM represents an in-
teger constant; both have arbitrary length. GA allows a clas-
sic example of ambiguity: the statement if(a)if(b)c;else
d; can be parsed in two ways: if(a){if(b)c;else d;}, or
if(a){if(b)c;}else d;.

StatementList: S | S StatementList
S : AS | IF ’(’E’)’ S | IF ’(’E’)’ S ELSE S | ’{’S’}’
AS : E ’;’ | ID ’=’ E ’;’
E : ID | NUM

Figure 6: An example of an ambiguous grammar GA

We purposely crafted grammar GA to be ambiguous and
to have few kinds of unambiguous statements. Yet, we de-
tected no outsiders for a slice size of 1000 in C = 10, 000
trials. One intuitive explanation is that an ambiguous string
requires an if..if..else construct. The keywords use 2 +
2 + 4 = 8 bytes out of the 1000 bytes slice size. We could
remove the keywords and ﬁll those 8 bytes with identiﬁers or
constants in many ways. Thus for a single ambiguous word
we can get potentially many more unambiguous ones, hence
a low density of ambiguous words. To test this explanation,
we reduced the number of possible lexical tokens, and we
changed the regexes of ID and NUM to match exactly one
value each. This should increase the density of ambiguous
strings. Indeed, in this case we counted U = 9 outsiders in
C = 100 trials, and estimated β (cid:24) C/(C − U ) (cid:24) 1.1. We re-
peated the experiment by enforcing various bounds for the
number of elements that ID and NUM can match. Table 1

1302Experiment Parameters

Limit on # of tokens Trials Measured
|L(ID)|

|L(NUM)|

U
9

1257
111
33
422
20
14
2
0

Inferred

β
1.1
1.14
1.01
1.003
1.004
1.002
1.001

1
1

1
1
2
3
3
4
4
10
15

1
1
2
2
2
2
3
10
10

C
102
104
104
104
105
104
104
105
105

Table 1: Counting the number of outsiders for GA. C is the
number of attempts to ﬁnd an outsider, and U is the number
of outsiders found, i.e. r (cid:18)= Rank(Unrank(r)). The notation
|L(X)| = 2 means that token X may only take one of two
values, say x1 or x2. The ambiguity-factor β is estimated as
β (cid:24) C/(C − U )

shows the results. It backs the hypothesis that, at least in
this case, the more values lexical tokens can take, the lower
it is the percentage of ambiguous strings (which require ﬁxed
keywords).

Discussion. The experiments indicate that our method for
relaxed ranking based on CFGs is both usable and eﬃcient.
Once the memoization is performed, ranking and unrank-
ing are fast, even for complex languages such as C99, and
run in under one second even for slices as large as 4,000
bytes. While theoretically an impediment, in practice am-
biguity is not a a problem for commonly used grammars.
Even when we used a highly ambiguous grammar, we could
not experimentally detect any outsiders unless we artiﬁcially
bounded the number of values that lexical tokens could take
on to be very small numbers. Even in the extreme case
when we allowed only one value for each token, the mea-
sured ambiguity-factor was less than 2. This means, for
instance, that the expected number of cycle walks for FPE
is at most 2 in the scheme from Section 2. Thus encryption
and decryption will be fast.

Acknowledgements
We thank the reviewers for their helpful comments. This
work was supported in part by the US National Science
Foundation (NSF) grants CNS-1228782, CNS-1228620, CNS-
1064944, CNS-1330308, CNS-1065134, CNS-1253870, CNS-
0845610 and CNS-1319061.

6. REFERENCES
[1] The gnu multiple precision arithmetic library.

http://gmplib.org/.

[2] M. Bellare, T. Ristenpart, P. Rogaway, and T. Stegers.

Format-preserving encryption. In Selected Areas in
Cryptography, pages 295–312. Springer-Verlag, 2009.

[3] J. Black and P. Rogaway. Ciphers with arbitrary ﬁnite

domains. In Topics in Cryptology–CT-RSA 2002,
pages 114–130. Springer Berlin Heidelberg, 2002.

[4] M. Brightwell and H. Smith. Using

datatype-preserving encryption to enhance data
warehouse security. In 20th National Information
Systems Security Conference Proceedings (NISSC),
pages 141–149, 1997.

[5] Ansi c99 grammar yacc speciﬁcation. http:

//www.quut.com/c/ANSI-C-grammar-y-1999.html.

[6] R. Dingledine, N. Mathewson, and P. Syverson. Tor:
the second-generation onion router. In Proceedings of
the 13th conference on USENIX Security Symposium -
Volume 13, pages 21–21, Berkeley, CA, USA, 2004.
USENIX Association.

[7] K. P. Dyer, S. E. Coull, T. Ristenpart, and

T. Shrimpton. Protocol misidentiﬁcation made easy
with format-transforming encryption. In Proceedings
of the 20th ACM Conference on Computer and
Communications Secuirty (CCS 2013), November
2013.

[8] A. Goldberg and M. Sipser. Compression and ranking.

In Proceedings of the seventeenth annual ACM
symposium on Theory of computing, STOC ’85, pages
440–448, New York, NY, USA, 1985. ACM.

[9] M. Holzer and M. Kutrib. Descriptional complexity of

(un)ambiguous ﬁnite state machines and pushdown
automata. In Proceedings of the 4th international
conference on Reachability problems, RP’10, pages
1–23, Berlin, Heidelberg, 2010. Springer-Verlag.
[10] O. H. Ibarra and B. Ravikumar. On sparseness,

ambiguity and other decision problems for acceptors
and transducers. In 3rd annual symposium on
theoretical aspects of computer science on STACS 86,
pages 171–179, New York, NY, USA, 1985.
Springer-Verlag New York, Inc.

[11] S. Kannan, Z. Sweedyk, and S. Mahaney. Counting

and random generation of strings in regular languages.
In Proceedings of the sixth annual ACM-SIAM
symposium on Discrete algorithms, SODA ’95, pages
551–557, Philadelphia, PA, USA, 1995. Society for
Industrial and Applied Mathematics.

[12] L. Lee. Fast context-free grammar parsing requires

fast boolean matrix multiplication. J. ACM,
49(1):1–15, Jan. 2002.

[13] J. Levine, T. Mason, and D. Brown. Lex & Yacc, 2Nd

Edition. O’Reilly, second edition, 1992.

[14] D. Luchaup, K. P. Dyer, S. Jha, T. Ristenpart, and

T. Shrimpton. Libfte: A user-friendly toolkit for
constructing practical format-abiding encryption
schemes. In Proceedings of the 14th conference on
USENIX Security Symposium, 2014.

[15] E. M¨akinen. Ranking and unranking left szilard

languages. Technical report, ISO/IEC
JTC1/SC29/WGll/N2467, Atlantic City, 1997.

[16] A. Nijenhuis and H. S. Wilf. Combinatorial

Algorithms. New York : Academic Press, 1975.

[17] B. Ravikumar and O. H. Ibarra. Relating the type of

ambiguity of ﬁnite automata to the succinctness of
their representation. SIAM J. Comput.,
18(6):1263–1282, Dec. 1989.

[18] R. Schroeppel and H. Orman. The hasty pudding

cipher. AES candidate submitted to NIST, page M1,
1998.

[19] M. Sipser. Introduction to the Theory of Computation.

Cengage Learning, 2012.

1303