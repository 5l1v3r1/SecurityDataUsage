PoWerStore: Proofs of Writing for Efﬁcient

and Robust Storage

Dan Dobre1 Ghassan O. Karame1 Wenting Li1 Matthias Majuntke2 Neeraj Suri3 Marko Vukoli´c4

1NEC Laboratories Europe
Heidelberg, 69115, Germany

{dan.dobre, ghassan.karame, wenting.li}@neclab.eu

2Capgemini Deutschland
Berlin, 10785, Germany

matthias.majuntke@capgemini.com

3TU Darmstadt

Darmstadt, 64289, Germany
suri@cs.tu-darmstadt.de

4EURECOM

Biot, 06410, France

vukolic@eurecom.fr

ABSTRACT
Existing Byzantine fault tolerant (BFT) storage solutions
that achieve strong consistency and high availability, are
costly compared to solutions that tolerate simple crashes.
This cost is one of the main obstacles in deploying BFT
storage in practice.

In this paper, we present PoWerStore, a robust and eﬃ-
cient data storage protocol. PoWerStore’s robustness com-
prises tolerating network outages, maximum number of Byzan-
tine storage servers, any number of Byzantine readers and
crash-faulty writers, and guaranteeing high availability (wait-
freedom) and strong consistency (linearizability) of read-
/write operations. PoWerStore’s eﬃciency stems from com-
bining lightweight cryptography, erasure coding and meta-
data write-backs, where readers write-back only metadata
to achieve strong consistency. Central to PoWerStore is the
concept of “Proofs of Writing” (PoW), a novel data storage
technique inspired by commitment schemes. PoW rely on
a 2-round write procedure, in which the ﬁrst round writes
the actual data and the second round only serves to “prove”
the occurrence of the ﬁrst round. PoW enable eﬃcient im-
plementations of strongly consistent BFT storage through
metadata write-backs and low latency reads.

We implemented PoWerStore and show its improved per-
formance when compared to existing robust storage proto-
cols, including protocols that tolerate only crash faults.

Categories and Subject Descriptors
D.4.5 [Operating Systems]: Reliability – Fault tolerance.

Keywords
Byzantine-Fault Tolerance; Secure Distributed Storage; Strong
Consistency.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright 2013 ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516750.

1.

INTRODUCTION

Byzantine fault-tolerant (BFT) distributed protocols have
attracted considerable research attention, due to their ap-
pealing promise of masking various system issues ranging
from simple crashes, through software bugs and misconﬁg-
urations, all the way to intrusions and malware. However,
the use of existing BFT protocols is questionable in practice
due to, e.g., weak guarantees under failures [41] or high cost
in performance and deployment compared to crash-tolerant
protocols [27]. This can help us derive the following require-
ments for the design of future BFT protocols:

• A BFT protocol should be robust, i.e., it should tol-
erate Byzantine faults and asynchrony (modeling net-
work outages) while maintaining correctness (data con-
sistency) and providing sustainable progress (availabil-
ity) even under worst-case conditions that still meet
the protocol assumptions. This requirement has often
been neglected in BFT protocols that focus primarily
on common, failure-free operation modes (e.g., [22]).
• Such a robust protocol should be eﬃcient. For exam-
ple, virtually all BFT protocols resort to data write-
backs, in which a reader must ensure that a value it is
about to return is propagated to a suﬃcient number
of servers before a read completes—eﬀectively, this en-
tails repeating a write after a read [5] and results in
performance deterioration. We believe that the eﬃ-
ciency of a robust BFT protocol is best compared to
the eﬃciency of its crash-tolerant counterpart.
Ide-
ally, a robust protocol should not incur signiﬁcant per-
formance and resource cost penalty with respect to a
crash-tolerant implementation, hence making the re-
placement of a crash-tolerant protocol a viable option.

In this paper, we present PoWerStore, a robust and eﬃ-
cient asynchronous BFT distributed read/write storage pro-
tocol. The notion of robustness subsumes [5]: (i) high avail-
ability, or wait-freedom [23], where read/write operations
invoked by correct clients always complete, and (ii) strong
consistency, or linearizability [24] of read/write operations.
At the heart of PoWerStore is a novel data storage tech-
nique we call Proofs of Writing (PoW). PoW are inspired
by commitment schemes; PoW incorporate a 2-round write
procedure, where the second round of write eﬀectively serves
to “prove” that the ﬁrst round has actually been completed

285before it is exposed to a reader. The second write round in
PoW is lightweight and writes only metadata; however, it is
powerful enough to spare readers of writing back the data
allowing them to only write metadata to achieve strong con-
sistency.1 We construct PoW using cryptographic hash func-
tions and eﬃcient message authentication codes (MACs); in
addition, we also propose an instantiation of PoW based on
polynomial evaluation.

PoWerStore’s eﬃciency is reﬂected in (i) metadata write-
backs where readers write-back only metadata, avoiding ex-
pensive data write-backs, (ii) use of lightweight cryptographic
primitives (such as hashes and MACs), (iii) optimal re-
silience, i.e., ensuring correctness despite the largest possible
number t of Byzantine server failures; this mandates using
3t + 1 servers [35]. Moreover, PoWerStore achieves these
desirable properties with optimal latency: namely, we show
that no robust protocol, including crash-tolerant ones, that
uses a bounded number of servers and avoids data write-
backs can achieve better latency than PoWerStore. More
speciﬁcally, in the single writer (SW) variant of PoWerStore,
this latency is two rounds of communication between a client
and servers for both reads and writes.

In addition, PoWerStore employs erasure coding at the
client side to oﬄoad the storage servers and to reduce net-
work traﬃc. Furthermore, PoWerStore tolerates an un-
bounded number of Byzantine readers and unbounded num-
ber of writers that can crash.

Finally, while our SW variant of PoWerStore demonstrates
the eﬃciency of PoW, for practical applications we propose
a multi-writer (MW) variant of PoWerStore (referred to
as M-PoWerStore). M-PoWerStore features 3-round writes
and reads, where the third read round is invoked only un-
der active attacks. M-PoWerStore also resists attacks spe-
ciﬁc to multi-writer setting that exhaust the timestamp do-
main [6]. We evaluate M-PoWerStore and demonstrate its
superiority even with respect to existing crash-tolerant ro-
bust atomic storage implementations. Our results show that
in typical settings, the peak throughput achieved by M-
PoWerStore improves over existing robust crash-tolerant [5]
and Byzantine-tolerant [34] atomic storage implementations,
by 50% and 100%, respectively.

The remainder of the paper is organized as follows.

In
Section 2, we outline our system and threat model. Sec-
tion 3 outlines the main intuition behind Proofs of Writing.
In Section 4, we introduce PoWerStore and we analyze its
correctness. In Section 5, we present the multi-writer variant
of PoWerStore, M-PoWerStore. In Section 6, we evaluate an
implementation of M-PoWerStore. In Section 7, we overview
related work and we conclude the paper in Section 8.

2. MODEL

We consider a distributed system that consists of three
disjoint sets of processes: a set servers of size S = 3t + 1,
where t is the failure threshold parameter, containing pro-
cesses {s1, ..., sS}; a set writers w1, w2, ... and a set read-
ers r1, r2, etc. The set clients is the union of writers and
readers. We assume the data-centric model [12, 40] with bi-
directional point-to-point channels between each client and
each server. Servers do not communicate among each other,

1As proved in [17],
“write”, i.e., modify the state of storage servers.

in any robust storage readers must

nor send messages other than in reply to clients’ messages.
In fact, servers do not even need to be aware of each other.
2.1 Threat Model

We model processes as probabilistic I/O automata [43]
where a distributed algorithm is a set of such processes. Pro-
cesses that follow the algorithm are called correct. Similar
to [3, 4, 16, 34], we assume that any number of honest writ-
ers that may fail by crashing while allowing any number of
readers and up to t servers to exhibit Byzantine [29] (or
arbitrary [25]) faults.

We assume a strong adversary that can coordinate the
Byzantine servers and readers to compromise the system.
We further assume that the adversary controls the network
and as such controls the scheduling of all transmitted mes-
sages in the network, resulting in asynchronous communica-
tion. However, we assume that the adversary cannot prevent
the eventual delivery of messages between correct processes.
We opt not to focus on Byzantine writers because they can
always obliterate the storage by constantly overwriting data
with garbage, even in spite of (expensive) techniques that
ensure that each individual write leaves a consistent state
(e.g., [10,19,22,31]. As a consequence, with Byzantine writ-
ers and asynchronous message schedule, the adversary can
make a correct reader return arbitrary data at will. There-
fore, we assume appropriate access control mechanisms to
prevent untrusted writers from having appropriate creden-
tials to modify data.

Finally, we assume that the adversary is computationally
bounded and cannot break cryptographic hash functions or
forge message authentication codes. In this context, we as-
sume the existence of a cryptographic (i.e., one way and
collision resistant) hash function H(·), and a secure message
authentication function M ACk(·), where k is a λ-bit sym-
metric key. We further assume that each server si pre-shares
one symmetric group key with each writer in W ; in the fol-
lowing, we denote this key by ki.2 Note that, in this paper,
we do not address the conﬁdentiality of the outsourced data.
2.2 Atomic Storage

We focus on a read/write storage abstraction [28] which
exports two operations: write(v), which stores value v and
read(), which returns the stored value. We assume that the
initial value of a storage is a special value ⊥, which is not a
valid input value for a write operation. While every client
may invoke the read operation, we assume that writes are
invoked only by writers.

We say that an operation (invocation) op is complete if the
client receives the response, i.e., if the client returns from
the invocation.
In any execution of an algorithm, we say
that a complete operation op2 follows op1 if the invocation
of op2 follows the response of op1 in that execution. We
further assume that each correct client invokes at most one
operation at a time (i.e., does not invoke the next operation
until it receives the response for the current operation).

We focus on robust storage with the strongest storage
progress consistency and availability semantics, namely lin-
earizability [24] (or atomicity [28]) and wait-freedom [23].

2Sharing group keys is not a requirement for the main func-
tionality of the single-writer nor the multi-writer versions of
PoWerStore. As we show in Section 5, this requirement is
only needed to prevent a speciﬁc type of DoS attack where
malicious readers exhaust the timestamp space.

286Construction costs

Veriﬁcation costs

Hash-based PoW

1 hash

Polynomial-based PoW O(|t3|) modular exponentiations O(|t2|) modular exponentiations
O(|M|) modular exponentiations O(|M|) modular exponentiations

RSA signatures

1 hash

Table 1: Construction and veriﬁcation costs of our PoW instantiations. Here, t is the failure threshold, M (cid:29) t
is the modulus used in RSA signatures.

(a) Complete Write

(b) Incomplete Write

Figure 1: Complete vs.
S = 4).

Incomplete writes (t = 1,

Wait-freedom states that if a correct client invokes an oper-
ation op, then op eventually completes. Linearizability pro-
vides the illusion that a complete operation op is executed
instantly at some point in time between its invocation and
response, whereas the operations invoked by faulty clients
appear either as complete or not invoked at all.

Finally, we measure the time-complexity of an atomic stor-
age implementation in terms of number of communication
round-trips (or simply rounds) between a client and servers.
Intuitively, a round consists of a client sending the message
to (a subset of) servers and receiving replies. A formal def-
inition can be found in, e.g., [18].

3. PROOFS OF WRITING

In this section, we give the main intuition behind Proofs
of Writing (PoW) and describe two possible instantiations
of PoW: (i) a hash-based variant of PoW that oﬀers compu-
tation security, and (ii) a polynomial evaluation-based PoW
variant that provides information theoretic guarantees.
3.1 Intuition behind PoW

A distributed storage that tolerates failures and asyn-
chrony must prevent clients from blocking while waiting
for t possibly faulty servers to reply. As depicted in Fig-
ure 1(a), this implies that operations by clients must return
after probing a quorum of S − t servers. Intuitively, by look-
ing at a strict subset of the servers, a reader cannot obtain
a global view of the system state and in particular, diﬀeren-
tiate a complete write from an incomplete write.

For example, in Figure 1, reader Alice cannot tell apart
a complete write (Fig. 1(a)) where one Byzantine server
deletes the data, from an incomplete write without Byzan-
tine servers (Fig. 1(b)). To ensure strong consistency, i.e.,
that a subsequent read by Bob does not observe stale data
(Fig. 1(b)), Alice must ensure that the data she is about
to read is propagated to a suﬃcient number of servers be-
fore her read completes. Essentially, Alice must complete
the write herself by performing a data write-back, involv-
ing a full write of the data she reads [5] in both executions.
However, if she somehow knew that the write in Figure 1(a)

was in fact complete, Alice could safely skip data writeback,
since Bob would anyway observe recent data.

In the context of BFT storage, data write-backs by readers
are undesirable for two reasons: (i) the overhead of crypto-
graphic schemes related to preventing malicious readers from
exploiting such write-backs to jeopardize the storage by over-
writing data with garbage, and (ii) the inherent bandwidth
and latency cost associated with writing-back data. In addi-
tion, when combined with erasure coded storage, data write-
backs are computationally expensive since readers may need
to erasure code data.

Essentially, since the data write-back technique is driven
by readers’ uncertainty in diﬀerentiating between a complete
write and an incomplete write, we aim at an eﬃcient tech-
nique that would allow readers to tell incomplete and com-
plete writes apart. Such a technique would allow readers
to safely discard incomplete writes altogether, obviating the
need for writing-back data. Currently, the most widespread
technique to achieve this diﬀerentiation would be to have
writers send an authenticated acknowledgement (e.g., by us-
ing digital signatures) upon completion of the write.

At the heart of PoWerStore is a novel storage technique we
call Proofs of Writing (PoW) which enables to achieve this
diﬀerentiation more eﬃciently. PoW are inspired by com-
mitment schemes [21]; similar to commitment schemes, PoW
consist of two rounds: (i) in the ﬁrst round, the writer com-
mits to a random value of its choice, whereas (ii) in the sec-
ond round, the writer “opens” his commitment. Unlike com-
mitment schemes, PoW are constructed by honest writers
and stored along with the data in a set of servers, enabling
them to collectively convince a reader that the requested
data has been stored in suﬃciently many correct servers.
Furthermore, we show that PoW can be constructed with-
out relying on any public-key infrastructure while incurring
little cost when compared to digital signatures (Table 1).

PoW obviate the need for writing-back data, allowing
readers to write-back metadata. Metadata write-backs (i)
help to prevent malicious readers from compromising the
storage, and (ii) feature low communication latency and
bandwidth usage even in worst-case conditions. By doing
so, PoW provide an eﬃcient alternative to digital signatures
in BFT storage protocols. In what follows, we describe two
eﬃcient instantiations of PoW using cryptographic hashes,
and polynomial evaluation; we also outline their relative per-
formance gains when compared to digital signatures.

3.2 PoW based on Cryptographic Hashes

We start by outlining a PoW implementation that is based
on the use of one-way collision-resistant functions seeded
with pseudo-random input.

In the ﬁrst write round, the writer generates a pseudo-
random nonce and writes the hash of the nonce together
with the data in a quorum of servers. In the second write
round, the writer discloses the nonce and stores it in a quo-

BobAliceBobAlice?287rum. During the ﬁrst round of a read operation, the client
collects the nonce from a quorum and sends (writes-back)
the nonce to a quorum of servers in the second round. The
server veriﬁes the nonce by checking that the received nonce
matches the hash of the stored nonce. If so, the server con-
ﬁrms the validity of the nonce by exposing the corresponding
stored data to the client. The client obtains a PoW after re-
ceiving t + 1 conﬁrmations pertaining to a nonce.

Since the writer keeps the nonce secret until the start of
the second round, it is computationally infeasible for the
adversary to fabricate the nonce unless the ﬁrst round of
write has completed, and hence the data is written. Thus,
if the nonce received in the ﬁrst read round hashes to the
stored hash at t + 1 servers (one of which is necessarily cor-
rect), then this provides suﬃcient proof that the nonce has
been disclosed by the writer, which implies that the data
has been written.
3.3 PoW based on Polynomial Evaluation

In what follows, we propose an alternative construction
of PoW based on polynomial evaluation. Here, at the start
of every write operation, the writer constructs a polyno-
mial P (·) of degree t with coeﬃcients {αt, . . . , α0} chosen at
random from Zq, where q is a public parameter. That is,

P (x) =(cid:80)j=t

j=0 αjxj.

The writer then constructs the PoW as follows: for each
server si, the writer picks a random point xi on P (·), and
constructs the share (xi, Pi), where Pi = P (xi). As such,
the writer constructs S diﬀerent shares, one for each server,
and sends them to each server si over a conﬁdential channel.
Note that since there are at most t Byzantine servers, these
servers cannot reconstruct the polynomial P (·) from their
In the second write round,
shares, even if they collude.
the writer reveals the polynomial P (·) to all servers. This
enables a correct server si to establish that the ﬁrst write
round has been completed by checking that the share (xi, Pi)
is indeed a point of P (·).

The argument of PoW relies on the assumption that the
correct servers holding shares agree on the validity of the
polynomial. Therefore, it is crucial to ensure that even after
the disclosure P (·), the adversary cannot fabricate a poly-
ˆP (·) (cid:54)= P (·) that intersects with P (·) in the share
nomial
of a correct server. By relying on randomly chosen xi, and
the fact that correct servers never divulge their share, our
construction prevents an adversary from fabricating ˆP (·).
Note that there exist other variant implementations for
PoW that do not leak the polynomial to the servers [26].
We point that, unlike our prior solution based on cryp-
tographic hash functions, this PoW construction provides
information-theoretic guarantees [4, 30]. Table 1 illustrates
the PoW construction and veriﬁcation costs incurred in our
PoW constructs. Owing to its reduced costs, we focus in
this paper on hash-based PoWs (Sec. 3.2).
4. PoWerStore

In this section, we provide a detailed description of the
PoWerStore protocol and we analyze its correctness. In Ap-
pendix A, we show that PoWerStore exhibits optimal worst-
case latency.
4.1 Overview of PoWerStore

In PoWerStore, the WRITE operation performs in two
consecutive rounds, called store and complete. Likewise,

Algorithm 1 Algorithm of the writer in PoWerStore.

ts : structure num, initially ts0 (cid:44) 0

1: Deﬁnitions:
2:
3: operation write(V )
ts ← ts + 1
4:
5: N ← {0, 1}λ
6: N ← H(N )
7:
8:
9:

store(ts, N , V )
complete(ts, N )
return ok

10: procedure store(ts, V, N )
{f r1, . . . , f rS} ← encode(V, t + 1, S)
11:
cc ← [H(f r1), . . . , H(f rS )]
12:
for 1 ≤ i ≤ S do send store(cid:104)ts, f ri, cc, N(cid:105) to si
13:
14: wait for store ack(cid:104)ts(cid:105) from S − t servers
15: procedure complete(ts, N )
16:
17: wait for complete ack(cid:104)ts(cid:105) from S − t servers

send complete(cid:104)ts, N(cid:105) to all servers

the read performs in two rounds, called collect and fil-
ter. For the sake of convenience, each round rnd ∈ {store,
complete, collect, filter} is wrapped by a procedure
rnd. In each round rnd, the client sends a message of type
rnd to all servers. A round completes at the latest when
the client receives messages of type rnd ack from S − t
correct servers. The server maintains a variable lc to store
the metadata of the last completed write, consisting of a
timestamp-nonce pair, and a variable LC that stores a set of
such tuples written-back by clients. In addition, the server
keeps a variable Hist storing the history, i.e., a log consist-
ing of the data written by the writer3 in the store round,
indexed by timestamp.
4.2 Write Implementation

The write implementation is given in Algorithm 1. To
write a value V , the writer increases its timestamp ts, gen-
erates a nonce N and computes its hash N = H(N ), and
invokes store with ts, V and N . When the store proce-
dure returns, the writer invokes complete with ts and N .
After complete returns, the write completes.
In store, the writer encodes V into S fragments f ri
(1 ≤ i ≤ S), such that V can be recovered from any sub-
set of t + 1 fragments. Furthermore, the writer computes
a cross-checksum cc consisting of the hashes of each frag-
ment. For each server si (1 ≤ i ≤ S), the writer sends a
store(cid:104)ts, f ri, cc, N(cid:105) message to si. On reception of such a
message, the server writes (f ri, cc, N ) into the history entry
Hist[ts] and replies to the writer. After the writer receives
S − t replies from diﬀerent servers, the store procedure
returns, and the writer proceeds to complete.
In complete, the writer sends a complete(cid:104)ts, N(cid:105) mes-
sage to all servers. Upon reception of such a message, the
server changes the value of lc to (ts, N ) if ts > lc.ts and
replies to the writer. After the writer receives replies from
S − t diﬀerent servers, the complete procedure returns.
4.3 Read Implementation

The read implementation is given in Algorithm 3; it con-
sists of the collect procedure followed by the filter pro-
cedure. In collect, the client reads the tuples (ts, N ) in-

3Recall that PoWerStore is a single-writer storage protocol.

288Algorithm 2 Algorithm of server si in PoWerStore.

lc : structure (ts, N ), initially c0 (cid:44) (ts0, null)
LC : set of structure (ts, N ), initially ∅

18: Deﬁnitions:
19:
20:
21: Hist[. . . ] : vector of (f r, cc, N ) indexed by ts, with all entries initialized to (null, null, null)
22: upon receiving store(cid:104)ts, f r, cc, N(cid:105) from the writer
23: Hist[ts] ← (f r, cc, N )
24:
25: upon receiving complete(cid:104)ts, N(cid:105) from the writer
26:
27:

if ts > lc.ts then lc ← (ts, N )
send complete ack(cid:104)ts(cid:105) to the writer

send store ack(cid:104)ts(cid:105) to the writer

28: procedure gc()
29:
30:
31:

chv ← max({c ∈ LC : valid(c)} ∪ {c0})
if chv.ts > lc.ts then lc ← chv
LC ← LC \ {c ∈ LC : c.ts ≤ lc.ts}

cluded in the set LC ∪ {lc} at the server, and accumulates
these tuples in a set C together with the tuples read from
other servers. We call such a tuple a candidate and C a
candidate set. Before responding to the client, the server re-
moves obsolete tuples from LC using the gc procedure. Af-
ter the client receives candidates from S−t diﬀerent servers,
collect returns.

In filter, the client submits C to each server. Upon
reception of C, the server performs a write-back of the can-
didates in C (metadata write-back ). In addition, the server
picks chv as the candidate in C with the highest timestamp
such that valid(chv) holds, or c0 if no such candidate ex-
ists. The predicate valid(c) holds if the server, based on
the history, is able to verify the integrity of c by check-
ing that H(c.N ) equals Hist[c.ts].N . The server then re-
sponds to the client with a message including the timestamp
chv.ts, the fragment Hist[chv.ts].f r and the cross-checksum
Hist[chv.ts].cc. The client waits until S − t diﬀerent servers
responded and either (i) safe(c) holds for the candidate with
the highest timestamp in C, or (ii) all candidates have been
excluded from C, after which collect returns. The pred-
icate safe(c) holds if at least t + 1 diﬀerent servers si have
responded with timestamp c.ts, fragment f ri and cross-
checksum cc such that H(f ri) = cc[i]. If C (cid:54)= ∅, the client
selects the candidate with the highest timestamp c ∈ C and
restores value V by decoding V from the t + 1 correct frag-
ments received for c. Otherwise, the client sets V to the
initial value ⊥. Finally, the read returns V .
4.4 Analysis

In what follows, we show that PoWerStore is robust, guar-
anteeing that read/write operations are linearizable and
wait-free. We start by proving a number of core lemmas, to
which we will refer to throughout the analysis.

Definition 4.1

(Valid candidate). A candidate c is

valid if valid(c) holds at some correct server.

Lemma 4.2

(Proofs of Writing). If c is a valid can-
didate, then there exists a set Q of t + 1 correct servers such
that for each server si ∈ Q, H(c.N ) = Histi[c.ts].N .

Proof:
If c is valid, then by Deﬁnition 4.1, H(c.N ) =
Histj[c.ts].N holds at some correct server sj. By the pre-
image resistance of H, no computationally bounded adver-
sary can acquire c.N from the sole knowledge of H(c.N ).

//last completed write
//set of written-back candidates

gc()
send collect ack(cid:104)tsr, LC ∪ {lc}(cid:105) to client r

32: upon receiving collect(cid:104)tsr(cid:105) from client r
33:
34:
35: upon receiving filter(cid:104)tsr, C(cid:105) from client r
36:
37:
38:
39:

LC ← LC ∪ C
chv ← max({c ∈ C : valid(c)} ∪ {c0})
(f r, cc) ← πf r,cc(Hist[chv.ts])
send filter ack(cid:104)tsr, chv.ts, f r, cc(cid:105) to client r

//write-back

40: Predicates:
41:

valid(c) (cid:44) (H(c.N ) = Hist[c.ts].N )

Hence, c.N stems from the writer in a write operation wr
with timestamp c.ts. By Algorithm 1, line 8, the value of
c.N is revealed only after the completion of the store round
in wr. Hence, by the time c.N is revealed, there is a set Q
of t + 1 correct servers such that each server si ∈ Q assigned
Histi[c.ts].N to H(c.N ).

Lemma 4.3

(No exclusion). Let c be a valid candi-
date and let rd be a read by a correct reader that includes
c in C during collect. Then c is never excluded from C.

Proof: As c is valid, by Lemma 4.2 there is a set Q of t + 1
correct servers such that each server si ∈ Q, H(c.N ) =
Histi[c.ts]. Hence, valid(c) is true at every server in Q.
Thus, no server in Q replies with a timestamp ts < c.ts
in line 39. Therefore, at most S − (t + 1) = 2t timestamps
received by the reader in the filter round are lower than
c.ts, and so c is never excluded from C.

We now explain why linearizability is satisﬁed by arguing
that if a read follows after a write(V ) (resp. after a read
that returns V ) then the read does not return a value older
than V .

Read/Write Linearizability.

Suppose a read rd by a correct client follows after a com-
pleted write(V ). If ts is the timestamp of write(V ), we
argue that rd does not select a candidate with a timestamp
lower than ts. Since a correct server never changes lc to
a candidate with a lower timestamp, after write(V ) com-
pleted, t+1 correct servers hold a valid candidate with times-
tamp ts or greater in lc. During collect in rd, the client
awaits to receive the value of lc from S − t diﬀerent servers.
Hence, a valid candidate c with timestamp ts or greater is
received from at least one of these t + 1 correct servers and
included in C. By Lemma 4.3 (no exclusion), c is never
excluded from C and by Algorithm 3, rd does not select a
candidate with timestamp lower than c.ts ≥ ts.

Read Linearizability.
Suppose a read rd(cid:48) by a correct client follows after rd.
If c is the candidate selected in rd, we argue that rd(cid:48) does
not select a candidate with a timestamp lower than c.ts. By
the time rd completes, t + 1 correct servers hold c in LC.
According to Algorithm 2, if a correct server removes c from

289Algorithm 3 Algorithm of client r in PoWerStore.

tsr: num, initially 0
Q, R: set of pid, initially ∅
C: set of (ts, N ), initially ∅

42: Deﬁnitions:
43:
44:
45:
46: W [1 . . . S]: vector of (ts, f r, cc), initially []
47: operation read()
48:
49:
50:
51:
52:
53:
54:
55:
56:

C, Q, R ← ∅
tsr ← tsr + 1
C ← collect(tsr)
C ← filter(tsr, C)
if C (cid:54)= ∅ then

c ← c(cid:48) ∈ C : highcand(c(cid:48)) ∧ safe(c(cid:48))
V ← restore(c.ts)

else V ← ⊥
return V

send collect(cid:104)tsr(cid:105) to all servers

57: procedure collect(tsr)
58:
59: wait until |Q| ≥ S − t
60:
61: upon receiving collect ack(cid:104)tsr, Ci(cid:105) from server si
62:
63:

Q ← Q ∪ {i}
C ← C ∪ {c ∈ Ci : c.ts > ts0}

return C

64: procedure filter(tsr, C)
65:
66: wait until |R| ≥ S − t ∧

send filter(cid:104)tsr, C(cid:105) to all servers
((∃c ∈ C : highcand(c) ∧ safe(c)) ∨ C = ∅)
return C

67:
68: upon receiving filter ack(cid:104)tsr, ts, f r, cc(cid:105) from server si
69:
70:

R ← R ∪ {i}; W [i] ← (ts, f r, cc)
C ← C \ {c ∈ C : invalid(c)}

cc ← cc(cid:48) s.t. ∃R(cid:48) ⊆ R : |R(cid:48)| ≥ t + 1 (cid:86)

71: procedure restore(ts)
72:

(∀i ∈ R(cid:48) : W [i].ts = ts ∧ W [i].cc = cc(cid:48))
F ← {W [i].f r : i∈R ∧ W [i].ts=ts ∧ H(W [i].f r)=cc[i]}
V ← decode(F, t + 1, S)
return V

76: Predicates:
77:

safe(c) (cid:44) ∃R(cid:48) ⊆ R : |R(cid:48)| ≥ t + 1 (cid:86)
(∀i ∈ R(cid:48) : W [i].ts = c.ts) (cid:86)

73:
74:
75:

78:

79:

(∀i, j ∈ R(cid:48):W [i].cc=W [j].cc ∧ H(W [i].f r)=W [j].cc[i])
highcand(c) (cid:44) (c.ts = max({c(cid:48).ts : c(cid:48) ∈ C}))
invalid(c) (cid:44) |{i ∈ R : W [i].ts < c.ts}| ≥ S − t

LC, then the server changed lc to a valid candidate with
timestamp c.ts or greater. As such, t+1 correct servers hold
in LC∪{lc} a valid candidate with timestamp c.ts or greater.
During collect in rd(cid:48), the client awaits to receive the value
of LC ∪{lc} from S− t diﬀerent servers. As such, it includes
a valid candidate c(cid:48) with timestamp c.ts or greater from at
least one correct server. By Lemma 4.3 (no exclusion), c(cid:48)
is never excluded from C and by Algorithm 3, rd(cid:48) does not
select a candidate with timestamp lower than c(cid:48).ts ≥ c.ts.
In the following, we argue that read does not block in
filter (line 66) while waiting for the candidate c with the
highest timestamp in C to become safe(c) or C to become
empty, which is the most sophisticated waiting condition.

Wait-freedom.

Suppose by contradiction that rd blocks during filter af-
ter receiving filter ack messages from all correct servers.
We distinguish two cases: (Case 1) C contains a valid can-
didate and (Case 2) C contains no valid candidate.

• Case 1: Let c be the valid candidate with the highest
timestamp in C. As c is valid, by Lemma 4.2, at least
t + 1 correct servers hold history entries matching c.
Since no valid candidate in C has a higher timestamp
than c.ts, these t + 1 correct servers responded with
timestamp c.ts, corresponding erasure coded fragment
f ri and cross-checksum cc such that H(f ri) = cc[i].
Therefore, c is safe(c). Furthermore, all correct servers
(at least S−t) responded with timestamps at most c.ts.
Hence, every candidate c(cid:48) ∈ C such that c(cid:48).ts > c.ts
becomes invalid(c(cid:48)) and is excluded from C. As such,
c is also highcand(c) and rd does not block.

• Case 2: Since no candidate in C is valid, all correct
servers (at least S − t) responded with timestamp ts0,
which is lower than any candidate timestamp. As such,
every candidate c ∈ C becomes invalid(c) is excluded
from C. Therefore, rd does not block.

5. M-PoWerStore

In what follows, we present the multi-writer variant of
our protocol, dubbed M-PoWerStore. M-PoWerStore re-
sists attacks speciﬁc to multi-writer setting that exhaust
the timestamp domain [6]. Besides its support for multiple
writers, M-PoWerStore protects against denial of service at-
tacks speciﬁc to PoWerStore, in which the adversary swamps
the system with bogus candidates. While this attack can
be contained in PoWerStore by a robust implementation of
the point-to-point channel assumption using, e.g., a separate
pair of network cards for each channel (in the vein of [13]),
this may impact practicality.
5.1 Overview

M-PoWerStore supports an unbounded number of clients.
In addition, M-PoWerStore features optimal read latency of
two rounds in the common case, where no process is Byzan-
tine. Under malicious attacks, M-PoWerStore gracefully de-
grades to guarantee reading in at most three rounds. The
write has a latency of three rounds, featuring non-skipping
timestamps [6], which prevents the adversary from exhaust-
ing the timestamp domain.

The main diﬀerence between M-PoWerStore and PoWer-
Store is that, here, servers store and transmit a single can-
didate instead of a (possibly unbounded) set. To this end,
it is crucial that servers are able to determine the validity
of a written-back candidate without consulting the history.
For this purpose, we enhance our original PoW scheme by
extending the candidate with message authentication codes
(MACs) on the timestamp and the nonce’s hash, one for
each server, using the corresponding group key. As such, a
valid MAC proves to a server that the written-back candi-
date stems from a writer, and thus, constitutes a PoW that
a server can obtain even without the corresponding history
entry. Note that in case of a candidate incorporating cor-
rupted MACs, servers might disagree about the validity of
a written-back candidate. Hence, a correct client might not
be able to write-back a candidate to t + 1 correct servers
as needed. To solve this issue, M-PoWerStore ”pre-writes”
the MACs, enabling clients to recover corrupted candidates
from the pre-written MACs.

To support multiple-writers, write in M-PoWerStore com-
prises an additional distributed synchronization round, called

290clock, which is prepended to store. The read performs
an additional round called repair, which is appended to
collect. The purpose of repair is to recover broken can-
didates prior to writing them back, and is invoked only un-
der attack by a malicious adversary that actually corrupts
candidates.

Similarly to PoWerStore, the server maintains the variable
Hist to store the history of the data written by the writer
in the store round, indexed by timestamp. In addition, the
server keeps the variable lc to store the metadata of the last
completed write consisting of the timestamp, the nonce and
a vector of MACs (with one entry per server) authenticating
the timestamp and the nonce’s hash.

The full write implementation is given in Algorithm 4.
The implementation of the server and the read operation
are given in Algorithm 5 and 6. In the following, we simply
highlight the diﬀerences to PoWerStore.
5.2 Write Implementation

As outlined before, M-PoWerStore is resilient to the ad-
versary skipping timestamps. This is achieved by having
the writer authenticate the timestamp of a write with a
key kW shared among the writers. Note that such a shared
key can be obtained by combining the diﬀerent group keys;
for instance, kW ← H(k1||k2|| . . . ).

To obtain a timestamp, in the clock procedure, the writer
retrieves the timestamp (held in variable lc) from a quorum
of S − t servers and picks the highest timestamp ts with a
valid MAC. Then, the writer increases ts and computes a
MAC for ts using kW . Finally, clock returns ts.

To write a value V , the writer, (i) obtains a timestamp ts
from the clock procedure, (ii) authenticates ts and the
nonce’s hash N by a vector of MACs vec, with one en-
try for each server si using group key ki, and (iii) stores
vec both in store and complete. Upon reception of a
store(cid:104)f ri, cc, N , vec(cid:105) message, the server writes the tuple
(f ri, cc, N , vec) into the history entry Hist[ts]. Upon recep-
tion of a complete(cid:104)ts, N, vec(cid:105) message, the server changes
the value of lc to (ts, N, vec) if ts > lc.ts.
5.3 Read Implementation

The read consists of three consecutive rounds, collect,
filter and repair. In collect, a client reads the candi-
date triple (ts, N, vec) stored in variable lc in the server, and
inserts it into the candidate set C together with the candi-
dates read from other servers. After the client receives S − t
candidates from diﬀerent servers, collect returns.

In filter, the client submits C to each server. Upon re-
ception of C, the server chooses chv as the candidate in C
with the highest timestamp such that valid(chv) is satisﬁed,
or c0 if no such candidate exists, and performs a write-back
by setting lc to chv if chv.ts > lc.ts. Roughly speaking,
the predicate valid(c) holds if the server veriﬁes the integrity
of the timestamp c.ts and nonce c.N either by the MAC,
or by the corresponding history entry. The server then re-
sponds to the client with the timestamp chv.ts, the fragment
Hist[chv.ts].f r, the cross-checksum Hist[chv.ts].cc and the
vector of MACs Hist[chv.ts].vec.
The client awaits responses from S − t servers and waits
until there is a candidate c with the highest timestamp in
C such that safe(c) holds, or until C is empty, after which
filter returns. The predicate safe(c) holds if at least t +
1 diﬀerent servers si have responded with timestamp c.ts,

Algorithm 4 Algorithm of writer w in M-PoWerStore.

80: Deﬁnitions:
81:
82:

Q: set of pid, (process id) initially ∅
ts: structure (num, pid, MAC{kW }(num||pid)),
initially ts0 (cid:44) (0, 0, null)

83: operation write(V )
84:
85:
86:
87:
88:
89:
90:
91:

Q ← ∅
ts ← clock()
N ← {0, 1}λ
N ← H(N )
vec ← [MAC{ki}(ts||N )1≤i≤S ]
store(ts, V, N , vec)
complete(ts, N, vec)
return ok

92: procedure clock()
send clock(cid:104)ts(cid:105) to all servers
93:
94: wait until |Q| ≥ S − t
ts.num ← ts.num + 1
95:
ts ← (ts.num, w, MAC{kW }(ts.num||w))
96:
97:
98: upon receiving clock ack(cid:104)ts, tsi(cid:105) from server si
99:
100:

Q ← Q ∪ {i}
if tsi > ts ∧ verify(tsi, kW ) then ts ← tsi

return ts

{f r1, . . . , f rS} ← encode(V, t + 1, S)
cc ← [H(f r1), . . . , H(f rS )]
foreach server si send store(cid:104)ts, f ri, cc, N , vec(cid:105) to si

101: procedure store(ts, V, N , vec)
102:
103:
104:
105: wait for store ack(cid:104)ts(cid:105) from S − t servers
106: procedure complete(ts, N, vec)
107:
108: wait for complete ack(cid:104)ts(cid:105) from S − t servers

send complete(cid:104)ts, N, vec(cid:105) to all servers

fragment f ri, cross-checksum cc such that H(f ri) = cc[i],
and vector vec. If C is empty, the client sets V to the initial
value ⊥. Otherwise, the client selects the highest candidate
c ∈ C and restores value V by decoding V from the t + 1
correct fragments received for c.

In repair, the client veriﬁes the integrity of c.vec by
matching it against the vector vec received from t + 1 dif-
ferent servers. If c.vec and vec match then repair returns.
Otherwise, the client repairs c by setting c.vec to vec and
invokes a round of write-back by sending a repair(cid:104)tsr, c(cid:105)
message to all servers. Upon reception of such a message,
if valid(c) holds then the server sets lc to c provided that
c.ts > lc.ts and responds with an repair ack message to
the client. Once the client receives acknowledgements from
S−t diﬀerent servers, repair returns. After repair returns,
the read returns V .
5.4 Analysis

We argue that M-PoWerStore satisﬁes linearizability by
showing that if a completed read rd by a correct client re-
turns V then a subsequent read rd(cid:48) by a correct client does
not return a value older than V . The residual correctness
arguments are similar to those of PoWerStore (Section 4.4),
and therefore omitted.
Suppose rd(cid:48) follows after rd. If c is the candidate selected
in rd, we argue that rd(cid:48) does not select a candidate with
a lower timestamp. By assumption c is selected in rd by
a correct client that checks the integrity of c.vec (during
repair). If c.vec passes the check, then each of the correct
servers (at least S−2t ≥ t+1) that received c during filter

291Algorithm 5 Algorithm of server si in M-PoWerStore.

Algorithm 6 Algorithm of client r in M-PoWerStore.

109: Deﬁnitions:
lc: structure (ts, N, vec), initially c0 (cid:44) (ts0, null, null)
110:
111: Hist[. . . ]: vector of (f r, cc, N , vec) indexed by ts, with all

entries initialized to (null, null, null, null)

send store ack(cid:104)ts(cid:105) to writer w

if ts > lc.ts then lc ← (ts, N, vec)
send complete ack(cid:104)ts(cid:105) to writer w

109: upon receiving clock(cid:104)ts(cid:105) from writer w
send clock ack(cid:104)ts, lc.ts(cid:105) to writer w
110:
111: upon receiving store(cid:104)ts, f r, cc, N , vec(cid:105) from writer w
112: Hist[ts] ← (f r, cc, N , vec)
113:
114: upon receiving complete(cid:104)ts, N, vec(cid:105) from writer w
115:
116:
117: upon receiving collect(cid:104)tsr(cid:105) from client r
118:
119: upon receiving filter(cid:104)tsr, C(cid:105) from client r
120:
121:
122:
123:
124: upon receiving repair(cid:104)tsr, c(cid:105) from client r
125:
126:

chv ← max({c ∈ C : valid(c)} ∪ {c0})
if chv.ts > lc.ts then lc ← chv
(f r, cc, vec) ← πf r,cc,vec(Hist[chv.ts])
send filter ack(cid:104)tsr, chv.ts, f r, cc, vec(cid:105) to client r

if c.ts > lc.ts ∧ valid(c) then lc ← c
send repair ack(cid:104)tsr(cid:105) to client r

send collect ack(cid:104)tsr, lc(cid:105) to client r

//write-back

//write-back

127: Predicates:
128: valid(c) (cid:44) (H(c.N ) = Hist[c.ts].N ) ∨

verify(c.vec[i], c.ts, H(c.N ), ki)

validates c (by verifying its own entry of c.vec) and sets lc to
c unless it has already changed lc to a higher timestamped
candidate. Otherwise, if c.vec fails the integrity check, then
the client in rd repairs c.vec and subsequently writes-back
c to t + 1 correct servers or more. Hence, by the time rd
completes, at least t + 1 correct servers have set lc to c or
to a valid candidate with a higher timestamp. Since during
collect in rd(cid:48) the client receives the value of lc from S − t
diﬀerent servers, a valid candidate c(cid:48) such that c(cid:48).ts ≥ c.ts
is included in C. By Lemma 4.3 (no exclusion), c(cid:48) is never
excluded from C and by Algorithm 6, rd(cid:48) does not select a
candidate with timestamp lower than c(cid:48).ts ≥ c.ts.

6.

IMPLEMENTATION & EVALUATION

In this section, we describe an implementation model-
ing a Key-Value Store (KVS) based on M-PoWerStore. To
model a KVS, we use multiple instances of M-PoWerStore
referenced by keys. We then evaluate the performance of
our implementation and we compare it to: (i) M-ABD, the
multi-writer variant of the crash-only atomic storage proto-
col of [5], and (ii) Phalanx, the multi-writer robust atomic
protocol of [34] that relies on digital signatures.

6.1 Implementation Setup

Our KVS implementation is based on the JAVA-based
framework Neko [2] that provides support for inter-process
communication, and on the Jerasure [1] library for construct-
ing the dispersal codes. To evaluate the performance of
our M-PoWerStore we additionally implemented two KVSs
based on M-ABD and Phalanx.

In our implementation, we relied on 160-bit SHA1 for

hashing purposes, 160-bit keyed HMACs to implement MACs,

tsr: num, initially 0
Q, R: set of pid, initially ∅
C: set of (ts, N, vec), initially ∅

C, Q, R ← ∅
tsr ← tsr + 1
C ← collect(tsr)
C ← filter(tsr, C)
if C (cid:54)= ∅ then

129: Deﬁnitions:
130:
131:
132:
133: W [1 . . . S]: vector of (ts, f r, cc, vec), initially []
134: operation READ()
135:
136:
137:
138:
139:
140:
141:
142:
143:
144:
145: upon receiving collect ack(cid:104)tsr, ci(cid:105) from server si
146:
147:
148: upon receiving filter ack(cid:104)tsr,ts,f r,cc,vec(cid:105) from server si
149:
150:

c ← c(cid:48) ∈ C : highcand(c(cid:48)) ∧ safe(c(cid:48))
V ← restore(c.ts)
repair(c)
else V ← ⊥
return V

R ← R ∪ {i}; W [i] ← (ts, f r, cc, vec)
C ← C \ {c ∈ C : invalid(c)}

Q ← Q ∪ {i}
if ci.ts > ts0 then C ← C ∪ {ci}

...

...

...

vec ← vec(cid:48) s.t. ∃R(cid:48) ⊆ R : |R(cid:48)| ≥ t + 1 (cid:86)

151: procedure repair(c)
152:

(∀i ∈ R(cid:48) : W [i].ts = c.ts ∧ W [i].vec = vec(cid:48))
if c.vec (cid:54)= vec then

c.vec ← vec
send repair(cid:104)tsr, c(cid:105) to all servers
wait for repair ack(cid:104)tsr(cid:105) from S − t servers

safe(c) (cid:44) ∃R(cid:48) ⊆ R : |R(cid:48)| ≥ t + 1 (cid:86)
(∀i ∈ R(cid:48) : W [i].ts = c.ts) (cid:86)
(∀i, j ∈ R(cid:48):W [i].cc=W [j].cc ∧ H(W [i].f r)=W [j].cc[i])(cid:86)

//repair

(∀i, j ∈ R(cid:48) : W [i].vec = W [j].vec)

157: Predicates:
158:

153:
154:
155:
156:

...

and 1024-bit DSA to generate signatures4. For simplicity,
we abstract away the eﬀect of message authentication in
our implementations; we argue that this does not aﬀect our
performance evaluation since data origin authentication is
typically handled as part of the access control layer in all
three implementations, when deployed in realistic settings
(e.g., in Wide Area Networks).

We deployed our implementations on a private network
consisting of a 4-core Intel Xeon E5443 with 4GB of RAM,
a 4 core Intel i5-3470 with 8 GB of RAM, an 8 Intel-Core
i7-37708 with 8 GB of RAM, and a 64-core Intel Xeon E5606
equipped with 32GB of RAM. In our network, the commu-
nication between various machines was bridged using a 1
Gbps switch. All the servers were running in separate pro-
cesses on the Xeon E5606 machine, whereas the clients were
distributed among the Xeon E5443, i7, and the i5 machines.
Each client invokes operation in a closed loop, i.e., a client
may have at most one pending operation. In all KVS imple-
mentations, all write and read operations are served by a
local database stored on disk.

We evaluate the peak throughput incurred in M-PoWerStore

in write and read operations, when compared to M-ABD
and Phalanx with respect to: (i) the ﬁle size (64 KB, 128

4Note that our benchmarks feature balanced reads/writes
in which case balanced sign/verify DSA performance seems
more appropriate for Phalanx than RSA [14].

292Parameter

Default Value

Failure threshold t

File size

Probability of Concurrency

Workload Distribution

1

256 KB

1%

100% read
100% write

Table 2: Default parameters used in evaluation.

KB, 256 KB, 512 KB, and 1024 KB), and (ii) to the server
failure threshold t (1, 2, and 3, respectively). For better
evaluation purposes, we vary each variable separately and
we ﬁx the remaining parameters to a default conﬁguration
(Table 2). We also evaluate the latency incurred in M-
PoWerStore with respect to the attained throughput.

We measure peak throughput as follows. We require that
each writer performs back to back write operations; we
then increase the number of writers in the system until the
aggregated throughput attained by all writers is saturated.
The peak throughput is then computed as the maximum ag-
gregated amount of data (in bytes) that can be written/read
to the servers per second.

Each data point in our plots is averaged over 5 indepen-
dent measurements; where appropriate, we include the cor-
responding 95% conﬁdence intervals. as data objects. On
the other hand, read operations request the data pertaining
to randomly-chosen keys. For completeness, we performed
our evaluation (i) in the Local Area Network (LAN) set-
ting comprising our aforementioned network (Section 6.2)
and (ii) in a simulated Wide Area Network (WAN) setting
(Section 6.3). Our results are presented in Figure 2.
6.2 Evaluation Results within a LAN Setting
Figure 2(a) depicts the latency incurred in M-PoWerStore
when compared to M-ABD and Phalanx, with respect to
the achieved throughput (measured in the number of oper-
ations per second). Our results show that, by combining
metadata write-backs with erasure coding, M-PoWerStore
achieves lower latencies than M-ABD and Phalanx for both
read and write operations. As expected, read latencies
incurred in PoWerStore are lower than those of write oper-
ations since a write requires an extra communication round
corresponding to the clock round. Furthermore, due to
PoW and the use of lightweight cryptographic primitives,
the read performance of PoWerStore considerably outper-
forms M-ABD and Phalanx. On the other hand, writing in
M-PoWerStore compares similarly to the writing in M-ABD.
Figure 2(b) depicts the peak throughput achieved in M-
PoWerStore with respect to the number of Byzantine servers
t. As t increases, the gain in peak throughput achieved in
M-PoWerStore’s read and write increases compared to M-
ABD and Phalanx. This mainly stems from the reliance
on erasure coding, which ensures that the overhead of ﬁle
replication among the servers is minimized when compared
to M-ABD and Phalanx. In typical settings, featuring t = 1
and the default parameters of Table 2, the peak throughput
achieved in M-PoWerStore’s read operation is almost twice
as large as that in M-ABD and Phalanx.

In Figure 2(c), we measure the peak throughput achieved
in M-PoWerStore with respect to the ﬁle size. Our ﬁndings
clearly show that as the ﬁle size increases, the performance
gain of M-PoWerStore compared to M-ABD and Phalanx
becomes considerable. For example, when the ﬁle size is 1

MB, the peak throughput of read and write operations in
M-PoWerStore approaches the (network-limited) bounds of
50 MB/s5 and 45 MB/s, respectively.
6.3 Evaluation Results within a Simulated WAN

Setting

We now proceed to evaluate the performance (latency, in
particular) of M-PoWerStore when deployed in WAN set-
tings. For that purpose, we rely on a 100 Mbps switch to
bridge the network outlined in Section 6.1 and we rely on
NetEm [37] to emulate the packet delay variance speciﬁc to
WANs. More speciﬁcally, we add a Pareto distribution to
our links, with a mean of 20 ms and a variance of 4 ms.

Our measurement results (Figure 2(d)) conﬁrm our pre-
vious analysis conducted in the LAN scenario and demon-
strate the superior performance of M-PoWerStore compared
to M-ABD and Phalanx in realistic settings. Here, we point
out that the performance of M-PoWerStore incurred in both
read and write operations does not deteriorate as the num-
ber of Byzantine servers in the system increases. This is
mainly due to the reliance on erasure coding. In fact, the
overhead of transmitting an erasure-coded ﬁle F to the 3t+1
servers, with a reconstruction threshold of t + 1 is given by
t+1 |F|. It is easy to see that, as t increases, this overhead
3t+1
asymptotically increases towards 3|F|.

7. RELATED WORK

HAIL [8] is a distributed cryptographic storage system
that implements a multi-server variant of Proofs of Retriev-
ability (PoR) [9] to ensure integrity protection and availabil-
ity (retrievability) of ﬁles dispersed across several storage
servers. Like PoWerStore, HAIL assumes Byzantine fail-
ure model for storage servers, yet the two protocols largely
cover diﬀerent design space. Namely, HAIL considers a mo-
bile adversary and a single client interacting with the storage
in a synchronous fashion. In contrast, PoWerStore assumes
static adversary, yet assumes a distributed client setting in
which clients share data in an asynchronous fashion. Multi-
ple clients are also supported by IRIS [42], a PoR-based dis-
tributed ﬁle system designed with enterprise users in mind
that stores data in the clouds and is resilient against poten-
tially untrustworthy service providers. However, in IRIS, all
clients are pre-serialized by a logically centralized, trusted
portal which acts as a fault-free gateway for communication
with untrusted clouds.
In contrast, PoWerStore relies on
the highly available distributed PoW technique, which elim-
inates the need for any trusted and/or fault-free component.
Notice that data conﬁdentiality is orthogonal to all of HAIL,
IRIS and PoWerStore protocols.

In the context of distributed storage asynchronously shared
among multiple fault-prone clients across multiple servers
without any fault-free component, a seminal crash-tolerant
storage implementation (called ABD) was presented in [5].
ABD assumes a majority of correct storage servers, and
achieves strong consistency by having readers write back
the data they read. As shown in [17], server state modiﬁca-
tions by readers introduced in ABD are unavoidable in ro-
bust storage such as ABD, where robustness is characterized
by both strong consistency and high availability. However,

5Note that an eﬀective peak throughput of 50MB/s in M-
PoWerStore reﬂects an actual throughput of almost 820
Mbps when t = 1.

293(a) Throughput vs. latency in a LAN setting.

(b) Peak throughput with respect to the failure threshold in
a LAN setting.

(c) Peak throughput with respect to the ﬁle size in a LAN
setting.

(d) Latency vs the failure threshold in a simulated WAN set-
ting.

Figure 2: Evaluation Results. Data points are averaged over 5 independent runs; where appropriate, we
include the corresponding 95% conﬁdence intervals.

robust storage implementations diﬀer in the writing strategy
employed by readers: in some protocols readers write-back
data (e.g., [4, 5, 15, 18, 20, 34]) whereas in others readers only
write metadata to servers (e.g., [11, 16, 17]).

Existing robust storage protocols in which readers write
only metadata, either do not tolerate Byzantine faults [11,
17], or require a total number of servers linear in number of
readers to tolerate t Byzantine servers [16], and hence are
prohibitively expensive. PoWerStore is hence the ﬁrst robust
BFT protocol that uses a bounded number of storage servers
and has readers write only metadata to servers.

Clearly, most distributed BFT storage implementations
have been focusing on using as few servers as possible, ideally
3t + 1, which deﬁnes optimal resilience in the asynchronous
model [35]. This was ﬁrst achieved by Phalanx [34], a BFT
variant of ABD [5]. Phalanx uses digital signatures, i.e.,
self-verifying data, to port ABD to the Byzantine model,
maintaining the latency of ABD, as well as its data write-
backs. However, since digital signatures introduce consid-
erable overhead [33, 38], protocols that feature lightweight

authentication, or no data authentication at all (unauthen-
ticated model) have been designed. Unfortunately, in the
unauthenticated model, optimal resilience in BFT storage
incurs considerable latency penalties: at least two rounds
of communication between clients and servers for writes [3]
and at least four rounds6 for reads [15], even in the single
writer case. To avoid such a considerable overhead, some
robust BFT storage protocols (e.g., PASIS [19]) store unau-
thenticated data across 4t + 1 servers.

Clearly, there is a big gap in eﬃciency (and, in partic-
ular, communication latency and the number of servers)
between storage protocols that use self-verifying data and
those that assume no authentication. Loft [22] aims at
bridging this gap and implements erasure-coded optimally
resilient linearizable storage while optimizing the failure-free
case. Loft uses homomorphic ﬁngerprints and MACs; it fea-
tures 3-round wait-free writes, but reads are based on data
write-backs and data might be unavailable in case of heavy
read/write concurrency. Similarly, our Proofs of Writing

6Under constant number of write rounds.

 0 50 100 150 200 250 40 60 80 100 120 140 160 180Latency (ms)Throughput (op/s)M-ABD Write M-ABD Read Phalanx Write Phalanx Read M-PoWerStore Write M-PoWerStore Read  5 10 15 20 25 30 35 40 45t=1t=2t=3Peak Throughput (MB/s)Failure ThresholdM-ABD WriteM-ABD Read Phalanx Write Phalanx Read M-PoWerStore Write M-PoWerStore Read 0 10 20 30 40 50 100 200 300 400 500 600 700 800 900 1000Peak Throughput (MB/s)File Size (KB)M-ABD WriteM-ABD Read Phalanx Write Phalanx Read M-PoWerStore Write M-PoWerStore Read 100 200 300 400 500 600 700 800 900 1000t=1t=2t=3Latency (ms)Failure ThresholdM-ABD WriteM-ABD Read Phalanx Write Phalanx Read M-PoWerStore Write M-PoWerStore Read294Protocol

Data Authentication Data Dispersal Read/Write Latency no. of readers/writers no. of messages Byz. clients

Phalanx [34]

[31]
[15]
[10]

Loft [22]

PoWerStore

M-PoWerStore

signatures
signatures

none

signatures
hash/MAC
hash/MAC
hash/MAC

replication
replication
replication

erasure coding
erasure-coding
erasure-coding
erasure-coding

2D/(D+d)
2D/(D+2d)

(2D+2d)/(D+d)

>(D+2d) / >(3D+d)

∞/D+2d

(D+d)/(D+d)

(D+2d)/(D+2d)

any/any
any/any

any/1

any/any
any/any

any/1

any/any

O(t)
O(t)
O(t)
O(t2)
O(t2)
O(t)
O(t)

readers

all
no
all
all

readers
readers

Table 3: Comparison of properties of existing strongly consistent optimally resilient BFT storage protocols.
Shown latency is worst case and distinguishes between data rounds (D) and metadata rounds (d), where
typically D >> d.

(PoW) incorporate lightweight authentication that is, how-
ever, suﬃcient to achieve optimal latency and to facilitate
metadata write-backs while guaranteeing optimal resilience,
high-availability and strong consistency. We ﬁnd PoW to
be a fundamental improvement in the light of BFT storage
implementations that explicitly renounce strong consistency
in favor of weaker consistency notions due to the high cost
of data write-backs (e.g., [7]). A summary of the key prop-
erties of existing BFT storage protocols when compared to
our protocols is shown in Table 3.

A separate line of research aims at a family of so-called
forking semantics (e.g., [36]), which relax atomic semantics,
yet require no trusted components whatsoever. Systems
guaranteeing forking semantics guarantee that after a sin-
gle atomicity violation by the service, the views seen by two
inconsistent clients can never again converge. PoWerStore
avoids the drawbacks of fork-consistent systems (reﬂected
in, e.g., diﬃculties in understanding forking semantics and
exploiting them in practice [39]), by oﬀering easily under-
standable, fully linearizable, (i.e., atomic) semantics.

8. CONCLUSION

In this paper, we presented PoWerStore, an eﬃcient ro-
bust storage protocol that achieves optimal latency, mea-
sured in maximum (worst-case) number of communication
rounds between a client and storage servers. We also sepa-
rately presented a multi-writer variant of our protocol called
M-PoWerStore. The eﬃciency of our proposals stems from
combining lightweight cryptography, erasure coding and meta-
data writebacks, where readers write-back only metadata to
achieve linearizability. While robust BFTs have been of-
ten criticized for being prohibitively ineﬃcient, our ﬁndings
suggest that eﬃcient and robust BFTs can be realized in
practice by relying on lightweight cryptographic primitives
without compromising worst-case performance.

At the heart of both PoWerStore and M-PoWerStore pro-
tocols are Proofs of Writing (PoW): a novel storage tech-
nique inspired by commitment schemes in the ﬂavor of [21],
that enables single-writer PoWerStore to write and read
in 2 rounds which we show optimal. Similarly, by rely-
ing on PoW, multi-writer M-PoWerStore features 3-round
writes/reads where the third read round is only invoked un-
der active attacks. Finally, we demonstrated M-PoWerStore’s
superior performance compared to existing crash and Byzantine-
tolerant atomic storage implementations.

9. ACKNOWLEDGEMENTS

This work is supported in part by the EU CLOUDSPACES
(FP7-317555) and SECCRIT (FP7-312758) projects and by
LOEWE TUD CASED.

10. REFERENCES
[1] Jerasure. https://github.com/tsuraan/Jerasure,

2008.

[2] The Neko Project. http://ddsg.jaist.ac.jp/neko/,

2009.

[3] Ittai Abraham, Gregory Chockler, Idit Keidar, and

Dahlia Malkhi. Byzantine Disk Paxos: Optimal
Resilience with Byzantine Shared Memory. Distributed
Computing, 18(5):387–408, 2006.

[4] Amitanand S. Aiyer, Lorenzo Alvisi, and Rida A.

Bazzi. Bounded Wait-free Implementation of
Optimally Resilient Byzantine Storage Without
(Unproven) Cryptographic Assumptions. In
Proceedings of DISC, 2007.

[5] Hagit Attiya, Amotz Bar-Noy, and Danny Dolev.

Sharing Memory Robustly in Message-Passing
Systems. J. ACM, 42:124–142, January 1995.

[6] Rida A. Bazzi and Yin Ding. Non-skipping

Timestamps for Byzantine Data Storage Systems. In
Proceedings of DISC, pages 405–419, 2004.

[7] Alysson Neves Bessani, Miguel P. Correia, Bruno

Quaresma, Fernando Andr´e, and Paulo Sousa. Depsky:
dependable and secure storage in a cloud-of-clouds. In
Proceedings of EuroSys, pages 31–46, 2011.

[8] Kevin D. Bowers, Ari Juels, and Alina Oprea. Hail: a
high-availability and integrity layer for cloud storage.
In CCS, pages 187–198, 2009.

[9] Kevin D. Bowers, Ari Juels, and Alina Oprea. Proofs

of retrievability: theory and implementation. In
CCSW, pages 43–54, 2009.

[10] Christian Cachin and Stefano Tessaro. Optimal

Resilience for Erasure-Coded Byzantine Distributed
Storage. In Proceedings of DSN, pages 115–124, 2006.

[11] Brian Cho and Marcos K. Aguilera. Surviving

congestion in geo-distributed storage systems. In
Proceedings of USENIX ATC, pages 40–40, 2012.

[12] Gregory Chockler, Dahlia Malkhi, and Danny Dolev.
Future directions in distributed computing. chapter A
data-centric approach for scalable state machine
replication, pages 159–163. 2003.

[13] Allen Clement, Edmund L. Wong, Lorenzo Alvisi,

Michael Dahlin, and Mirco Marchetti. Making
byzantine fault tolerant systems tolerate byzantine
faults. In Proceedings of NSDI, pages 153–168, 2009.

[14] Wei Dai. Crypto++ 5.6.0 benchmarks. Website, 2009.

Available online at
http://www.cryptopp.com/benchmarks.html.

[15] Dan Dobre, Rachid Guerraoui, Matthias Majuntke,
Neeraj Suri, and Marko Vukoli´c. The Complexity of

295Robust Atomic Storage. In Proceedings of PODC,
pages 59–68, 2011.

[16] Partha Dutta, Rachid Guerraoui, Ron R. Levy, and

Marko Vukoli´c. Fast Access to Distributed Atomic
Memory. SIAM J. Comput., 39:3752–3783, December
2010.

[17] Rui Fan and Nancy Lynch. Eﬃcient Replication of
Large Data Objects. In Proceedings of DISC, pages
75–91, 2003.

[34] Dahlia Malkhi and Michael K. Reiter. Secure and
Scalable Replication in Phalanx. In Proceedings of
SRDS, pages 51–58, 1998.

[35] Jean-Philippe Martin, Lorenzo Alvisi, and Michael

Dahlin. Minimal Byzantine Storage. In Proceedings of
DISC, pages 311–325, 2002.

[36] David Mazi`eres and Dennis Shasha. Building secure
ﬁle systems out of byantine storage. In PODC, pages
108–117, 2002.

[18] Chryssis Georgiou, Nicolas C. Nicolaou, and

[37] NetEm. NetEm, the Linux Foundation. Website, 2009.

Alexander A. Shvartsman. Fault-tolerant Semifast
Implementations of Atomic Read/Write Registers. J.
Parallel Distrib. Comput., 69(1):62–79, January 2009.
[19] Garth R. Goodson, Jay J. Wylie, Gregory R. Ganger,

and Michael K. Reiter. Eﬃcient Byzantine-Tolerant
Erasure-Coded Storage. In Proceedings of DSN, 2004.

[20] Rachid Guerraoui and Marko Vukoli´c. Reﬁned quorum

systems. Distributed Computing, 23(1):1–42, 2010.

[21] Shai Halevi and Silvio Micali. Practical and
provably-secure commitment schemes from
collision-free hashing. In Proceedings of CRYPTO,
pages 201–215, 1996.

[22] James Hendricks, Gregory R. Ganger, and Michael K.
Reiter. Low-overhead Byzantine fault-tolerant storage.
In Proceedings of SOSP, pages 73–86, 2007.

[23] Maurice Herlihy. Wait-Free Synchronization. ACM

Trans. Program. Lang. Syst., 13(1), 1991.

[24] Maurice P. Herlihy and Jeannette M. Wing.
Linearizability: A Correctness Condition for
Concurrent Objects. ACM Trans. Program. Lang.
Syst., 12(3), 1990.

[25] Prasad Jayanti, Tushar Deepak Chandra, and Sam
Toueg. Fault-tolerant Wait-free Shared Objects. J.
ACM, 45(3), 1998.

[26] Aniket Kate, Gregory M. Zaverucha, and Ian

Goldberg. Constant-size commitments to polynomials
and their applications. In Proceedings of
ASIACRYPT, volume 6477, pages 177–194, 2010.

[27] Petr Kuznetsov and Rodrigo Rodrigues. Bftw3: Why?
When? Where? workshop on the theory and practice
of Byzantine fault tolerance. SIGACT News,
40(4):82–86, 2009.

[28] Leslie Lamport. On Interprocess Communication.

Distributed Computing, 1(2):77–101, 1986.

[29] Leslie Lamport, Robert E. Shostak, and Marshall C.
Pease. The byzantine generals problem. ACM Trans.
Program. Lang. Syst., 4(3):382–401, 1982.

[30] Harry C. Li, Allen Clement, Amitanand S. Aiyer, and
Lorenzo Alvisi. The Paxos Register. In Proceedings of
SRDS, pages 114–126, 2007.

[31] Barbara Liskov and Rodrigo Rodrigues. Tolerating
Byzantine Faulty Clients in a Quorum System. In
Proceedings of ICDCS, 2006.

[32] Nancy A. Lynch and Mark R. Tuttle. An introduction
to input/output automata. CWI Quarterly, 2:219–246,
1989.

[33] Dahlia Malkhi and Michael K. Reiter. A

High-Throughput Secure Reliable Multicast Protocol.
J. Comput. Secur., 5(2):113–127, March 1997.

Available online at http://www.linuxfoundation.
org/collaborate/workgroups/networking/netem.

[38] Michael K. Reiter. Secure Agreement Protocols:

Reliable and Atomic Group Multicast in Rampart. In
Proceedings of CCS, pages 68–80, 1994.

[39] Alexander Shraer, Christian Cachin, Asaf Cidon, Idit

Keidar, Yan Michalevsky, and Dani Shaket. Venus:
veriﬁcation for untrusted cloud storage. In CCSW,
pages 19–30, 2010.

[40] Alexander Shraer, Jean-Philippe Martin, Dahlia

Malkhi, and Idit Keidar. Data-centric reconﬁguration
with network-attached disks. In Proceedings of LADIS,
pages 22–26, 2010.

[41] Atul Singh, Tathagata Das, Petros Maniatis, Peter
Druschel, and Timothy Roscoe. Bft protocols under
ﬁre. In Proceedings of NSDI, pages 189–204, 2008.

[42] Emil Stefanov, Marten van Dijk, Ari Juels, and Alina
Oprea. Iris: a scalable cloud ﬁle system with eﬃcient
integrity checks. In ACSAC, pages 229–238, 2012.

[43] Sue-Hwey Wu, Scott A. Smolka, and Eugene W.

Stark. Composition and behaviors of probabilistic i/o
automata. In Proceedings of CONCUR, pages
513–528, 1994.

APPENDIX
A. OPTIMALITY OF PoWerStore

In this section, we prove that PoWerStore features optimal
latency, by showing that writing in two rounds is necessary
and we refer the reader to [16] for the necessity of reading in
two rounds. We start by giving some informal deﬁnitions.

A distributed algorithm A is a set of automata [32], where
automaton Ap is assigned to process p. Computation pro-
ceeds in steps of A and a run is an inﬁnite sequence of steps
of A. A partial run is a ﬁnite preﬁx of some run. We say
that a (partial) run r extends some partial run pr if pr is
a preﬁx of r. We say that an implementation is selﬁsh,
if clients write-back metadata to achieve linearizability (in-
stead of the full value) [17]. Furthermore, we say that an
operation is fast if it completes in a single round.

Theorem A.1. There is no fast WRITE implementa-
tion I of a multi-reader selﬁsh robust storage that makes
use of less than 4t + 1 servers.

Preliminaries. We prove Theorem A.1 by contradiction
assuming at most 4t servers. An illustration of the proof
is given in Figure 3. We partition the set of servers into
four distinct subsets (we call blocks), denoted by T1, T2, T3
each of size exactly t, and T4 of size at least 1 and at most
t. Without loss of generality we assume that each block
contains at least one server. We say that an operation op
skips a block Ti, (1 ≤ i ≤ 4) when all messages by op to

296• Let run2 be the partial run similar to run(cid:48)

1, in which
all servers except T2 are correct, but due to asynchrony,
all messages from w to T4 are delayed. Like in run(cid:48)
1, wr
completes by writing v to all servers except T4, which
it skips. To see why, note that wr cannot distinguish
run2 from run(cid:48)
1. After wr completes, T2 fails Byzantine
by reverting its memory to the initial state.
• Let run3 extend run1 by appending a complete READ
rd1 invoked by r1. By our assumption, I is wait-free.
As such, rd1 completes by skipping T1 (because T1
crashed) and returns (after a ﬁnite number of rounds)
a value vR.

• Let run4 extend run2 by appending rd1. In run4, all
servers except T2 are correct, but due to asynchrony all
messages from r1 to T1 are delayed indeﬁnitely. More-
over, since T2 reverted its memory to the initial state,
v is held only by T3. Note that r1 cannot distinguish
run4 from run3 in which T1 has crashed. As such, rd1
completes by skipping T1 and returns vR. By lineariz-
ability, vR = v.

• Let run5 be similar to run3 in which all servers except
T3 are correct but, due to asynchrony, all messages from
r1 to T1 are delayed. Note that r1 cannot distinguish
run5 from run3. As such, rd1 returns vR in run5, and
by run4, vR = v. After rd1 completes, T3 fails by
crashing.

• Let run6 extend run5 by appending a READ rd2 in-
voked by r2 that completes by returning v(cid:48). Note that
in run5, (i) T3 is the only server to which v was writ-
ten, (ii) rd1 did not write-back v (to any other server)
before returning v, and (iii) T3 crashed before rd2 is
invoked. As such, rd2 does not ﬁnd v in any server and
hence v(cid:48) (cid:54)= v, violating linearizability.

It is important to note that Theorem A.1 allows for self-
verifying data and assumes clients that may fail only by
crashing. Furthermore, the impossibility extends to crash-
tolerant storage using less than 3t + 1 servers when deleting
the Byzantine block T2 in the above proof.

Ti are delayed indeﬁnitely (due to asynchrony) and all other
blocks Tj receive all messages by op and reply.

wr(v)

wr(v)

T1

T2

T3

T4

T1

T2

T3

T4

T1

T2

T3

T4

T1

T2 @

T3

T4

(a) run1

(b) run2

wr(v) rd1 → vR

wr(v) rd1 → v

T1

T2 @

T3

T4

...

...

...

...

...

...

(c) run3

(d) run4

wr(v) rd1 → v

wr(v) rd1 → v

rd2 → v(cid:48)

T1

T2

T3

T4

...

...

...

...

...

...

...

...

...

(e) run5

(f) run6

Figure 3: Sketch of the runs used in the proof of
Theorem A.1.

Proof: We construct a series of runs of a linearizable im-
plementation I towards a partial run that violates lineariz-
ability, i.e., that features two consecutive read operations
by distinct readers that return diﬀerent values.

• Let run1 be the partial run in which all servers are
correct except T1 which crashed at the beginning of
run1. Let wr be the operation invoked by the writer
w to write a value v (cid:54)= ⊥ in the storage. The WRITE
wr is the only operation invoked in run1 and w crashes
after writing v to T3. Hence, wr skips blocks T1, T2
and T4.
• Let run(cid:48)

1 be the partial run in which all servers are
correct except T4, which crashed at the beginning of
run(cid:48)
1, w is correct and wr completes by writing
v to all blocks except T4, which it skips.

1. In run(cid:48)

297