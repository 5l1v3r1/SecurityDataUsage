Demystifying the Dark Side of the Middle: A Field Study of

Middlebox Failures in Datacenters

Rahul Potharaju
Purdue University

Navendu Jain

Microsoft Research

rpothara@purdue.edu

navendu@microsoft.com

n
o
i
t
u
b
i
r
t
n
o
C

 
t
n
e
c
r
e
P

100

75

50

25

0

n
o
i
t
u
b
i
r
t
n
o
C

 
t
n
e
c
r
e
P

100

75

50

25

0

ABSTRACT
Network appliances or middleboxes such as ﬁrewalls, intru-
sion detection and prevention systems (IDPS), load bal-
ancers, and VPNs form an integral part of datacenters and
enterprise networks. Realizing their importance and short-
comings, the research community has proposed software im-
plementations, policy-aware switching, consolidation appli-
ances, moving middlebox processing to VMs, end hosts, and
even oﬄoading it to the cloud. While such eﬀorts can use
middlebox failure characteristics to improve their reliability,
management, and cost-eﬀectiveness, little has been reported
on these failures in the ﬁeld.

In this paper, we make one of the ﬁrst attempts to perform
a large-scale empirical study of middlebox failures over two
years in a service provider network comprising thousands of
middleboxes across tens of datacenters. We ﬁnd that mid-
dlebox failures are prevalent and they can signiﬁcantly im-
pact hosted services. Several of our ﬁndings diﬀer in key as-
pects from commonly held views: (1) Most failures are grey
dominated by connectivity errors and link ﬂaps that exhibit
intermittent connectivity, (2) Hardware faults and overload
problems are present but they are not in majority, (3) Mid-
dleboxes experience a variety of misconﬁgurations such as
incorrect rules, VLAN misallocation and mismatched keys,
and (4) Middlebox failover is ineﬀective in about 33% of the
cases for load balancers and ﬁrewalls due to conﬁguration
bugs, faulty failovers and software version mismatch. Fi-
nally, we analyze current middlebox proposals based on our
study and discuss directions for future research.

Categories and Subject Descriptors
C.2.3 [Computer-Communication Network]: Network
Operations—network management

General Terms
Network management; Reliability

Keywords
Datacenters; Network reliability; Middleboxes

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
IMC’13, October 23–25, 2013, Barcelona, Spain.
Copyright 2013 ACM 978-1-4503-1953-9/13/10 ...$15.00..

81

20

High Severity Incidents
Population

36

43

 2

11

 1

 3

L2 Switches

L3 Routers Middleboxes

Others

42

35

11

9

c

e

n

y

t i v i t
H i g

e

t

a

h   L

y

c

n

r

e

S

r

o

r

e   E r

v i c

y

r i t

u

c

e

S

n

o

t   c

s

o

L

3

L A

S

Figure 1: Middleboxes contribute to 43% of high-
severity incidents despite being 11% of the popula-
tion (top). The top-5 categories of service impact in
these incidents caused by middleboxes (bottom).

1.

INTRODUCTION

Today’s datacenters and enterprises deploy a variety of
intermediary network devices or middleboxes to distribute
load (e.g., load balancers), enable remote connectivity (e.g.,
VPNs),
improve performance (e.g., proxies) and security
(e.g., ﬁrewalls, IDPS), as well as to support new traﬃc
classes and applications [1–4]. Given these valuable beneﬁts,
the market for middleboxes is estimated to exceed $10B by
2016 [5], and their number is becoming comparable to that
of routers in enterprise networks [1, 6].

These beneﬁts, however, come at a high cost: middle-
boxes constitute a signiﬁcant fraction of the network capital
and operational expenses [1]; they are complex to manage
and expensive to troubleshoot [6]; and their outages can
greatly impact service performance and availability [7]. For
instance, in December 2012, a load balancing misconﬁgura-
tion aﬀected multiple Google services including Chrome and
Gmail [8]. In a 2011 survey of 1,000 organizations [9], 36%
and 42% of the respondents indicated failure of a ﬁrewall
due to DDoS attacks at the application layer and network
layer, respectively; the very attack ﬁrewalls are deployed to
protect against.

9Table 1: The major ﬁndings of our study of middlebox failures and their implications.

Major ﬁndings

Implications

(1) Middlebox failures are prominent and can greatly
degrade service performance and availability.
(2) Most failures are grey dominated by connectivity
errors and link ﬂaps that exhibit intermittent
connectivity; fail-stop device failures occur, but they
are not as often.

(3) Hardware faults, misconﬁgurations and overload
problems are present, but they are not in majority in
contrast to common perception [6].

(4) There are a variety of misconﬁgurations such as
incorrect rules, VLAN misallocation and mismatched
keys.
(5) Middlebox redundancy is ineﬀective in about 33%
cases for load balancers and ﬁrewalls due to
conﬁguration bugs, faulty failovers and software
version mismatch.
(6) A family of middleboxes having faulty PSUs and
another exhibiting the few bad apples eﬀect, could
have been detected as early problem indicators.

(1) The problem of middlebox failures is signiﬁcant and worth
special attention from the research community.
(2) Connectivity errors degrade service performance and
availability e.g., due to timeouts, delayed protocol convergence,
packet retransmissions, and parity errors. Robust network
protocols (e.g., MP-TCP [10]) need to be developed to route
traﬃc around interfaces exhibiting intermittent connectivity.
(3) Hardware repairs risk long periods during which the network
is operating at reduced or no redundancy. Load testing of
middleboxes is needed to measure the maximum operating
capacity under peak traﬃc.
(4) Software-deﬁned networks (SDNs) based conﬁguration and
management tools need to be developed to handle a wide range
of middlebox misconﬁgurations.
(5) Middlebox failovers need rigorous testing and ensuring
software version compatibility in redundant units. Explore
commoditizing middleboxes to dynamically scale-out/in and to
avoid single points of failure.
(6) Trend analysis can be a useful tool in identifying failure
trends in network datasets e.g., compare device
categories/models to ﬁnd the ones less reliable than others.

Our analysis, Figure 1 (top) of high-severity incidents (sig-
nifying high customer or business impact), over four years
in the network we studied indicates that middleboxes con-
tributed to about 43% of the incidents despite being 11%
of the population. Figure 1 (bottom) shows the top-5 cate-
gories of service impact caused by middleboxes in this dataset.
Operators tagged each incident to a problem category based
on the primary issue experienced by impacted customers.
We observe that loss of connectivity and high latency issues
dominate, with a long tail of other issues such as service
errors, security problems and SLA violations. Thus, under-
standing the characteristics and the impact of middlebox
failures is important to improve performance and availabil-
ity of hosted services.

Recent research proposals on middleboxes [2, 3, 6, 11–13]
aim to alleviate many of their shortcomings, but they do not
address how to improve middlebox reliability in existing de-
ployments. These eﬀorts, in turn, would beneﬁt from a ﬁeld
study on middlebox failures as understanding their failures,
causes and impact may help guide researchers to improve
their reliability, cost-eﬀectiveness and management, in spirit
of recent eﬀorts [14, 15].

Unfortunately, very few empirical studies have been con-
ducted on middlebox failures and even they have a lim-
ited focus. For example, Allman et al. [16] perform ac-
tive measurements of TCP traﬃc across two ﬁrewalls and
load balancers to quantify their impact on end-to-end per-
formance [16]. Sekar et al. [1, 11] presented anecdotal data
on middleboxes from a large enterprise network. Sherry et
al. [6] provide a useful survey of 57 network administrators
to study enterprise middlebox deployments. They, however,
do not report any empirical failure data, but rather the frac-
tion of administrators who estimated one of three pre-deﬁned
issues to be the most common root cause. Our prior work on
datacenter network failures observed that load balancers ex-
hibit many short-term, transient faults [17]. However, it did
not analyze their root causes, service impact, eﬀectiveness
of middlebox redundancy or study other types of middle-

boxes; we provide a detailed comparison to related work in
Section 8.

1.1 Our contributions

In this paper, we present a large-scale empirical study
of middleboxes in datacenters to (a) understand their fail-
ure characteristics, (b) analyze current middlebox proposals
through the lens of reliability, and (c) derive implications for
research on middleboxes. Speciﬁcally, our study focuses on
answering the following key questions:

1. What are the failure characteristics of middleboxes?
2. What are the root causes and what actions are taken for
resolution? How prevalent are misconﬁgurations and what
are their diﬀerent types?

3. Does reliability improve with new generations? Can we

ﬁnd early problem symptoms to enable proactive repair?

4. Is middlebox redundancy eﬀective in masking failures?

To answer these questions, we collect network event logs
over two years (July 24, 2010-12) across 10+ datacenters
hosting 100k+ servers and 2k+ middleboxes; absolute counts
omitted due to conﬁdentiality reasons. Our measurement
data covers a wide range of network event sources, including
syslog and SNMP alerts, conﬁguration ﬁles, a maintenance
tracking system, trouble tickets, and traﬃc data. The de-
tails of our datasets and methodology are described in Sec-
tion 2. Our major ﬁndings are summarized in Table 1.

To identify failure trends in network datasets, we propose
using a simple technique of trend analysis which can un-
cover global problem trends rather than analyzing incidents
in isolation. For instance, we observed that only a few de-
vices contributed to a signiﬁcant number of failures in one
device family exhibiting the few bad apples eﬀect, and a fam-
ily of load balancers having faulty PSUs could have been
detected as early problem indicators by aggregating failure
root causes across that type, but they kept being repaired
in isolation as observed in our failure dataset.

10INTERNET

OTHER

DATACENTERS

IDPS

VPN

CORE 
ROUTER

ACCESS
ROUTER

IDPS

VPN

FIREWALL

FIREWALL

AGGREGATION

SWITCH

Core

L3

Aggregation

L2 

Aggregation

LOAD 

BALANCER

LOAD

BALANCER

TOP-OF-RACK

 SWITCH

RACK MOUNTED

SERVERS

Figure 2: Example of a conventional datacenter net-
work comprising middleboxes, switches and routers.

While we studied only our dataset, we believe it is fairly
representative of a larger class of datacenters based on the
scale (2k+ middleboxes across 10+ datacenters), diversity
(multiple middlebox types and platforms), measurement du-
ration (over two years), and validation with operators having
high experience with middleboxes and some having previ-
ously worked at other sites. Further, the middleboxes we
studied are from multiple leading network vendors having a
signiﬁcant share of the middlebox market.

2. BACKGROUND AND METHODOLOGY
In this section we provide a brief overview of the datacen-
ter network architecture, and then describe the challenges
and methodology for analyzing middlebox failures.

2.1 Datacenter network architecture

Figure 2 illustrates a topology1 of a conventional data-
center network [18] comprising a variety of L2 switches, L3
routers and middleboxes such as ﬁrewalls, IDPS, load bal-
ancers, and VPNs.

Firewalls (FW) protect applications from unwanted traﬃc
(e.g., DoS attacks) by examining packet ﬁelds at IP, trans-
port and application layers against a speciﬁed set of rules.
Network-based IDPS devices analyze traﬃc to safeguard
against attacks such as malware and intrusions. Typically,
hardware FWs and IDPSes are deployed to handle high traf-
ﬁc rates [19]. Load Balancers (LB) distribute application re-
quests across hosted servers. Redundant pairs of LBs typi-
cally connect to each aggregation switch (AGG) and perform
mapping between static (exposed to clients through DNS)
and dynamic IP addresses of the servers. Recent hardware
LBs also provide other features such as NAT, SSL accelera-
tion, and data caching. VPNs are typically used to facilitate
secure remote access for web and client/server applications.
Some commercial oﬀerings such as Cisco Catalyst 6509-E
provide VPN functionality through IPSec and SSL. Due to
the low population of other middleboxes such as NATs and
media converters, our study did not include them.

1

Other conﬁgurations are possible e.g., ﬁrewalls at the network edge.

2.2 Network Datasets

We examined multiple sources of network failure data for
middleboxes over two years logged in the monitoring servers
of a large service provider comprising 100k+ servers and 2k+
middleboxes across 10+ datacenters. The service provider
uses middleboxes to deliver a variety of applications e.g.,
load balancers for web services, search, email, cloud com-
puting and video streaming; ﬁrewalls and IDPS to protect
conﬁdential data (e.g., high business impact applications),
and VPNs to enable remote access (e.g., to oﬃce network).

• Event Logs: Operators typically detect network fail-
ures from monitoring alarms such as syslog and SNMP
traps, and track device/link status via ping and SNMP
polling. These logs contain information about what type
of network element experienced the event, event type, the
other end-point of the interface, and a short machine-
generated description.
• Conﬁguration data: A conﬁguration database stores
and tracks changes to device conﬁgurations using a revi-
sion control system. It also provides a ‘diﬀ’-like function-
ality.
• Maintenance Tracking System: This system tracks
network changes such as device provisioning, repairs, and
code updates. Network engineers use this system to check
for on-going and planned maintenance during failure triage.
• Link-level traﬃc: Traﬃc carried on each network in-
terface is logged using SNMP [20] polling which averages
packet and byte counts observed every ﬁve minutes. Most
traﬃc monitoring systems use the MIB [21] format to
store the data that includes ﬁelds such as the interface
type (e.g., token ring, ethernet), the other endpoint, in-
terface status (up/down), and the number of bytes sen-
t/received. We correlate this data with network events to
extract failures impacting network traﬃc, and to reverse-
engineer the topology.
• Trouble Tickets: Network tickets record the activities
of engineers while troubleshooting a problem [22]. Each
ticket is assigned a unique identiﬁer (TicketID) and con-
tains multiple ﬁelds such as when and how a failure was
discovered, and a diary of free-form text written by engi-
neers describing the steps taken to mitigate the problem.

Note that since the datacenters we studied are managed by
a central operations team, their failure characteristics are
similar and thus we do not compare between datacenters.

2.3 Methodology
Failure deﬁnition. Intuitively, a failure can be deﬁned as
an event that causes a network device or link to be unavail-
able to carry traﬃc. However, precisely deﬁning a middlebox
failure is complicated because its impact is dependent on the
device type. For instance, a faulty load balancer may cause
a loss of throughput while a misconﬁgured ﬁrewall may in-
correctly forward or drop legitimate traﬃc. Therefore, we
consider all logged events as failures which cause a traﬃc
impact (in terms of loss in traﬃc volume) or cause a device
to function incorrectly in routing or processing traﬃc (based
on problems observed in trouble tickets).

Challenges in analyzing data. There are several chal-
lenges in utilizing the network event logs to derive an accu-
rate set of failure events and to characterize their impact:

11LB−1

LB−2

LB−3

SYSLOG / 

SNMP Events

Maintenance 

Tracking System

Time window 
correlation

Network 

Traffic Data

Problem Inference

from Trouble Tickets

+

Timing
Filter

Maintenance

Filter

X

Redundant
Failures 

Filter

+

Impact 
Filter

Failures

with

Impact

]
x
>
X
P

[

0.25

0.20

0.15

0.10

0.05

0.00

3

10

68%

71.8%

91.9%

96.4%

1.96M

627.2K

552.6K

160K

69K

Figure 4: A pipeline of event ﬁlters applied to
extract meaningful failures from the network data
sources. The cumulative noise reduction at each
step is shown below.

A. Traﬃc Impact: Since the network is shared and we did
not have access to application logs, we estimate the failure
impact by leveraging network traﬃc data and computing
the ratio of the median traﬃc on a failed device/link dur-
ing a failure and its value in the recent past (preceding a
two hour window): a failure has traﬃc impact if this ratio
is less than one [17]. Note that this ﬁlter is applied after a
network alarm has been logged and a ticket opened to re-
solve the issue. Hence, it will not misclassify failures due
to the inherent time-varying nature of traﬃc e.g., time of
day eﬀects. We perform hypothesis testing to validate this
approach. Speciﬁcally, we use the Mann-Whitney-Wilcoxon
(MWW) [23] test for validation, which is a non-parametric
signiﬁcance test to compare two populations. The null hy-
pothesis to test is that the traﬃc values observed before a
failure and during a failure have identical distribution func-
tions. We randomly sampled 7k middlebox failure events
and 7k time points from periods during which no known
failures occurred. For the former, we obtained traﬃc values
up to eight hours before the failure and during the failure,
and up to eight hours before the event and up to two hours
afterwards for the latter.

Figure 5 plots the distribution of the p-value to quantify
the fraction of cases where we can reject the null hypothesis
at a signiﬁcance level of 0.05. The top ﬁgure shows that in
99.3% of the failures, p-value is less than 0.05; the remaining
cases had insuﬃcient traﬃc data points (e.g., due to best-
eﬀort logging). Hence the null hypothesis can be rejected
in 99.3% of the cases. Similarly, the bottom ﬁgure shows
that the distributions are identical during periods when no
failures occurred. Further, a time window size of two hours
and eight hours of traﬃc before the event yields the highest
and lowest accuracy, respectively. Indeed, under more strict
assumptions, a signiﬁcant MWW test can be interpreted as
showing a diﬀerence in medians [24]. Our prior work [17]
also correlated failures with traﬃc deviations, but it did not
evaluate its accuracy or specify the time window size before
a failure to compute the traﬃc ratio with highest accuracy.
As Figure 5 (top) shows, the accuracy is clearly dependent
on the time window size.

B. Device Malfunction Impact: To measure impact due to
incorrect device function, we leverage the information logged
by operators in trouble tickets to determine the problem
root causes when middleboxes fail. Speciﬁcally, we apply
NetSieve [25], an automated problem inference system that
analyzes the free-form text in a trouble ticket to generate
its synopsis: (1) the problems observed e.g., link down, mis-
conﬁgured rule, (2) troubleshooting performed e.g., check
conﬁguration, verify routes, and (3) actions taken for reso-
lution e.g., replaced line card, cleaned ﬁber.

32

Number of failures

100

316

1,000 3,162

Figure 3: CCDF plot showing that a small fraction
of devices log ≫ 100 failure events.
1. How can we diﬀerentiate noisy events from meaning-
ful failures? For instance, syslog messages can be spurious
where a device may log multiple ‘down’ events (even when it
is operational) or when neighbors may send redundant noti-
ﬁcations for the same device. Similarly, how to handle link
ﬂaps generating multiple and possibly overlapping down/up
messages which get logged as separate events?

2. Is it accurate to consider all failures to characterize reli-
ability? In particular, some down events are expected due to
routine and scheduled maintenance e.g., code update. Op-
erators aim to limit the planned downtime, and at the same
time, prioritize detection and mitigation of unexpected fail-
ures. While prior eﬀorts [16, 17] did not diﬀerentiate them,
“lumping” all events together risks inﬂating failure rates.

3. How to handle redundant failures from devices that keep
logging errors while undergoing repairs or being scheduled
for replacement? Figure 3 shows the CCDF for diﬀerent
types of LBs in our dataset with some devices logging more
than 1000 down messages over a few hours as the monitoring
system did not suppress them during triage. Such events
may bias measurement of device failures.

Event Filters: To address these challenges, we apply a
pipeline of four event ﬁlters (Figure 4): (1) timing ﬁlter, (2)
maintenance ﬁlter, (3) redundant failures ﬁlter, and (4) im-
pact ﬁlter.

1. Timing Filter: This ﬁlter ﬁxes various timing incon-
sistencies. First, it groups all events with the same start and
end time originating on the same interface to remove dupli-
cate events. Second, to avoid any problems due to clock
drifts (typically small) and log buﬀering, multiple events
within a 60 second window on the same interface are grouped
into a single event by picking the earliest start and end times.
If two events on the same interface have the same start time
but diﬀerent end times, we group them into a single event
and assign the earliest end time as events may not always
be quickly cleared after being ﬁxed.

2. Maintenance Filter: We use the maintenance track-
ing system to ﬁlter failure events due to planned mainte-
nance. As operators likely have a good understanding of
scheduled repairs, this ﬁltering enables computing device re-
liability only due to unexpected or unplanned problems.

3. Redundant Failures Filter: To ﬁlter redundant fail-
ures, we group events based on their unique trouble ticket
identiﬁers as events within the same ticket are typically part
of the common problem. We validated these events with op-
erators assigned to trouble tickets for these failures.

4. Impact Filter: We measure the impact of a failure in
two ways based on our deﬁnition of a middlebox failure: (a)
traﬃc loss and (b) device malfunctioning.

12Table 2: Annual number of failures per middlebox and annualized failure rate (AFR) for devices that were
present for at least a month in production.

Type Mean
FW
IDPS

2.1
3.5
1.5
3.4

LB
VPN

Stddev

25P

50P

75P

95P

COV

AFR% (July 2010-11) AFR% (July 2011-12)

2
2.9
0.9
2.2

1
1
1
2

2
1
1
3

3
6
2
5

5
8.3
3
7

0.9
0.8
0.6
0.7

19
23
31
7

13
18
19
12

]
x
<
X
P

[

1.00

0.75

0.50

0.25

0.00

1 hour
1.5 hours
2 hours
4 hours
6 hours
8 hours

significance

level of 0.05

FW IDPS

LB

VPN

-4

10

-3

10

-2

10

p−value (log scale)

-1

10

0

10

0

5

10

15

Number of Failures

20

Comparing traffic before and during failure periods

1.0

0.8

0.6

0.4

0.2

]
x
<
X
P

[

1.0

0.8

0.6

0.4

0.2

]
x
<
X
P

[

significance

level of 0.05

-4

10

-3

10

-2

10

-1

10

0

10

p−value (log scale)

Comparing traffic during non−failure periods

Figure 5: MWW test: p-value < 0.05 in 99.3% of the
failures (top), and p-value > 0.05 in 99.2% of the
events during no failure periods (bottom). Hence
the null hypothesis can be rejected.

Our dataset comprised 1.96M raw events collected over
two years for FWs, IDPSes, LBs and VPNs. Note that the
high-severity incidents described in Figure 1 is a very small
fraction compared to this dataset; we use the latter for anal-
ysis throughout the rest of this paper. Applying the pipeline
of ﬁlters on this data resulted in about 17k events from the
traﬃc ﬁlter and about 52k events from the device malfunc-
tion ﬁlter. To ensure 100% coverage of impactful failure
events, we veriﬁed that our ﬁltered dataset includes all the
events recorded in tickets deemed ‘actionable’ by operators.
Since typically multiple events logged close in time for a de-
vice attribute to a common problem and hence are likely
to be merged in the same ticket, the number of tickets is
signiﬁcantly smaller than the number of events.

3. FAILURE CHARACTERIZATION

In this section we characterize reliability of the four types
of middleboxes in our dataset: ﬁrewalls, IDPS, VPNs each
spanning two generations (FW[1-2], IDPS[1-2], and VPN[1-
2], respectively), and load balancers spanning three of them
(LB[1-3]). For a middlebox type, its diﬀerent generations
are from the same network vendor, and they are ordered by
their release date (1 is oldest). Where applicable, we ﬁrst
provide an inter-type comparison and then compare devices
across diﬀerent generations within a type.

Figure 6: Failures per device per year for middle-
boxes.
3.1 How reliable are middleboxes?

Figure 6 shows the CDF of the number of failures per de-
vice per year for devices with at least a month in operation.
Most devices experience few failures per year with a median
value of at most three (Table 2), and they occur with low
variability (COV < 1: COV [26] is deﬁned as the ratio of
standard deviation to the sample mean where a COV >> 1
indicates high variability) as expected in a service provider
network carrying customer traﬃc. Across middlebox types,
IDPS devices show relatively highest annual failures per de-
vice with a 95th percentile value of 8.3 while LBs exhibit
lowest with a 95th percentile value of 3. The distribution
for ﬁrewalls have a relatively long tail of up to 24 annual
failures per device compared to other types.

We next compute the annualized failure rate (AFR) by
dividing the number of devices of a given type that observe
at least one failure by the population of that type.

Inter-type: Table 2 shows the AFR for diﬀerent device
types during July 2010-11 and July 2011-12. We observe
that LBs are the least reliable with about a 1 in 3 chance of
failure during July 2010-11 which improved to about a 1 in
5 chance in the next year. The reason for this improvement
is due to on-boarding of the more reliable LB-3 platform in
the deployment and the end-of-life retirement of some of the
older LB-1 devices. FWs and IDPSes also exhibit improve-
ment in AFR over the two years. VPNs show relatively the
lowest AFR amongst all middleboxes.

Intra-type: Figure 7 shows the AFR and Figure 8 shows
the fraction of failures and downtime, respectively, across
diﬀerent generations of each middlebox type. We ﬁnd that
during the ﬁrst year, FW-2 exhibits relatively low failure
rate of 14% whereas FW-1 exhibits 2x higher failure rate
at 32%. Through trouble tickets, we found that the relia-
bility of FW-2 started improving when the vendor released
an end-of-life notice for FW-1 and provided faster bug ﬁxes
and better technical support to handle FW-2 failures. Fig-
ure 8 shows that FW-2 contributed majority to the total
failures due to its dominant population while FW-1 devices
decreased due to being decommissioned. This observation

13FW

2010−11
2011−12

14

12

32

21

)

%

(
 

R
F
A

100

75

50

25

0

LB

2010−11
2011−12

31

18

34 37

29

16

100

75

50

25

0

100

75

50

25

0

VPN

2010−11
2011−12

 9

13

 7

11

IDPS

2010−11
2011−12

28

16

33

21

100

75

50

25

0

FW−1

FW−2

LB−1

LB−2

LB−3

VPN−1

VPN−2

IDPS−1

IDPS−2

Figure 7: Comparing annualized failure rate across middleboxes over the two years measurement period.

combined with the relatively low AFR of FW-2 (Figure 7)
indicates that a few devices contributed the most, due to
limited device familiarity of operators with the new platform
and unexpected hardware problems. Further, the number of
failures contributed by FW-2 is roughly proportional to its
downtime while they are relatively higher for FW-1. This in-
dicates that most FW-1 failures seem to be relatively short-
lived and resolved through reboot or rule reconﬁguration.

In the case of load balancers, LB-1 exhibits a large AFR
and contributes signiﬁcantly (Figure 8) toward the total
number of failures and downtime. However, note that in
comparison to other LBs, the fraction of failures contributed
is higher than the downtime indicating that most problems
are short-lived. We validate this observation by using the
time-to-repair (TTR) plots (Figure 9(c)) where a short TTR
indicates transient problems. We ﬁnd that a majority of
these problems were due to link ﬂapping i.e., interface con-
tinually goes up and down due to a bad cable, faulty line
card, etc. In most cases, the network engineer adjusts/re-
seats the cable/network card or the problem gets self-corrected.
LB-2, on the other hand, exhibited an increased AFR indi-
cating that this generation experienced new types of prob-
lems such as unexpected reboots. However, these problems
seem to have been resolved in its successor, LB-3, which ex-
hibits relatively the highest reliability. Similar to the case
of ﬁrewalls, note that the signiﬁcant improvement in LB-3
reliability occurred due to the vendor releasing an end-of-life
notice for older generations, as we observed through tickets.
Across the four middlebox types, VPN devices exhibit rel-
atively higher reliability with failure rates between 7%-9%
which increased slightly year over year to 11%-13%. The
increase in AFR over time is due to (a) the older genera-
tion VPN-1 which was being decommissioned exhibiting an
increasing number of connectivity errors, and (b) the newer
generation VPN-2 exhibiting the infant mortality eﬀect e.g.,
memory errors. Further, the failures contributed is roughly
proportional to the downtime caused by both VPN-1 and
VPN-2 suggesting that each failure roughly causes the same
downtime on average. We ﬁnd a common root cause of VPN
failures to be VLAN misconﬁgurations. Similar to other
middleboxes, we also observed an end-of-life announcement
for VPN-1 recently.

Findings (1): (1) Firewalls and IDPS devices exhibit annual-
ized failure rate of 19% and 23%, respectively, which improved
to about 13% and 18% year over year due to timely bug ﬁxes
and decommissioning of older generation of devices. (2) About
1 in 3 load balancers experience a failure and exhibit the high-
est annualized failure rate during July 2010-11, which improved
next year to about a 1 in 5 chance of failure due to new gener-
ations exhibiting relatively higher reliability. (3) VPN devices
exhibit the relatively lowest annualized failure rate across mid-
dleboxes.
(4) When vendors release an end-of-life notice for
an old device generation, it often results in improved reliabil-

ity of newer generations because of faster bug ﬁxes and better
technical support.

3.2 How often do middleboxes fail?

We deﬁne the time to failure (TTF) of a device as the time
elapsed between two consecutive failures. As this metric
requires that a device fail at least twice, we exclude devices
having a single failure during our measurement period.
Inter-type: Figure 9(a) shows the TTF across middle-
boxes. We observe that the lines of FWs and LBs nearly
overlap exhibiting a high similarity with a median TTF of
7.5 hours and 5.2 hours, respectively. Many FW failures
are caused due to connectivity errors, hardware faults and
overload conditions (Figure 11). Similarly, LB failures are
mostly due to short-term transient faults, hardware and par-
ity errors e.g., due to incompatible protocol versions between
devices. For IDPS, the median is about 20 minutes while the
95th percentile is around 40 days. This indicates that even
amongst devices that fail multiple times, there are two types:
(a) robust ones that fail at most once in one or two months
and (b) failure prone ones that experience numerous failures
(the few bad apples eﬀect), mostly within a few hours. In
comparison, VPN devices show relatively the smallest me-
dian TTF of about 10 minutes due to connectivity errors
(Figure 11) and 95th percentile of about three days.

In terms of availability, Figure 9(b) shows that FWs ex-
hibit a median of four 9’s and a 95th percentile of two
9’s. VPNs perform slightly better with their 95th percentile
above two 9’s. While the median value for LBs is above four
9’s, their 95th percentile is two 9’s due to all three LB[1-3]
generations exhibiting high AFR over the two year measure-
ment period as observed in Figure 7.

3.3 How long do repairs take?

We deﬁne the time to repair (TTR) for a device as the time
between a down notiﬁcation for a device and when it is next
reported as being back online. There are two main types of
failures: short-lived transient faults where an engineer may
not always intervene to resolve them and long-term failures
where the device or its component is usually replaced or de-
commissioned. Note that for long-term failures, the failure
durations may be skewed (up to days) by when the trouble
ticket got closed e.g., till a spare is on-site. Figure 9(c) shows
the TTR results. The 95th percentile values show that the
distribution has a long tail for both LBs and FWs. Using
problems inferred from trouble tickets, we found several inci-
dents for device replacements and ambiguous customer com-
plaints which resulted in an increased TTR. For instance, in
several cases, after receiving vague complaints from many
customers about occasional slow ﬁle-transfer rates between
two datacenters, determining the rootcause was challenging
as the problem could not be easily reproduced. The root-

14FW

99

93

Downtime
Failures

t
n
e
c
r
e
P

90

60

30

0

 1

 7

FW−1

LB

Downtime
Failures

27

15

28

13

63

41

90

60

30

0

90

60

30

0

FW−2

LB−1

LB−2

LB−3

VPN−1

VPN

98

90

Downtime
Failures

10

 2

VPN−2

90

60

30

0

IDPS

99

78

Downtime
Failures

22

 1

IDPS−1

IDPS−2

Figure 8: Comparing fraction of failures and downtime across middleboxes.

1.00

95th Percentile

1.00

95th Percentile

1.00

95th Percentile

0.75

]
x
<
X
P

[

0.50

0.25

0.00

5 minutes

Median

1 hour

1 day 1 week

1e+02

1e+05
Time Between Failures (s)

(a)

FW

IDPS

LB

VPN

0.75

]
x
<
X
P

[

0.50

0.25

0.00

five 9’s 

(0.09 hours)

Median

FW

IDPS

LB

VPN

three 9’s 

(8.77 hours)

four 9’s 

(0.88 hours)

two 9’s 

(87.66 hours)

5 minutes

Median

0.75

]
x
<
X
P

[

0.50

0.25

0.00

1e+08

1e+02

1e+04

1e+06

1e+01

Annualized Downtime (s)

(b)

FW

IDPS

LB

VPN

1 hour

1 day

1 week

1e+03
Time To Repair (s)

1e+05

1e+07

(c)

Figure 9: Comparing (a) Time to Failure, (b) Annualized downtime, and (c) Time to Repair across middle-
boxes.

cause, determined after three weeks, was a software bug that
prevented some FWs from recognizing large ﬂows.

Across device types, middleboxes exhibit typically a low
median TTR, indicating short-lived failures that get resolved
within a few minutes to at most a few hours in most cases.
The short-lived failures for LBs are due to interface ﬂaps
and software problems (e.g, OS watchdog timer triggering
a reboot) and the long-lived failures to delays in equipment
replacement (due to spare unavailability), faulty spares and
problem misdiagnosis. VPNs exhibited mostly link ﬂapping
and connectivity errors which get resolved quickly (e.g., pro-
tocol convergence, port disabled) resulting in a small TTR.

Findings (2): Middleboxes exhibit a low median TTR, sug-
gesting most failures are short-lived due to connectivity errors
and link ﬂaps that get resolved within a few minutes to a few
hours.

3.4 How to model failures and repairs?

Building statistical models of middlebox failures is impor-
tant as researchers can use them to understand their fail-
ure modes, analyze a large number of network design solu-
tions [14, 16, 27], and derive implications to improve fault de-
tection and troubleshooting operations. Although this sec-
tion discusses the speciﬁc case of LBs, note that our analysis
can be extended to other types of middleboxes.

Before modeling the failure distribution, we need to ﬁrst
characterize their statistical properties. As Figures 9(a) and
9(c) show, the range of TTF and TTR durations can be sev-
eral orders of magnitude across devices. To reduce this high
variance, we use the Box-Cox transformation [28], a family of
parametric power transformation techniques that transform
data such that it approximately follows the normal distri-
bution. Using this technique, we found that the logarithmic
power transform yielded the best normal ﬁt for our failure
data which we veriﬁed using a Q-Q plot (not shown).

Figures 10(a) and 10(d) show the kernel density plot of
the log-transformed TTF and TTR respectively. We observe
that the distributions are right skewed and are clearly not

unimodal (observe the peaks). To ﬁnd these peaks automat-
ically, we use a local peak ﬁnding algorithm where a peak
is deﬁned as the locally highest value in a time series, be-
fore a change in signal direction has occurred. Observe that
the main peaks occur at 15 minutes for TTF (Figure 10(a))
and 5.6 minutes for TTR (Figure 10(d)). Further, there are
secondary peaks, which are relatively small, at 14 days for
TTF and 24 hours and 2.5 days for TTR.

These ﬁndings indicate that there are two or three qual-
itatively diﬀerent types of failures and repairs in eﬀect, re-
spectively. For TTF, the peak at 15 minutes indicates that
connection errors and interface ﬂaps dominate middlebox
failures, and the secondary peak at 14 days indicates prob-
lems due to both hardware faults (e.g., line card, memory
errors) and software bugs. For TTR, most repairs take a
short time (< 6 minutes), but once the repair window ex-
ceeds a day (the 24 hours peak), it will likely take longer
e.g., hardware replacement.

Failure Modeling: Based on prior work [14, 29], we ﬁrst
tried to ﬁt existing heavy-tailed distributions to our failure
data. However, they did not capture the multi-mode prop-
erties of our dataset. For instance, in the case of lognormal
distribution, the Q-Q plots in Figures 10(b) and 10(e) show
that the data does not follow the absolute line.

Intuitively, the presence of multiple peaks indicates that
a two or three-component mixture of right-skewed distribu-
tions might be able to provide a good approximation of the
process generating the data. We next present the mixture
model framework [30] we used. Suppose the real-valued vari-
ables X1, ..., Xn are a random sample of time periods from
a ﬁnite mixture of m(> 1) arbitrary distributions, called
components. The density of each Xi may then be written
as:

m

hθ(xi) =

X

j=1

λjφj (xi), xi ∈ Rr

(1)

where θ = (λ, φ) = (λ1, ...λm, φ1, ..., φm) denotes the model
parameter and Pm
j=1 λm = 1. We further assume that φj

15y
t
i

s
n
e
D

0.09

0.06

0.03

0.00

0.2

0.1

y
t
i

s
n
e
D

0.0

0

15.1  minutes

13.9  days

1e+08

1e+05

1e+02

a
t
a
D

 
l

a
c
i
r
i
p
m
E

0
Time to Failure  (log transformed)

10

15

5

20

1e+01

1e+05

1e+09

Theoretical Data

(a)

(b)

5.6  minutes

2.5  days

23.3  hours

1e+06

a
t
a
D

 
l

a
c
i
r
i
p
m
E

1e+04

1e+02

y
t
i
s
n
e
D

y
t
i
s
n
e
D

2
1

.

0

8
0

.

0

4
0

.

0

0
0

.

0

0
2

.

0

0
1

.

0

0
0

.

0

5

10

15

Time to Repair  (log transformed)

1e+01

1e+03

1e+05

1e+07

Theoretical Data

0

5

10

15

Time to Failure (log transformed)

(c)

2

4

6

8

10

12

14

Time to Repair (log transformed)

(d)

(e)

(f)

Figure 10: Modeling Time to Failure [a-c] and Time to Repair [d-f ] for load balancers. Figures (a, d) show
the kernel density plot of the log transformed TTF and TTR. Figures (b, e) show ﬁt of a single log-normal
distribution. Figures (c, f ) show ﬁt of a mixture of log-normal distributions.
are drawn from some family F of univariate density func-
tions on R. We consider a univariate lognormal family
F = {φ(·|µln, σ2
ln}, in which µln and σln denote the mean
and standard deviation in log scale. Thus, the model param-
eter reduces to θ = (λ, (µln1 , σ2
lnm )). By
substituting these parameters, Equation (1) can be written
as:

Table 3: Parameters for the two-component lognor-
mal mixture distribution

0.516284
TTF
TTR 0.428609

ln1 ), ..., (µlnm , σ2

14.210978
8.524983

2.090600
2.541691

6.557005
6.045474

1.963986
0.690533

µln1

σln1

Type

λ

µln2

σln2

hθ(xi) =

m

X

j=1

λj

1

σlnj xi√2π

−

e

(ln(xi)−µlnj

)2

2σ2

lnj

, xi ∈ Rr

(2)

We obtained a reasonable ﬁt by choosing a two-component
(m = 2) lognormal mixture, where our model from Equa-
tion (1) becomes: λf (µln1 , σln1 ) +(1 − λ)f (µln2 , σln2 ). To
obtain the model parameter λ, we use the well-known Expec-
tation Maximization(EM) [31] algorithm. Table 3 gives the
values of the parameters for the two-component lognormal
mixture distribution. We use the two-sample Kolmogorov-
Smirnov (KS) test [32] as a goodness-of-ﬁt test to compare
data generated from the two-component lognormal mixture
distribution with our failure data. We obtain a p-value of
0.111 indicating that we cannot reject the null hypothesis
that the samples are drawn from the same distribution.

Figures 10(c) and 10(f) show how this model ﬁts our data;
the dotted-line is the kernel density curve of our data and
the solid lines are the individual mixture components. We
observe that our model closely approximates the real-world
data of load balancer failures in our study.

4. ROOT CAUSE ANALYSIS

In this section, we analyze the root causes of middlebox

problems and the actions taken to mitigate them.

4.1 What are the dominant problems?

As described in Section 2.3, we apply NetSieve to do auto-
mated problem inference on network trouble tickets [25] for
determining the problem root causes when middleboxes fail.
Table 4 shows the classiﬁcation of the problems into ﬁve cat-
egories based on the ticket data and their example causes.
Note that there are two potential issues to be considered in
interpreting these categories. First, a problem observed in
one category can be due to issues in another one. For in-
stance, parity errors in VPNs indicate failed sanity checks
which may be caused due to misconﬁgured pre-shared keys.
Another example is that a malformed message could be due
to incompatible protocol versions between devices. Second,
in some cases, there may be multiple problems pertaining
to the same failure. In these cases, we take a conservative
approach and attribute each observed problem to its respec-
tive category.

Inter-type: Figure 11 shows the breakdown of problems
observed across the four types of middleboxes. We ﬁnd that
connectivity errors dominate failures across all middleboxes
as also validated from the results in Section 3. Connectiv-
ity errors are mainly interface/link ﬂaps which can be due
to many factors such as protocol convergence, line card er-
rors, cpu overload, bit corruption and cable problems. As a

16Table 4: Classiﬁcation and examples of problem types observed across middleboxes.

Problem Classiﬁcation Example causes or explanation

Connectivity errors

Hardware

Misconﬁguration

Software
Overload

Device/neighbor unreachable, link ﬂaps, ARP conﬂicts, address not found, message/frame parity errors,
checksum failed, malformed message, port errors
faulty PSU/ASIC, memory error, defective chassis/disk/fan, failed SUP engine/line card
bad rule, VLAN misallocation, conﬁguration conﬂict/out-of-sync/corrupted/missing, mismatched crypto
keys, expired security association, incorrect clock causing certiﬁcate expiry, incorrect root password
reboot/reload, ﬁrmware bug, OS errors
High utilization exceeding a speciﬁed load threshold

78

55 57

39

t
n
e
c
r
e
P

90

60

30

0

Firewall

IDPS

LB

VPN

25

28

19

1

12

4

13

15

19

7

4

4

5

4

7

1

Connectivity errors

Hardware

Misconfiguration

Overload

Software

Figure 11: Problem types observed across middleboxes.

result, these errors can signiﬁcantly degrade service perfor-
mance and availability due to TCP timeouts, convergence
delays, and retransmissions e.g., due to malformed packets
or parity errors. The second main root cause is hardware
problems except for VPNs where it accounts for only 1% of
the problems. One implication of high percentage of hard-
ware problems is long repair times as the faulty component
or the device gets replaced. Another one is that this obser-
vation is in direct contrast to the industry view of equating
hardware boxes with high reliability.

VPN problems are dominated by parity errors which may
be attributed to faulty cables (e.g., copper, SFP ﬁber op-
tics) and their common root causes are power surge, bad
connectors, or a short circuit. For these problems, we ob-
served mainly two repair actions of either cleaning the ﬁber
or replacing them.
In comparison, misconﬁgurations and
overload problems are relatively in minority, contrary to the
common view of being a dominant contributor [6].

Table 4 (row 3) gives the common categories of miscon-
ﬁgurations across device types. While bad rules as a com-
mon root cause (≈70% cases) are expected, there are a wide
variety of other problems such as VLAN misallocation, cor-
rupted or missing conﬁguration ﬁles, mismatched crypto-
graphic keys, and expired certiﬁcates.

Across middleboxes, misconﬁgurations contribute lowest
to IDPS problems due to the fact that centralized man-
agement in a datacenter continuously monitors and imme-
diately enforces policy changes (e.g., security access) upon
violation. Software problems contribute the minimum per-
centage to problems indicating that the software running on
these devices exhibits reasonable stability. Finally, software
problems are ﬁxed relatively quickly e.g., via code update or
power cycling the device.

Intra-type: Figure 12 shows the problems observed split
by diﬀerent generations for each of the four types of mid-
dleboxes. For Firewalls, FW-1 exhibits lower hardware and
software problems compared to FW-2. However, the frac-
tion of misconﬁgurations and connectivity errors is higher,
due to better conﬁguration tools and more reliable port con-
nectors and cables for the new platform FW-2. Surprisingly,

even though FW-2 has a higher capacity compared to FW-
1, it experiences a high fraction of overload induced fail-
ures which are not observed in FW-1. Unlike ﬁrewalls, for
IDPS devices, the port errors increased with the newer de-
vice IDPS-2, while contribution of the other problem types
reduced. For load balancers, LB-3 exhibits fewer problems
due to hardware and software improvements compared to its
predecessors, LB-1 and LB-2. In sharp contrast to trends
observed across FWs, IDPSes and LBs, VPN-2 exhibits a
higher percentage of hardware faults and overload problems
compared to VPN-1 due to the instability of the newer plat-
form.

Overall, these trends provide a ﬁne-grained analysis of
what aspects of a new platform are improving over its pre-
decessor and vice versa, to guide researchers and vendors in
developing better designs and tools to improve middleboxes.

Findings (3): (1) Connection errors and interface problems
dominate causes of middlebox failures, and (2) Hardware, mis-
conﬁgurations and overload problems are present, but they are
not in majority.

4.2 What are the main resolution actions?

Figure 13 shows the breakdown of actions taken to miti-
gate middlebox failures. The nine categories show the res-
olutions applied to faulty components at a ﬁne granularity.
For instance, the cable category implies that the cable was
either cleaned or replaced. The software category denotes
that either a patch was applied or the code was upgraded
to the latest stable version. Similarly, the categories of dif-
ferent hardware components such as chassis, disk, fan, and
PSU denote that they were replaced using on-site spare or
sent back to the vendor for repairs, or warranty purposes.

We observe that across middleboxes, the most prevalent
actions taken are replacing the cable and disks, and reboot
to ﬁx software-related issues. The former can be attributed
to the relatively low-cost and high failure rates of these
commodity components as operators tend to replace them
quickly after observing a problem. In comparison, the de-
cisions to replace specialized components such as chassis,
supervisor engines, or even the device, requires a detailed

1769

46

35

71

57

45

58

86

44

Connectivity errors

6

33

28

19

25

4

9

5

0

10

23

0

FW−1

FW−2

0

5

IDPS−1

IDPS−2

6

5

18

24

17

4

8

16

10

11

4

10

12

5

LB−1

LB−2

LB−3

41

1
Hardware

10

3

Misconfiguration

10
2
Overload

VPN−2

VPN−1
5
2
Software

Figure 12: Comparison of problem types across diﬀerent generations of middleboxes.

Firewall

IDPS

LB

VPN

24

2127

21

8

4 6

0

0

0

11

0

2922

20

31

0

4 0

0

12

0 0

14

16

8 8

0

3326

21

20

0

0 0

7

0

0 0

7

Cable

Chassis Device

Disk

Fan

Line Card

PSU

Reboot Software Supervisor

Figure 13: Resolution actions observed across middleboxes.

t
n
e
c
r
e
P

t
n
e
c
r
e
P

t
n
e
c
r
e
P

t
n
e
c
r
e
P

90
60
30
0

90
60
30
0

90
60
30
0

90
60
30
0

t
n
e
c
r
e
P

90

60

30

0

t
n
u
o
C

Cumulative Devices

Cumulative Failures

Devices

Failures

LB−1

LB−2

LB−3

D
e
v

i

c
e
s

F
a

i
l

u
r
e
s

S e p − 1 0

D ec− 1 0

M ar− 1 1

Ju n − 1 1

N ov− 1 1

S e p − 1 0

D ec− 1 0

M ar− 1 1

Ju n − 1 1

N ov− 1 1

S e p − 1 0

D ec− 1 0

M ar− 1 1

Ju n − 1 1

N ov− 1 1

Figure 14: Failure trends across diﬀerent genera-
tions of load balancers. The y-axes are unlabeled
for conﬁdentiality.

investigation due to their high cost and long turnaround
time from their vendor. The reason for a high percentage of
reboots is that it is a quick-ﬁx solution in many cases and
hence run as a popular ﬁrst attempt to mitigate a problem
to reduce downtime.

Findings (4): (1) Cables and disks are the most failing com-
ponents, and (2) Device reboot is a popular quick-ﬁx solution
to mitigate middlebox problems.

5. FAILURE TREND ANALYSIS

In this section we analyze failure trends across diﬀerent
device generations of middleboxes. We use trend analysis as
it is a useful tool to uncover outliers i.e., set of devices which

0.6

y
t
i
s
n
e
D

0.4

0.2

LB−1

LB−2

LB−3

0.0

0

5

10

15

Failures

20

25

Figure 15: Distribution of load balancer failures.

are more failure-prone than the rest of the population. We
present detailed case studies on LB[1-3].

Figure 14 shows the timeline for the number of failure
events and the number of devices that failed; the Y-axis is
on log scale. Each subplot shows two curves: a cumulative
curve that sums up the number of failures (or devices that
failed) across time and the other showing the actual number
of failures (or devices that failed). For LB-1, the number of
failure events far exceeds the number of devices that failed
indicating that a few bad devices are responsible for a ma-
jority of their failures.

Validation. We validate this inference by computing two
statistical metrics from the failure distribution (Figure 15).
(1) skewness [33], an indicator used in distribution analy-
sis as a sign of asymmetry where positive values indicate a
right-skewed distribution and vice versa, and (2) COV. The
observed skewness values are 2.2 (LB-1), 1.5 (LB-2) and 3.9
(LB-3), indicating that the distributions for LB-1 and LB-3
are right-skewed (tail on the right). Though the COV across
all LBs is 0.6 (Table 2), the COV for LB-1 is 5.8 whereas it is
1.3 for LB-3 indicating that even though both have a right-
skewed distribution, the “few bad devices” eﬀect is higher
in LB-1. Further, in Figure 14, the slope of the cumulative

18curve indicates the rate of increase of the observed value.
Sudden increase in the slope indicates major problems dur-
ing that period of observation. A key root cause of this
behavior is connectivity problems as observed in Figure 12.
Combining failure rates with trouble tickets, trend analy-

sis revealed two key issues:

1. Early problem symptoms: Inference on the prob-
lems observed in trouble tickets for LB-1 devices revealed
a recurring PSU problem. Aggregating problem statistics
across LB-1 tickets indicated that 46% of the tickets called
for a device replacement due to a narrow focus of operators
in mitigating a problem (compared to its diagnosis) and lim-
ited tools at their disposal in checking vendor-speciﬁc prob-
lems. In retrospect, a simple approach of aggregating root
causes across a device generation can identify the problem
patterns. In this case, failing PSUs could have been used as
early indicators of problems with the product line. A Cost
of Ownership analysis (§7) can then help decide whether
to continue using LB-1 or gracefully upgrade it to a newer,
more stable generation.

2. The few bad apples aﬀect: LB-1 suﬀers from the few
bad apples eﬀect i.e., the rate of increase of the slope of the
cumulative count of failures curve is higher than the rate of
increase of the slope of the cumulative count of devices curve.
This indicates that only a few devices are contributing to a
majority of failures and is validated by high COV value for
LB-1 of 5.8. There were several reasons why this happened.
First, we observed a re-occurring fault using LB-1 trouble
tickets with the regulated PSU, despite being redundant,
that frequently caused power ﬂuctuations leading to dam-
aged parts. Even if the damaged parts are replaced, unless
the fault is localized to the power supply unit, the problem
likely will repeat itself. Second, a memory bit corruption
(e.g., SDRAM ECC error) kept being masked temporarily
by the quick-ﬁx solution of reboots as observed in Figure 13.

These observations can be leveraged to (a) prioritize re-
placement of frequently failing devices and scheduling of re-
pair actions based on the top-k observed problems, and (b)
aid repair vs. replace analysis by combining early problem
symptoms with a cost of ownership metric. Finally, fre-
quently occurring hardware problems can aid in detecting
faults at the vendor.

Findings (5): (1) A signiﬁcant number of failures in one gener-
ation of load balancers exhibited the few bad apples eﬀect, and
(2) A family of load balancers having faulty PSUs could have
been detected as early problem indicators using trend analysis.

6. REDUNDANCY ANALYSIS

In this section we ﬁrst analyze the loss of traﬃc due to
middlebox failures and then evaluate the eﬀectiveness of re-
dundancy in masking them.

6.1 How much trafﬁc loss do failures cause?

Quantifying the impact of a failure is diﬃcult as it re-
quires attributing discrete outage levels to annotations used
by network operators such as severe, mild, or some impact.
As described in Section 2.3, we estimate the failure impact
in terms of lost network traﬃc that would have been routed
on a failed device in the absence of failure. Speciﬁcally, we
ﬁrst compute the median traﬃc (bytes/sec) on the device

two hours before the failure and during the failure. The
traﬃc loss is estimated as product of the failure duration
and the diﬀerence between the two median traﬃc values.

Figure 16(a) shows the estimated traﬃc loss across the
three LB generations. We observe the estimated median
number of bytes lost during failures is about 1 GB for LB-
1 (exhibiting a relatively high failure rate as seen in §3.1
and §5) while it is close to 5 MB for newer LB-3 generation
devices, which have a low failure rate. Notice that LB-2,
which also exhibited high failure rates compared to LB-3,
shows a median traﬃc loss close to LB-1.

6.2 How effective is middlebox redundancy?

We next analyze how eﬀective is middlebox redundancy,
a de-facto technique for fault tolerance,
in masking fail-
ures. Typically, middleboxes are deployed in 1:1 redundant
groups, where one device is designated the primary and the
other as backup. Based on the failure deﬁnition in Sec-
tion 2.3, we evaluate redundancy eﬀectiveness for LBs based
on the traﬃc impact, and based on device malfunctioning for
the other middlebox types.

Evaluating LB redundancy. To estimate the eﬀective-
ness of LB redundancy, we ﬁrst compute the ratio of me-
dian traﬃc on a device during a failure and its median traf-
ﬁc before the failure, and then compute this ratio across all
devices in the redundancy group where the failure occurred.
Network redundancy is considered 100% eﬀective if this ra-
tio is close to one across a redundancy group. We refer to
this ratio as normalized traﬃc [17].

Figures 16(b) and 16(c) show the distribution of normal-
ized traﬃc for individual LBs and their redundancy groups.
We observe that the redundancy groups are eﬀective at mov-
ing the ratio close to one with 40% of the events experiencing
no failure impact at the redundancy group level. The me-
dian traﬃc carried at the redundancy group level is 92%
compared with 55% at the individual level, a relative im-
provement of 67.2% in median traﬃc as a result of middle-
box redundancy.

Evaluating redundancy for other middleboxes. To
analyze redundancy eﬀectiveness for FWs, IDPSes, and VPNs,
we measure the fraction of overlapping failure events where
both the redundant pairs were down simultaneously. In par-
ticular, after one of the pair fails, we check if the other de-
vice also undergoes a failure before the former becomes op-
erational. Our analysis of redundancy eﬀectiveness across
FWs, IDPSes and VPNs revealed that both the redundant
pairs failed in 33% and 1% of the failure events for FWs
and IDPS, respectively.
In comparison, VPN redundancy
exhibits 100% eﬀectiveness in mitigating failures.

Analyzing unsuccessful redundancy failover. Based
on applying NetSieve [25] on trouble tickets, we observed two
main reasons why the redundancy failover was not successful
for LBs and FWs in masking the failure impact:

• Faulty Failovers: The primary device failed when the
backup was experiencing an unrelated problem and hence
led to a failed failover. In other cases, protocol bugs, bad
cables, and software issues such as version incompatibility
caused an unsuccessful failover, often sending devices into
a deadlock state.

191.00

0.75

]
x
<
X
P

[

0.50

95th Percentile

Median

0.25

0.00

LB−1
LB−2
LB−3

s
e
t
y
b
#
d
e
z
i
l

 

 

a
m
r
o
N
n
a
d
e
M

i

1.00

0.75

0.50

0.25

0.00

1e−04

1e−01

1e+02

1e+05

Traffic Loss in GB (log scale)

(a)

LB−AGG

Topology Level

(b)

Individual

Redundancy Group

0.92

Individual

Redundancy Group

0.55

1.00

0.75

]
x
<
X
P

[

0.50

95th Percentile

Median

0.25

0.00

0.00

0.25

0.75
Traffic during/Traffic before

0.50

1.00

(c)

Figure 16: (a) Estimated traﬃc loss during failure events for LBs, (b) Normalized traﬃc (bytes) during
failure events for a device as well as within a redundancy group and (c) Normalized bytes (quartiles) during
failure events per device and across redundancy group compared across LBs.
• Misconﬁgurations: The same conﬁguration error was
made on both primary and backup devices. Primary rea-
sons were due to copy-paste of the conﬁguration ﬁles, mis-
matched cryptographic keys, and expired certiﬁcates.

Findings (6): (1) Middlebox redundancy is ineﬀective in 33%
of the cases for load balancers and ﬁrewalls, and (2) Unsuc-
cessful redundancy failover is mainly due to misconﬁgurations,
faulty failovers and software version mismatch.

7. DISCUSSION

In this section, we discuss the research implications based

on our middlebox reliability study.

Commoditize middleboxes: §1 and §3 indicated that
hardware middleboxes contribute signiﬁcantly to network
failures, and that their failovers are not 100% eﬀective (§6.2).
Therefore, a natural alternative is to commoditize them [11–
13] as they can enable a scale-out design and provide n:1
redundancy. While there has been promising work on soft-
ware routers [34], achieving high performance, scalability
and fault tolerance simultaneously poses several challenges.
First, achieving high performance and programmability are
often competing goals. For instance, the communication
overhead introduced by software LBs may impact perfor-
mance of latency-sensitive applications. Second, well-known
software issues such as code bugs, frequent updates, and
misconﬁgurations, may cause a commodity middlebox to be
less reliable than hardware boxes. Finally, running a large
number of commodity servers or VMs as middleboxes risks
high operational costs such as power and cooling. Eﬀorts
such as [35, 36] to reduce network energy footprint can be
explored to address them. In some cases, strict application
requirements for hardware accelerators such as hardware-
based deep packet inspection (DPI) for IDPS and deep-
packet modiﬁcation for WAN optimizers, may become a lim-
iting factor.
Middlebox veriﬁcation: §4 indicated that connection er-
rors and interface problems dominate middlebox failures,
and that there is a wide range of misconﬁgurations observed
on middleboxes. As a result, a key requirement for net-
work operations is to automate management and debugging
of network problems e.g., using SDN [37, 38]. To improve
network debugging, one approach is to do static and run-
time veriﬁcation of desired properties e.g., to check conﬁgu-
rations, verify reachability, detect routing loops, guarantee

isolation [37, 39, 40]. However, there are several challenges
that need to be addressed. First, current tools lack sup-
port for common middlebox features such as tunneling and
load balancing. Second, the task of parsing policy speci-
ﬁcations and conﬁgurations is non-trivial and error-prone,
as vendors use varying semantics which in-turn may change
with device generations. Third, checking for correctness of
policies involves designing a set of constraints that can be
tested against the device conﬁguration. However, deﬁning
a complete constraint set is diﬃcult and evaluating it, com-
putationally expensive [41]. Finally, the large-scale of data
center networks makes it challenging to apply formal veriﬁ-
cation techniques due to the well-known state space explo-
sion problem.
Detect early faulty product-line: In §5, we uncovered
using trend analysis that an LB generation had lower relia-
bility compared to other generations. This raises a serious
dilemma: keep replacing faulty devices in existing deploy-
ment or upgrade to a new product line? While capital and
operating costs dominate the cost vs. beneﬁt analysis, an-
swering this question is challenging for two additional rea-
sons:
• Device familiarity: Upgrading devices with their high-
speed, feature-rich counterparts will require additional
training for network engineers (and has the undesirable
consequence of increasing TTR due to unfamiliarity).
• Active/Standby Compatibility: Where 1:1 redundancy
is dominant, both devices should be simultaneously up-
graded to avoid unsuccessful redundancy failovers.

By computing a Cost of Ownership (COO) [42] metric, de-
cisions such as whether to buy more spares for a faulty prod-
uct line or to gracefully replace a product line can be made.
The primary cost elements of the COO model are: (i) ini-
tial investment in purchasing the product, (ii) depreciation,
(iii) cost of operation that includes space, personnel, and
power expenses, (iv) cost of maintenance operations includ-
ing downtime, repair cost, failure rates, and cost of spares,
and (v) enhancement costs that include hardware/software
upgrades and personnel training. Our reliability analysis at-
tempts to provide some of these metrics as described in §3.1
and §3.3.
Feasibility of making middlebox processing as a cloud
service: A recent proposal is to oﬄoad middlebox pro-
cessing to the cloud [6]. Similar to CDNs, the idea is to
use DNS-based redirection to route requests across multiple

20cloud PoPs and deploy middlebox instances in the cloud to
process them based on deﬁned policies. While this approach
shifts the burden of owning and managing middleboxes from
enterprises to the cloud, our study indicates that it risks
making the problem worse as middlebox failures are preva-
lent in datacenters (§3.1) and redundancy is not 100% eﬀec-
tive (§6.2) . Several other challenges also need to addressed
before pushing middleboxes as a cloud hosted service: (a)
how to guarantee correct middlebox traversal and coordi-
nate state during churn? (b) how to manage the state and
complexity for multiple tenants sharing the same set of mid-
dlebox instances? (c) how to address grey failures (§4.1) that
can degrade service performance and availability? and (d)
how to elastically scale-out and scale-in the processing capa-
bilities (e.g., for stateful services) as demand changes? For
instance, the stateful session nature of many middleboxes
(e.g., load balancers) can limit how quickly instances can be
(de)instantiated or risk a traﬃc spike as disconnected cus-
tomers attempt to simultaneously rejoin when an instance is
shutdown. To deliver traﬃc reliably in the presence of grey
failures, multi-path protocols such as MP-TCP [10] can be
explored.
Make the cost-metric of middlebox hardware relia-
bility aware: To compare network hardware costs across
platforms and vendors, the conventional metric is to use
$/Gb/s as the unit-cost basis e.g., $ per 10 Gbps port. For
instance, using this metric, we ﬁnd that hardware LBs are
about an order of magnitude more expensive than commod-
ity Layer 3 switches. As §3 shows, middleboxes experience
a signiﬁcant fraction of failures, and incur substantial op-
erating costs for repair and upgrade [6]. Thus, to balance
the trade-oﬀ between improving service availability while
cutting costs down, the cost metric should comprise both
capital and operational expenses, e.g., $/Gbps/99.9% avail-
ability, so that administrators can compute the total cost of
ownership (TCO) and determine the network costs to pro-
vide a target SLA (e.g., 99.9%), to hosted services. Note
that the accuracy of this metric needs to be continuously
validated and improved by leveraging data from middlebox
deployments in production.

8. RELATED WORK

Broadly, the research community has pursued two key di-

rections to build better middleboxes:

1. Middlebox Architectures: Research in this direction
proposes techniques for explicit control of the middleboxes
e.g., middlebox communication (MIDCOM) [4, 43–45] and
IETF Next Steps in Signaling (NSIS) [46]. The motivation
behind these eﬀorts draws from the fact that while opera-
tors increasingly require support for new applications (e.g.,
teleconferencing, cloud and mobile applications), they are
unable to use third-party software/hardware speciﬁc to the
application and are tied to their middlebox vendor to make
the necessary upgrade [4, 6]. There has been some recent
work [47, 48] in separating policy from reachability and cen-
tralized management of networks. Joseph et al. [2] proposed
a policy-aware switching layer for data centers comprising
inter-connected policy-aware switches that enable forward-
ing of diﬀerent types of traﬃc through a speciﬁed sequence of
middleboxes. dFence proposes adding DoS mitigation mid-
dleboxes on-demand on the data path to servers under at-
tack [27].

2. Middlebox Processing Analysis: Research in this
direction largely focused on either characterizing NAT prop-
erties such as mapping type and ﬁltering rules [49, 50] or
quantifying the end-to-end performance degradation induced
by middleboxes [16]. Our work falls into the latter category
of middlebox reliability analysis. We do not argue the bene-
ﬁts or shortcomings of current middlebox architectures but
instead focus on analyzing their performance.

Failures in datacenters have received signiﬁcant attention
in the recent years [51–56]. Wang et al. measure the impact
of middlebox policies on performance, energy and security in
cellular networks [15]. However, they did not study middle-
box failures. Our work is complementary to these eﬀorts in
that we focus on characterizing the device reliability across
middleboxes and understanding the causes and impact of
their failures.

Markopoulou et al. [54] studied failure in the Sprint back-
bone using passive optical taps and high-speed packet cap-
ture hardware. Their results indicated that 20% of all fail-
ures is due to planned maintenance and that maintenance
is usually scheduled during periods of low network usage to
minimize the impact on performance. As operators likely
have a good understanding of problems under scheduled
maintenance, we apply a planned maintenance ﬁlter on net-
work events to analyze device-level reliability due to unex-
pected outages.

Our work draws some similarity with the analysis carried
out by Turner et al. [56] and Gill et al. [17]. Turner et al. [56]
used router conﬁgurations, syslog and email records to an-
alyze network failures in the CENIC network that serves
educational and research institutions in California. Gill et
al. [17] study network failures in datacenters. Similar to us,
they observe that LBs exhibit high annualized failure rates.
While we share the broader goal of studying network fail-
ures with these eﬀorts, this work diﬀers in three important
ways driven by our goal of understanding the causes and
impact of middlebox failures. First, we focus on several new
questions to characterize middlebox reliability e.g., What
are the key problems observed and their resolution actions?
How prevalent are misconﬁgurations and what are their dif-
ferent types? Does reliability improve with new generations?
and How eﬀective is middlebox redundancy? that have not
been addressed in the past. Second, our methodology ﬁl-
ters maintenance events to identify “unexpected/unplanned”
outages, ﬁlters events for devices with redundant failures,
and performs hypothesis testing to validate traﬃc based im-
pact ﬁltering. Finally, our results provide useful guidelines
to improve middlebox reliability.

9. CONCLUSION

Middleboxes form a critical component of the network in-
frastructure in datacenters and enterprises, yet, there have
been few real-world studies on understanding the character-
istics of middlebox failures. In this paper, we make one of
the ﬁrst attempts to do a large-scale characteristic study of
real-world middleboxes to understand the causes and im-
pact of their failures in datacenters. Our study reveals sev-
eral surprising ﬁndings that diﬀer from some commonly held
views. We hope that this study motivates further research
to improve middlebox reliability. In future work, we aim to
leverage the middlebox failure models to predict their fail-
ures, develop techniques to improve middlebox redundancy,
and build better tools for conﬁguration management.

2110. ACKNOWLEDGEMENTS

We thank our shepherd Z. Morley Mao and the anony-
mous reviewers for their feedback. Moises Goldszmidt and
Nachiappan Nagappan provided invaluable help in verifying
the failure analysis methodology. We are grateful to David
St. Pierre for helping us understand the network data sets.

11. REFERENCES
[1] V. Sekar, S. Ratnasamy, M. Reiter, N. Egi, and G. Shi, “The

Middlebox Manifesto: Enabling Innovation in Middlebox
Deployment,” in HotNets, 2011.

[2] D. Joseph, A. Tavakoli, and I. Stoica, “A Policy-Aware

Switching Layer for Data Centers,” in SIGCOMM CCR, 2008.

[3] M. Walﬁsh, J. Stribling, M. Krohn, H. Balakrishnan, R. Morris,
and S. Shenker, “Middleboxes No Longer Considered Harmful,”
in OSDI, 2004.

[4] P. Srisuresh, J. Kuthan, J. Rosenberg, A. Molitor, and

A. Rayhan, “Middlebox Communication Architecture and
Framework,” RFC 3303, 2002.

[5] ABI, “Enterprise Network and Data Security Spending Shows

Remarkable Resilience,” http://goo.gl/t43ax, January 2011.

[6] J. Sherry, S. Hasan, C. Scott, A. Krishnamurthy, S. Ratnasamy,

and V. Sekar, “Making Middleboxes Someone Else’s Problem:
Network Processing As a Cloud Service,” in SIGCOMM, 2012.

[7] I. Week, “Data Center Outages Generate Big Losses,”

http://goo.gl/zBOfv, May 2011.

[8] “Why Gmail went down: Google misconﬁgured load balancing

servers,” http://goo.gl/NXTeW.

[9] “2011 ADC Security Survey Global Findings,”

http://goo.gl/A3b2Q.

[10] M. Scharf and A. Ford, “MP-TCP Application Interface

Considerations,” draft-ietf-mptcp-api-00, 2010.

[11] V. Sekar, N. Egi, S. Ratnasamy, M. Reiter, and G. Shi, “Design

and Implementation of a Consolidated Middlebox
Architecture,” in NSDI, 2012.

[12] H. Uppal, V. Brajkovic, D. Brandon, T. Anderson, and

A. Krishnamurthy, “ETTM: A Scalable Fault Tolerant Network
Manager,” in NSDI, 2011.

[13] A. Greenhalgh, F. Huici, M. Hoerdt, P. Papadimitriou,

M. Handley, and L. Mathy, “Flow Processing and the Rise of
Commodity Network Hardware,” SIGCOMM CCR, 2009.

[14] V. Liu, A. Krishnamurthy, and T. Anderson, “F10:

Fault-Tolerant Engineered Networks,” in NSDI, 2013.

[15] Z. Wang, Z. Qian, Q. Xu, Z. Mao, and M. Zhang, “An Untold

Story of Middleboxes in Cellular Networks,” SIGCOMM CCR,
2011.

[16] M. Allman, “On Performance of Middleboxes,” in IMC, 2003.
[17] P. Gill, N. Jain, and N. Nagappan, “Understanding Network

Failures in Data Centers: Measurement, Analysis, and
Implications,” in SIGCOMM, 2011.

[18] “Cisco Data Center Network Architecture,”

http://goo.gl/kP28P.

[19] J. Lockwood, “Open Platform for Development of Network

Processing Modules in Reprogrammable Hardware,” in
DesignCon, 2001.

[20] J. Case, M. Fedor, M. Schoﬀstall, and J. Davin, “Simple

Network Management Protocol,” SRI International Network
Information Center, May 1990.

[21] M. McCloghrie, K. ad Rose, “Management Information Base for

Network Management of TCP/IP-based internets,” RFC 1213.

[22] D. Johnson, “NOC Internal Integrated Trouble Ticket System,”

RFC 1297, 1992.

[23] H. Mann and D. Whitney, “On a Test of Whether One of Two
Random Variables is Stochastically Larger than the Other,” in
The Annals of Mathematical Statistics, 1947.

[24] A. Hart, “Mann-Whitney test is not just a test of medians:

diﬀerences in spread can be important,” 2001.

[25] R. Potharaju, N. Jain, and C. Nita-Rotaru, “Juggling the

Jigsaw: Towards Automated Problem Inference from Network
Trouble Tickets,” in NSDI, 2013.

[26] C. E. Brown, “Coeﬃcient of Variation,” in AMSGRS, 1998.
[27] A. Mahimkar, J. Dange, V. Shmatikov, H. Vin, and Y. Zhang,

“dFence: Transparent network-based denial of service
mitigation,” in NSDI, 2007.

[28] R. Sakia, “The Box-Cox Transformation Technique: A Review,”

in JSTOR Statistician, 1992.

[29] W. E. Leland, M. S. Taqqu, W. Willinger, and D. V. Wilson,

“On the Self-Similar Nature of Ethernet Traﬃc (extended
version),” in IEEE ToN, 1994.

[30] T. L. Bailey and C. Elkan, “Fitting a Mixture Model by

Expectation Maximization to Discover Motifs in Bipolymers,”
in ISMB, 1994.

[31] T. K. Moon, “The Expectation-Maximization Algorithm,” 1996.
[32] H. W. Lilliefors, “On the Kolmogorov-Smirnov Test for

Normality with Mean and Variance Unknown,” JASA, 1967.

[33] R. Bendel, S. Higgins, J. Teberg, and D. Pyke, “Comparison of

Skewness Coeﬃcient, Coeﬃcient of Variation, and Gini
Coeﬃcient as Inequality Measures within Populations,” in
Oecologia, 1989.

[34] K. Argyraki, S. Baset, B. Chun, K. Fall, G. Iannaccone,

A. Knies, E. Kohler, M. Manesh, S. Nedevschi, and
S. Ratnasamy, “Can Software Routers Scale?” in PRESTO,
2008.

[35] S. Nedevschi, L. Popa, G. Iannaccone, S. Ratnasamy, and

D. Wetherall, “Reducing Network Energy Consumption via
Sleeping and Rate-adaptation,” in NSDI, 2008.

[36] A. Greenberg, P. Lahiri, D. Maltz, P. Patel, and S. Sengupta,

“Towards a Next Generation Data Center Architecture:
Scalability and Commoditization,” in PRESTO. ACM, 2008.

[37] P. Kazemian, G. Varghese, and N. McKeown, “Header space

analysis: Static checking for networks,” in NSDI, 2012.

[38] N. Handigol, B. Heller, V. Jeyakumar, D. Mazi`eres, and

N. McKeown, “Where is the debugger for my software-deﬁned
network?” in Proceedings of the ﬁrst workshop on Hot topics
in software deﬁned networks. ACM, 2012, pp. 55–60.

[39] H. Mai, A. Khurshid, R. Agarwal, M. Caesar, P. Godfrey, and

S. King, “Debugging the Data Plane with Anteater,”
SIGCOMM CCR, 2011.

[40] N. Feamster and H. Balakrishnan, “Detecting BGP

conﬁguration Faults with Static Analysis,” in NSDI, 2005.

[41] A. Feldmann and J. Rexford, “IP network Conﬁguration for

Intra-domain Traﬃc Engineering,” Network, IEEE, 2001.

[42] L. Ellram, “Total Cost of Ownership: An Analysis Approach

for Purchasing,” in Journal of PDLM, 1995.

[43] R. Swale, P. Mart, P. Sijben, S. Brim, and M. Shore,

“Middlebox Communications Protocol Requirements,” RFC
3304, 2002.

[44] B. Carpenter and S. Brim, “Middleboxes: Taxonomy and

Issues,” RFC 3234, 2002.

[45] M. Stiemerling and J. Quittek, “Middlebox communication

(MIDCOM) protocol semantics,” RFC 4097, 2008.

[46] R. Hancock, S. Bosch, G. Karagiannis, and J. Loughney, “Next

steps in signaling (NSIS): Framework,” in IETF RFC 4080,
2005.

[47] A. Greenberg, G. Hjalmtysson, D. Maltz, A. Myers, J. Rexford,

G. Xie, H. Yan, J. Zhan, and H. Zhang, “Clean slate 4D
approach to Network Control and Management,” in
SIGCOMM, 2005.

[48] M. Casado, M. Freedman, J. Pettit, J. Luo, N. McKeown, and

S. Shenker, “Ethane: Taking Control of the Enterprise,” in
SIGCOMM CCR, 2007.

[49] J. Eppinger, “TCP connections for P2P Apps: Software
Approach to Solving the NAT Problem,” in ISR, 2005.

[50] A. Biggadike, D. Ferullo, G. Wilson, and A. Perrig,

“NATBLASTER: Establishing TCP Connections Between
Hosts Behind NATs,” in ACM SIGCOMM Workshop, 2005.
[51] S. Kandula, R. Mahajan, P. Verkaik, S. Agarwal, J. Padhye,
and P. Bahl, “Detailed Diagnosis in Enterprise Networks,” in
SIGCOMM, 2009.

[52] V. Padmanabhan, S. Ramabhadran, S. Agarwal, and J. Padhye,
“A Study of End-to-End Web Access Failures,” in CoNext, 2006.

[53] C. Labovitz, A. Ahuja, and F. Jahanian, “Experimental Study

of Internet Stability and Backbone Failures,” in FTC, 1999.

[54] A. Markopoulou, G. Iannaccone, S. Bhattacharyya, C. Chuah,

Y. Ganjali, and C. Diot, “Characterization of Failures in an
Operational IP Backbone Network,” in IEEE/ACM TON,
2008.

[55] A. Shaikh, C. Isett, A. Greenberg, M. Roughan, and

J. Gottlieb, “A Case Study of OSPF Behavior in a Large
Enterprise Network,” in ACM SIGCOMM WIM, 2002.

[56] D. Turner, K. Levchenko, A. Snoeren, and S. Savage,

“California Fault Lines: Understanding the Causes and Impact
of Network Failures,” in ACM SIGCOMM CCR, 2010.

22