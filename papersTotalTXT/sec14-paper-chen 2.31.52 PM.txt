Peeking into Your App without Actually Seeing it: 

UI State Inference and Novel Android Attacks
Qi Alfred Chen, University of Michigan; Zhiyun Qian, NEC Laboratories America;  

Z. Morley Mao, University of Michigan

https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/chen

This paper is included in the Proceedings of the 23rd USENIX Security Symposium.August 20–22, 2014 • San Diego, CAISBN 978-1-931971-15-7Open access to the Proceedings of  the 23rd USENIX Security Symposium is sponsored by USENIXPeeking into Your App without Actually Seeing It: UI State Inference and

Novel Android Attacks

Qi Alfred Chen, Zhiyun Qian†, Z. Morley Mao

University of Michigan, †NEC Labs America, Inc.

alfchen@umich.edu, zhiyunq@nec-labs.com, zmao@umich.edu

Abstract

The security of smartphone GUI frameworks remains
an important yet under-scrutinized topic.
In this pa-
per, we report that on the Android system (and likely
other OSes), a weaker form of GUI conﬁdentiality can
be breached in the form of UI state (not the pixels) by a
background app without requiring any permissions. Our
ﬁnding leads to a class of attacks which we name UI state
inference attack. The underlying problem is that popular
GUI frameworks by design can potentially reveal every
UI state change through a newly-discovered public side
channel — shared memory. In our evaluation, we show
that for 6 out of 7 popular Android apps, the UI state in-
ference accuracies are 80–90% for the ﬁrst candidate UI
states, and over 93% for the top 3 candidates.

Even though the UI state does not reveal the exact pix-
els, we show that it can serve as a powerful building
block to enable more serious attacks. To demonstrate
this, we design and fully implement several new attacks
based on the UI state inference attack, including hijack-
ing the UI state to steal sensitive user input (e.g., login
credentials) and obtain sensitive camera images shot by
the user (e.g., personal check photos for banking apps).
We also discuss non-trivial challenges in eliminating the
identiﬁed side channel, and suggest more secure alterna-
tive system designs.

1 Introduction

For instance,

The conﬁdentiality and integrity of applications’ GUI
content are well recognized to be critical in achiev-
ing end-to-end security [1–4].
in the
desktop and browser environment, window/UI spooﬁng
(e.g., fake password dialogs) breaks GUI integrity [3, 4].
On the Android platform, malware that takes screenshots
breaches GUI conﬁdentiality [5]. Such security issues
can typically lead to the direct compromise of the con-
ﬁdentiality of user input (e.g., keystrokes). However, a
weaker form of conﬁdentiality breach has not been thor-
oughly explored, namely the knowledge of the applica-
tion UI state (e.g., knowing that the application is show-
ing a login window) without knowing the exact pixels of
the screen, especially in a smartphone environment.

Surprisingly, in this paper we report that on the An-
droid system (and likely on other OSes), such GUI con-
ﬁdentiality breach is indeed possible, leading to serious
security consequences. Speciﬁcally, we show that UI
state can be inferred without requiring any Android per-
missions. Here, UI state is deﬁned as a mostly consis-
tent user interface shown in the window level, reﬂecting
a speciﬁc piece of program functionality. An example
of a UI state is a login window, in which the text con-
tent may change but the overall layout and functionality
remain the same. Thus, we call our attack UI state infer-
ence attack. In this attack, an attacker ﬁrst builds a UI
state machine based on UI state signatures constructed
ofﬂine, and then infers UI states in real time from an un-
privileged background app. In Android terminology, the
UI state is known as Activity, so we also call it Activity
inference attack in this paper.

Although UI state knowledge does not directly reveal
user input, due to a lack of direct access to the exact pix-
els or screenshots, we ﬁnd that it can effectively serve as
a building block and enable more serious attacks such as
stealing sensitive user input. For example, based on the
inferred UI states, we can further break the GUI integrity
by carefully exploiting the designed functionality that al-
lows UI preemption, which is commonly used by alarm
or reminder apps on Android.

The fundamental

reason for such conﬁdentiality
breach is in the Android GUI framework design, where
every UI state change can be unexpectedly observed
through publicly accessible side channels. Speciﬁcally,
the major enabling factor is a newly-discovered shared-
memory side channel, which can be used to detect win-
dow events in the target application. This side channel
exists because shared memory is commonly adopted by
window managers to efﬁciently receive window changes
or updates from running applications. For more details,
please refer to §2.1 where we summarize the design and
implementation of common window managers, and §3.2
where we describe how shared memory plays a critical
role. In fact, this design is not speciﬁc to Android: nearly
all popular OSes such as Mac OS X, iOS, and Windows
also adopt this shared-memory mechanism for their win-

USENIX Association  

23rd USENIX Security Symposium  1037

1

dow managers. Thus, we believe that our attack on An-
droid is likely to be generalizable to other platforms.

Since the window manager property we exploit has no
obvious vulnerabilities in either design or implementa-
tion, it is non-trivial to construct defense solutions. In §9,
we discuss ways to eliminate the identiﬁed side channels,
and also suggest more secure alternative system designs.
Our discovered Activity inference attack enables a
number of serious follow-up attacks including (1) Ac-
tivity hijacking attack that can unnoticeably hijack
the UI state to steal sensitive user input (e.g.,
lo-
gin credentials), and (2) camera peeking attack that
captures sensitive camera images shot by the user
(e.g., personal check photos for banking apps). We
have fully designed and implemented these attacks and
strongly encourage readers to view several short video
demos at https://sites.google.com/site/
uistateinferenceattack/demos [6].

Furthermore, we demonstrate other less severe but
also interesting security consequences: (1) existing at-
tacks [5, 7–10] can be enhanced in stealthiness and ef-
fectiveness by providing the target UI states; (2) user be-
havior can be inferred through tracking UI state changes.
Previous work has demonstrated other interesting An-
droid side-channel attacks, such as inferring the web
pages a user visits [11] as well as the identity, loca-
tion, and disease information of users [12]. However,
these attacks are mostly application-speciﬁc with limited
scope. For instance, Memento [11] only applies to web
browsers, and Zhou et al. [12] reported three side chan-
nels speciﬁc to three different apps. In contrast, the UI
state concept in this paper applies generally to all An-
droid apps, leading to not only a larger attack coverage
but also many more serious attacks, such as the Activity
hijacking attack which violates GUI integrity.

The contributions of this paper are as follows:
• We formulate the general UI state inference attack
that violates a weaker form of GUI conﬁdentiality, aimed
at exposing the running UI states of an application. It ex-
ploits the unexpected interaction between the design and
implementation of the GUI framework (mainly the win-
dow manager) and a newly-discovered shared-memory
side channel.

• We design and implement the Android version of
this attack and ﬁnd an accuracy of 80–90% in determin-
ing the foreground Activity for 6 out of 7 popular apps.
The inference itself does not require any permissions.

• We develop several attack scenarios using the UI
state inference technique and demonstrate that an at-
tacker can steal sensitive user input and sensitive camera
images shot by the user when using Android apps.

For the rest of the paper, we ﬁrst provide the attack
background and overview in §2. The newly-discovered
side channel and Activity transition detection are detailed

in §3, and based on that, the Activity inference technique
is described in §4. In §5, we evaluate this attack with
popular apps, and §6, §7 and §8 show concrete follow-
up attacks. We cover defense in §9, followed by related
work in §10, before concluding in §11.

2 Background and Overview
2.1 Background: Window Manager
Window manager is system software that interacts with
applications to draw the ﬁnal pixels from all application
windows to the frame buffer, which is then displayed
on screen. After evolving for decades, the most recent
design is called compositing window manager, which is
used virtually in all modern OSes. Unlike its predeces-
sors, which allow individual applications to draw to the
frame buffer directly, a compositing window manager re-
quires applications to draw the window content to off-
screen buffers ﬁrst, and use a dedicated window compos-
itor process to combine them into a ﬁnal image, which is
then drawn to the frame buffer.
Client-drawn and server-drawn buffer design. There
are two types of compositing window manager design,
as shown in Fig. 1. In this ﬁgure, client and server refer
to an application and the window compositor1 respec-
tively.
In the client-drawn buffer design, the applica-
tions draw window content to off-screen buffers, and use
IPC to communicate these buffers with the server, where
the ﬁnal image is composited and written to the frame
buffer. This design is very popular and is used in Mac
OS X, iOS, Windows, Android, and Wayland for the fu-
ture Linux [13, 14]. In the server-drawn buffer design,
the main difference is that the off-screen buffers are al-
located and drawn by the window compositor instead of
by the applications. Applications send commands to the
window compositor to direct the drawing process. Only
the X window system on the traditional Linux and Mir
for the future Linux [15] use this design.

Both designs have their advantages. The client-drawn
buffer design provides better isolation between applica-
tions, more ﬂexible window drawing and more balanced
overhead between the client and the server. For the
server-drawn buffer design, the server has control over
all applications’ window buffers, which is better for cen-
tralized resource management. Interestingly, some prior
work choose the former to enhance GUI security [1], but
we ﬁnd that it actually enables our attacks (shown in §3).

2.2 Background: Android Activity and Ac-

tivity Transition

In Android, the UI state our attack infers is called Activ-
ity. An Activity provides a user interface (UI) for user in-

1For traditional Linux the server is an X server, and the window
compositor is a separate process talking to the X server. In Fig. 1 we
refer to the combination of them as the window compositor.

1038  23rd USENIX Security Symposium 

USENIX Association

2

Draw

App1

Draw

App2

Draw

App3

Off-screen
buffer 1

Off-screen
buffer 2

Off-screen
buffer 3

IPC

IPC

IPC

(a)

Final
image

Frame
buffer

Window
compositor

App1

App2

App3

IPC

IPC

IPC

Off-screen
buffer 1

Off-screen
buffer 2

Draw

Draw

Final
image

Window compositor

Frame
buffer

Draw
Off-screen
buffer 3
(b)

1

SendMoney

Activity
7

SendMoney
Verification

Activity

8

2

6

Choose
Recipient
Activity
5

3

4

AddRecipient

Activity

ChooseRecipient
NotificationActivity

Figure 1: Two types of compositing window manager design: (a) client-
drawn buffer design, and (b) server-drawn buffer design. Client refers to the
application, and server refers to the window compositor.

Figure 2: Activities involved in send-
ing money in Android Chase app. The
numbers denote the action order.

teractions, and is typically a full-screen window serving
as a functionality unit in Android. We denote Activities
as a, b, ..., and the set of Activities for an app as A. Due
to security concerns, by default apps cannot know which
Activity is currently shown in the foreground unless they
are the owners or the central Activity manager.

An Activity may display different content depending
on the app state. For instance, a dictionary app may have
a single “deﬁnition” Activity showing different texts for
each word lookup. We call these distinct displays View-
States, and denote the set of them for Activity a as a.V S.
Activity transition. In Android, multiple Activities typ-
ically work together and transition from one to another
to support the functionality of an app as a whole. An
example is shown in Fig. 2. During a typical transition,
the current foreground Activity pauses and a new one is
created. A Back Stack [16] storing the current and past
Activities is maintained by Android. To prevent exces-
sive memory usage, at any point in time, only the top
Activity has its window buffer allocated. Whenever an
Activity transition occurs, the off-screen buffer alloca-
tion for the new Activity window and the deallocation
for the existing Activity window take place.

Activity transitions can occur in two ways: a new Ac-
tivity is created (create transition), or an existing one re-
sumes when the BACK key is pressed (resume transi-
tion), corresponding to push and pop actions in the Back
Stack. Fig. 3 shows the major function calls involved
in these two transition types. Both transition types start
by pausing the current foreground Activity, and then
launching the new one. During launch, the create transi-
tion calls both onCreate() and onResume(), while
the resume transition only calls onResume(). Both
onCreate() and onResume() are implemented by
the app. After that, performTraversal() is called,
in which measure() and layout() calculate the
sizes and locations of UI components, and draw() puts
them into a data structure as the new Activity UI. Fi-
nally, the create transition pushes the new Activity into
the Back Stack and stops the current one, while the re-
sume transition pops the current one and destroys it.

Activity transition graph.

Immediately after a tran-

Perform-
Pause()
onPause()

c

r

performLaunch()

onCreate()
onResume()

performLaunch()

onResume()

performTr-
aversal()
measure()
layout()
draw()

c

performStop()

onStop()

r performDestroy()

onDestroy()

Figure 3: The function call trace for create (denoted by
c) and resume (denoted by r) transitions.

sition, the user lands on one of the ViewStates of the
new Activity, which we call a LandingState. We de-
note the set of LandingStates for Activity a as a.LS,
and a.LS ⊆ a.V S. Individual LandingStates are denoted
as a.ls1 , a.ls2 , .... Activity transition is a relationship
a.V S → b.LS, a, b ∈ A. As the ViewState before the tran-
sition is not of interest in this study, we simplify it to
a → b.LS, which forms the graph in Fig. 4. Note that our
deﬁnition is slightly different from that in previous work
[17] as the edge tails in our graph are more ﬁne-grained:
they are LandingStates instead of Activities.

2.3 Attack Overview

Our proposed UI state inference is a general side-channel
attack against GUI systems, aimed at exposing the run-
ning UI state of an application at the window level, i.e.,
the currently displayed window (without knowing the ex-
act pixels). To achieve that, the attack exploits a newly-
discovered shared-memory side channel, which may ex-
ist in nearly all popular GUI systems used today (shown
in §3). In this paper, we focus on the attack on the An-
droid platform: Activity inference attack. We expect the
technique to be generalizable to all GUI systems with the
same window manger design as that in Android, such as
the GUI systems in Mac OS X, iOS, Windows, etc.
Threat model. We require an attack app running in the
background on the victim device, which is a common re-
quirement for Android-based attacks [7–11, 18]. To en-
sure stealthiness, the app should be low-overhead, not
draining the battery too quickly. Also, as the purpose
of permissions is to alert users to privacy- or security-
invasive apps [19], the attack should not require any ad-
ditional permissions besides a few very common ones,
for example the INTERNET permission.

USENIX Association  

23rd USENIX Security Symposium  1039

3

a

a.ls1 a.ls2 a.ls3

b
b.ls1

c

c.ls1

c.ls2

d
d.ls1

Figure 4: An example Activity transition
graph. Solid and dotted edges denote cre-
ate and resume transitions respectively.

Activity

transition period

Activity
transition
detection

Activity (UI state)

inference

Shared-

memory side

channels

Other side channels

such as CPU utilization

time, network, etc.

Activity hijacking attack

Camera peeking attack

Enhance existing attacks

Monitor user behavior

UI state based attacks

Figure 5: Activity inference attack overview.

General steps. As shown in Fig. 5, Activity inference is
performed in two steps:

1. Activity transition detection: we ﬁrst detect an Ac-
tivity transition event, which reports a single bit of infor-
mation on whether an Activity transition just occurred.
This is enabled by the newly-discovered shared-memory
side channel. As shown later in §3.3, the change ob-
served through this channel is a highly-distinct “signal”.
2. Activity inference: upon detecting an Activity tran-
sition, we need to differentiate which Activity is entering
the foreground. To do so, we design techniques to train
the “signature” for the landing Activity, which roughly
characterizes its starting behavior through publicly ob-
servable channels,
including the new shared-memory
side channel, CPU utilization time, and network activity
(described in §4).

Finally, using our knowledge of the foreground Activ-
ity in real time, we develop novel attacks that can effec-
tively steal sensitive user input as well as other informa-
tion as detailed in §6, §7 and §8.

3 Shared-Memory Side Channel and Ac-

tivity Transition Detection

In this section, we ﬁrst report the newly-discovered side
channel along with the fundamental reason for its exis-
tence, and then detail the transition detection technique.

3.1 Shared-Memory Side Channels

As with any modern OS design, the memory space of
an Android app process consists of the private space
and the shared space. Table 1 lists memory counters in
/proc/pid/statm and their names used in the Linux com-
mand top and the Linux source code.
Inherited from
Linux, these counters can be freely accessed without any
privileges. With these counters, we can calculate both the
private and shared sizes for virtual memory and physical
memory. In this paper, we leverage mm->shared_vm
and file_rss as our shared-memory side channels,
the former for virtual memory and the latter for phys-
ical memory. For convenience, we refer to them as
shared vm and shared pm. In this section, we focus on
using shared vm to detect Activity transition events. In

§4.1, we use both shared vm and shared pm to infer An-
droid Content Provider usages in the Activity inference,
which is another use case we discovered.

3.2 Android Window Events and Shared-

Memory Side Channel

We ﬁnd that shared vm changes are correlated with An-
droid window events. In this section, we detail its root
cause and prevalence in popular GUI systems.
Shared-memory IPC used in the Android window
manager. As mentioned earlier in §2.1, Android adopts
the client-drawn buffer design, where each client (app)
needs to use a selected IPC mechanism to communicate
their off-screen buffers with the window compositor. In
practice, we ﬁnd that shared memory is often used, since
it is the most efﬁcient channel (without large memory
copy overhead). On Android, when an Activity tran-
sition occurs, shared vm size changes can be found in
both the app process and the window compositor process
named SurfaceFlinger. More investigations into Android
source code reveal that the size changes correspond to the
allocations and deallocations of a data structure named
GraphicBuffer, which is the off-screen buffer in Android.
In the Android window drawing process shown in Fig. 3,
GraphicBuffer is shared between the app process and the
SurfaceFlinger process using mmap() at the beginning
of draw() in performTraversal().

Interestingly, this implies that if we know the Graph-
icBuffer size for a target window, we can detect its al-
location and deallocation events by monitoring the size
changes of shared vm. Since the GraphicBuffer size is
proportional to the window size, and an Activity is a full-
screen window, its GraphicBuffer size is ﬁxed for a given
device, which can be known beforehand.

It is noteworthy that different from private memory
space, shared memory space changes only when shared
ﬁles or libraries are mapped into the virtual memory.
This keeps our side channel clean; as a result,
the
changes in shared vm are distinct with minimum noise.
Shared-memory side-channel vulnerability on other
OSes. To understand the scope, we investigate other
OSes besides Android. On Linux, Wayland makes it
clear that it uses shared buffers as IPC between the win-

1040  23rd USENIX Security Symposium 

USENIX Association

4

Item in /proc/-

Description

pid/statm
VmSize

drs

Total virtual memory size
Private virtual memory size

Name
in top
VIRT

/

Name in Linux

source code

mm->total vm
mm->total vm-
mm->shared vm

resident

share

Total physical memory size
Shared physical memory size

RES
SHR

file rss+anon rss

file rss

(cid:1132) shared_vm
0

(cid:1132) shared_vm
0

time

Sampling

time
Sampling

pulse

Table 1: Android/Linux memory counters in /proc/pid/statm and their
names in the Linux command top, and the Linux source code (obtained
from task statm() in task mmu.c). The type of mm is mm struct.

Figure 6: A successful sampling of an
Activity transition signal for the Activ-
ity transition detection

dow compositor and clients to render the windows [13].
Similar to Android, attackers can use /proc/pid/statm to
get the shared memory size and detect window events.

Mac OS X, iOS and Windows neither explain this IPC
in their documentations nor have corresponding source
code for us to explore, so we did some reverse engi-
neering using memory analysis tools such as VMMap
[20]. On Windows 7, we found that whenever we open
and close a window, a memory block appears and dis-
appears in the shared virtual memory space of both the
window compositor process, Desktop Window Manager,
and the application process. Moreover, the size of this
memory block is proportional to the window size on the
screen. This strongly implies that this memory block is
the off-screen buffer and shared memory is the IPC used
for passing it to the window compositor. Thus, using
the GetProcessMemoryInfo() API that does not
require privilege, similar inference may be possible.

Mac OS X is similar to Windows except that the mem-
ory block in shared memory space is named CG back-
ing store. On iOS this should be the same as Mac
OS X since they share the same window compositor,
Quartz Compositor. But on Mac OS X and iOS, only
system-wide aggregated memory statistics seem avail-
able through host statistics() API, which may
still be usable for this attack but with a less accuracy.

3.3 Activity Transition Detection

With the above knowledge, detecting Activity transition
is simply a matter of detecting the corresponding window
event pattern by monitoring shared vm size changes.

The left half of Fig. 6 shows the typical shared vm
changing pattern for an Activity transition, and we name
it Activity transition signal.
In this signal, the posi-
tive and negative spikes are increases and decreases in
shared vm respectively, corresponding to GraphicBuffer
allocations and deallocations. The GraphicBuffer allo-
cation for the new Activity usually happens before the
deallocation for the current Activity, which avoids user-
visible UI delays. Since Activity windows all have full-
screen sizes, the increase and decrease amount are the
same. With the multiple buffer mechanism for UI draw-
ing acceleration on Android [21], 1–3 GraphicBuffer al-
locations or deallocations can be observed during a sin-

gle transition, resulting in multiple spikes in Fig. 6. The
delay between allocations is usually 100–500 ms due to
measurement and layout computations, while the delay
between deallocations is usually under 10 ms. An exam-
ple result of a successful sampling is shown on the right
half of Fig. 6 with the sampling period being 30–100 ms.
To detect this signal, we monitor the changes of
shared vm, and conclude an Activity transition period by
observing (1) both full-screen size shared vm increase
and decrease events, (2) the idle time between two suc-
cessive events is longer than a threshold idle thres. A
successful detection is shown on the top of Fig. 10.

We evaluate this method and ﬁnd a very low false posi-
tive rate, which is mainly because the shared vm channel
is clean. In addition, it is rare that the following unique
patterns happen randomly in practice: (1) the shared vm
increase and decrease amounts are exactly the same as
the full-screen GraphicBuffer size (920 pages for Sam-
sung Galaxy S3); (2) both the increase and decrease
events occur very closely in time.

On the other hand, this method may have false neg-
atives due to a cancellation effect — when an increase
and a decrease are in the same sampling period, they can-
cel each other and the shared vm size stays unchanged.
Raising the sampling rate can overcome this problem, but
at the cost of increased sampling overhead. Instead, we
solve the problem using the number of minor page faults
(minﬂt), in /proc/pid/stat. When allocating memory for a
GraphicBuffer, the physical memory is not actually allo-
cated until it is used. At time of use, the same number of
pages faults as the allocated GraphicBuffer page size is
generated. Since minﬂt monotonically increases as a cu-
mulative counter, we can use it to deduce the occurrence
of a cancellation event.

4 Activity Inference

After detecting an Activity transition, we infer the iden-
tity of the new Activity using two kinds of information:
1. Activity signature. Among functions involved in
the transition (as shown in Fig. 3), onCreate() and
onResume() are deﬁned in the landing Activity, and
the execution of performTraversal() depends on
the UI elements and layout in its LandingState. Thus, ev-
ery transition has behavior speciﬁc to the landing Activ-

USENIX Association  

23rd USENIX Security Symposium  1041

5

Training phase:

Trigger Activity

transitions
automatically

Automated
transition tool
Attacking phase:

Trigger Activity

transitions

Mobile user

Activity
transition

graph

Collect
transition
feature data

Collect
transition
feature data

Transition

model

Activity
signature

Activity
inference

result

Figure 7: Overview of the Activity inference

)
s
m

(
 
e
m

i
t
 

n
o

i
t

a
z

i
l
i
t

 

U
U
P
C

 1200

 1000

 800

 600

 400

 200

 0

onCreate()
onResume()
measure()
layout()
draw()
other

 100

 80

 60

 40

 20

)

%

(
 
F
D
C

c1

c2

c3

c4

r1

r2

Activity Transitions

 0
 470  480  490  500  510  520  530

First Packet Size (byte)

Figure 8: CPU utilization time
breakdown for 6 Activity tran-
sitions in WebMD

Figure 9: CDF of the ﬁrst
packet sizes for various Land-
ingStates in H&R Block

ity, giving us opportunities to infer the landing Activity
based on feature data collected during the transition.
2. Activity transition graph. If the Activity transition
graph of an app is sparse, once the foreground Activity is
known, the set of the next candidate Activities is limited,
which can ease the inference difﬁculty. Thus, we also
consider Activity transition graph in the inference.

Fig. 7 shows an overview of the Activity inference pro-
cess. This process has two phases, the training phase and
the attacking phase. The training phase is executed ﬁrst
ofﬂine. We build a tool to automatically generate Activ-
ity transitions in an app, and at the same time collect fea-
ture data to build the Activity signature and construct the
Activity transition graph. In the attacking phase, when a
user triggers an Activity transition event, the attack app
ﬁrst collects feature data like in the training phase, then
leverages Activity signature and a transition model based
on the Activity transition graph to perform inference.

4.1 Activity Signature Design

During the transition, we identify four types of features
described below and use them jointly as the signature.
Input method events. Soft keyboard on smartphones is
commonly used to support typing in Activities. It usually
pops up automatically at the landing time. There is also a
window event for the keyboard process, which again can
be inferred through shared vm. This is a binary feature
indicating whether the LandingState requires typing.
Content Provider events. Android component Content
Provider manages access to a structured set of data using
SQLite as backend. To deliver content without memory
copy overhead, Android uses anonymous shared mem-
ory (ashmem) shared by the Content Provider and the app
process. Similar to the compositing window manger de-
sign, by monitoring shared vm, we can detect the query
and release events of the Content Provider. Speciﬁcally,
in Android design, we found that the virtual memory size
of ashmem allocated for a Content Provider query is a
ﬁxed large value, e.g., 2048 KB for Android 4.1, which
creates a clear signal. Usually its content only consti-
tutes a small portion. To know the content size, we also

monitor shared pm introduced in §3.1, which indicates
the physical memory allocation size for the content.

The Content Provider is queried in onCreate() and
onResume() to show content in the landing Activity.
For signature construction, we collect Content Provider
query events and the corresponding content size by mon-
itoring shared vm and shared pm. As shared pm may be
noisy, we use a normal distribution to model the size.
CPU utilization time. Fig. 8 shows the CPU utilization
time collected by DDMS [22] for each function in Fig. 3
during the transition. For the 6 transitions, c and r denote
create and resume transition, and 1–4 denote 4 different
LandingStates. The time collected may be inﬂated due
to the overhead added by DDMS proﬁling. The ﬁgure
shows that CPU utilization time spent in each function
differs across distinct LandingStates due to distinct draw-
ing complexity, and for the same LandingState, resume
transitions usually take less time than create ones since
the former lacks onCreate(). Thus, the CPU utiliza-
tion time can be used to distinguish Activity transitions
with different transition types and LandingStates.

To collect data for

this feature, we record the
user space CPU utilization time value (utime),
in
/proc/pid/stat for the Activity transition. Similar to pre-
vious work [23, 24], we ﬁnd that the value for the same
repeated transition roughly follows normal distribution.
Thus, we model it using a normal distribution.
Network events. For LandingStates with content from
the network, network connection(s) are initiated in
performLaunch() during the transition. For a given
LandingState, the request command string such as HTTP
GET is usually hard-coded, with only minor changes
from app states or user input. This leads to similar size
of the ﬁrst packet immediately after the connection estab-
lishment. We do not use the response packet size, since
the result may be dynamically generated. Fig. 9 shows
the CDF of the ﬁrst packet sizes for 14 Activity Land-
ingStates in H&R Block. As shown, most distributions
are quite stable, with variations of less than 3 bytes.

To capture the ﬁrst packet size, we monitor the send
packet size value in /proc/uid stat/uid/tcp snd. We con-

1042  23rd USENIX Security Symposium 

USENIX Association

6

∆ shared_vm

Activity transition period

Activity
transition
signal:

Activity
signature
features:

idle_thres

Content
Provider
feature

Network

event
feature

Input method

feature

CPU utilization time feature

time

Activity
inference
starts here

time

Figure 10: Signature feature data collection timeline.

currently monitor /proc/net/tcp6, which contains net-
work connection information such as the destination IP
address and app uid for all apps. We use the uid to ﬁnd
the app which the connection belongs to, and use time
correlation to match the ﬁrst packet size to the connec-
tion. For the LandingState with multiple connections,
we use the whois IP address registry database [25] to get
the organization names and thus distinguish connections
with different purpose. To read ﬁrst packet sizes accu-
rately, we raise the sampling rate to be 1 in 5 ms during
the transition period. Since this period usually lasts less
than 1 second, the overall sampling overhead is still low.
For signature construction, we keep separate records
for connections with different organization names and
occurrence ordering. For each record, we use the ﬁrst
packet size appearance frequencies to calculate the prob-
ability for this feature.

Fig. 10 shows the data collection timeline for these
feature data and their relationship with the shared vm
signal. The Content Provider event feature is collected
before the ﬁrst shared vm increase, and the input method
event feature is collected after the ﬁrst shared vm de-
crease. Network events are initiated before the ﬁrst
shared vm increase, while the ﬁrst packet size is usually
captured after that. The CPU utilization time feature is
collected throughout the whole transition period.

With these four types of features, our signature prob-
ability Prob(�·, a.lsi�), a ∈ A, a.lsi ∈ a.LS is obtained
by computing the product of each feature’s probability,
based on the observation that they are fairly independent.
In §5, we evaluate our signature design with these four
features both jointly and separately.

4.2 Transition Model and Inference Result

Transition model. In our inference, the states (i.e., Ac-
tivities) are not visible, so we use Hidden Markov Model
(HMM) to model Activity transitions. We denote the
foreground Activity trace with n Activity transitions as
{a0 , a1 , ..., an}. The HMM can be solved using the
Viterbi algorithm [26] with initialization Prob({a0}) =
1
|A| , and inductive steps Prob({a0, ..., an}) = argmax
an .lsi∈an .LS

Prob(�·, an .lsi�)Prob(an|an−1)Prob({a0 , ..., an−1}).
In inductive steps, Prob(�·, an .lsi�) denotes

the
probability calculated from Activity signature, and
Prob(an|an−1) denotes the probability that an−1 transi-
tions to an. If an−1 has x egress edges in the transition
graph, Prob(an|an−1) = 1
x , assuming that user choices
are uniformly distributed.

The typical Viterbi algorithm [26] calculates the most
likely Activity trace {a0, a1 , ..., an}, with computation
complexity O((n + 1)|A|2). However, for our case,
only the new foreground Activity an is of interest, so
we modify the Viterbi algorithm by only calculating
Prob({an−c+1 , ..., an}), where c is a constant. This re-
duces the computation complexity to O(c|A|2).
In our
implementation, we choose c = 2.
Inference result. After inference, our attack outputs a
list of Activities in decreasing order of their probabilities.

4.3 Automated Activity Transition Tool

Our

tool

By design, both the Activity signature and Activity tran-
sition graph are mostly independent of user behavior;
therefore, the training phase does not need any victim
participation. Thus, we also develop an automated tool
to accelerate the training process ofﬂine.
is built on top of
Implementation.
ActivityInstrumentationTestCase, an An-
droid class for building test cases of an app [27]. The
implementation has around 4000 lines of Java code.
Activity transition graph generation with depth-ﬁrst
search. To generate the transition graph, we send and
record user input events to Activities to drive the app in
a depth-ﬁrst search (DFS) fashion like the depth-ﬁrst ex-
ploration described in [17]. The DFS graph has View-
States as nodes, user input event traces as edges (create
transitions), and the BACK key as the back edge (resume
transitions). Once the foreground Activity changes, tran-
sition information such as the user input trace and the
landing Activity name is recorded. The graph generated
is in the form of the example shown in Fig. 4.
Activity transition graph traversal. With the transi-
tion graph generated, our tool supports automatic graph
traversals in deterministic and random modes. In the ran-
dom mode, the tool chooses random egress edges during
the traversal, and goes back with some probabilities.
Tool limitations. We assume Activities are independent
from each other. If changes in one Activity affect Ac-
tivity transition behavior in another, our tool may not be
aware of that, leading to missed transition edges. For
some user input such as text entering, the input genera-
tion is only a heuristic and cannot ensure 100% cover-
age. To address such limitations, some human effort is
involved to ensure that we do not miss the important Ac-
tivities and ViewStates.

USENIX Association  

23rd USENIX Security Symposium  1043

7

Application

name

Activity
number

WebMD

Chase
Amazon
NewEgg
GMail

H&R Block
Hotel.com

38
34
19
55
7
20
24

create
type
274
257
209
242
10
58
29

Activity transitions

Activity LandingStates

resume

Total

type
129
39
190
253
10
39
41

403
296
399
495
20
97
70

Graph
density
14.0%
17.4%
55.3%
8.2%
20.4%
12.1%
6.1%

Average egress
edge per Activity

Number

10.0
8.71
21.0
9.0
2.86
4.85
2.92

92
50
39
80
17
42
35

w/ content
provider
18.7%

4%

0.00%
0.00%
0.00%
0.00%
0.00%

w/ input
method
15.3%
0.00%
7.69%
8.75%
5.71%
2.3%
2.8%

w/ network

70.3%
44.0%
74.3%
97.5%
5.8%
100%
100%

Table 2: Characteristics of Activity, Activity LandingStates and Activity transitions of selected apps (numbers are
obtained manually as the ground truth). The CPU utilization time feature is omitted since it is always available.

Application

name

WebMD

Chase
Amazon
NewEgg
GMail

H&R Block
Hotel.com

Activity transition detection
FN
Accuracy
0.50%
1.0%
0.53% 0.63%
0.7%
1.6%
0.8%
2.3%
3.5%

99%
99.5%
99.3%
98.4%
99.2%
97.7%
96.5%

FP

4%
0.1%
0%
2%
0.6%

Activity inference accuracy

Top 1 cand.

Top 2 cand.

Top 3 cand.

84.5%
83.1%
47.6%
85.9%
92.0%
91.9%
82.6%

91.4%
91.8%
65.6%
92.6%
98.3%
96.7%
92.7%

93.6%
95.7%
74.1%
96.3%
99.3%
98.1%
96.7%

WebMD
Chase
Amazon
NewEgg
Gmail
H&R Block
Hotel.com

 100

 80

 60

 40

 20

)

%

(
 
F
D
C

 0

 0

 20

 40

 60

 80

 100

Top 1 Candidate Accuracy (%)

Table 3: Activity transition detection and inference result for selected apps.
All results are generated using Activity traces with more than 3000 transitions.

Figure 11: CDF of the average ac-
curacy for top 1 candidates

5 Evaluation

In this section, we evaluate (1) the effectiveness of the
automated Activity transition tool, (2) the performance
of the Activity inference, and (3) the attack overhead.
Attack implementation. We implement the Activity in-
ference attack with around 2300 lines of C++ code com-
piled with Android NDK packaged in an attack app.
Data collection. We use the automated tool in §4.3 to
generate Activity transitions. We use random traversals
to simulate user behavior, and deterministic traversals in
controlled experiments for training and parameter selec-
tion, e.g.,
the sampling rate. We run all experiments on
Samsung Galaxy S3 devices with Android 4.2. We do
not make use of any device-speciﬁc features and expect
our ﬁndings to apply to other Android phones.
App selection and characteristics. In our experiments,
we choose 7 Android apps, WebMD, GMail, Chase,
H&R Block, Amazon, NewEgg, and Hotel.com, all of
which are popular and also contain sensitive information.
Table 2 characterizes these apps according to properties
relevant to our attack techniques. NewEgg and GMail
have the highest and the lowest number of Activities, and
Amazon has the highest graph density. Chase app is the
only one with no automatic soft keyboard pop-up during
the transition among these apps. The Content Provider
is only extensively used by WebMD. Except GMail, the
percentage of the network feature is usually high.

5.1 Activity Transition Tool Evaluation

For Activity transition graph generation, the tool typi-
cally spends 10 minutes to 1 hour on a single Activity,
depending on the UI complexity. For all apps except

WebMD, the generated transition graphs are exactly the
same as the ones we generate manually. The transition
graph of WebMD misses 4 create transition edges and 3
resume transition edges, which is caused by dependent
Activity issues described in §4.3. Our tool generates no
fake edges for all selected apps.

5.2 Activity Inference Attack Evaluation

Evaluation methodology. We run the attack app in the
background while the tool triggers Activity transitions.
The triggered Activity traces are recorded as the ground
truth. To simulate the real attack environment, the at-
tack is launched with popular apps such as GMail and
Facebook running in the background. For the Activity
transition detection, we measure the accuracy, false pos-
itive (FP) and false negative (FN) rates. For the Activity
inference, we consider the accuracy for the top N candi-
dates — the inference is considered correct if the right
Activity is ranked top N on the result candidate list.

5.2.1 Activity Transition Detection Results

Aggregated Activity transition detection results are
shown in columns 2–4 in Table 3. For all selected apps,
the detection accuracies are more than 96.5%, and the FP
and FN rates are both less than 4%.

When changing the sampling period from 30 to 100
ms in our experiment, for all apps the increases of FP and
FN rates are no more than 5%. This shows a small im-
pact of the sampling rate on the detection; thus, a lower
sampling rate can be used to reduce sampling overhead.
We also measure Activity transition detection delay,
which is the time from the ﬁrst shared vm increase to
the moment when the Activity transition is detected in

1044  23rd USENIX Security Symposium 

USENIX Association

8

Fig. 10. For all apps, 80% of the delay is shorter than
1300 ms, which is fast enough for Activity tracking.

5.2.2 Activity Inference Results

The aggregated Activity transition inference result is
shown in column 5–7 in Table 3. For all apps except
Amazon, the average accuracies for the top 1 candidates
are 82.6–92.0%, while the top 2 and top 3 candidates’ ac-
curacies exceed 91.4% and 93.6%. Amazon’s accuracy
remains poor, and can achieve 80% only when consid-
ering the top 5 candidates. In the next section, we will
investigate more into the reason of these results.

Fig. 11 shows the CDF of the accuracy for top 1 candi-
dates per Activity in the selected apps. Except Amazon,
all apps have more than 70% of Activities with more than
80% accuracy. For WebMD, NewEgg, Chase and Ho-
tel.com, around 20% Activities have less than 70% accu-
racy. For these Activities, they usually lack some signa-
ture features, or the features they have are too common to
be distinct enough. However, such Activities usually do
not have sensitive data due to a lack of important UI fea-
tures such as text ﬁelds for typing, and thus are not rele-
vant to the proposed attacks. For example, in Hotel.com,
the two Activities with less than 70% accuracy are Coun-
trySelectActivity for switching language and Opinion-
LabEmbeddedBrowserActivity for rating the app.

5.2.3 Breakdown Analysis and Discussion

To better understand the performance results, we break
down the contributions of each signature feature and the
transition model further. Table 4 shows the decrease of
the average accuracy for top 1 candidates if leaving out
certain features or the transition model. Without the CPU
utilization time feature, the accuracy decreases by 36.2%
on average, making it the most important contributor.
Contributions from the network feature and the transi-
tion model are also high, which generally improves the
accuracy by 12–30%. As low-entropy features, the Con-
tent Provider and the input method contribute under 5%.
Thus, the CPU utilization time, the network event and
the transition model are the three most important contrib-
utors to the ﬁnal accuracy. Note that though the Content
Provider and input method features have lower contribu-
tions, we ﬁnd that the top 2 and top 3 candidates’ accu-
racies beneﬁt more from them. This is because they are
more stable features, and greatly reduce the cases with
extremely poor results due to the high variance in the
CPU utilization time and the network features.

Thus, considering that the CPU utilization time is al-
ways available, apps with a high percentage of network
features, or a sparse transition graph, or both, should
have a high inference accuracy. In Table 2 and Table 3,
this rule applies to all the selected apps except Amazon.

Application

name

WebMD

Chase
Amazon
NewEgg
GMail

H&RBlock
Hotel.com

∆ Accuracy for top 1 candidates

no IM no CP
-3.8%
-0%

-10.2%
-0.5%
-13.7%
-0.7%
-0.3%

no Net

no CPU
-2.6% -19.1% -25.7%
-2.0% -12.8% -71.5%
-3.2%
-0%
-32.0%
-31.7% -20.0%
-0%
-0%
-0.9%
-58.6%
-30.7% -27.9%
-0%
-0%
-28.8% -17.9%

no HMM
-16.6%
-28.7%
-5.9%
-13.0%
-19.4%
-16.5%
-12.2%

Table 4: Breakdown of individual contributions to accu-
racy. IM, CP, Net, and CPU stand for input method, Con-
tent Provider, network event and CPU utilization time.

Amazon has a low accuracy mainly because it bene-
ﬁts little from either the transition model or the network
event feature due to high transition graph density and in-
frequent network events. The reason for the high transi-
tion graph density is that in Amazon each Activity has
a menu which provides options to transition to nearly
all other Activities. The infrequent network events are
due to its extensively usage of caching, presumably be-
cause much of its content is static and cacheable. How-
ever, we note that many network events are typically not
cacheable, e.g., authentication events and dynamic con-
tent (generated depending on the user input and/or the
context). Compared to the other 6 apps, we ﬁnd that
these two properties for Amazon are not typical, not
present in another shopping app NewEgg.

The Amazon app case indicates that our inference
method may not work well if certain features are not suf-
ﬁciently distinct, especially the major contributors such
as the transition model and the network event feature.
To better understand the general applicability of our in-
ference technique, a more extensive measurement study
about the Activity and Activity transition graph proper-
ties is needed, which we leave as future work.

5.2.4 Attack overhead

We use the Monsoon Power Monitor [28] to measure
the attack energy overhead. Using an Activity trace of
WebMD on the same device, with our attack in the back-
ground the power level increases by 2.2 to 6.0% when
the sampling period changes from 100 to 30 ms.

6 Enabled Attack: Activity Hijacking

In this section, based on the UI state tracking, we de-
sign a new Android attack which breaches GUI integrity
— Activity hijacking attack — based on a simple idea:
stealthily inject into the foreground a phishing Activity
at the right timing and steal sensitive information a user
enters, e.g., the password in login Activity.

Note that this is not the ﬁrst attack popping up a phish-
ing Activity to steal user input, but we argue that it is the
ﬁrst general one that can hijack any Activities during an
app’s lifetime. Previous study [29] pops up a fake login

USENIX Association  

23rd USENIX Security Symposium  1045

9

Foreground

Activity

(cid:258)

Activity

1

Activity

2

Foreground

Activity
(cid:258)
Login
Activity

Phishing
Login
Activity

Waiting
for Login
Activity

Phishing
Login
Activity

Login
Activity
appears,

start

hijacking!

Go back
to the
victim
Login
Activity

Foreground

Activity
(cid:258)

Phishing
Login
Activity

Login
Activity

Activity
transition
signal:

Activity
hijacking
attack:

∆ shared_vm

Activity entering

animation

Activity
injection

delay

t0

t1

t2

Activity
transition
period starts

Early

inference

Inject phishing
Activity with
no animation

time

Original
inference
starts here

Step 1

Step 2

Step 3

Figure 12: General steps of the Activity hijacking attack

Figure 13: Activity injection process with early inference

time

Activity every time the attack app detects the launching
of the target app, tricking users into entering login cre-
dentials. However, this can easily cause user suspicion
due to the following: (1) most apps nowadays do not re-
quire login right after the app starts, even for banking
apps like Chase; (2) the attack requires suspicious per-
missions such as BOOT COMPLETED to be notiﬁed of
system boot, based on the assumption that login is ex-
pected after the system reboot. With the Activity infer-
ence attack, we no longer suffer from these limitations.

6.1 Activity Hijacking Attack Overview

Fig. 12 shows the general steps of Activity hijacking at-
tack. In step 1, the background attack app uses Activity
inference to monitor the foreground Activity, waiting for
the attack target, for example, LoginActivity in Fig. 12.
In step 2, once the Activity inference reports that the
target victim Activity, e.g., LoginActivity, is about to
enter the foreground, the attack app simultaneously in-
jects a pre-prepared phishing LoginActivity into the fore-
ground. Note that the challenge here is that this intro-
duces a race condition where the injected phishing Ac-
tivity might enter the foreground too early or too late,
causing visual disruption (e.g., broken animation). With
carefully designed timing, we prepare the injection at the
perfect time without any human-observable glitches dur-
ing the transition (see video demos [6]). Thus, the user
will not notice any differences, and continue entering the
password. At this point, the information is stolen and the
attack succeeds.

In step 3, the attack app ends the attack as unsuspi-
ciously as possible. Since the attack app now becomes
the foreground app, it needs to somehow transition back
to the original app without raising much suspicion.

6.2 Attack Design Details

Activity injection. To understand how it is possible to
inject an Activity from one app into the foreground and
preempt the existing one, we have to understand the de-
sign principle of smartphone UI. If we think about apps
such as the alarm and reminder apps, they indeed require
the ability to pop up a window and preempt any fore-
ground Activities. In Android, such functionality is sup-

ported in two ways without requiring any permissions:
(1) starting an Activity with a restricted launching mode
“SingleInstance” [30];
(2) starting an Activity from an
Android broadcast receiver [31]. In our design, since the
timing of the injection is critical, we choose the former
as it can be launched 30 ms faster.
UI phishing. To ensure that the phishing Activity’s UI
appears the same as the victim Activity, we disassem-
ble the victim app’s apk using apktool [32] and copy all
related UI resources to the attack app. However, some-
times the Activity UI may have dynamically loaded ar-
eas which are not determined by the UI resources, e.g.,
the account veriﬁcation image in banking apps. To solve
that, the attacker can make those areas transparent, given
that Android supports partially transparent Activity [33].
Activity transition animation modifying. Since our in-
jection introduces an additional Activity transition which
is racing with the original transition, the animation of
the additional transition would clearly disrupt the ﬂow.
Fortunately, this problem can be solved by disabling the
transition animation (allowed by Android) by modifying
an Activity conﬁguration of the attack app without need-
ing any permissions. This helps the injection become to-
tally seamless, and as will be discussed in §9, enforcing
this animation may be a feasible mitigation of our attack.
Injection timing constraint. For the attack to succeed,
the Activity injection needs to happen before any user in-
teraction begins, otherwise the UI change triggered by it
may be disrupted by the injected Activity. Since the in-
jection with the current inference technique takes quite
long (the injected Activity will show up after around
1300 ms from the ﬁrst detected shared vm increase as
measured in §5), any user interaction during this period
would cause disruptions. To reduce the delay, we adapt
the inference to start much earlier. As shown in Fig. 13,
we now start the inference as soon as the shared vm de-
crease is observed (roughly corresponding to the Activity
entering animation start time). In contrast, our original
inference starts after the last shared vm increase.

Note that this would limit the feature collection up to
the point of the shared vm decrease, thus impacting the
inference accuracy. Fortunately, as indicated in Fig. 10,
such change does allow the network event feature, the

1046  23rd USENIX Security Symposium 

USENIX Association

10

majority of the CPU utilization time features, and the
transition model to be included in the inference, which
are the three most important contributors to the ﬁnal ac-
curacy as discussed in §5.2.3. Based on our evaluation,
this would reduce the delay to only around 500 ms.
Unsuspicious attack ending. As described in §6.1, in
step 3 we try to transition from the attack app back to the
victim unsuspiciously. Since the phishing Activity now
has the information entered on the screen, it is too abrupt
to directly close it and jump back to the victim. In our
design, we leverage “benign” abnormal events to hide
the attack behavior, e.g., the attack app can show “server
error” after the user clicks on the login button, and then
quickly destroy itself and fall back to the victim.
Deal with cached user data. It is common that some
sensitive data may be cached, thus won’t be entered at all,
e.g., the user name in login Activity. Without waiting for
them to expire, it is difﬁcult to capture any fresh input.

Note that we can simply inject the phishing Activity
with all ﬁelds left blank. The challenge is to not alert
the user with any other observable suspicious behavior.
Speciﬁcally, depending on the implementation, we ﬁnd
that the cached user data sometimes show up immedi-
ately in the very ﬁrst frame of the Activity entering an-
imation (t0 in Fig. 13). Thus, our later injection would
clear the cached ﬁelds, which causes visual disruption.

Our solution is to pop up a tailored cache expiration
message (replicating the one from the app), and then
clear such cached data, prompting users to re-enter them.

6.3 Attack Implementation and Evaluation

Implementation. We implement Activity hijacking at-
tack against 4 Activities: H&R Block’s LoginActivity
and RefundInfoActivity for stealing the login creden-
tials and SSN, and NewEgg’s ShippingAddressAddAc-
tivity and PaymentOptionsModifyActivity for stealing
the shipping/billing address and credit card information.
The latter two Activities do not appear frequently in the
check-out process since the corresponding information
may be cached. Thus, to force the user to re-enter them,
our attack injects these two Activities into the check-out
process. The user would simply think that the cached
information has expired. In this case the fake cache ex-
piration messages are not needed, since the attack can
fall back to the check-out process naturally after entering
that information. Attack demos can be found in [6].
Evaluation. The most important metric for our attack is
the Activity injection delay, which is the time from t1 to
t2 in Fig. 13. In Android, it is hard to know precisely the
animation ending time t1, so the delay is measured from
t0 to t2 as an upper bound. In the evaluation the Activity
injection is performed 50 times for the LoginActivity of
H&R Block app, and the average injection delay is 488
ms. Most of the delay time is spent in onCreate()

(242 ms) and performTraverse() (153 ms). From
our experience, the injection is fast enough to complete
before any user interaction starts.

7 Enabled Attack: Camera Peeking

In this section, we show another new attack enabled by
the Activity inference: camera peeking attack.

7.1 Camera Peeking Attack Overview

Due to privacy concerns, many apps store photo images
shot by the camera only in memory and never make them
publicly accessible, for example by writing them to ex-
ternal storage. This applies to many apps such as bank-
ing apps (e.g., Chase), shopping apps (e.g., Amazon and
Best Buy), and search apps (e.g., Google Goggles). Such
photo images contain highly-sensitive information such
as the user’s life events, shopping interests, home address
and signature (on the check). Surprisingly, we show that
with Activity tracking such sensitive and well-protected
camera photo images can be successfully stolen by a
background attack app. Different from PlaceRaider [34],
our attack targets at the camera photo shot by the user,
instead of random ones of the environment.

Our attack follows a simple idea: when an Activity is
using the camera, the attack app quickly takes a separate
image while the camera is still in the same position. In
the following, we detail our design and implementation.

7.2 Attack Design Details

Background on Android camera resource manage-
ment. With the camera permission, an Android app can
obtain and release the camera by calling open() and
release(). Once the camera is obtained, an app can
then take pictures by calling takePicture(). There
are two important properties: (1) exclusive usage. The
camera can be used by only one app at any point in time;
(2) slow initialization. Camera initialization needs to
work with hardware, so open() typically takes 500–
1000 ms (measured on Samsung Galaxy S3 devices).
Obtain camera images in the background. In the An-
droid documentation, taking pictures in the background
is explicitly disallowed due to privacy concerns [35].
Though PlaceRaider [34] succeeded in doing so, we ﬁnd
that their technique is restricted to certain devices run-
ning old Android systems which do not follow the docu-
mentation correctly, e.g., Droid 3 with Android 2.3.

Interestingly, we ﬁnd camera preview frames to be
the perfect alternative interface for obtaining camera
images without explicitly calling takePicture().
When using the camera,
the preview on the screen
shows a live video stream from the camera. Using
PreviewCallback(), the video stream frames are
returned one by one, which are actually the camera im-
ages we want. SurfaceTexture is used to capture this

USENIX Association  

23rd USENIX Security Symposium  1047

11

Camera shooting Activity Picture review Activity

...

Camera usage checking

Obtain camera image

time

Foreground

app

Background
attack app

App

behavior
Camera
App

behavior
Camera

time
Figure 14: Camera peeking attack process when the
foreground Activity is using the camera

Camera peeking

Success

attack type

Blind attack (3s idle time)
Blind attack (4s idle time)
Blind attack (5s idle time)

UI state based attack

rate
81%
83%
79%
99%

DoS
rate
19%
14%
8%
0%

# of camera poss-
ession per round

30.5
20.9
18.9
1.4

Table 5: User study evaluation result for the camera peeking
attack

image stream, and we ﬁnd that it can be created with a
nonexistent OpenGL texture object name, thus prevent-
ing any visible preview on the screen. We suspect that
the less restrictive interface is managed by OpenGL li-
brary which bypasses the Android framework and its as-
sociated security checks. Compared to PlaceRaider [34],
this technique not only has no requirement of the sensi-
tive MODIFY AUDIO SETTINGS permission to avoid
shutter sound, but also has much faster “shutter speed” of
24 frames per second. Note that even if this interface is
blocked, our attack can still use techniques in §6 to inject
an Activity to the foreground to get the preview frames.

Obtain photo images shot by the user. Fig. 14 shows
how our attack gets the photo image the user shoots in
the victim app. The photo taking functionality usually
involves a camera shooting Activity and a picture review
Activity. Once the user clicks on the shutter button in the
former, the latter pops up with the picture just taken. Due
to the exclusive usage property, when the foreground Ac-
tivity is using the camera the attack app cannot get the
camera. Thus, once knowing that the camera is in use,
the attack app keeps calling open() to request the cam-
era until it succeeds right after the user presses the shutter
button and the camera gets released during the Activity
transition. Since the delay to get a camera preview frame
is only the initialization time (500–1000 ms), the cam-
era is very likely still pointing at the same object, thus
obtaining a similar image.

Capture the camera usage time. To trigger the attack
process in Fig. 14, the attack app needs to know when
the camera is in use in the foreground. However, due
to the slow initialization, a naive solution which peri-
odically calls open() to check the camera usage will
possess the camera for 500–1000 ms for each checking
action when the camera is not in use. During that time,
if the foreground app tries to use the camera, a denial of
service (DoS) will take place. With 12 popular apps, we
ﬁnd that when failing to get the camera, most apps pop
up a “camera error” or ”camera is used by another app”
message and some even crash immediately. These errors
may indicate that an attack is ongoing and alert the user.
Besides, the frequent camera resource possessing behav-
ior is easily found suspicious with increasing concerns
about smartphone camera usage [34].

To solve the problem, our attack uses Activity infer-

ence to capture the camera usage time by directly waiting
for the camera shooting Activity. To increase the infer-
ence accuracy for Activities using the camera, we add
camera usage as a binary feature (true or false on the
camera usage status) and it is only tested when the land-
ing Activity is very likely to be the camera shooting Ac-
tivity based on other features to prevent DoS and overly
frequent camera possessions.

7.3 Attack Evaluation

Implementation. We implement the camera peeking at-
tack against the check deposit functionality in Chase app,
which allows users to deposit personal checks by taking
a picture of the check. Besides the network permission,
the attack app also needs the the camera permission to
access camera preview frames. On the check photo, the
attacker can steal much highly-sensitive personal infor-
mation, including the user name, home address, bank
routing/account number, and even the user’s signature.
A video demo is available at [6].
Evaluation methodology. We compare our UI state
based camera peeking attack against the blind attack,
which periodically calls open() to check the fore-
ground camera usage as described in §7.2. We add pa-
rameter idle time for the blind attack as the camera usage
checking period. The longer the idle time is, the lower
the DoS possibility and the camera possession frequency
are. However, the idle time cannot be so long that the at-
tack misses the camera shooting events. Thus, the blind
attack faces a trade-off between the DoS risk, the camera
possession frequency, and the attack success rate.
User study. We evaluate our attack with a user study of
10 volunteers. In the study we use 4 Samsung Galaxy S3
phones with Android 4.1. Three of them use the blind
attacks with idle time being 3, 4 and 5 seconds respec-
tively, and the last one uses the UI state based attack.
Each user performs 10 rounds, and in each round, the
users are asked to ﬁrst interact with the app as usual, and
then go to the check deposit Activity and take a picture
of a check provided by us. We emphasize that they are
expected to perform as if it is their own check. The IRB
for this study has been approved and we took steps to
ensure that no sensitive user data were exposed, e.g., by
using a fake bank account and personal checks.
Performance metrics. For evaluation we measure: (1)

1048  23rd USENIX Security Symposium 

USENIX Association

12

DoS rate, the ratio that when the user wants to use the
camera but fails; (2) number of camera possessions, the
number of events that the camera is possessed by the at-
tack app; (3) success rate, the ratio that the attacker gets
the check image after the user shoots the check.
Result. Table 5 shows the user study evaluation results.
With the camera usage feature, the UI state based attack
can achieve 99% success rate, and the only failure case
is due to a failure in detecting the Activity transition. For
the blind attack, the success rate is less than 83%, and
when the idle time increases, the success rate increases
then decreases. The increase is due to lower DoS proba-
bility, and the decrease is because the users usually ﬁnish
shooting in around 4 seconds (found in our user study),
so when the idle time increases to 5 seconds, the blind
attack misses some camera shooting events.

UI state based attack causes no DoS during the user
study. For the blind attack, the DoS rate is around 8–
19%, and decreases when the idle time increases. Con-
sidering that a single DoS case may likely cause “sud-
den death” for the attack app, this risk is high, especially
compared to the UI state based attack.

The camera possession number for the UI state based
attack is also a magnitude lower. Every round, except the
necessary one for camera shooting, the UI state based at-
tack only needs 0.4 excessive camera possessions, which
is mainly caused by inaccurate inference. For the blind
attack, to ensure a high success rate, the camera posses-
sion number is proportional to time, making it hard to
avoid suspicious frequent camera possessions.

Fig. 15 includes an average quality check image
“stolen” from a real user, showing that the image is clear
enough to read all the private information.

Figure 15: An example check image “stolen” using the
camera peeking attack.

8 Other Attack Scenarios

Enhance existing attacks. Generally, a class of existing
attacks that are launched only at speciﬁc timings bene-
ﬁts from UI state information. Since many attacks need
to be launched at a speciﬁc timing, with the UI state
information, both stealthiness and effectiveness can be
achieved. For example, for the phishing attack using

TCP connection hijacking [9, 10], the attack app can pre-
cisely target at connections established in Activities with
web pages instead of unrelated ones, e.g., database up-
dating, keepalive messages, etc. The attack thus becomes
more efﬁcient and less suspicious by avoiding frequently
sending large amounts of attack trafﬁc [9]. Similar en-
hancement can also be applied to keystroke inference at-
tacks [7, 8] and screenshot taking attack [5] where only
keystrokes entered in login Activities may be of interest.
User behavior monitoring and analysis. UI states
themselves are user behavior related internal states of an
app. As shown in Fig. 2, due to the limited screen size on
the smartphone, full-screen window-level UI state infor-
mation breaks user-app interaction to very ﬁne-grained
states. Thus, by tracking UI states, a background app can
monitor and analyze user behavior, e.g., in a health app
the user is more often looking for drugs or physicians.

In addition, with Activity tracking, the attacker can
even infer which choice is made inside an Activity (e.g.,
which medical condition a user is interested in). This is
achieved using the size of the request packet obtained by
the technique described in §4.1. For example, for QAL-
istActivity of H&R Block app, we can infer which tax
question a user is interested in based on the length of the
question that is embedded in the query packet. In this
question list, we ﬁnd that 10 out of 11 question queries
are distinguishable (with different lengths).

A similar technique was proposed recently [12], but
built upon a network event based state machine, with two
limitations: (1) packet size itself can be highly variable
(ads connections may co-occur) and different Activities
may generate similar packet size traces, e.g., login Activ-
ities and user account Activities both have the authentica-
tion connection thus may have similar packet size trace.
UI state knowledge would limit the space of possible
connections signiﬁcantly as we infer the Activity based
on a more comprehensive set of features; (2) not all user
choices in Android are reﬂected in network packets —
database/Content Provider can also be used to fetch con-
tent. With our UI state machine, we can further extend
the attack to the Content Provider based user choice in-
ference. For example, in WebMD, DrugSearchMainAc-
tivity has a list of letter A to Z. Once one letter is clicked,
Content Provider is queried to fetch a list of drug names
starting from that letter. With the Content Provider query
event and content size inference technique (described in
§4.1), we characterized all of the choices and found fairly
good entropy: the responding content sizes have 16 dif-
ferent values for the 26 letters, corresponding to 4 bits
out of 4.7 bits of information for the user choice.

9 Defense Discussion

In this section, we suggest more secure system designs
to defend against the attacks found in this paper.

USENIX Association  

23rd USENIX Security Symposium  1049

13

In our attack,
Proc ﬁle system access control.
shared vm and features of Activity signature such as
CPU and network all rely on freely accessible ﬁles in
proc ﬁle system. However, as pointed out by Zhou et
[12], simply removing them from the list of pub-
al.
lic resources may not be a good option due to the large
amount of existing apps depending on them. To better
solve the problem, Zhou et al. [12] proposed a mitiga-
tion strategy which reduces the attack effectiveness by
rounding up or down the actual value. This can work for
the network side channel, but may not be effective for
shared vm and shared pm, which are already rounded to
pages (4KB) but still generate a high transition detec-
tion accuracy. This is mainly because the window buffer
size is large enough to be distinct and the side channel
is pretty clean, as discussed in §3.3. Thus, Android sys-
tem may need to reconsider its access control strategy for
these public accessible ﬁles to better balance functional-
ity and security. In fact, Android has already restricted
access to certain proc ﬁles that are publicly accessible in
the standard Linux, e.g., /proc/pid/smaps. However, our
work indicates that it is still far from being secure.
Window manager design. As described in §3.2, the ex-
istence of the shared-memory side channel is due to the
requirement of the window buffer sharing in the client-
drawn buffer design. Thus, a fundamental way of de-
fending against the UI state inference attack in this paper
is to use the server-drawn buffer design in GUI systems,
though this means that any applications that are exposed
to the details of the client-drawn buffer design need to be
updated, which may introduce other side effects.
Window buffer reuse. The Activity transition signal
consists of shared vm increases and decreases, corre-
sponding to window buffer allocations and deallocations.
To eliminate such signal, the system can avoid them by
pre-allocating two copies of the buffers and reuse them
for all transitions in an app. Note that this is at the cost of
much more memory usage for each app, as each buffer is
several megabytes in size. However, with increasingly
larger memory size in future mobile devices [36], we
think this overhead may be acceptable.

In this paper, the most serious security breaches are
caused by follow-up attacks based on UI state inference.
Thus, we provide suggestions as follows that can miti-
gate the attacks even if the UI state information is leaked.
Enforce UI state transition animation. Animation is an
important indicator for informing users about app state
changes. In the Activity hijacking attack in §6, the seam-
less Activity injection is possible because this indicator
can be turned off in Android. With UI state tracking,
the attacker can leverage this to replace the foreground
UI state with a phishing one without any visible indica-
tions. Thus, one defense on GUI system design side is
to always keep this indicator on by enforcing animation

in all UI state transitions. This helps reduce the attack
stealthiness though it cannot fully eliminate the attack.
Limit background application functionality. In GUI
systems, background applications do not directly interact
with users, so they should not perform privacy-sensitive
actions freely. In §7, a background attacker can still get
camera images, indicating that Android did not sufﬁ-
ciently restrict the background app functionality. With
UI state tracking, an attacker can leverage precise timing
to circumvent app data isolation. Thus, more restrictions
should be imposed to limit background applications’ ac-
cess to sensitive resources like camera, GPS, sensor, etc.
To summarize, we propose solutions that eliminate de-
pendencies of the attack such as the proc ﬁle side chan-
nel, which may prevent the attack. However, more inves-
tigation is required to understand their effectiveness and
most of them do require signiﬁcant changes that have im-
pact on either backward-compatibility or functionality.

10 Related Work

Android malware. The Android OS, like any systems,
contains security vulnerabilities and is vulnerable to mal-
ware [37–39]. For instance, the IPC mechanisms leave
Android vulnerable to confused deputy attacks [38, 39].
Malware can collect privacy-sensitive information by re-
questing certain permissions [37, 40]. To combat these
ﬂaws, a number of defenses have been proposed [38,41],
such as tracking the origin of inter-process calls to pre-
vent unauthorized apps from indirectly accessing priv-
ileged information. Our attack requires neither spe-
ciﬁc vulnerabilities nor privacy-sensitive permissions, so
known defense mechanisms will not protect against it.
Side-channel attacks. Much work has been done on
studying side channels. Proc ﬁle systems have been
long abused for side-channel attacks. Zhang et al. [24]
found that
the ESP/EIP value can be used to infer
keystrokes. Qian et al. [10] used “sequence-number-
dependent” packet counter side channels to infer TCP
sequence number. In memento [11], the memory foot-
prints were found to correlate with the web pages a user
visits. Zhou et al. [12] found three Android/Linux public
resources to leak private information. These attacks are
mostly app-dependent, while in this paper the UI state
inference applies generally to all Android apps, leading
to not only a larger attack coverage but also many more
serious attacks. Timing is another popular form of side
channels. Studies have shown that timing can be used
to infer keystrokes as well as user information revealed
by web applications [23, 42–44]. Sensors are more re-
cent, popular side-channel sources. The sound made by
the keyboard [45], electromagnetic waves [46], and spe-
cial software [47] can be used to infer keystrokes. More
recently, a large number of sensor-based side channels
have been discovered on Android, including the micro-

1050  23rd USENIX Security Symposium 

USENIX Association

14

phone [18], accelerometer [7, 8] and camera [34]. Our
attack does not rely on sensors which may require suspi-
cious permissions. Instead, we leverage only data from
the proc ﬁle system, which is readily available with no
permission requirement.
Root causes of side-channel attacks. All side-channel
attacks exist because of certain behavior in the soft-
ware/hardware stack that can be distinguished through
some forms of observable channels by attackers. For ex-
ample, the inter-keystroke timing attack exploits the ap-
plication and OS behavior that handles user input. SSH
programs will send whatever keys the user types immedi-
ately to the network, so the timing is observable through
a network packet trace [23]. For VIM-like programs, cer-
tain program routines are triggered whenever a new key
is captured, so the timing can be captured through snap-
shots of the program’s ESP/EIP values [24]. The TCP
sequence number inference attack [10] exploits the TCP
stack of the OS that exposes internal states through ob-
servable packet counters. In our attack, we exploit a new
side channel caused by popular GUI framework behav-
ior, in particular how user interaction and window events
are designed and implemented.

11 Conclusion

In this paper, we formulate the UI state inference attack
designed at exposing the running UI state of an appli-
cation. This attack is enabled by a newly-discovered
shared-memory side channel, which exists in nearly all
popular GUI systems. We design and implement the An-
droid version of this attack, and show that it has a high
inference accuracy by evaluating it on popular apps. We
then show that UI state tracking can be used as a power-
ful attack building block to enable new Android attacks,
including Activity hijacking and camera peeking. We
also discuss ways of eliminating the side channel, and
suggest more secure system designs.

Acknowledgments

We would like to thank Sanae Rosen, Denis Foo Kune,
Zhengqin Luo, Yu Stephanie Sun, Earlence Fernandes,
Mark S. Gordon, the anonymous reviewers, and our
shepherd, Jaeyeon Jung, for providing valuable feed-
back on our work. This research was supported in part
by the National Science Foundation under grants CNS-
131830, CNS-1059372, CNS-1039657, CNS-1345226,
and CNS-0964545, as well as by the ONR grant N00014-
14-1-0440.

[2] J. S. Shapiro, J. Vanderburgh, E. Northup, and
D. Chizmadia, “Design of the EROS trusted win-
dow system,” in USENIX Security Symposium,
2004.

[3] T. Fischer, A.-R. Sadeghi, and M. Winandy, “A pat-
tern for secure graphical user interface systems,” in
20th International Workshop on Database and Ex-
pert Systems Application.

IEEE, 2009.

[4] S. Chen, J. Meseguer, R. Sasse, H. J. Wang, and
Y.-M. Wang, “A Systematic Approach to Uncover
Security Flaws in GUI Logic,” in IEEE Symposium
on Security and Privacy, 2007.

[5] C.-C. Lin, H. Li, X. Zhou, and X. Wang, “Screen-
milker: How to Milk Your Android Screen for Se-
crets,” in NDSS, 2014.

[6] “Video Demos for this Paper,” https://sites.google.

com/site/uistateinferenceattack/demos.

[7] Z. Xu, K. Bai, and S. Zhu, “Taplogger: Inferring
user inputs on smartphone touchscreens using on-
board motion sensors,” in WiSec, 2012.

[8] E. Miluzzo, A. Varshavsky, S. Balakrishnan, and
R. R. Choudhury, “Tapprints: your ﬁnger taps have
ﬁngerprints,” in Mobisys, 2012.

[9] Z. Qian and Z. M. Mao, “Off-Path TCP Sequence
Number Inference Attack – How Firewall Middle-
boxes Reduce Security,” in IEEE Symposium on Se-
curity and Privacy, 2012.

[10] Z. Qian, Z. M. Mao, and Y. Xie, “Collaborative tcp
sequence number inference attack: how to crack se-
quence number under a second,” in CCS, 2012.

[11] S. Jana and V. Shmatikov, “Memento: Learning Se-
crets from Process Footprints,” in IEEE Symposium
on Security and Privacy, 2012.

[12] X. Zhou, S. Demetriou, D. He, M. Naveed, X. Pan,
X. Wang, C. A. Gunter, and K. Nahrstedt, “Iden-
tity, Location, Disease and More: Inferring Your
Secrets from Android Public Resources,” in CCS,
2013.

[13] “Wayland,” http://wayland.freedesktop.org/.

[14] “Ubuntu Move

to Wayland,”
markshuttleworth.com/archives/551.

http://www.

References

[15] “Mir,” https://wiki.ubuntu.com/Mir.

[1] N. Feske and C. Helmuth, “A Nitpickers guide to a
minimal-complexity secure GUI,” in ACSAC, 2005.

[16] “Back Stack,” http://developer.android.com/guide/

components/tasks-and-back-stack.html.

USENIX Association  

23rd USENIX Security Symposium  1051

15

[17] T. Azim and I. Neamtiu, “Targeted and depth-ﬁrst
exploration for systematic testing of android apps,”
in OOPSLA, 2013.

[33] “Transparent Activity Theme,” http://developer.

android.com/guide/topics/ui/themes.html#
ApplyATheme.

[18] R. Schlegel, K. Zhang, X. yong Zhou, M. Int-
wala, A. Kapadia, and X. Wang, “Soundcomber:
A Stealthy and Context-Aware Sound Trojan for
Smartphones,” in NDSS, 2011.

[19] A. P. Felt, E. Chin, S. Hanna, D. Song, and D. Wag-
ner, “Android Permissions Demystiﬁed,” in CCS,
2011.

[20] “VMMap,”

http://technet.microsoft.com/en-us/

sysinternals/dd535533.aspx.

[21] “project-butter,” http://www.androidpolice.com/
2012/07/12/getting-to-know-android-4-1-part-3-
project-butter-how-it-works-and-what-it-added/.

[22] “Android DDMS,” http://developer.android.com/

tools/debugging/ddms.html.

[34] R. Templeman, Z. Rahman, D. Crandall, and
A. Kapadia, “PlaceRaider: Virtual Theft in Phys-
ical Spaces with Smartphones,” in NDSS, 2013.

[35] “Android Camera,” http://developer.android.com/

reference/android/hardware/Camera.html.

[36] “Samsung Wants to Cram 4GB of RAM into Your

Next Phone,” http://www.pcworld.com/article/
2083320/samsung-lays-groundwork-for-
smartphones-with-more-ram.html.

[37] Y. Zhou and X. Jiang, “Dissecting Android mal-
ware: Characterization and evolution,” in IEEE
Symposium on Security and Privacy, 2012.

[38] A. P. Felt, H. J. Wang, A. Moshchuk, S. Hanna, and
E. Chin, “Permission Re-delegation: Attacks and
Defenses,” in USENIX Security Symposium, 2011.

[23] D. X. Song, D. Wagner, and X. Tian, “Timing Anal-
ysis of Keystrokes and Timing Attacks on SSH,” in
USENIX Security Symposium, 2001.

[39] Y. Zhou and X. Jiang, “Detecting Passive Content
Leaks and Pollution in Android Applications,” in
NDSS, 2013.

[24] K. Zhang and X. Wang,

“Peeping Tom in
the Neighborhood: Keystroke Eavesdropping on
Multi-User Systems,” in USENIX Security Sympo-
sium, 2009.

[40] W. Enck, P. Gilbert, B.-G. Chun, L. P. Cox, J. Jung,
P. McDaniel, and A. Sheth, “TaintDroid: An Infor-
mation Flow Tracking System for Real-Time Pri-
vacy Monitoring on Smartphones,” in OSDI, 2010.

[25] “Whois IP Address Database,” http://whois.net/.

[26] L. R. Rabiner, “A tutorial on hidden Markov mod-
els and selected applications in speech recogni-
tion,” Proceedings of the IEEE, vol. 77, no. 2, pp.
257–286, 1989.

[27] “Android Activity Testing,”

http://developer.

android.com/tools/testing/activity testing.html.

[28] “Monsoon Power Monitor,” http://www.msoon.

com/LabEquipment/PowerMonitor/.

[29] “Focus Stealing Vulnerability,” http://blog.

spiderlabs.com/2011/08/twsl2011-008-focus-
stealing-vulnerability-in-android.html.

[30] “Android

Launching

Mode,”

http://

developer.android.com/guide/topics/manifest/
activity-element.html#lmode.

[31] “Android Broadcast Receiver,” http://developer.

android.com/reference/android/content/
BroadcastReceiver.html.

[32] “Android Apktool,”

http://code.google.com/p/

android-apktool/.

[41] M. Dietz, S. Shekhar, Y. Pisetsky, A. Shu, and
D. S. Wallach, “Quire: Lightweight Provenance for
Smart Phone Operating Systems,” in USENIX Se-
curity Symposium, 2011.

[42] S. Chen, R. Wang, X. Wang, and K. Zhang, “Side-
channel Leaks in Web Applications: A Reality To-
day, a Challenge Tomorrow,” in IEEE Symposium
on Security and Privacy, 2010.

[43] Y. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart,
“Cross-VM side channels and their use to extract
private keys,” in CCS, 2012.

[44] A. Bortz and D. Boneh, “Exposing private informa-

tion by timing web applications,” in WWW, 2007.

[45] L. Zhuang, F. Zhou, and J. D. Tygar, “Keyboard

acoustic emanations revisited,” in CCS, 2005.

[46] M. Vuagnoux and S. Pasini, “Compromising elec-
tromagnetic emanations of wired and wireless key-
boards,” in USENIX security symposium, 2009.

[47] K. Killourhy and R. Maxion,

“Comparing
for Keystroke

Anomaly-Detection Algorithms
Dynamic,” in DSN, 2009.

16

1052  23rd USENIX Security Symposium 

USENIX Association

