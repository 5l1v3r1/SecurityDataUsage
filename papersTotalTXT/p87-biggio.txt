Is Data Clustering in Adversarial Settings Secure?

Battista Biggio

Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy

battista.biggio@diee.unica.it

Davide Ariu

Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy

davide.ariu@diee.unica.it

Ignazio Pillai

Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy
pillai@diee.unica.it

Marcello Pelillo

Università Ca’ Foscari di

Venezia

Via Torino, 155

30172 Venezia-Mestre
pelillo@dais.unive.it

Samuel Rota Bulò

FBK-irst

Via Sommarive, 18
38123, Trento, Italy
rotabulo@fbk.eu

Fabio Roli

Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy
roli@diee.unica.it

ABSTRACT
Clustering algorithms have been increasingly adopted in se-
curity applications to spot dangerous or illicit activities.
However, they have not been originally devised to deal with
deliberate attack attempts that may aim to subvert the
clustering process itself. Whether clustering can be safely
adopted in such settings remains thus questionable. In this
work we propose a general framework that allows one to
identify potential attacks against clustering algorithms, and
to evaluate their impact, by making speciﬁc assumptions on
the adversary’s goal, knowledge of the attacked system, and
capabilities of manipulating the input data. We show that
an attacker may signiﬁcantly poison the whole clustering
process by adding a relatively small percentage of attack
samples to the input data, and that some attack samples
may be obfuscated to be hidden within some existing clus-
ters. We present a case study on single-linkage hierarchical
clustering, and report experiments on clustering of malware
samples and handwritten digits.

Categories and Subject Descriptors
D.4.6 [Security and Protection]: Invasive software (e.g.,
viruses, worms, Trojan horses); G.3 [Probability and Statis-
tics]: Statistical computing; I.5.1 [Models]: Statistical;
I.5.2 [Design Methodology]: Clustering design and eval-
uation; I.5.3 [Clustering]: Algorithms

General Terms
Security, Clustering.

Keywords
Adversarial learning, Unsupervised Learning, Clustering, Se-
curity Evaluation, Computer Security, Malware Detection.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
AISec’13, November 4, 2013, Berlin, Germany.
Copyright 2013 ACM 978-1-4503-2488-5/13/11 ...$15.00.
http://dx.doi.org/10.1145/2517312.2517321

1.

INTRODUCTION

Clustering algorithms are nowadays a fundamental tool
for the data analysts as they allow them to make inference
and gain insights on large sets of unlabeled data. Appli-
cations of clustering span across a large number of diﬀerent
domains, such as market segmentation [14, 26], classiﬁcation
of web pages [10], and image segmentation [12]. In the spe-
ciﬁc domain of computer security, clustering algorithms have
been recently exploited to solve plenty of diﬀerent problems,
e.g., spotting fast-ﬂux domains in DNS traﬃc [24], gaining
useful insights on tools and sources of attacks against Inter-
net websites [25], detecting repackaged Android applications
[16] and (Android) mobile malware [9], and even automati-
cally generating signatures for anti-virus software to enable
detection of HTTP-based malware [23].

In many of the aforementioned scenarios, a large amount
of data is often collected in the wild, in an unsupervised man-
ner. For instance, malware samples are often collected from
the Internet, by means of honeypots, i.e., machines that pur-
posely expose known vulnerabilities to be infected by mal-
ware [28], or other ad hoc services, like VirusTotal.1 Given
that these scenarios are intrinsically adversarial, it may thus
be possible for an attacker to inject carefully crafted samples
into the collected data in order to subvert the clustering pro-
cess, and make the inferred knowledge useless. This raises
the issue of evaluating the security of clustering algorithms
against carefully designed attacks, and proposing suitable
countermeasures, when required. It is worth noting that re-
sults from the literature of clustering stability [29] can not
be directly exploited to this end, since the noise induced
by adversarial manipulations is not generally stochastic but
speciﬁcally targeted against the clustering algorithm.

The problem of learning in adversarial environments has
recently gained increasing popularity, and relevant research
has been done especially in the area of supervised learning
algorithms for classiﬁcation [6, 8, 17, 3], and regression [13].
On the other hand, to the best of our knowledge only few
works have implicitly addressed the issue of security evalua-
tion related to the application of clustering algorithms in ad-
versarial settings through the deﬁnition of suitable attacks,

1http://virustotal.com

87while we are not aware of any work that proposes speciﬁc
countermeasures to attacks against clustering algorithms.

The problem of devising speciﬁc attacks to subvert the
clustering process was ﬁrst brought to light by Dutrisac and
Skillicorn [11, 27]. They pointed out that some points can be
easily hidden within an existing cluster by forming a fringe
cluster, i.e., by placing such points suﬃciently close the bor-
der of the existing cluster. They further devised an attack
that consists of adding points in between two clusters to
merge them, based on the notion of bridging. Despite this
pioneering attempts, a framework for the systematic security
evaluation of clustering algorithms in adversarial settings is
still missing, as well as a more general theory that takes
into account the presence of the adversary to develop more
secure clustering algorithms.

In this work we aim to take a ﬁrst step to ﬁll in this gap, by
proposing a framework for the security evaluation of cluster-
ing algorithms, which allows us to consider several potential
attack scenarios, and to devise the corresponding attacks, in
a more systematic manner. Our framework, inspired from
previous work on the security evaluation of supervised learn-
ing algorithms [6, 17, 3], is grounded on a model of the
attacker that allows one to make speciﬁc assumptions on
the adversary’s goal, knowledge of the attacked system, and
capability of manipulating the input data, and to subse-
quently formalize a corresponding optimal attack strategy.
This work is thus explicitly intended to provide a cornerstone
for the development of an adversarial clustering theory, that
should in turn foster research in this area.

The proposed framework for security evaluation is pre-
sented in Sect. 2.
In Sect. 3 we derive worst-case attacks
in which the attacker has perfect knowledge of the attacked
system.
In particular, we formalize the notion of (worst-
case) poisoning and obfuscation attacks against a clustering
algorithm, respectively in Sects. 3.1 and 3.2.
In the for-
mer case, the adversary aims at maximally compromising
the clustering output by injecting a number of carefully de-
signed attack samples, whereas in the latter one, she tries to
hide some attack samples into an existing cluster by manipu-
lating their feature values, without signiﬁcantly altering the
clustering output on the rest of the data. As a case study, we
evaluate the security of the single-linkage hierarchical clus-
tering against poisoning and obfuscation attacks, in Sect. 4.
The underlying reason is simply that the single-linkage hier-
archical clustering has been widely used in security-related
applications [4, 16, 23, 24]. To cope with the computa-
tional problem of deriving an optimal attack, in Sects. 4.1
and 4.2 we propose heuristic approaches that serve well our
purposes. Finally, in Sect. 5 we conduct synthetic and real-
world experiments that demonstrate the eﬀectiveness of the
proposed attacks, and subsequently discuss limitations and
future extensions of our work in Sect. 6.

2. ATTACKING CLUSTERING

In this section we present our framework to analyze the
security of clustering approaches from an adversarial pattern
recognition perspective. It is grounded on a model of the ad-
versary that can be exploited to identify and devise attacks
against clustering algorithms. Our framework is inspired by
a previous work focused on attacking (supervised) machine
learning algorithms [6], and it relies on an attack taxonomy
similar to the one proposed in [17, 3]. As in [6], the adver-

sary’s model entails the deﬁnition of the adversary’s goal,
knowledge of the attacked system, and capability of manip-
ulating the input data, according to well-deﬁned guidelines.
Before moving into the details of our framework, we intro-
duce some notation. Clustering is the problem of organizing
a set of data points into groups referred to as clusters in a
way that some criteria is satisﬁed. A clustering algorithm
can thus be formalized in terms of a function f mapping a
given dataset D = {xi}n
i=1 to a clustering result C = f (D).
We do not specify the mathematical structure of C at this
point of our discussion because there exist diﬀerent types
of clustering requiring diﬀerent representations, while our
model applies to any of them. Indeed, C might be a hard or
soft partition of D delivered by partitional clusterings algo-
rithms such as k-means, fuzzy c-means or normalized cuts,
or it could be a more general family of subsets of D such as
the one delivered by the dominant sets clustering algorithm
[22], or it can even be a parametrized hierarchy of subsets
(e.g., linkage-type clustering algorithms).

2.1 Adversary’s goal

Similarly to [6, 17, 3], the adversary’s goal can be deﬁned
according to the attack speciﬁcity, and the security violation
pursued by the adversary. The attack speciﬁcity can be
targeted, if it aﬀects solely the clustering of a given subset
of samples; or indiscriminate, if it potentially aﬀects the
clustering of any sample. Security violations can instead
aﬀect the integrity or the availability of a system, or the
privacy of its users.

Integrity violations amount to performing some malicious
activity without signiﬁcantly compromising the normal sys-
tem operation.
In the supervised learning setting [17, 3],
they are deﬁned as attacks aiming at camouﬂaging some
malicious samples (e.g., spam emails) to evade detection,
without aﬀecting the classiﬁcation of legitimate samples. In
the unsupervised setting, however, this deﬁnition can not be
generally applied since the notion of malicious or legitimate
class is not generally available. Therefore, we regard in-
tegrity violations as attacks aiming at deﬂecting the group-
ing for speciﬁc samples, while limiting the changes to the
original clustering. For instance, an attacker may obfuscate
some samples to hide them in a diﬀerent cluster, without
excessively altering the initial clusters.

Availability violations aim to compromise the functional-
ity of the system by causing a denial of service. In the super-
vised setting, this translates into causing the largest possible
classiﬁcation error [17, 6, 7]. According to the same ratio-
nale, in the unsupervised setting we can consider attacks
that signiﬁcantly aﬀect the clustering process by worsening
its result as much as possible.

Finally, privacy violations may allow the adversary to ob-
tain information about the system’s users from the clustered
data by reverse-engineering the clustering process.

2.2 Adversary’s knowledge

The adversary can have diﬀerent degrees of knowledge of
the attacked system. They can be deﬁned by making speciﬁc
assumptions on the points (k.i)-(k.iv) described below.

(k.i) Knowledge of the data D: The adversary might
know the data D or only a portion of it. More realistically,
she may not know D exactly, but she may be able to obtain a
surrogate dataset sampled from the same distribution as D.

88In practice, this can be obtained by collecting samples from
the same source from which samples in D were collected;
e.g., honeypots for malware samples [28].

(k.ii) Knowledge of the feature space: The adversary
could know how features are extracted from each sample.
Similarly to the previous case, she may know how to com-
pute the whole feature set, or only a subset of the features.
(k.iii) Knowledge of the algorithm: The adversary’s
could be aware of the targeted clustering algorithm and how
it organizes the data into clusters; e.g., the criterion used to
determine the cluster set from a hierarchy in hierarchical
clustering.

(k.iv) Knowledge of the algorithm’s parameters:
The attacker may even know how the parameters of the
clustering algorithm have been initialized (if any).

Perfect knowledge. The worst-case scenario in which
the attacker has full knowledge of the attacked system, is
usually referred to as perfect knowledge case [6, 7, 19, 8,
17, 3].
(k.i) the
data, (k.ii) the feature representation, (k.iii) the clustering
algorithm, and (k.iv) its initialization (if any).

In our case, this amounts to knowing:

2.3 Adversary’s capability

The adversary’s capability deﬁnes how and to what extent
the attacker can control the clustering process. In the super-
vised setting [17, 6], the attacker can exercise a causative or
exploratory inﬂuence, depending on whether she can control
training and test data, or only test data. In the case of clus-
tering, however, there is not a test phase in which some data
has to be classiﬁed. Accordingly, the adversary may only ex-
ercise a causative inﬂuence by manipulating part of the data
to be clustered.2 This is often the case, though, since this
data is typically collected in an unsupervised manner.

We thus consider a scenario in which the attacker can
add a maximum number of (potentially manipulated) sam-
ples to the dataset D. This is realistic in several practical
cases, e.g., in the case of malware collected through honey-
pots [28], where the adversary may easily send (few) samples
without having access to the rest of the data. This amounts
to controlling a (small) percentage of the input data. An
additional constraint may be given in terms of a maximum
amount of modiﬁcations that can be done to the attack sam-
ples. In fact, to preserve their malicious functionality, mali-
cious samples like spam emails or malware code may not be
manipulated in an unconstrained manner. Such a constraint
can be encoded by a suitable distance measure between the
original, non-manipulated attack samples and the manipu-
lated ones, as in [6, 20, 17, 3].

2.4 Attack strategy

Once the adversary’s goal, knowledge and capabilities have
been deﬁned, one can determine an optimal attack strategy
that speciﬁes how to manipulate the data to meet the ad-
versary’s goal, under the restriction given by the adversary’s
knowledge and capabilities. In formal terms, we denote by
Θ the knowledge space of the adversary. Elements of Θ hold
information about the dataset D, the clustering algorithm

2One may however think of an exploratory attack to a clus-
tering algorithm as an attack in which the adversary aims to
gain information on the clustering algorithm itself, although
she may not necessarily manipulate any data to this end.

f , and its parametrization, according to (k.i)-k(.iv). To
model the degree of knowledge of the adversary we con-
sider a probability distribution µ over Θ. The entropy of
µ indicates the level of uncertainty of the attacker. For
example, if we consider a perfect-knowledge scenario like
the one addressed in the next section, we have that µ is a
Dirac measure peaked on an element θ0 ∈ Θ (with null en-
tropy), where θ0 = (D, f, · · · ) holds the information about
the dataset, the algorithm and any other of the informations
listed in Sect.2.2. Further, we assume that the adversary is
given a set of attack samples A that can be manipulated be-
fore being added to the original set D. We model with the
function Ω(A) the family of sample sets that the attacker
can generate according to her capability as a function of the
set of initial attack samples A. The set A can be empty, if
the attack samples are not required to fulﬁll any constraint
on their malicious functionality, i.e., they can be generated
from scratch (as we will see in the case of poisoning attacks).
Finally, the adversary’s goal given the knowledge θ ∈ Θ is
expressed in terms of an objective function g(A′; θ) ∈ R
that evaluates how close the modiﬁed data set integrating
the (potentially manipulated) attack samples A′ is to the ad-
versary’s goal. In summary, the attack strategy boils down
to ﬁnding a solution to the following optimization problem:

maximize Eθ∼µ[g(A′; θ)]

s.t. A′ ∈ Ω(A) .

(1)

where Eθ∼µ[·] denotes the expectation with respect to θ be-
ing sampled according to the distribution µ.

3. PERFECT KNOWLEDGE ATTACKS

In this section we provide examples of worst-case integrity
and availability security violations in which the attacker has
perfect knowledge of the system, as described in Sect. 2.2.
We respectively refer to them as poisoning and obfuscation
attacks. Since the attacker has no uncertainty about the sys-
tem, we set µ = δ{θ0}, where δ is the Dirac measure and θ0
represents exact knowledge of the system. The expectation
in (1) thus yields g(A′; θ0).

3.1 Poisoning attacks

Similarly to poisoning attacks against supervised learn-
ing algorithms [7, 19], we deﬁne poisoning attacks against
clustering algorithms as attacks in which the data is tainted
to maximally worsen the clustering result. The adversary’s
goal thus amounts to violating the system’s availability by
indiscriminately altering the clustering output on any data
point. To this end, the adversary may aim at maximizing
a given distance measure between the clustering C obtained
from the original data D (in the absence of attack) and the
clustering C′ = fD(D′) obtained by running the clustering
algorithm on the contaminated data D′, and restricting the
result to the samples in D, i.e., fD = πD ◦ f where πD is
a projection operator that restricts the clustering output to
the data samples in D. We regard the tainted data D′ as
the union of the original dataset D with the attack samples
in A′, i.e., D′ = D ∪ A′. The goal can thus be written as
g(A′; θ0) = dc(C, fD(D ∪ A′)), where dc is the chosen dis-
tance measure between clusterings. For instance, if f is a
partitional clustering algorithm, any clustering result can be
represented in terms of a matrix Y ∈ Rn×k, each (i, k)th com-
ponent being the probability that the ith sample is assigned

89to the kth cluster. Under this setting, a possible distance
measure between clusterings is given by:

scalar. Consequently, the function Ω representing the at-
tacker’s capacity is given by

dc(Y, Y′) = kYY⊤ − Y′Y′⊤kF ,

(2)

where k · kF is the Frobenius norm. The components of the
matrix YY⊤ represent the probability of two samples to be-
long to the same cluster. When Y is binary, thus encoding
hard clustering assignments, this distance counts the num-
ber of times two samples have been clustered together in one
clustering and not in the other, or vice versa. In general, de-
pending on the nature of the clustering result, other ad-hoc
distance measures can be adopted.

As mentioned in Sect. 2.3, we assume that the attacker
can inject a maximum of m data points into the original
data D, i.e. |A′| ≤ m. This realistically limits the adversary
to manipulate only a given, potentially small fraction of the
dataset. Clearly, the value of m will be considered as a pa-
rameter in our evaluation to investigate the robustness of
the given clustering algorithm against an increasing control
of the adversary over the data. We further deﬁne a box con-
straint on the feature values xlb ≤ x ≤ xub, to restrict the
attack points to lie in some ﬁxed interval (e.g., the smallest
box that includes all the data points). Hence, we deﬁne the
function Ω encoding the adversary’s capabilities as follows:

Ωp = n{a′
i}m

i=1 ⊂ R

d : xlb ≤ a′

i ≤ xub for i = 1, · · · , mo .

Note that Ω depends on a set of target samples A in (1), but
since A is empty in this case, we write Ωp instead of Ω(∅).
The reason is simply that, in the case of a poisoning attack,
the attacker aims to ﬁnd a set of attack samples that do
not have to carry out any speciﬁc malicious activity besides
worsening the clustering process.

In summary, the optimal attack strategy under the afore-
mentioned hypothesis amounts to solving the following op-
timization problem derived from (1):

maximize dc(C, fD(D ∪ A′))

s.t. A′ ∈ Ωp .

(3)

3.2 Obfuscation attacks

Obfuscation attacks are violations of the system integrity
through targeted attacks. The adversary’s goal here is to
hide a given set of initial attack samples A within some ex-
isting clusters by obfuscating their content, possibly without
altering the clustering results for the other samples. We de-
note by Ct the target clustering involving samples in D ∪ A′
the attacker is aiming to, being A′ the set of obfuscated
attack samples. With the intent to preserve the cluster-
ing result C on the original data samples, we impose that
πD(Ct) = C, while the cluster assignments for the samples
in A′ are freely determined by the attacker. As opposed to
the poisoning attack, here the attacker is interested in push-
ing the ﬁnal clustering towards the target clustering and
therefore her intention is to minimize the distance between
Ct and C′ = f (D ∪ A′). Accordingly, the goal function g in
this case is deﬁned as g(A′; θ0) = −d(Ct, f (D ∪ A′)).

As for the adversary’s capability, we assume that the at-
tacker can perturb the target samples in A to some maxi-
mum extent. We model this by imposing that ds(A, A′) ≤
dmax, where ds is a measure of divergence between the two
sets of samples A and A′ and dmax is a nonnegative real

Ωo(A) = n{a′

i=1 : ds(A, A′) ≤ dmaxo .
i}|A|

The distance ds can be deﬁned in diﬀerent ways. For in-
stance, in the next section we deﬁne ds(A, A′) as the largest
Euclidean distance among corresponding elements in A and
A′, i.e.,

ds(A, A′) = max

i=1,...,m

kai − a′

ik2

(4)

where we assume A = {ai}m
i=1. This
choice allows us to bound the divergence between the origi-
nal target samples in A and the manipulated ones, as typi-
cally done in adversarial learning [20, 17, 8, 6].

i=1 and A′ = {a′

i}m

In summary, the attack strategy in the case of obfusca-
tion attacks can be obtained as the solution of the following
optimization program derived from (1):

minimize dc(Ct, f (D ∪ A′))

s.t. A′ ∈ Ωo(A) .

(5)

4. A CASE STUDY ON SINGLE-LINKAGE

HIERARCHICAL CLUSTERING

In this section we solve a particular instance of the opti-
mization problems (3) and (5), corresponding respectively to
the poisoning and obfuscation attacks described in Sects. 3.1
and 3.2, against the single-linkage hierarchical clustering.
The motivation behind this speciﬁc choice of clustering algo-
rithm is that, as mentioned in Sect. 1, it has been frequently
exploited in security-sensitive tasks [4, 16, 23, 24].

Single-linkage hierarchical clustering is a bottom-up al-
gorithm that produces a hierarchy of clusterings, as any
other hierarchical agglomerative clustering algorithm [18].
The hierarchy is represented by a dendrogram, i.e., a tree-
like data structure showing the sequence of cluster fusion
together with the distance at which each fusion took place.
To obtain a given partitioning of the data into clusters, the
dendrogram has to be cut at a certain height. The leaves
that form a connected sub-graph after the cut are considered
part of the same cluster. Depending on the chosen distance
between clusters (linkage criterion), diﬀerent variants of hi-
erarchical clustering can be deﬁned.
In the single-linkage
variant, the distance between any two clusters C1, C2 is de-
ﬁned as the minimum Euclidean distance between all pairs
of samples in C1 × C2.

For both poisoning and obfuscation attacks, we will model
the clustering output as a binary matrix Y ∈ {0, 1}n×k, in-
dicating the sample-to-cluster assignments (see Sect. 3.1).
Consequently, we can make use of the distance measure dc
between clusterings deﬁned in Eq. (2). However, to obtain
a given set of clusters from the dendrogram obtained by the
single-linkage clustering algorithm, we will have to specify
an appropriate cut criterion.

4.1 Poisoning attacks

For poisoning attacks against single-linkage hierarchical
clustering, we aim to solve the optimization problem given
by Eq. (3). As already mentioned, since the clustering is ex-
pressed in terms of a hierarchy, we have to determine a suit-
able dendrogram cut in order to model the clustering output

90 ✂✄

 

✁✂✄

✁

☎✂✄

☎

✲☎✂✄

✲✁

✲✁✂✄

✲ 

✲ ✂✄

✁✻

 ✂✄

✁✹

✁ 

✁☎

✽

✻

✹

 

 

✁✂✄

✁

☎✂✄

☎

✲☎✂✄

✲✁

✲✁✂✄

✲ 

✲ ✂✄

✹✂✄

✹

✸✂✄

✸

 ✂✄

 

✁✂✄

✁

☎✂✄

✲  ✲✁✂✄ ✲✁ ✲☎✂✄ ☎ ☎✂✄ ✁ ✁✂✄

✲  ✲✁✂✄ ✲✁ ✲☎✂✄ ☎ ☎✂✄ ✁ ✁✂✄

Figure 1: Poisoning single-linkage hierarchical clustering. In each plot, samples belonging to diﬀerent clusters
are represented with diﬀerent markers and colors. The left and middle plot show the initial partitioning of
the given 100 data points into k = 4 clusters. The objective function of Eq. 3 (shown in colors) for our greedy
attack (|A′| = 1) is respectively computed with hard (left plot) and soft assignments (middle plot), i.e., with
binary Y and posterior estimates. The k − 1 = 3 bridges obtained from the dendrogram are highlighted with
red lines. The rightmost plot shows how the partitioning changes after m = 20 attack samples (highlighted
with red circles) have been greedily added.

as a binary matrix Y. In this case, we assume that the clus-
tering algorithm selects the cut, i.e., the number of clusters,
that achieves the minimum distance between the clustering
obtained in the absence of attack C and the one induced by
the cut, i.e., min dc(C, fD(D ∪ A′)). Although this may not
be a realistic cut criterion, as the ideal clustering C is not
known to the clustering algorithm, this worst-case choice for
the adversary gives us the minimum performance degrada-
tion incurred by the clustering algorithm under attack.

Let us now discuss how Problem (3) can be solved. First,
note that it is not possible to predict analytically how the
clustering output Y′ changes as the set of attack samples
A′ is altered, since hierarchical clustering does not have a
tractable, underlying analytical interpretation.3 One possi-
ble answer consists in a stochastic exploration of the solution
space (e.g. by simulated annealing). This is essentially done
by perturbing the input data A′ a number of times, and eval-
uating the corresponding values of the objective function by
running the clustering algorithm (as a black box) on D ∪ A′.
The set A′ that provides the highest objective value is even-
tually retained. However, to ﬁnd an optimal conﬁguration of
attack samples A′, one should repeat this procedure a very
large number of times. To reduce computational complexity,
one may thus consider eﬃcient search heuristics speciﬁcally
tailored to the considered clustering algorithm.

For the above reason, we consider a greedy optimization
approach where the attacker aims at ﬁnding a local maxi-
mum of the objective function by adding one attack sample
at a time, i.e., |A′| = m = 1.
In this case, we can more
easily understand how the objective function changes as the
inserted attack point varies, and deﬁne a suitable heuris-
tic approach. An example is shown in the leftmost plot of
Fig. 1. This plot shows that the objective function exhibits a
global maximum when the attack point is added in between
clusters that are suﬃciently close to each other. The reason
is that, when added in such a location, the attack point op-

3In general, even if the clustering algorithm has a clearer
mathematical formulation, it is not guaranteed that a good
analytical prediction can be found. For instance, though k-
means clustering is well-understood mathematically, its vari-
ability to diﬀerent initializations makes it almost impossible
to reliably predict how its output may change due to data
perturbation.

erates as a bridge, causing the two clusters to be merged in
a single cluster, and the objective function to increase.

Bridge-based heuristic search. Based on this observa-
tion, we devised a search heuristic that considers only k − 1
potential attack samples, being k the actual number of clus-
ters found by the single-linkage hierarchical clustering at a
given dendrogram cut.
In particular, we only considered
the k − 1 points lying in between the connections that have
been cut to separate the k given clusters from the top of
the hierarchy, highlighted in our example in the leftmost
plot of Fig. 1. These connections can be directly obtained
from the dendrogram, i.e., we do not have to run any post-
processing algorithm on the clustering result. Thus, one is
only required to evaluate the objective function k − 1 times
for selecting the best attack point. We will refer to this ap-
proach as Bridge (Best) in Sect. 5.1. The rightmost plot in
Fig. 1 shows the eﬀect of our greedy attack after that m = 20
attack points have been inserted. Note how the initial clus-
ters are fragmented into smaller clusters that tend to contain
points which originally belonged to diﬀerent clusters.

Approximating Y′. To further reduce the computational
complexity of our approach, i.e., to avoid re-computing the
clustering and the corresponding value of the objective func-
tion k − 1 times for each attack point, we consider another
heuristic approach. The underlying idea is simply to select
the attack sample (among the k − 1 bridges suggested by
our bridge-based heuristic search) that lies in between the
largest clusters.
In particular, we assume that the attack
point will eﬀectively merge the two adjacent clusters, and
thus modify Y′ accordingly (without re-estimating its real
value by re-running the clustering algorithm). To this end,
for each point belonging to one of the two clusters, we set
to 1 (0) the value of Y′ corresponding to the ﬁrst (second)
cluster. Once the estimated Y′ is computed, we evaluate the
objective function using the estimated Y′, and select the at-
tack point that maximizes its value. We will refer to this
approach as Bridge (Hard) in Sect. 5.1.

Approximating Y′ with soft clustering assignments.
Finally, we discuss another variation to the latter discussed
heuristic approach, which we will refer to as Bridge (Soft),
in Sect. 5.1. The problem arises from the fact that our ob-
jective function exhibits really abrupt variations, since it is

91computed on hard cluster assignments (i.e., binary matri-
ces Y′). Accordingly, adding a single attack point at a time
may not reveal connections that can potentially merge large
clusters after few attack iterations, i.e., using more than one
attack sample. To address this issue, we approximate Y′ with
soft clustering assignments. To this end, the element y′
ik of Y′
is estimated as the posterior probability of point xi belong-
ing to cluster ck, i.e., y′
ik = p(ck|xi) = p(xi|ck)p(ck)/p(xi).
The prior p(ck) is estimated as the number of samples be-
longing to ck divided by the total number of samples, the
likelihood p(xi|ck) is estimated with a Gaussian Kernel Den-
sity Estimator (KDE) with bandwidth parameter h:

p(xi|ck) =

1

|ck| Xxj ∈ck

exp(cid:18)−

||xi − xj||2

h

(cid:19) ,

(6)

and the evidence p(xi) is obtained by marginalization over
the given set of clusters.

Worth noting, for too small values of h, the posterior es-
timates tend to the same value, i.e., each point is likely to
be assigned to any cluster with the same probability. When
h is too high, instead, each point is assigned to one cluster,
and the objective function thus equals that corresponding to
the original hard assignments. In our experiments we simply
avoid these limit cases by selecting a value of h comparable
to the average distance between all possible pairs of samples
in the dataset, which gave reasonable results.

An example of the smoother approximation of the objec-
tive function provided by this heuristic is shown in the mid-
dle plot of Fig. 1. Besides, this technique also provides a reli-
able approximation of the true objective: although its values
are signiﬁcantly re-scaled, the global maximum is still found
in the same location. The smooth variations that character-
ize the approximated objective inﬂuence the choice of the
best candidate attack point.
In fact, attack points lying
on bridges that may potentially connect larger clusters after
some attack iterations may be sometimes preferred to attack
points that can directly connect smaller and closer clusters.
This may lead to a larger increase in the true objective func-
tion as the number of injected attack points increases.

4.2 Obfuscation attacks

In this section we solve (5) assuming the worst-case (perfect-

knowledge) scenario against the single-linkage clustering al-
gorithm. Recall that the attacker’s goal in this case is to
manipulate a given set of non-obfuscated samples A such
that they are clustered according to a desired conﬁguration,
e.g., together with points in an existing, given cluster, with-
out altering signiﬁcantly the initial clustering that would be
obtained in the absence of manipulated attacks.

As in the previous case, to represent the output of the clus-
tering algorithm as a binary matrix Y representing clustering
assignments, and thus compute dc as given by Eq. 2, we have
to deﬁne a suitable criterion for cutting the dendrogram.
Similarly to poisoning attacks, we deﬁne an advantageous
criterion for the clustering algorithm, that gives us the low-
est performance degradation incurred under this attack: we
select the dendrogram cut that minimizes dc(C⋆, f (D ∪ A′)),
where C⋆ represents the optimal clustering that would be ob-
tained including the non-manipulated attack samples, i.e.,
C⋆ = f (D ∪ A). The reason is that, to better contrast an ob-
fuscation attack, the clustering algorithm should try to keep
the attack points corresponding to the non-manipulated set

A into their original clusters. For instance, in the case of
malware clustering, non-obfuscated malware may easily end
up in a well-deﬁned cluster, and, thus, it may be subse-
quently categorized in a well-behaved malware family. While
the adversary tries to manipulate malware to have it clus-
tered diﬀerently, the best solution for the clustering algo-
rithm would be to obtain the same clusters that would be
obtained in the absence of attack manipulation.

We derive a simple heuristic to get an approximate solu-
tion of (5) assuming ds to be deﬁned as in (4). We assume
that, for each sample ai ∈ A, the attacker selects the closest
sample di ∈ D belonging to the cluster to which ai should
belong to, according to the attacker’s desired clustering Ct.
To meets the constraint given by Ωo in Eq. 5, the attacker
then determines for each ai ∈ A a new sample a′
i ∈ A along
the line connecting ai and di in a way not to exceed the
maximum distance dmax from ai, i.e., a′
i = ai + α(di − ai),
where α = min(1, dmax/kdi − aik2).

5. EXPERIMENTS

We present here some experiments to evaluate the eﬀec-
tiveness of the poisoning and obfuscation attacks devised
in Sect. 4 against the single-linkage hierarchical clustering
algorithm, under perfect knowledge of the attacked system.

5.1 Experiments on poisoning attacks

For the poisoning attack, we consider three distinct cases:
a two-dimensional artiﬁcial data set, a realistic application
example on malware clustering, and a task in which we aim
to cluster together distinct handwritten digits.

5.1.1 Artiﬁcial data

We consider here the standard two-dimensional banana-
shaped dataset from PRTools,4 for which a particular in-
stance is shown in Fig. 1 (right and middle plot). We ﬁx the
number of initial clusters to k = 4, which yields our original
clustering C in the absence of attack.

We repeat the experiment ﬁve times, each time by ran-
domly sampling 80 data points. In each run, we add up to
m = 20 attack samples, that simulates a scenario in which
the adversary can control up to 20% of the data. As de-
scribed in Sect. 4.1, the attack proceeds greedily by adding
one sample at a time. After adding each attack sample, we
allow the clustering algorithm to change the number of clus-
ters from a minimum of 2 to a maximum of 50. The criterion
used to determine the number of clusters is to minimize the
distance of the current partitioning with the clustering in
the absence of attack, as explained in details in Sect. 4.1.

We consider ﬁve attack strategies, described in the follow-

ing.

Random: the attack point is selected at random in the

minimum box that encloses the data.

Random (Best): k − 1 attack points are selected at ran-
dom, being k the actual number of clusters at a given attack
iteration. Then, the objective function is evaluated for each
point, and the best one is chosen.

Bridge (Best): The k−1 bridges suggested by our heuristic

approach are evaluated, and the best one is chosen.

4http://prtools.org

92n
o

 

i
t
c
n
u
F
e
v
i
t
c
e
b
O

j

)
k
(
 
s
r
e
t
s
u
C
m
u
N

 

l

Banana

60
50
40
30
20
10
0
0% 2% 5% 7% 9% 12% 15% 18% 20%
6
4
14
12
10
8
0% 2% 5% 7% 9% 12% 15% 18% 20%
Fraction of samples controlled by the attacker

n
o

 

i
t
c
n
u
F
e
v
i
t
c
e
b
O

j

180
160
140
120
100
80
60
40
20
0
0%

Malware

n
o
i
t
c
n
u
F
 
e
v
i
t
c
e
b
O

j

1%

2%

3%

4%

5%

Digits

 

800
700
600
500
400
300
200
100
0
0.0% 0.2% 0.4% 0.6% 0.8% 1.0%

Random
Random (Best)
Bridge (Best)
Bridge (Soft)
Bridge (Hard)

 

)
k
(
 
s
r
e
t
s
u
C
m
u
N

 

l

30
25
20
15
10
5
0%
5%
Fraction of samples controlled by the attacker

1%

2%

3%

4%

)
k
(
 
s
r
e
t
s
u
C
m
u
N

 

l

100
80
60
40
20
0
0.0% 0.2% 0.4% 0.6% 0.8% 1.0%
Fraction of samples controlled by the attacker

Figure 2: Results for the poisoning attack averaged over ﬁve runs on the Banana-shaped dataset (ﬁrst
column), the Malware dataset (second column), and the Digit dataset (third column). Top plots show the
variation of the objective function dc(f (D), fD(D ∪ A′)) as the fraction of samples controlled by the adversary
increases. Bottom plots report the number of clusters selected after the insertion of each attack sample.

Banana (20%)

Malware (5%)

Digits (1%)

Split

Merge

Split

Merge

Split

Merge

Random
Random (Best)
Bridge (Best)
Bridge (Soft)
Bridge (Hard)

1.15 ± 0.22
1.40 ± 0.34
2.40 ± 0.60
3.85 ± 1.35
3.75 ± 1.43

1.29 ± 0.06
1.54 ± 0.30
1.40 ± 0.23
1.22 ± 0.11
1.21 ± 0.23

1.00 ± 0.00
1.00 ± 0.00
1.49 ± 0.23
2.76 ± 0.84
2.41 ± 0.73

1.00 ± 0.00
1.34 ± 0.39
1.31 ± 0.17
1.12 ± 0.09
1.10 ± 0.10

1.00 ± 0.00
1.00 ± 0.00
33.9 ± 0.15
33.9 ± 0.15
34.0 ± 0.00

1.00 ± 0.00
1.00 ± 0.00
1.02 ± 0.00
1.02 ± 0.00
1.02 ± 0.00

Table 1: Split and Merge averaged values and standard deviations for the Banana-shaped dataset (at 20%
poisoning), the Malware dataset (at 5% poisoning), and the Digit dataset (at 1% poisoning).

Bridge (Hard): The k − 1 bridges are evaluated here by
predicting the clustering output Y′ as discussed in Sect. 4.1
(i.e., assuming that the corresponding clusters will be merged),
using hard clustering assignments.

Bridge (Soft): This is the same strategy as Bridge (Hard),
except for the fact that we consider soft clustering assign-
ments when modifying Y′. To this end, as discussed in
Sect. 4.1, we use a Gaussian KDE. We set the kernel band-
width h as the average distance between each possible pair
of samples in the data. On average, h ≈ 2 in each run.

It is worth remarking that Random (Best) and Bridge
(Best) require the objective function to be evaluated k − 1
times at each iteration to select the best candidate attack
sample. This means that the clustering algorithm has to be
run k − 1 times at each step. Instead, the other methods
do not require us to re-run the clustering algorithm to select
the attack point. Their complexity is therefore signiﬁcantly
lower than the aforementioned methods.

The results averaged over the ﬁve runs are reported in
Fig. 2 (ﬁrst column). From the top plot one may appre-
ciate how the methods based on the bridge-based heuris-
tics achieve similar values of the objective function, while
clearly outperforming the random-based methods. Further,
as reasonably expected, Random (Best) outperforms Ran-
dom since it considers the best point over k − 1 attempts.
Nevertheless, even selecting a random attack sample, in this
case, turned out to signiﬁcantly aﬀect the clustering results.
The bottom plot provides us a better understanding of
how the attack eﬀectively works. The main eﬀect is in-

deed to fragment the original clusters into a high number of
smaller clusters. In particular, after the insertion of m = 20
data points, i.e., when 20% of the data is controlled by the
attacker, the selected number of clusters increases from 4 to
about 7-14 clusters depending on the considered method.

To further clarify the eﬀect of the attack on the clustering
algorithm, we consider two measures referred to as Split and
Merge in Table 5.1, which are given as follows. Let C and C′
be the initial and the ﬁnal clustering restricted to elements
in D, respectively, and let C be a binary matrix, each entry
Ckk′ indicating the co-occurrence of at least one sample in
the kth cluster of C and in the k′th cluster of C ′. Then, the
above measures are given as:

Split = mean

i Xj

Cij , Merge = mean

j Xi

Cij .

Intuitively, split quantiﬁes to what extent the initial clusters
are fragmented across diﬀerent ﬁnal clusters, while merge
quantiﬁes to what extent the ﬁnal clusters contain samples
that originally belonged to diﬀerent initial clusters.

From Table 5.1, it can be appreciated how, for the most
eﬀective attacks, i.e., Bridge (Soft) and Bridge (Hard), the
initial clusters are split into approximately 3.8 clusters, while
the ﬁnal clusters merge approximately 1.2 initial clusters, on
average. This clariﬁes how the proposed attack eventually
compromises the initial clustering: it tends to fragment the
initial clusters into smaller ones, and to merge together ﬁnal
clusters which originally came from diﬀerent clusters. Bridge
(Best) tends instead to induce a lower number of ﬁnal clus-

93❘ ✁✂✄☎

❘ ✁✂✄☎

✭✝✞✟✠✡

✝❇☛✂☞✞

✭✝✞✟✠✡

✝❇☛✂☞✞
✭✌✄✍✠✡

✝❇☛✂☞✞
✭✎ ❇✂✡

✥

✆

✥✶

Figure 3: Attack samples produced by the ﬁve
strategies at iterations 1, 2 and 10, for the digit data.

ters, i.e., the clustering algorithm tends to merge more ﬁnal
clusters than splitting initial ones. However, this is not the
optimal choice according to the attacker’s goal.

5.1.2 Malware clustering

We consider here a more realistic application example in-
volving malware clustering, and in particular a simpliﬁed
version of the algorithm for behavioral malware clustering
proposed in [23]. The ultimate goal of this approach is to
obtain malware clusters that can aid the automatic gen-
eration of high quality network signatures, which can be
used in turn to detect botnet command-and-control (C&C)
and other malware-generated communications at the net-
work perimeter. With respect to the original algorithm, we
made the following simpliﬁcations:

(a) we consider only the ﬁrst of the two clustering steps
carried out by the original system. The algorithm pro-
posed in [23] clusters samples through two consecutive
stages, named coarse-grain and ﬁne-grain clustering, re-
spectively. Here, we just focus on the coarse-grain clus-
tering, which is based on a set of numeric features.

(b) We consider a subset of six statistical features (out of
the seven used by the original algorithm). They are: (1)
number of GET requests; (2) number of POST requests;
(3) average length of the URLs; (4) average number of
parameters in the request; (5) average amount of data
sent by POST requests; and (6) average length of the
response. We exclude the seventh feature, i.e., the total
number of HTTP requests, as it is redundant with respect
to the ﬁrst and the second feature. All feature values are
re-scaled in [0, 1] as in the original work.

(c) We use the single-linkage hierarchical clustering instead
of the BIRCH algorithm [30], since this modiﬁcation does
not signiﬁcantly aﬀect the quality of the clustering re-
sults, as the authors demonstrated in [23].

For the purpose of this evaluation, we use a subset of 1,000
samples taken from Dataset 1 of [23]. This dataset consists
of distinct malware samples (no duplicates) collected dur-
ing March 2010 from a number of diﬀerent malware sources,
including MWCollect [1], Malfease [2], and commercial mal-
ware feeds. As in the previous setting, we repeat the ex-
periments ﬁve times, by randomly selecting a subset of 475
samples from the available set of 1, 000 malware data in each
run. The initial set of clusters C, as in [23], is selected as the
partitioning that minimizes the value of the Davies-Bouldin
Index (DBI) [15], a measure that characterizes dispersion

and closeness of clusters. We consider the cuts of the initial
dendrogram that yield from 2 to 25 clusters, and choose the
one corresponding to the minimum DBI. This yields approx-
imately 9 clusters in each run. While the attack proceeds,
the clustering algorithm can choose a number of clusters
ranging from 2 to 50. The attacker can inject up to 25 at-
tack samples, that amounts to controlling up to 5% of the
data. The value of h for the KDE used in Bridge (Soft) is
set as the average distance between pairs of samples, which
turns out to be approximately 0.2 in each run.

Results are shown in Fig. 2 (second column). The eﬀect of
the attack is essentially the same as in the previous experi-
ments on the Banana-shaped data, although here there is a
signiﬁcant diﬀerence among the performances of the bridge-
based methods. In particular, Bridge (Soft) gradually out-
performs the other approaches as the fraction of injected
samples approaches 5%. The reason is that, as qualitatively
discussed in Sect. 4.1, this heuristic approach tends to bridge
clusters which are too far to be bridged with a single attack
point, and are thus disregarded by Bridge (Best) and not al-
ways chosen by Bridge (Hard). It is also worth noting that,
in this case, the Random approach is totally ineﬀective. In
particular, no change in the objective function is observed
for this method, and the number of clusters increases lin-
early as the attack proceeds. This means simply that the
clustering algorithm produces a new cluster for each newly-
injected attack point, making the attack totally ineﬀective.
The behavior exhibited by the diﬀerent attack strategies is
also conﬁrmed by the Split and Merge values reported in Ta-
ble 5.1. Here, the most eﬀective methods, i.e., again Bridge
(Soft) and Bridge (Hard), split the 3 initial clusters each
into 2.7 and 2.4 ﬁnal clusters, on average, yielding a total
number of clusters of about 20-25 clusters. Similarly to the
previous experiments, Bridge (Best) yields a lower number
of ﬁnal clusters, as it induces more the clustering algorithm
to cluster together samples that originally belonged to dif-
ferent initial clusters.

5.1.3 Handwritten digits

We ﬁnally repeat the experiments described in the pre-
vious sections on the MNIST handwritten digit data [21].5
In this dataset, each digit is size-normalized and centered,
and represented as a grayscale image of 28 × 28 pixels. Each
pixel is raster-scan ordered and its value is directly consid-
ered as a feature. The dimensionality of the feature space is
thus 784, a much higher value than that considered in the
previous cases. We further normalize each feature (pixel) in
[0, 1] by dividing its value by 255.

We focus here on a subset of data consisting of the three
digits ‘0’, ‘1’, and ‘6’. To obtain three initial clusters, each
representing one of the considered digits, we ﬁrst compute
the average digit for each class (i.e., the average ‘0’, ‘1’,
and ‘6’), and then select 700 samples per class, by retaining
the closest samples to the corresponding average digit. We
repeat the experiments ﬁve times, each time by randomly
selecting 330 samples per digit from the corresponding set
of 700 pre-selected samples. While the attack proceeds, the
clustering algorithm can choose a number of clusters ranging
from 2 to 100. We assume that the attacker can inject up to
10 attack samples, that amounts to controlling up to 1% of

5This dataset is publicly available in Matlab format at http:
//cs.nyu.edu/~roweis/data.html.

94the data. The value of h for the KDE used in Bridge (Soft)
is set as in the previous case, based on the average distance
between all pairs of samples. For this dataset, it turns out
that h ≈ 1 in each run.

Results are shown in Fig. 2 (third column). With respect
to the previous experiments on the Banana-shaped data,
and on the Malware data, the results here are signiﬁcantly
diﬀerent. In particular, note how the Random and Random
(Best) approaches are totally ineﬀective here. Similarly to
the previous case in malware clustering, the clustering algo-
rithm essentially defeats the attack inﬂuence by creating a
new cluster for each attack sample. The underlying reason
is that, in this case, the feature space has a very high dimen-
sionality, and, thus, sampling only k − 1 points at random is
not enough to ﬁnd a suitable attack point. In other words,
if an attack sample is not very well crafted, it may be easily
isolated from the rest of the data. Although increasing the
dimensionality may thus seem a suitable countermeasure to
protect clustering against random attacks, this drastically
increases its vulnerability to well designed attack samples.
Note indeed how the clustering is already signiﬁcantly wors-
ened when the adversary only controls a fraction as small
as of 0.2% of the data. In fact, the number of ﬁnal clusters
raises immediately to the maximum allowed number of 100.
This is also clariﬁed in Table 5.1, where it can be appreci-
ated how the initial clusters are fragmented into an average
of 33 ﬁnal clusters for the bridge-based methods. Note how-
ever that, in this case, the ﬁnal clusters are almost pure, i.e.,
the attack algorithm does not succeed in merging together
samples coming from diﬀerent initial clusters.

In Fig. 3 we also show some of the attack samples that
are produced by the ﬁve attack strategies, at diﬀerent attack
iterations. The random-based attacks clearly produce very
noisy images which yield a completely ineﬀective attack, as
already mentioned. Instead, the initial attacks considered
by bridge-based methods (at iteration 1 and 2) resemble
eﬀectively the digits corresponding to the two initial clusters
that they aim to connect (‘0’ and ‘6’, and ‘1’ and‘6’). Since
the attack completely destroys the three initial clusters after
very few attack samples have been added, at later iterations
(e.g., iteration 10), the bridge-based methods tend to enforce
some connection within the cluster belonging to the ‘0’ digit,
probably trying to merge some of the ﬁnal clusters together.
However, since the maximum number of allowed clusters has
been already reached, no further improvement is observed in
the objective function.

5.2 Experiments on obfuscation attacks

For the obfuscation attack, we present an experiment on
handwritten digits, using again the MNIST digit data de-
scribed in Sect. 5.1.3.

5.2.1 Handwritten digits

We consider the same initial clusters of Sect. 5.1.3, con-
‘0’,
sisting of 330 samples for each of the following digits:
‘1’, and ‘6’. As in the previous case, we average the results
over ﬁve runs, each time selecting the initial 330 samples per
cluster from the pre-selected sets of 700 samples per digit.
In this case, however, we consider a further initial cluster of
100 samples corresponding to the digit ‘3’ (which are also
randomly sampled from a pre-selected set of 700 samples of
‘3’, chosen with the same criterion used in Sect. 5.1.3 to end
up in the same cluster, initially). These represent the attack

)
k
(
 
s
r
e

l

t
s
u
C
m
u
N

 

5
4.6
4.2
3.8
3.4
3
0

1

2

3

4

5
d max

6

7

8

9 10

Figure 4: Results for the obfuscation attack aver-
aged over ﬁve runs on the Digit dataset. The top
plots shows the variation of the objective function
for the attacker dc(Ct, f (D ∪ A′)) and for the cluster-
ing algorithm dc(f (D ∪ A), f (D ∪ A′)) as the maximum
amount of modiﬁcations dmax to the initial attack
samples A increases. The bottom plot reports the
corresponding average number of selected clusters.

✵ ✵

✷ ✵

Figure 5: An example of how a digit ‘3’ is gradually
manipulated to resemble the closest ‘6’, for diﬀerent
values of dmax.

✸ ✵

✹ ✵

✺ ✵

✼ ✵

samples A that the attacker aims to obfuscate. We remind
the reader that the attacker’s goal in this case is to manip-
ulate some samples to have them clustered according to a
desired criterion, without aﬀecting signiﬁcantly the normal
system operation.
In particular, we assume here that the
attacker can manipulate samples corresponding to the digit
‘3’ in order to have them clustered together with the cluster
corresponding to the digit ‘6’, while preserving the initial
clusters. In other words, the desired clustering output for
the attacker consists of three clusters: one corresponding to
the ‘0’ digit, one corresponding to the ‘1’ digit, and the latter
corresponding to the digits ‘6’ and ‘3’. These constraints can
be easily encoded as a desired clustering output Ct through
a binary matrix Yt. This reﬂects exactly Problem 5, where
the attacker aims at minimizing dc(Ct, f (D ∪ A′)).

On the other hand, as explained in Sect. 3.2, the clustering
algorithm attempts to keep the attack points corresponding
to the digit ‘3’ into a well-separated cluster from the re-
maining digits, i.e., it selects the number of clusters that
minimizes dc(C⋆, f (D ∪ A′)), which can thus be regarded as
the objective function for the clustering algorithm. In this
case, C⋆ is the clustering obtained on the initial data and
the non-manipulated attack samples, i.e., C ⋆ = f (D ∪ A).
The results for the above discussed obfuscation attack are
given in Fig. 4, where we report the values of the objective
function for the attacker and for the clustering algorithm,
as well as the number of selected clusters, as a function of
the maximum amount of allowed modiﬁcations to the at-
tack samples, given in terms of the maximum Euclidean dis-

95tance dmax (see Eq. 4). The results clearly show that the
objective function of the attacker tends to decrease, while
that of the clustering algorithm generally increases. The
reason is that, initially, the clustering algorithm correctly
separates the four clusters associated to the four distinct
digits, whereas as dmax increases, the attack digits ‘3’ are
more and more altered to resemble the closest ‘6’s, and are
then gradually merged to their cluster. The number of clus-
ters does not decrease immediately to 3 as one would expect
since, while manipulating the attack samples, their cluster
is fragmented into smaller ones (typically, two or three clus-
ters). The reason is that, to remain as close as possible to
the ideal C⋆, the clustering algorithm avoids some of the ‘3’s
to immediately join the cluster of ‘6’s by fragmenting the
cluster of ‘3’s.

When dmax takes on values approximately in [3, 4], the
clustering algorithm creates only three clusters, correspond-
ing eﬀectively to the attacker’s goal Ct (this is witnessed
by the fact that the averaged attacker’s objective is almost
zero). Surprisingly, though, as soon as dmax becomes greater
than 4, the number of clusters raises again to 4, and some
of the attack samples are again separated from the cluster
of ‘6’s, worsening the adversary’s objective. This is due to
the fact that, when dmax ≈ 3 or 4, some of the attack points
work as bridges and successfully connect the remaining ‘3’s
to the cluster of ‘6’s, whereas when these points are further
shifted towards the cluster of ‘6’s, the algorithm can success-
fully split the two clusters again. Based on this observation,
a smarter attacker may even manipulate only a very small
subset of her attack samples to create proper bridges and
connect the remaining non-manipulated samples to the de-
sired cluster. We however left a quantitatively investigation
of this approach to future work.

In Fig. 5 we ﬁnally report an example of how a digit ‘3’
is manipulated by our attack to be hidden in the cluster
associated to the digit ‘6’.
It is worth noting how, when
dmax ∈ [2, 4], the original attack sample still signiﬁcantly re-
sembles the initial ‘3’: this shows that the adversary’s goal
can be achieved without altering too much the initial at-
tack samples, which is clearly a strong desideratum for the
attacker in adversarial settings.

6. CONCLUSIONS AND FUTURE WORK

In this paper, we addressed the problem of evaluating the
security of clustering algorithms in adversarial settings, by
providing a framework for simulating potential attack sce-
narios. We devised two attacks that can signiﬁcantly com-
promise availability and integrity of the targeted system.
We demonstrated with real-world experiments that single-
linkage clustering may be signiﬁcantly vulnerable to delib-
erate attacks, either when the adversary can only control a
very small fraction of the input data, or when she slightly
manipulates her attack samples. This shows that attack-
ing clustering algorithms with tailored strategies can signif-
icantly alter their output to meet the adversary’s goal.

Admittedly, one of the causes of the vulnerability of single-
linkage resides in its inter-cluster distance, which solely de-
pends on the closest points between clusters, and thus al-
lowed for an eﬃcient constructing of bridges. It is reason-
able to assume that algorithms based on computing averages
(e.g., k-means) or density estimation might be more robust
to poisoning, although not necessarily robust to obfuscation

attacks. However, the results of our empirical evaluation can
not be directly generalized to diﬀerent algorithms, and more
investigation should thus be carried out in this respect.

In general, ﬁnding the optimal attack strategy given an
arbitrary clustering algorithm is a hard problem and we have
to rely on heuristic algorithms in order to carry out our
analysis. For the sake of eﬃciency, these heuristics should
be heavily dependent on the targeted clustering algorithm,
as in our case. However, it would be interesting to exploit
more general approaches that ideally treat the clustering
algorithm as a black box and ﬁnd a solution by performing
a stochastic search on the solution space (e.g. by simulated
annealing), or an educated exhaustive search (e.g. by using
branch-and-bound techniques).

In this work we did not address the problem of countering
attacks by designing more secure clustering algorithms. We
only assumed that the clustering algorithm can select a dif-
ferent number of clusters (optimal according to its goal) after
each attack iteration. More generally, though, one can de-
sign a clustering algorithm that explicitly takes into account
the adversary’s presence, and her optimal attack strategy,
e.g., by modeling clustering in adversarial settings as a game
between the clustering algorithm and the attacker. This has
been done in the case of supervised learning, to improve
the security of learning algorithms against evasion attempts
[8], and similarly, in the regression setting [13]. Other ap-
proaches may more directly encode explicit assumptions on
how the data distribution changes under attack, similarly to
[5]. We left this investigation to future work.

Another possible future extension of our work would be
to consider a more realistic setting in which the attacker has
limited knowledge of the attacked system. To this end, the
upper bound on the performance degradation incurred under
attack provided by our worst-case analysis may be exploited
to evaluate the eﬀectiveness of attacks devised under limited
knowledge (i.e., how close they can get to the worst case).
One limitation of our approach may be the so-called in-
verse feature-mapping problem [17, 6], i.e., the problem of
ﬁnding a real attack sample corresponding to a desired fea-
ture vector (as the ones suggested by our attack strategies).
In the reported experiments, this was not a signiﬁcant prob-
lem since modiﬁcations to the given feature values could be
directly mapped to manipulations on the real attack sam-
ples. Although inverting the feature mapping may be a cum-
bersome task for more complicated feature representations,
this remains a common problem of optimal attacks in adver-
sarial learning, and it has to be addressed in an application-
speciﬁc manner, depending on the given feature space.

As a further future development, we plan to establish a
link between the evaluation of the security of clustering al-
gorithms and the problem of determining the stability of a
clustering, which has been already addressed in the liter-
ature and used as a device for model selection (see, e.g.,
[29]). Indeed, stable clusterings can be regarded as secure
under speciﬁc non-targeted attacks like, e.g., perturbation
of points with Gaussian noise.

Understanding robustness of clustering algorithms against
carefully targeted attacks under a more theoretical perspec-
tive (e.g., by devising theoretical bounds that evaluate the
impact of single attack points on the clustering output) may
also be a promising research direction. Some results from
clustering stability may be also exploited to this end.

967. ACKNOWLEDGMENTS

This work has been partly supported by the Regional Ad-
ministration of Sardinia (RAS), Italy, within the projects
“Security of pattern recognition systems in future internet”
(CRP-18293), and “Advanced and secure sharing of mul-
timedia data over social networks in the future Internet”
(CRP-17555). Both projects are funded within the frame-
work of the regional law L.R. 7/2007, Bando 2009. The
opinions, ﬁndings and conclusions expressed in this paper
are solely those of the authors and do not necessarily reﬂect
the opinions of any sponsor.

8. REFERENCES

[1] Collaborative Malware Collection and Sensing.

https://alliance.mwcollect.org.

[2] Project Malfease. http://malfease.oarci.net.
[3] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and

J. D. Tygar. Can machine learning be secure? In
ASIACCS ’06: Proc. 2006 ACM Symposium on
Information, Computer and Communications Security,
pages 16–25, NY, USA, 2006. ACM.

[4] U. Bayer, P. M. Comparetti, C. Hlauschek, C. Kr¨ugel,

and E. Kirda. Scalable, behavior-based malware
clustering. In NDSS. The Internet Society, 2009.

[5] B. Biggio, G. Fumera, and F. Roli. Design of robust

classiﬁers for adversarial environments. In IEEE Int’l
Conf. on Systems, Man, and Cybernetics (SMC),
pages 977–982, 2011.

[6] B. Biggio, G. Fumera, and F. Roli. Security evaluation

of pattern classiﬁers under attack. IEEE Trans. on
Knowledge and Data Eng., 99(PrePrints):1, 2013.

[7] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks

against support vector machines. In J. Langford and
J. Pineau, editors, 29th Int’l Conf. on Machine
Learning. Omnipress, 2012.

[8] M. Br¨uckner, C. Kanzow, and T. Scheﬀer. Static

prediction games for adversarial learning problems. J.
Mach. Learn. Res., 13:2617–2654, 2012.

[9] I. Burguera, U. Zurutuza, and S. Nadjm-Tehrani.

Crowdroid: behavior-based malware detection system
for android. In Proc. 1st ACM workshop on Security
and Privacy in Smartphones and Mobile devices,
SPSM ’11, pages 15–26, NY, USA, 2011. ACM.

[15] M. Halkidi, Y. Batistakis, and M. Vazirgiannis. On

clustering validation techniques. Journal of Intelligent
Information Systems, 17(2-3):107–145, Dec. 2001.
[16] S. Hanna, L. Huang, E. Wu, S. Li, C. Chen, and

D. Song. Juxtapp: a scalable system for detecting
code reuse among android applications. In Proc. 9th
Int’l Conf. on Detection of Intrusions and Malware,
and Vulnerability Assessment, DIMVA’12, pages
62–81, Berlin, Heidelberg, 2013. Springer-Verlag.

[17] L. Huang, A. D. Joseph, B. Nelson, B. Rubinstein,

and J. D. Tygar. Adversarial machine learning. In 4th
ACM Workshop on Artiﬁcial Intelligence and Security
(AISec 2011), pages 43–57, Chicago, IL, USA, 2011.

[18] A. K. Jain and R. C. Dubes. Algorithms for clustering

data. Prentice-Hall, Inc., NJ, USA, 1988.

[19] M. Kloft and P. Laskov. Online anomaly detection

under adversarial impact. In Proc. 13th Int’l Conf. on
Artiﬁcial Intell. and Statistics, pages 405–412, 2010.

[20] A. Kolcz and C. H. Teo. Feature weighting for

improved classiﬁer robustness. In Sixth Conf. on
Email and Anti-Spam (CEAS), CA, USA, 2009.

[21] Y. LeCun, L. Jackel, L. Bottou, A. Brunot, C. Cortes,

J. Denker, H. Drucker, I. Guyon, U. M¨uller,
E. S¨ackinger, P. Simard, and V. Vapnik. Comparison
of learning algorithms for handwritten digit
recognition. In Int’l Conf. on Artiﬁcial Neural
Networks, pages 53–60, 1995.

[22] M. Pavan and M. Pelillo. Dominant sets and pairwise

clustering. IEEE Trans. on Pattern Analysis and
Machine Intelligence, 29(1):167–172, 2007.

[23] R. Perdisci, D. Ariu, and G. Giacinto. Scalable
ﬁne-grained behavioral clustering of http-based
malware. Computer Networks, 57(2):487 – 500, 2013.

[24] R. Perdisci, I. Corona, and G. Giacinto. Early

detection of malicious ﬂux networks via large-scale
passive DNS traﬃc analysis. IEEE Trans. on
Dependable and Secure Comp., 9(5):714–726, 2012.

[25] F. Pouget, M. Dacier, J. Zimmerman, A. Clark, and

G. Mohay. Internet attack knowledge discovery via
clusters and cliques of attack traces. J. Information
Assurance and Security, Vol. 1, Issue 1, March 2006.

[26] G. Punj and D. W. Stewart. Cluster analysis in
marketing research: Review and suggestions for
application. J. Marketing Res., 20(2):134, May 1983.

[10] C. Castillo and B. D. Davison. Adversarial web search.

[27] D. B. Skillicorn. Adversarial knowledge discovery.

Foundations and Trends in Information Retrieval,
4(5):377–486, May 2011.

[11] J. G. Dutrisac and D. Skillicorn. Hiding clusters in

adversarial settings. In IEEE Int’l Conf. on
Intelligence and Security Informatics (ISI 2008),
pages 185–187, 2008.

[12] D. A. Forsyth and J. Ponce. Computer Vision: A

Modern Approach. Prentice Hall, 2011.

[13] M. Großhans, C. Sawade, M. Br¨uckner, and

T. Scheﬀer. Bayesian games for adversarial regression
problems. In J. Mach. Learn. Res. - Proc. 30th Int’l
Conf. on Machine Learning (ICML), volume 28, 2013.

[14] P. Haider, L. Chiarandini, and U. Brefeld.

Discriminative clustering for market segmentation. In
Proc. 18th ACM SIGKDD Int’l Conf. Knowledge
Discovery and Data Mining, KDD ’12, pages 417–425,
NY, USA, 2012. ACM.

IEEE Intelligent Systems, 24:54–61, 2009.
[28] L. Spitzner. Honeypots: Tracking Hackers.

Addison-Wesley Professional, 2002.

[29] U. von Luxburg. Clustering stability: An overview.

Foundations and Trends in Machine Learning,
2(3):235–274, 2010.

[30] T. Zhang, R. Ramakrishnan, and M. Livny. Birch: an

eﬃcient data clustering method for very large
databases. In Proc. 1996 ACM SIGMOD Int’l Conf.
on Management of data, SIGMOD ’96, pages 103–114,
NY, USA, 1996. ACM.

97