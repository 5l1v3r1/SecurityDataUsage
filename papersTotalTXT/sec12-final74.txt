Data Node Encrypted File System:

Eﬃcient Secure Deletion for Flash Memory

Joel Reardon, Srdjan Capkun, David Basin

Department of Computer Science, ETH Zurich

Abstract

We propose the Data Node Encrypted File Sys-
tem (DNEFS), which uses on-the-ﬂy encryption and
decryption of ﬁle system data nodes to eﬃciently
and securely delete data on ﬂash memory systems.
DNEFS is a generic modiﬁcation of existing ﬂash
ﬁle systems or controllers that enables secure data
deletion while preserving the underlying systems’ de-
sirable properties: application-independence, ﬁne-
grained data access, wear-levelling, and eﬃciency.

We describe DNEFS both abstractly and in the
context of the ﬂash ﬁle system UBIFS. We propose
UBIFSec, which integrates DNEFS into UBIFS. We
implement UBIFSec by extending UBIFS’s Linux
implementation and we integrate UBIFSec in the
Android operating system running on a Google
Nexus One smartphone. We show that it is eﬃ-
cient and usable; Android OS and applications (in-
cluding video and audio playback) run normally on
top of UBIFSec. To the best of our knowledge,
this work presents the ﬁrst comprehensive and fully-
implemented secure deletion solution that works
within the speciﬁcation of ﬂash memory.

1 Introduction

Flash memory is used near universally in portable
devices. However, the way modern systems use ﬂash
memory has a serious drawback—it does not guaran-
tee deletion of stored data. To the user, data appears
to be deleted from the ﬁle system, but in reality
it remains accessible after deletion [39]. This prob-
lem is particularly relevant for modern smartphones,
as they store private data, such as communications,
browsing, and location history as well as sensitive
business data. The storage of such data on portable
devices necessitates guaranteed secure deletion.

no longer possible on that storage medium [9]. This
is in contrast to standard deletion, where metadata
simply indicates that the data’s storage location is
no longer needed and can be reused. The time be-
tween marking data as deleted and its actual (se-
cure) deletion is called the deletion latency. We use
the term guaranteed secure deletion to denote secure
deletion with a ﬁxed, (small) ﬁnite upper bound on
the deletion latency for all data.

On magnetic storage media, secure data deletion
is implemented by overwriting a ﬁle’s content with
non-sensitive information [29], or by modifying the
ﬁle system to automatically overwrite any discarded
sector [2]. However, ﬂash memory cannot perform
in-place updates of data (i.e., overwrites) [8]; it in-
stead performs erasures on erase blocks, which have
a larger granularity than read/write operations. A
single erase block may store data for diﬀerent ﬁles,
so it can only be erased when all the data in the
erase block is marked as deleted or when the live
data is replicated elsewhere. Moreover, ﬂash mem-
ory degrades with each erasure, so frequent erasures
shorten the device’s lifetime. Therefore, the simplis-
tic solution of erasing any erase block that contains
deleted data is too costly with regards to time and
device wear [35].

In this work, we present the Data Node Encrypted
File System (DNEFS), which securely and eﬃciently
deletes data on ﬂash memory; it requires only a few
additional erasures that are evenly distributed over
the erase blocks. DNEFS uses on-the-ﬂy encryption
and decryption of individual data nodes (the small-
est unit of read/write for the ﬁle system) and relies
on key generation and management to prevent access
to deleted data. We design and implement an in-
stance of our solution for the ﬁle system UBIFS [14]
and call our modiﬁcation UBIFSec.

Secure deletion is the operation of sanitizing data
on a storage medium, so that access to the data is

UBIFSec has the following attractive properties.
It provides a guaranteed upper bound on deletion

1

It provides ﬁne-grained deletion, also for
latency.
truncated or overwritten parts of ﬁles. It runs eﬃ-
ciently and produces little wear on the ﬂash memory.
Finally, it is easy to integrate into UBIFS’s existing
Linux implementation, and requires no changes to
the applications using UBIFS. We deploy UBIFSec
on a Google Nexus One smartphone [11] running an
Android OS. The system and applications (includ-
ing video and audio playback) run normally on top
of UBIFSec.

Even though DNEFS can be implemented on
YAFFS (the ﬁle system used on the Android OS),
this would have required signiﬁcant changes to
YAFFS. We test DNEFS within UBIFS, which is
a supported part of the standard Linux kernel (since
version 2.6.27) and which provides interfaces that
enable easy integration of DNEFS.

We summarize our contributions as follows. We
design DNEFS, a system that enables guaranteed
secure data deletion for ﬂash memory—operating
within ﬂash memory’s speciﬁcation [26]. We in-
stantiate DNEFS as UBIFSec, analyze its security,
and measure its additional battery consumption,
throughput, computation time, and ﬂash memory
wear to show that it is practical for real-world use.
We provide our modiﬁcation freely to the commu-
nity [37].

2 Background

Flash Memory. Flash memory is a non-volatile
storage medium consisting of an array of electronic
components that store information [1]. Flash mem-
ory has very small mass and volume, does not incur
seek penalties for random access, and requires little
energy to operate. As such, portable devices almost
exclusively use ﬂash memory.

Flash memory is divided into two levels of gran-
ularity. The ﬁrst level is called erase blocks, which
are on the order of 128 KiB [11] in size. Each erase
block is divided into pages, which are on the order
of 2 KiB in size. Erase blocks are the unit of era-
sure, and pages are the unit of read and write oper-
ations [8]. One cannot write data to a ﬂash memory
page unless that page has been previously erased ;
only the erasure operation performed on an erase
block prepares the pages it contains for writing.

Erasing ﬂash memory causes signiﬁcant physical
wear [22]. Each erasure risks turning an erase block
into a bad block, which cannot store data. Flash
erase blocks tolerate between 104 to 105 erasures be-
fore they become bad blocks. To promote a longer
device lifetime, erasures should be evenly levelled
over the erase blocks.

MTD Layer. On Linux, ﬂash memory is accessed
through the Memory Technology Device (MTD)
layer [23]. MTD has the following interface: read
and write a page, erase an erase block, check if an
erase block is bad, and mark an erase block as bad.
Erase blocks are referenced sequentially, and pages
are referenced by the erase block number and oﬀset.

Flash File Systems. Several ﬂash memory ﬁle
systems have been developed at the MTD layer [4,
40]. These ﬁle systems are log-structured: a class
of ﬁle systems that (notably) do not perform in-
place updates. A log-structured ﬁle system con-
sists of an ordered list of changes from an initial
empty state, where each change to the ﬁle system
is appended to the log’s end [34]. Therefore, stan-
dard log-structured ﬁle systems do not provide se-
cure deletion because new data is only appended.

When a change invalidates an earlier change then
the new, valid data is appended and the erase block
containing the invalidated data now contains wasted
space. Deleting a ﬁle, for example, appends a change
that indicates the ﬁle is deleted. All the deleted ﬁle’s
data nodes remain on the storage medium but they
are now invalid and wasting space. A garbage col-
lection mechanism detects and recycles erase blocks
with only invalid data; it also copies the remaining
valid data to a new location so it may recycle erase
blocks mostly ﬁlled with invalid data.

Flash Translation Layer. Flash memory is com-
monly accessed through a Flash Translation Layer
(FTL) [1, 15], which is used in USB sticks, SD cards,
and solid state drives. FTLs access the raw ﬂash
memory directly, but expose a typical hard drive in-
terface that allows any regular ﬁle system to be used
on the memory. FTLs can either be a hardware con-
troller or implemented in software. An FTL trans-
lates logical block addresses to raw physical ﬂash ad-
dresses, and internally implements a log-structured
ﬁle system on the memory [6]. Therefore, like log-
structured ﬁle systems, FTLs do not provide secure
data deletion. In Section 5 we explain how to mod-
ify an FTL to use DNEFS to enable eﬃcient secure
deletion for any ﬁle system mounted on it.

UBI Layer. Unsorted Block Images (UBI) is an
abstraction of MTD, where logical erase blocks are
transparently mapped to physical erase blocks [10].
UBI’s logical mapping implements wear-levelling
and bad block detection, allowing UBI ﬁle systems
to ignore these details. UBI also permits the atomic
updating of a logical erase block—the new data is
either entirely available or the old data remains.

2

journal is full, it is committed to the main storage
area by logically moving the journal to an empty
location and growing the main storage area to en-
compass the old journal. An index is used to lo-
cate data nodes, and this index is also written to
the storage medium. At its core, UBIFS is a log-
structured ﬁle system; in-place updates are not per-
formed. As such, UBIFS does not provide guaran-
teed secure data deletion.

In this work, we model a
Adversarial Model.
novel kind of attacker that we name the peek-a-boo
attacker. This attacker is more powerful than the
strong coercive attacker considered in other secure
deletion works [27, 30]. A coercive attacker can,
at any time, compromise both the storage medium
containing the data along with any secret keys or
passphrases required to access it. The peek-a-boo
attacker extends the coercive attacker to also allow
the attacker to obtain (“to peek into”) the content of
the storage medium at some point(s) in time prior
to compromising the storage medium.

Coercive attacks model legal subpoenas that re-
quire users to forfeit devices and reveal passwords.
Since the time of the attack is arbitrary and therefore
unpredictable, no extraordinary sanitization proce-
dure can be performed prior to the compromise time.
Since the attacker is given the user’s secret keys,
it is insuﬃcient to simply encrypt the storage me-
dia [17]. The peek-a-boo attacker models an at-
tacker who additionally gets temporary read-access
to the medium (e.g., a hidden virus that is forced
to send suicide instructions upon being publicly ex-
posed) and then subsequently performs a coercive
attack. It is roughly analogous to forward secrecy
in the sense that if a secure deletion scheme is re-
silient to a peek-a-boo attacker, it prevents recovery
of deleted data even if an earlier snapshot of the data
from the storage medium is available to the attacker.

Figure 2 shows a timeline of data storage and an
adversarial attack. We divide time into discrete in-
tervals called purging epochs. At the end of each
purging epoch any data marked for deletion is se-
curely deleted (purged). We assume that purging is
an atomic operation. The lifetime of a piece of data
is then deﬁned as all the purging epochs from the one
when it was written to the one when it was deleted.
We say that data is securely deleted if a peek-a-boo
attacker cannot recover the data when performing
peek and boo attacks in any purging epochs outside
the data’s lifetime.

Figure 1: Erase block relationships among MTD, UBI,
and UBIFS. Diﬀerent block shades label diﬀerent areas
of the ﬁle system. Empty LEBs are labelled by ε and are
not mapped to a corresponding PEB by UBI. Similarly,
bad PEBs are labelled and not mapped onto by UBI.

UBI exposes the following interface:

read and
write to a Logical Erase Block (LEB), erase an LEB,
and atomically update the contents of an LEB. UBI
LEBs neither become bad due to wear, nor should
their erasure counts be levelled.

Underlying this interface is an injective partial
mapping from LEBs to physical erase blocks (PEBs),
where PEBs correspond to erase blocks at the MTD
layer. The lower half of Figure 1 illustrates this re-
lationship. Wear monitoring is handled by tracking
the erasures at the PEB level, and a transparent
remapping of LEBs occurs when necessary. Remap-
ping also occurs when bad blocks are detected. De-
spite remapping, an LEB’s number remains con-
stant, regardless of its corresponding PEB.

Atomic updates occur by invoking UBI’s update
function, passing as parameters the LEB number to
update along with a buﬀer containing the desired
contents. An unused and empty PEB is selected and
the page-aligned data is then written to it. UBI then
updates the LEB’s mapping to the new PEB, and
the previous PEB is queued for erasure. This erasure
can be done either automatically in the background
or immediately with a blocking system call. If the
atomic update fails at any time—e.g., because of a
power loss—then the mapping is unchanged and the
old PEB is not erased.

UBIFS. The UBI ﬁle system, UBIFS [14], is de-
signed speciﬁcally for UBI, and Figure 1 illustrates
UBIFS’s relationship to UBI and MTD. UBIFS di-
vides ﬁle data into ﬁxed-sized data nodes. Each data
node has a header that stores the data’s inode num-
ber and its ﬁle oﬀset. This inverse index is used by
the garbage collector to determine if the nodes on
an erase block are valid or can be discarded.

UBIFS ﬁrst writes all data in a journal. When this

3

εεεPEBεεεεbadbadεεεεbadbadMTD (flash)εεεUBIFSblock, etcsuper main storage areajournalLEBUBI3.2 Our Solution

We now present our secure deletion solution and
show how it fulﬁlls the listed requirements.

In the spirit of Boneh and Lipton [3], DNEFS
uses encryption to provide secure deletion.
It en-
crypts each individual data node (i.e., the unit of
read/write for the ﬁle system) with a diﬀerent key,
and then manages the storage, use, and purging of
these keys in an eﬃcient and transparent way for
both users and applications. Data nodes are en-
crypted before being written to the storage medium
and decrypted after being read; this is all done in-
memory. The keys are stored in a reserved area of
the ﬁle system called the key storage area.

DNEFS works independent of the notion of ﬁles;
neither ﬁle count/size nor access patterns have any
inﬂuence on the size of the key storage area. The en-
crypted ﬁle data stored on the medium is no diﬀerent
than any reversible encoding applied by the storage
medium (e.g., error-correcting codes) because all le-
gitimate access to the data only observes the unen-
crypted form. This is not an encrypted ﬁle system,
although in Section 5 we explain that it can be easily
extended to one. In our case, encryption is simply
a coding technique that we apply immediately be-
fore storage to reduce the number of bits required
to delete a data node from the data node size to the
key size.

Key Storage Area. Our solution uses a small mi-
grating set of erase blocks to store all the data nodes’
keys—this set is called the Key Storage Area (KSA).
The KSA is managed separately from the rest of the
ﬁle system. In particular, it does not behave like a
log-structured ﬁle system: when a KSA erase block
is updated, its contents are written to a new erase
block, the logical reference to the KSA block is up-
dated, and the previous version of the KSA erase
block is then erased. Thus, except while updating,
only one copy of the data in the KSA is available on
the storage medium. Our solution therefore requires
that the ﬁle system or ﬂash controller that it mod-
iﬁes can logically reference the KSA’s erase blocks
and erase old KSA erase blocks promptly after writ-
ing a new version.

Each data node’s header stores the logical KSA
position that contains its decryption key. The erase
blocks in the KSA are periodically erased to securely
delete any keys that decrypt deleted data. When the
ﬁle system no longer needs a data node—i.e, it is
removed or updated—we mark the data node’s cor-
responding key in the KSA as deleted. This solution
is independent of the notion of ﬁles; keys are marked

Figure 2: Example timeline for secure deletion. Time
is divided into discrete purging epochs. Data is written
in epoch 2 and deleted in epoch 5, and the data’s life-
time includes all epochs between these. Here, the peek
attack (read access to the entire storage medium) occurs
in epoch 1 and the boo attack (full compromise of the
storage medium and secret keys/passphrases) in epoch
6. More generally, they can occur in any purging epochs
outside the data’s lifetime.

3 DNEFS

In this section we describe our main contribution: a
solution for eﬃcient secure deletion for ﬂash mem-
ory. We ﬁrst list our requirements for secure dele-
tion, and afterwards describe our solution.

3.1 Secure Deletion Requirements

We present four requirements for secure deletion so-
lutions. The solution must be sound, ﬁne-grained,
eﬃcient, and simple.

Soundness requires that the solution ensures guar-
anteed secure data deletion against a strong at-
tacker; we use the peek-a-boo attacker deﬁned in
Section 2.

Fine-grained requires the solution to securely
delete data, however small. This includes overwrit-
ten or truncated ﬁles, such as data removed from a
long-lived database.

The solution must be eﬃcient in terms of resource
consumption. For ﬂash memory and portable de-
vices, the relevant resources are battery consump-
tion, computation time, storage space, and device
lifetime, i.e., minimizing and levelling wear.

Finally, simplicity requires that the solution can
be easily implemented as part of existing systems.
For our purposes, this means that adding secure
deletion to existing ﬁle systems must be straightfor-
ward. We want to minimize the necessary changes
to the existing code and isolate the majority of the
implementation in new functions and separate data
structures. We want the change to be easily au-
dited and analyzed by security-minded professionals.
Moreover, we must not remove or limit any existing
feature of the underlying ﬁle system.

4

...timeepoch345621delete datawrite databoo attackpeek attackdata’s lifetimeas deleted whenever a data node is discarded. A key
remains marked as deleted until it is removed from
the storage medium and its location is replaced with
fresh, unused random data, which is then marked as
unused.

Figure 3: (a) writing a new data node DN3: DN3 is ﬁrst
encrypted with an unused key k3 and then written to an
empty position in the main storage. A reference to the
key’s position in the KSA is stored alongside Ek3 (DN3).
(b) reading a data node DN1: Ek1 (DN1) is ﬁrst read
from the storage medium along with a reference to its
key k1 in the KSA. The key is then read and used to
decrypt and return DN1.

When a new data node is written to the storage
medium, an unused key is selected from the KSA
and its position is stored in the data node’s header.
DNEFS does this seamlessly, so applications are un-
aware that their data is being encrypted. Figure 3
illustrates DNEFS’s write and read algorithms.

Purging. Purging is a periodic procedure that se-
curely deletes keys from the KSA. Purging proceeds
iteratively over each of the KSA’s erase blocks: a
new version of the erase block is prepared where the
used keys remain in the same position and all other
keys (i.e., unused and deleted keys) are replaced
with fresh, unused, cryptographically-appropriate
random data from a source of hardware random-
ness. Such random data is inexpensive and easy to
generate, even for resource-constrained devices [38].
Fresh random data is then assigned to new keys as
needed. We keep used keys logically-ﬁxed because
their corresponding data node has already stored—
immutably until an erasure operation—its logical

5

position. The new version of the block is then writ-
ten to an arbitrary empty erase block on the storage
medium. After completion, all erase blocks contain-
ing old versions of the logical KSA erase block are
erased, thus securely deleting the unused and deleted
keys along with the data nodes they encrypt.

The security of our system necessitates that the
storage medium can be properly instructed to erase
an erase block. Therefore, for ﬂash memory, DNEFS
must be implemented either into the logic of a ﬁle
system that provides access to the raw ﬂash memory
(e.g., UBIFS) or into the logic of the ﬂash controller
(e.g., solid state drive). As Swanson et al. [36] ob-
serve, any implementation of secure deletion on top
of an opaque ﬂash controller cannot guarantee dele-
tion, as its interface for erase block erasure is not
security focused and may neglect to delete internally
created copies of data due to wear levelling. Our use
of UBI bypasses obfuscating controllers and allows
direct access to the ﬂash memory.

By only requiring the secure deletion of small
densely-packed keys, DNEFS securely deletes all the
storage medium’s deleted data while only erasing a
small number of KSA erase blocks. Thus, encryption
is used to reduce the number of erasures required to
achieve secure deletion. This comes at the cost of as-
suming a computationally-bounded adversary—an
information-theoretic adversary could decrypt the
encrypted ﬁle data. We replace unused keys with
new random data to thwart the peek-a-boo attacker:
keys are discarded if they are not used to store data
in the same deletion epoch as they are generated.

While DNEFS is designed to batch deleted data
nodes, thus erasing fewer erase blocks per deleted
data node, there is no technical reason that prohibits
immediate secure deletion. In particular, ﬁles can be
marked as sensitive [2] so that purging is triggered
whenever a data node for such a ﬁle is deleted, re-
sulting in one erase block erasure. Purging can also
be triggered by an application, for example after it
clears its cache.

If a KSA erase block becomes a bad block while
erasing it, it is possible that its contents will remain
readable on the storage medium without the ability
to remove them [21].
In this case, it is necessary
to re-encrypt any data node whose encryption key
remains available and to force the garbage collection
of those erase blocks on which the data nodes reside.

Key State Map. The key state map is an in-
memory map that maps key positions to key states
{unused, used, deleted}. Unused keys can be as-
signed and then marked as used. Used keys are keys
that encrypt some valid data node, so they must be

kE  (DN  ) 33kE  (DN  ) 221k 1E  (DN  )......k3k4k1k2kE  (DN  ) 33(1) encryptkE  (DN  ) 33kE  (DN  ) 221k 1E  (DN  )......k3k4k1k2DN1(3) associate key(2) write dataKSAmain storage(a) DNEFS write operationWRITE    DN  31READ    DN(1) read encrypted data and key position(3) decrypt and return data(2) read encryption key(b) DNEFS read operationmain storageKSAﬂash memory. We require only that the ﬁle system
also determines the key location for the data node
in the index, and so the state of each key position
can be generated by marking these key locations as
used and assuming all other locations are deleted.

We deﬁne a correct key state map as one that has
(with high probability) the following three proper-
ties: (1) every unused key must not decrypt any
data node—either valid or invalid, (2) every used
key must have exactly one data node it can decrypt
and this data node must be referred to by the index,
and (3) every deleted key must not decrypt any data
node that is referred to by the index. Observe that
an unused key that is marked as deleted will still re-
sult in a correct key state map, as it aﬀects neither
the security of deleted data nor the availability of
used data.

The operation of purging performed on a cor-
rect key state map guarantees DNEFS’s soundness:
purging securely deletes any key in the KSA marked
as deleted; afterwards, every key decrypts at most
one valid data node, and every data node referred to
by the index can be decrypted. While the encrypted
version of the deleted data node still resides in ﬂash
memory, our adversary is thereafter unable to ob-
tain the key required to decrypt and thus read the
data. A correct key state map also guarantees the
integrity of our data during purging, because no key
that is used to decrypt valid data will be removed.
We deﬁne DNEFS’s purging epoch’s duration
(Section 2) as the time between two consecutive
purging operations. When a data node is written, it
is assigned a key that is currently marked as unused
in the current purging epoch. The purging opera-
tion’s execution at the purging epochs’ boundaries
ensures that all keys currently marked as unused
were not available in any previous purging epoch.
Therefore, a peek or boo attack that occurs in any
prior purging epoch reveals neither the encrypted
data node nor its encryption key. When data is
deleted, its encryption key is marked as deleted in
the current purging epoch. Purging’s execution be-
fore the next purging epoch guarantees that key
marked as deleted in one epoch is unavailable in the
KSA in the next epoch. Therefore, a peek or boo
attack that occurs in any later purging epoch may
reveal the encrypted data node but not the key. A
computationally-bounded peek-a-boo attacker is un-
able to decrypt the data node, ensuring that the data
is not recoverable and therefore securely deleted.

Conclusion. DNEFS provides guaranteed secure
deletion against a computationally-bounded peek-a-
boo attacker. When an encryption key is securely

Figure 4: Example of a key state map, key storage area,
and main storage area during a purging operation. (a)
shows the state before and (b) shows the state after
purging. Some keys are replaced with new values af-
ter purging, corresponding to data nodes that were un-
used or deleted. The table of data nodes illustrate a
log-structured ﬁle system, where newer versions of data
nodes for the same ﬁle/oﬀset invalidate older versions.

preserved to ensure availability of the ﬁle system’s
data. Deleted keys are keys used to encrypt deleted
data—i.e., data nodes that are no longer referenced
by the index—and should be purged from the sys-
tem to achieve secure deletion. Figure 4 shows an
example key state map and a KSA before and after
a purging operation: unused and deleted keys are
replaced with new values and used keys remain on
the storage medium.

While mounting, the key state map must be cor-
rectly constructed; the procedure for this depends on
the ﬁle system in which it is integrated. However,
log-structured ﬁle systems are capable of generating
a ﬁle system index data structure that maps data
nodes to their (most recently written) location in

6

          kkkkk43210kkkk567k89nextassignedkey1234567seq #   file #offsetkeyposdata1112212409600081920[...][...][...][...][...][...][...]81921234560valid   noyesnoyesnoyesyesnextassignedkeyerase block 2erase block 1erase block 2erase block 10−4 5−910−14 15−19kkkkkkkkkk10111213141516171819KSAkey state mapposstatedeleted1234567890deletedusedusedusedusedunusedunuseddeletedused*......main storage areadata nodes(a) state before purging keyskey state mapposstateKSA1234567890usedusedusedusedunusedunusedusedunusedunusedunused*0−4 5−910−14 15−19kkkkkkkkk10111213141516171819kkkk567k89kk43210kk......(b) state after purging keyskkdeleted, the data it encrypted is then inaccessible,
even to the user. All invalid data nodes have their
corresponding encryption keys securely deleted dur-
ing the next purging operation. Purging occurs pe-
riodically, so during normal operation the deletion
latency for all data is bounded by this period. Nei-
ther the key nor the data node is available in any
purging epoch prior to the one in which it is writ-
ten, preventing any early peek attacks from obtain-
ing this information.

4 UBIFSec

We now describe UBIFSec: our instantiation of
DNEFS for the UBIFS ﬁle system. We ﬁrst give
an overview of the aspects of UBIFS relevant for in-
tegrating our solution. We then describe UBIFSec
and conclude with an experimental validation.

4.1 UBIFS

UBIFS is a log-structured ﬂash ﬁle system, where
all ﬁle system updates occur out of place. UBIFS
uses an index to determine which version of data is
the most recent. This index is called the Tree Node
Cache (TNC), and it is stored both in volatile mem-
ory and on the storage medium. The TNC is a B+
search tree [7] that has a small entry for every data
node in the ﬁle system. When data is appended to
the journal, UBIFS updates the TNC to reference its
location. UBIFS implements truncations and dele-
tions by appending special non-data nodes to the
journal. When the TNC processes these nodes, it
ﬁnds the range of TNC entries that correspond to
the truncated or deleted data nodes and removes
them from the tree.

UBIFS uses a commit and replay mechanism to
ensure that the ﬁle system can be mounted after an
unsafe unmounting without scanning the entire de-
vice. Commit periodically writes the current TNC
to the storage medium, and starts a new empty jour-
nal. Replay loads the most recently-stored TNC into
memory and chronologically processes the journal
entries to update the stale TNC, thus returning the
TNC to the state immediately before unmounting.
UBIFS accesses ﬂash memory through UBI’s log-
ical interface, which provides two features useful for
our purposes. First, UBI allows updates to KSA
erase blocks (called KSA LEB’s in the context of
UBIFSec) using its atomic update feature; after
purging, all used keys remain in the same logical po-
sition, so references to KSA positions remain valid
after purging. Second, UBI handles wear-levelling
for all the PEBs, including the KSA. This is useful

because erase blocks assigned to the KSA see more
frequent erasure; a ﬁxed physical assignment would
therefore present wear-levelling concerns.

4.2 UBIFSec Design

UBIFSec is a version of UBIFS that is extended
to use DNEFS to provide secure data deletion.
UBIFS’s data nodes have a size of 4096 bytes, and
our solution assigns each of them a distinct 128-
bit AES key. AES keys are used in counter mode,
which turns AES into a semantically-secure stream
cipher [20]. Since each key is only ever used to
encrypt a single block of data, we can safely omit
the generation and storage of initialization vectors
(IVs) and simply start the counter at zero. There-
fore, our solution requires about 0.4% of the stor-
age medium’s capacity for the KSA, although there
exists a tradeoﬀ between KSA size and data node
granularity, which we discuss in Section 4.3.

Key Storage Area. The KSA is comprised of a
set of LEBs that store random data used as en-
cryption keys. When the ﬁle system is created,
cryptographically-suitable random data is written
from a hardware source of randomness to each of the
KSA’s LEBs and all the keys are marked as unused.
Purging writes new versions of the KSA LEBs us-
ing UBI’s atomic update feature; immediately after,
ubi_flush is called to ensure all PEBs containing
old versions of the LEB are synchronously erased
and the purged keys are inaccessible. This ﬂush
feature ensures that any copies of LEBs made as
a result of internal wear-levelling are also securely
deleted. Figure 5 shows the LEBs and PEBs during
a purging operation; KSA block 3 temporarily has
two versions stored on the storage medium.

Key State Map. The key state map (Section 3.2)
stores the key positions that are unused, used, and
deleted. The correctness of the key state map is
critical in ensuring the soundness of secure deletion
and data integrity. We now describe how the key
state map is created and stored in UBIFSec. As an
invariant, we require that UBIFSec’s key state map
is always correct before and after executing a purge.
This restriction—instead of requiring correctness at
all times after mounting—is to allow writing new
data during a purging operation, and to account for
the time between marking a key as used and writing
the data it encrypts onto the storage medium.

The key state map is stored, used, and updated
in volatile memory. Initially, the key state map of a
freshly-formatted UBIFSec ﬁle system is correct as it

7

Figure 5: Erase block relationships among MTD, UBI, and UBIFSec, showing the new regions added by UBIFSec
(cf. Figure 1). In this example, a purging operation is ongoing—the ﬁrst three KSA LEBs have been updated and
the remaining LEBs still have their old value. In the MTD layer, an old version of KSA 3 is temporarily available.

consists of no data nodes, and every key is fresh ran-
dom data that is marked as unused. While mounted,
UBIFSec performs appropriate key management to
ensure that the key state map is always correct when
new data is written, deleted, etc. We now show that
we can always create a correct key state map when
mounting an arbitrary UBIFSec ﬁle system.

The key state map is built from a periodic check-
point combined with a replay of the most recent
changes while mounting. We checkpoint the current
key state map to the storage medium whenever the
KSA is purged. After a purge, every key is either
unused or used, and so a checkpoint of this map can
be stored using one bit per key—less than 1% of the
KSA’s size—which is then compressed. A special
LEB is used to store checkpoints, where each new
checkpoint is appended; when the erase block is full
then the next checkpoint is written at the beginning
using an atomic update.

The checkpoint is correct when it is written to the
storage medium, and therefore it is correct when it is
loaded during mounting if no other changes occurred
to the ﬁle system. If the ﬁle system changed after
committing and before unmounting, then UBIFS’s
replay mechanism is used to generate the correct
key state map: ﬁrst the checkpoint is loaded, then
the replay entries are simulated. Therefore, we al-
ways perform purging during regular UBIFS com-
mits; the nodes that are replayed for UBIFS are ex-
actly the ones that must be replayed for UBIFSec.
If the stored checkpoint gets corrupted, then a full
scan of the valid data nodes rebuilds the correct key
state map. A consistency check for the ﬁle system
also conﬁrms the correctness of the key state map
with a full scan.

As it is possible for the storage medium to fail
during the commit operation (e.g., due to a loss of

power), we now show that our invariant holds re-
gardless of the condition of unmounting. Purging
consists of atomically updating each LEB contain-
ing deleted keys and afterwards writing a new check-
point. UBI’s atomic update feature ensures that any
failure before completing the update is equivalent
to failing immediately before beginning. Therefore,
the following is the complete list of possible failure
points: before the ﬁrst purge, between some purges,
after all the purges but before the checkpoint, dur-
ing the checkpoint, or after the checkpoint but before
ﬁnishing other UBIFS commit actions.

First, failure can occur before purging the ﬁrst
LEB, which means the KSA is unchanged. When
remounting the device, the loaded checkpoint is up-
dated with the replay data, thereby constructing the
exact key state map before purging—taken as cor-
rect by assumption.

Second, failure can occur after purging one, sev-
eral, or indeed all of the KSA’s LEBs. When re-
mounting the device, the loaded checkpoint merged
with the replay data reﬂects the state before the ﬁrst
purge, so some purged LEBs contain new unused
data while the key state map claims it is a deleted
key. As these are cryptographically-suitable random
values, with high probability they cannot success-
fully decrypt any existing valid data node.

Third, failure can occur while writing to the check-
point LEB. When the checkpoint is written using
atomic updates, then failing during the operation
is equivalent to failing before it begins (cf. previ-
ous case). Incomplete checkpoints are detected and
so the previous valid checkpoint is loaded instead.
After replaying all the nodes, the key state map is
equal to its state immediately before purging the
KSA. This means that all entries marked as deleted
are actually unused entries, so the invariant holds.

8

                                  ckptkey stateKSA   1’KSAKSAKSAKSA   2’   3’   4   5KSA   1’KSA   2’εckpt 2ckpt 1KSA   4KSA   5KSA   3’KSA   1’εckpt 2ckpt 1KSA   4KSA   2’KSA   5(flash)MTD PEBsUBIFSUBI LEBsmain storage areakey storage areaεckpt 2ckpt 1UBIFSec changesKSA   3’KSA   3Old ckpt

value
unused
unused
unused

used
used
used

Replay’s

eﬀect

nothing

mark used

Ckpt
value
unused

used

mark deleted

unused

nothing

mark used

used
used

recovery
unused

used

deleted

used
used

mark deleted

unused

deleted

Value after

Cause

Key’s state

no event

key assigned

key assigned, deleted

no event

cannot occur
key deleted

correct
correct
correct
correct
correct
correct

Table 1: Consequences of replaying false information during committing.

Finally, failure can occur after successfully purg-
ing the KSA and checkpointing the key state map,
but before completing the regular UBIFS commit.
In this case, the current key state map correctly re-
ﬂects the contents of the KSA. When mounting, the
replay mechanism incorrectly updates it with the
journal entries of the previous iteration. Table 1
shows the full space of possibilities when replaying
old changes on the post-purged checkpoint. It shows
that it is only possible for an unused key to be er-
roneously marked as deleted, which still results in a
correct key state map.

In summary, the correctness of the key state map
before and after a purge is invariant, regardless of
when or how the ﬁle system was unmounted. This
ensures secure deletion’s soundness as well as the
integrity of the valid data on the storage medium.

instantiates DNEFS for
Summary. UBIFSec
UBIFS, and so it provides eﬃcient ﬁne-grained guar-
anteed secure deletion. UBIFSec is eﬃcient in stor-
age space: the overhead for keys is ﬁxed and it needs
less than one percent of the total storage medium’s
capacity. The periodic checkpointing of UBIFSec’s
key state map ensures that UBIFS’s mounting time
is not signiﬁcantly aﬀected by our approach.

Our implementation of UBIFSec is available as a
Linux kernel patch for version 3.2.1 [37]. As of the
time of writing, we are in the process of integrating
UBIFSec into the standard UBIFS distribution.

4.3 Experimental Validation

We have patched an Android Nexus One smart
phone’s Linux kernel to include UBIFSec and modi-
ﬁed the phone to use it as the primary data partition.
In this section, we describe experiments with our im-
plementation on both the Android mobile phone and
on a simulator.

Our experiments measure our solution’s cost: ad-
ditional battery consumption, wear on the ﬂash
memory, and time required to perform ﬁle opera-
tions. The increase in ﬂash memory wear is mea-

sured using a simulator, and the increase in time
is measured on a Google Nexus One smartphone
by instrumenting the source code of UBIFS and
UBIFSec to measure the time it takes to perform ba-
sic ﬁle system operations. We further collected tim-
ing measurements from the same smartphone run-
ning YAFFS: the ﬂash ﬁle system currently used on
Android phones.

Android Implementation. To test the feasibil-
ity of our solution on mobile devices, we ported
UBIFSec to the Android OS. The Android OS is
based on the Linux kernel and it was straightfor-
wards to add support for UBIFS. The source code
was already available and we simply applied our
patch and conﬁgured the kernel compiler to include
the UBI device and the UBIFS ﬁle system.

Wear Analysis. We measured UBIFSec’s wear on
the ﬂash memory in two ways: the number of erase
cycles that occur on the storage medium, and the
distribution of erasures over the erase blocks. To re-
duce the wear, it is desirable to minimize the number
of erasures that are performed, and to evenly spread
the erasures over the storage medium’s erase blocks.
We instrumented both UBIFS and UBIFSec to
measure PEB erasure frequency during use. We var-
ied UBIFSec’s purging frequency and computed the
resulting erase block allocation rate. This was done
by using a low-level control (ioctl) to force UBIFS
to perform a commit. We also measured the ex-
pected number of deleted keys and updated KSA
LEBs during purging operation.

We simulated the UBI storage medium based on
Nexus One speciﬁcations [11]. We varied the period
between UBIFSec’s purging operation, i.e., the du-
ration of a purging epoch: one of 1, 5, 15, 30, and
60 minutes. We used a discrete event simulator to
write ﬁles based on the writing behaviour collected
from an Android mobile phone [32]. Writing was
performed until the ﬁle system began garbage collec-
tion; thenceforth we took measurements for a week

9

Purge
period
Stardard UBIFS
60 minutes
30 minutes
15 minutes
5 minutes
1 minute

-

-

PEB erasures Updates per KSA updates
per hour
21.3 ± 3.0
26.4 ± 1.5
34.9 ± 3.8
40.1 ± 3.6
68.5 ± 4.4
158.6 ± 11.5

KSA purge
6.8 ± 0.5
5.1 ± 0.6
3.7 ± 0.4
2.6 ± 0.1
1.0 ± 0.1

per hour
6.8 ± 0.5
9.7 ± 2.0
14.9 ± 1.6
30.8 ± 0.7
61.4 ± 4.6

Deleted keys

per purged LEB

-

64.2 ± 9.6
50.3 ± 9.5
36.3 ± 8.2
22.1 ± 4.3
14.1 ± 4.4

Wear

ineq (%)
16.6 ± 0.5
17.9 ± 0.2
17.8 ± 0.3
19.0 ± 0.3
19.2 ± 0.5
20.0 ± 0.2

Lifetime
(years)

841
679
512
447
262
113

Table 2: Wear analysis for our modiﬁed UBIFS ﬁle system. The expected lifetime is based on the Google Nexus One
phone’s data partition, which has 1571 erase blocks with a (conservative) lifetime estimate of 104 erasures.

of simulated time. We averaged the results from four
attempts and computed 95% conﬁdence intervals.

To determine if our solution negatively impacts
UBI’s wear levelling, we performed the following ex-
periment. Each time UBI unmaps an LEB from
a PEB (thus resulting in an erasure) or atomically
updates an LEB (also resulting in an erasure), we
logged the erased PEB’s number. From this data,
we then compute the PEBs’ erasure distribution.

To quantify the success of wear-levelling, we use
the Hoover economic wealth inequality indicator—a
metric that is independent of the storage medium
size and erasure frequency. This metric comes
from economics, where it quantiﬁes the unfairness
of wealth distributions. It is the simplest measure,
corresponding to an appropriately normalized sum
of the diﬀerence of each measurement to the mean.
For our purposes, it is the fraction of erasures that
must be reassigned to other erase blocks to obtain
completely even wear. Assuming the observations
i=1 ci, then the inequality
C − 1
n(cid:107).

are c1, . . . , cn, and C =(cid:80)n

(cid:80)n
i=1 (cid:107) ci

measure is 1
2

Table 2 presents the results of our experiment. We
see that the rate of block allocations increases as
the purging period decreases, with 15 minutes pro-
viding a palatable tradeoﬀ between additional wear
and timeliness of deletion. The KSA’s update rate
is computed as the product of the purging frequency
and the average number of KSA LEBs that are up-
dated during a purge. As such, it does not include
the additional costs of executing UBIFS commit,
which is captured by the disparity in the block al-
locations per hour. We see that when committing
each minute, the additional overhead of committing
compared to the updates of KSA blocks becomes sig-
niﬁcant. While we integrated purging with commit
to simplify the implementation, it is possible to sep-
arate these operations. Instead, UBIFSec can add
purging start and ﬁnish nodes as regular (non-data)
journal entries. The replay mechanism is then ex-
tended to correctly update the key state map while
processing these purging nodes.

10

The expected number of keys deleted per purged
KSA LEB decreases sublinearly with the purging pe-
riod and linearly with the number of purged LEBs.
This is because a smaller interval results in fewer ex-
pected deletions per interval and fewer deleted keys.
Finally, UBIFSec aﬀects wear-levelling slightly,
but not signiﬁcantly. The unfairness increases with
the purging frequency, likely because the set of un-
allocated PEBs is smaller than the set of allocated
PEBs; very frequent updates will cause unallocated
PEBs to suﬀer more erasures. However, the eﬀect
is slight. It is certainly the case that the additional
block erasures are, for the most part, evenly spread
over the device.

Throughput and Battery Analysis A natural
concern is that UBIFSec might introduce signiﬁ-
cant costs that discourage its use. We therefore
experimentally evaluated the read/write through-
put, battery consumption, and computation time of
UBIFSec’s Android implementation (Linux version
2.6.35.7) on a Google Nexus One mobile phone. We
compare measurements taken for both Android’s de-
fault ﬁle system (YAFFS) and for the standard ver-
sion of UBIFS.

To measure battery consumption over time, we
disabled the operating system’s suspension ability,
thus allowing computations to occur continuously
and indeﬁnitely. This has the unfortunate conse-
quence of maintaining power to the screen of the
mobile phone. We ﬁrst determined the power con-
sumption of the device while remaining idle over the
course of two hours starting with an 80% charged
battery with a total capacity of 1366 mAh. The re-
sult was nearly constant at 121 mA. We subtract this
value from all other power consumption measures.

To measure read throughput and battery use, we
repeatedly read a large (85 MiB) ﬁle; we mounted
the drive as read-only and remounted it after each
read to ensure that all read caches were cleared.
We read the ﬁle using dd, directing the output to
/dev/null and recorded the observed throughput.

We began each experiment with an 80% charged
battery and ran it for 10 minutes observing con-
stant behaviour. Table 3 presents the results for
this experiment. For all ﬁlesystems, the additional
battery consumption was constant: 39 mA, about
one-third of the idle cost. The throughput achieved
with that power varied, and so along with our re-
sults we compute the amount of data that can be
read using 13.7 mAh—1% of the Nexus One’s bat-
tery. The write throughput and battery consump-
tion was measured by using dd to copy data from
/dev/zero to a ﬁle on the ﬂash ﬁle system. Com-
pression was disabled for UBIFS for comparison with
YAFFS. When the device was full, the throughput
was recorded. We immediately started dd to write
to the same ﬁle, which begins by overwriting it and
thus measuring the battery consumption and reduc-
tion in throughput imposed by erase block erasure
concomitant with writes.

We observe that the use of UBIFSec reduces the
throughput for both read and write operations when
compared to UBIFS. Some decrease is expected, as
the encryption keys must be read from ﬂash while
reading and writing. To check if the encryption op-
erations also induce delay, we performed these ex-
periments with a modiﬁed UBIFSec that immedi-
ately returned zeroed memory when asked to read
a key, but otherwise performed all cryptographic
operations correctly. The resulting throughput for
read and write was identical to UBIFS, suggesting
that (for multiple reads) cryptographic operations
are easily pipelined into the relatively slower ﬂash
memory read/write operations.

Some key caching optimizations can be added to
UBIFSec to improve the throughput. Whenever a
page of ﬂash memory is read, the entire page can be
cached at no additional read cost, allowing eﬃcient
sequential access to keys, e.g., for a large ﬁle. Long-
term use of the ﬁle system may reduce its eﬃciency
as gaps between used and unused keys result in new
ﬁles not being assigned sequential keys.
Improved
KSA organization can help retain this eﬃciency.

Write throughput, alternatively,

is easily im-
proved with caching. The sequence of keys for data
written in the next purging epoch is known at purg-
ing time when all these keys are randomly generated
and written to the KSA. By using a heuristic on the
expected number of keys assigned during a purging
epoch, the keys for new data can be kept in mem-
ory as well as written to the KSA. Whenever a key
is needed, it is taken and removed from this cache
while there are still keys available.

YAFFS UBIFS UBIFSec

Read rate (MiB/s)
Power usage (mA)

GiB read per %

Write rate (MiB/s)
Power usage (mA)
GiB written per %

4.4
39
5.4
2.4
30
3.8

3.9
39
4.8
2.1
46
2.2

3.0
39
3.7
1.7
41
2.0

Table 3: I/O throughput and battery consumption for
YAFFS, UBIFS, and UBIFSec.

ing keys are overwritten when the key is no longer
needed during normal decryption and encryption op-
erations. Caches contain keys for a longer time
but are cleared during a purging operation to en-
sure deleted keys never outlive their deletion purging
epoch. Applications storing sensitive data in volatile
memory may remain after the data’s deletion and so
secure memory deallocation should be provided by
the operating system to ensure its unavailability [5].

Timing Analysis. We timed the following ﬁle
system functions: mounting/unmounting the ﬁle
system and writing/reading a page. Addition-
ally, we timed the following functions speciﬁc to
UBIFSec: allocation of the cryptographic context,
reading the encryption key, performing an encryp-
tion/decryption, and purging a KSA LEB. We col-
lected dozens of measurements for purging, mount-
ing and unmounting, and hundreds of measurements
for the other operations (i.e., reading and writing).
We controlled the delay caused by our instrumenta-
tion by repeating the experiments instead of execut-
ing nested measurements, i.e., we timed encryption
and writing to a block in separate experiments.

We mounted a partition of the Android’s ﬂash
memory ﬁrst as a standard UBIFS ﬁle system and
then as UBIFSec ﬁle system. We executed a se-
quence of ﬁle I/O operations on the ﬁle system. We
collected the resulting times and present the 80th
percentile measurements in Table 4. Because of
UBIFS’s implementation details, the timing results
for reading data nodes contain also the time required
to read relevant TNC pages (if they are not currently
cached) from the storage medium, which is reﬂected
in the increased delay. Because the data node size
for YAFFS is half that of UBIFS, we also doubled
the read/write measurements for YAFFS for com-
parison. Finally, the mounting time for YAFFS is
for mounting after a safe unmount—for an unsafe
unmount, YAFFS requires a full device scan, which
takes several orders of magnitude longer.

Caching keys in memory opens UBIFSec to at-
tacks. We ensure that all memory buﬀers contain-

The results show an increase in the time required
for each of the operations. Mounting and unmount-

11

File system
operation
mount
unmount
read data node
write data node
prepare cipher
read key
encrypt
decrypt
purge one block

80th percentile execution time (ms)
YAFFS UBIFS

UBIFSec

43
44
0.92
1.1
-
-
-
-
-

179
0.55
2.8
1.3
-
-
-
-
-

236
0.67
4.0
2.5
0.05
0.38
0.91
0.94
21.2

Table 4: Timing results for various ﬁle system function-
ality on an android mobile phone.

ing the storage medium continues to take a frac-
tion of a second. Reading and writing to a data
node increases by a little more than a millisecond,
an expected result that reﬂects the time it takes to
read the encryption key from the storage medium
and encrypt the data. We also tested for notice-
able delay by watching a movie in real time from a
UBIFSec-formatted Android phone running the An-
droid OS: the video was 512x288 Windows Media
Video 9 DMO; the audio was 96.0 kbit DivX au-
dio v2. The video and audio played as expected
on the phone; no observable latency, jitter, or stut-
ter was observed during playback while background
processes ran normally.

Each atomic update of an erase block takes about
22 milliseconds. This means that if every KSA LEB
is updated, the entire data partition of the Nexus
One phone can be purged in less than a ﬁfth of a
second. The cost to purge a device grows with its
storage medium’s size. The erasure cost for purging
can be reduced in a variety of ways: increasing the
data node size to use fewer keys, increasing the dura-
tion of a purging epoch, or improving the KSA’s or-
ganization and key assignment strategy to minimize
the number of KSA LEBs that contain deleted keys.
The last technique works alongside lazy on-demand
purging of KSA LEBs that contain no deleted keys,
i.e., only used and unused keys.

Granularity Tradeoﬀ Our
solution encrypts
each data node with a separate key allowing eﬃ-
cient secure deletion of data from long-lived ﬁles,
e.g., databases. Other related work instead encrypts
each ﬁle with a unique key, allowing secure deletion
only at the granularity of an entire ﬁle [19]. This is
well suited for media ﬁles, such as digital audio and
photographs, which are usually created, read, and
deleted in their entirety. However, if the encrypted
ﬁle should permit random access and modiﬁcation,

Data node size
(ﬂash pages)
1
8
64
512
4096

KSA size

Copy cost

(EBs per GiB)

(EBs)

64
8
1

0.125
0.016

0

0.11
0.98
63.98
511.98

Table 5: Data node granularity tradeoﬀs assuming 64
2-KiB pages per erase block.

then one of the following is true: (i) the cipher is used
in an ECB-like mode, resulting in a system that is
not semantically secure, (ii) the cipher is used in a
CBC-like mode where all ﬁle modiﬁcations require
re-encryption of the remainder of the ﬁle, (iii) the
cipher is used in a CBC-like mode with periodic IVs
to facilitate eﬃcient modiﬁcation, (iv) the cipher is
used in counter mode, resulting in all ﬁle modiﬁca-
tions requiring rewriting the entire ﬁle using a new
IV to avoid the two-time pad problem [20], or (v)
the cipher is used in counter mode with periodic IVs
to facilitate eﬃcient modiﬁcations.

We observe the that ﬁrst option is inadequate as a
lack of semantic security means that some informa-
tion about the securely deleted data is still available.
The second and fourth options are special cases of
the third and ﬁfth options respectively, where the IV
granularity is one per ﬁle and ﬁle modiﬁcations are
woefully ineﬃcient. Thus, a tradeoﬀ exists between
the storage costs of IVs and additional computation
for modiﬁcations. As the IV granularity decreases to
the data node size, the extra storage cost required
for IVs is equal to the KSA storage cost for DNEFS’s
one key per data node, and the modiﬁcation cost is
simply that of the single data node.

We emphasize that a scheme where IVs were not
stored but instead deterministically computed, e.g.,
using the ﬁle oﬀset, would inhibit secure deletion: so
long as the ﬁle’s encryption key and previous version
of the data node were available, the adversary could
compute the IV and decrypt the data. Therefore, all
IVs for such schemes must be randomly generated,
stored, and securely deleted.

Table 5 compares the encryption granularity trade
oﬀ for a ﬂash drive with 64 2-KiB pages per erase
block. To compare DNEFS with schemes that en-
crypt each ﬁle separately, simply consider the data
node size as equal to the IV granularity or the ex-
pected size ﬁle size. The KSA size, measured in
erase blocks per GiB of storage space, is the amount
of storage required for IVs and keys, and is the worst
case number of erase blocks that must be erased
during each purging operation. The copy cost, also

12

measured in erase blocks, is the amount of data that
must be re-written to the ﬂash storage medium due
to a data node modiﬁcation that aﬀects only one
page of ﬂash memory. For example, with a data
node size of 1024 KiB and a page size of 2 KiB, the
copy cost for a small change to the data node is 1022
KiB. This is measured in erase blocks because the
additional writes, once ﬁlling an entire erase block,
result in an additional erase block erasure, otherwise
unnecessary with a smaller data node size.

As we observed earlier, reducing the number of
keys required to be read from ﬂash per byte of data
improves read and write throughput. From these
deﬁnitions, along with basic geometry of the ﬂash
drive, it is easy to compute the values presented in
Table 5. When deploying DNEFS, the administra-
tor can choose a data node size by optimizing for
the costs given how frequently small erasures and
complete purges are executed.

5 Extensions and Optimizations

Compatibility with FTLs. The most widely-
deployed interface for ﬂash memory is the Flash
Translation Layer (FTL) [1], which maps logical
block device sectors (e.g., a hard drive) to physi-
cal ﬂash addresses. While FTLs vary in implemen-
tation, many of which are not publicly available, in
principle DNEFS can be integrated with FTLs in the
following way. All ﬁle-system data is encrypted be-
fore being written to ﬂash, and decrypted whenever
it is read. A key storage area is reserved on the ﬂash
memory to store keys, and key positions are assigned
to data. The FTL’s in-memory logical remapping
of sectors to ﬂash addresses must store alongside a
reference to a key location. The FTL mechanism
that rebuilds its logical sector to address mapping
must also rebuild the corresponding key location.
Key locations consist of a logical KSA erase block
number and the actual oﬀset inside the erase block.
Logically-referenced KSA erase blocks are managed
by storing metadata in the ﬁnal page of each KSA
erase block. This page is written immediately after
successfully writing the KSA block and stores the
following information: the logical KSA number so
that key references need not be updated after purg-
ing, and an epoch number so that the most recent
version of the KSA block is known. With this infor-
mation, the FTL is able to replicate the features of
UBI that DNEFS requires.

Generating a correct key state map when mount-
ing is tied to the internal logic of the FTL. Assuming
that the map of logical to physical addresses along
with the key positions is correctly created, then it

is trivial to iterate over the entries to mark the cor-
responding keys as used. The unmarked positions
are then purged to contain new data. The FTL
must also generate cryptographically-secure random
data (e.g., with an accelerometer [38]) or be able
to receive it from the host. Finally, the ﬁle sys-
tem mounted on the FTL must issue TRIM com-
mands [16] when a sector is deleted, as only the ﬁle
system has the semantic context to know when a
sector is deleted.

Purging Policies. Purging is currently performed
after a user-controlled period of time and before un-
mounting the device. More elaborate policies are
deﬁnable, where purging occurs once a threshold of
deleted keys is passed, ensuring that the amount of
exposable data is limited, so the deletion of many
ﬁles would thus act as a trigger for purging. A low-
level control allows user-level applications to trigger
a purge, such as an email application that purges
the ﬁle system after clearing the cache. We can al-
ternatively use a new extended attribute to act a
trigger: whenever any data node belonging to a sen-
sitive ﬁle is deleted, then DNEFS triggers an imme-
diate purge. This allows users to have conﬁdence
that most ﬁles are periodically deleted, while sensi-
tive ﬁles are promptly deleted.

Securely Deleting Swap. A concern for secure
deletion is to securely delete any copies of data made
by the operating system. Data that is quite large
may be written to a swap ﬁle—which may be on the
same ﬁle system or on a special cache partition. We
leave as future work to integrate our solution to a
secure deleting cache. (There exist encrypted swap
partitions [31], but not one that securely deletes the
memory when it is deallocated.) We expect it to
be simple to design, as cache data does not need to
persist if power is lost; an encryption-based approach
can keep all the keys in volatile memory and delete
them immediately when they are no longer needed.

Encrypted File System. Our design can be triv-
ially extended to oﬀer a passphrase-protected en-
crypted ﬁle system: we simply encrypt the KSA
whenever we write random data, and derive the
decryption key from a provided passphrase when
mounting.
Since, with high probability, each
randomly-generated key in the KSA is unique, we
can use a block cipher in ECB mode to allow
rapid decryption of randomly accessed oﬀsets with-
out storing additional initialization vectors [20].

Encrypted storage media are already quite pop-
ular, as they provide conﬁdentiality of stored data

13

against computationally-bounded non-coercive at-
tackers, e.g., thieves, provided the master secret is
unavailable in volatile memory when the attack oc-
curs [13]. It is therefore important to oﬀer our en-
crypted ﬁle system design to avoid users doubly-
encrypting their data: ﬁrst as an encrypted ﬁle sys-
tem and then for secure deletion.

6 Related Work

Secure deletion for magnetic media is a well-
researched area. Various solutions exist at diﬀer-
ent levels of system integration. User-level solutions
such as shred [29] open a ﬁle and overwrite its en-
tire content with insensitive data. This requires that
the ﬁle system performs in-place updates, otherwise
old data still remains. As such, these solutions are
inappropriate for ﬂash memory.

Kernel-level secure deletion has been designed and
implemented for various popular block-structured
ﬁle systems [2, 17]. These solutions typically change
the ﬁle system so that whenever a block is discarded,
its content is ﬁrst sanitized before it is added to the
list of free blocks. This ensures that even if a ﬁle is
truncated or the user forgets to use a secure deletion
tool, the data is still sanitized. It is also beneﬁcial
for journalled ﬁle systems where in-place updates do
not immediately occur, so overwriting a ﬁle may not
actually overwrite the original content. The saniti-
zation of discarded blocks still requires in-place up-
dates, and is therefore inapplicable to ﬂash memory.
The use of encryption to delete data was originally
proposed by Boneh and Lipton [3], where they used
the convenience of deleting small encryption keys to
computationally-delete data from magnetic backup
tapes. Peterson et al. [28] used this approach to im-
prove the eﬃciency of secure deletion in a versioned
backup system on a magnetic storage medium. They
encrypt each data block with an all-or-nothing cryp-
tographic expansion transformation [33] and colo-
cate the resulting key-sized tags for every version of
the same ﬁle in storage. They use in-place updates
to remove tags, and keys are colocated to reduce
the cost of magnetic disk seek times when deleting
all versions of a single ﬁle. DNEFS also colocates
keys separately to improve the eﬃciency of secure
deletion. However, DNEFS is prohibited from per-
forming in-place updates, and our design focuses on
minimizing and levelling erasure wear.

Another approach is Perlman’s Ephemerizer [27],
a system that allows communication between par-
ties where messages are securely deleted in the pres-
ence of a coercive attacker. Data is encrypted with
ephemeral keys that are manged by the eponymous

trusted third party. Each message is given an ex-
piration time at creation, and an appropriate key
is generated accordingly. The Ephemerizer stores
the keys and provides them when necessary to the
communicating parties using one-time session keys.
When the expiration time passes, it deletes any ma-
terial required to create the key, thus ensuring se-
cure deletion of the past messages. Perlman’s work
is a protocol using secure deletion as an assumed
primitive oﬀered by the storage medium. Indeed, if
this approach is implemented on a ﬂash-based smart
card, DNEFS can be used to implement it.

Reardon et al. [32] have shown how to securely
delete data from log-structured ﬁle systems from
user-space—that is, without modifying the hardware
or the ﬁle system—provided that the user can ac-
cess the ﬂash directly and is not subjected to disk
quota limitations. Their proposal is to ﬁll the stor-
age medium to capacity to ensure that no wasted
space remains on the storage medium, thus ensuring
the secure deletion of data. This is a costly approach
in terms of ﬂash memory wear and execution time,
but from user-space it is the only solution possible.
They also showed that occupying a large segment of
the storage medium with unneeded data improves
the expected time deleted data remains on the stor-
age medium, and reduces the amount of space that
needs to be ﬁlled to guarantee deletion.

Swanson et al. [36] considered veriﬁable sanitiza-
tion for solid state drives—ﬂash memory accessed
through an opaque FTL. They observed that the
manufacturers of the controller are unreliable even
when implementing their own advertised sanitiza-
tion procedure, and that the use of cryptography
is insuﬃcient when the ultimate storage location of
the cryptographic key cannot be determined from
the logical address provided by the FTL. They pro-
pose a technique for static sanitization of the entire
ﬂash memory—that is, all the storage medium’s con-
tained information is securely removed. It works by
originally encrypting all the data on the drive be-
fore being written. When a sanitize command is is-
sued, ﬁrst the memory containing the keys is erased,
and then every page on the device is written and
erased. Our solution focuses on the more typical case
of a user wanting to securely delete some sensitive
data from their storage media but not wanting to
completely remove all data available on the device.
Our solution requires access to the raw ﬂash, or a
security-aware abstraction such as UBI that oﬀers
the ubi_flush() function to synchronously remove
all previous versions (including copies) of a particu-
lar LEB number.

Wei et al. [39] have considered secure deletion on

14

ﬂash memory accessed through an FTL (cf. Sec-
tion 2). They propose a technique, called scrubbing,
which writes zeros over the pages of ﬂash memory
without ﬁrst erasing the block. This sanitizes the
data because, in general, ﬂash memory requires an
erasure to turn a binary zero to a binary one, but
writes turn ones into zeros. Sun et al. [35] pro-
pose a hybrid secure deletion method built on Wei
et al.’s scheme, where they also optimize the case
when there is less data to copy oﬀ a block then data
to be zero overwritten.

Scrubbing securely deletes data immediately, and
no block erasures must be performed. However, it re-
quires programming a page multiple times between
erasures, which is not appropriate for ﬂash mem-
ory [22]. In general, the pages of an erase block must
be programmed sequentially [26] and only once. An
option exists to allow multiple programs per page,
provided they occur at diﬀerent positions in the
page; multiple overwrites to the same location oﬃ-
cially result in undeﬁned behaviour [26]. Flash man-
ufacturers prohibit this due to program disturb [21]:
bit errors that can be caused in spatially proximate
pages while programming ﬂash memory.

Wei et al. performed experiments to quantify
the rate at which such errors occur: they showed
that they do exist but their frequency varies widely
among ﬂash types, a result also conﬁrmed by Grupp
et al. [12]. Wei et al. use the term scrub budget to re-
fer to the number of times that the particular model
of ﬂash memory has experimentally allowed multi-
ple overwrites without exhibiting a signiﬁcant risk
of data errors. When the scrub budget for an erase
block is exceeded, then secure deletion is instead per-
formed by invoking garbage collection: copying all
the remaining valid data blocks elsewhere and eras-
ing the block. Wei et al. state that modern densely
packed ﬂash memories are unsuitable for their tech-
nique as they allow as few as two scrubs per erase
block [39]. This raises serious concerns for the fu-
ture utility of Wei et al.’s approach and highlights
the importance of following hardware speciﬁcations.
Lee et al. [19] propose secure deletion for YAFFS.
They encrypt each ﬁle with a diﬀerent key, store
the keys in the ﬁle’s header, and propose to modify
YAFFS to colocate ﬁle headers in a ﬁxed area of the
storage medium. They achieve secure deletion by
erasing the erase block containing the ﬁle’s header,
thus deleting the entire ﬁle. More recently, Lee et
al. [18] built on this approach by extending it to per-
form standard data sanitization methods prescribed
by government agencies (e.g., NSA [25], DoD [24])
on the erase blocks containing the keys.

We can only compare our approach with theirs in

15

design, as their approaches were not implemented.
First, the approach causes an erase block deletion
for every deleted ﬁle. This results in rapid wear
for devices that create and delete many small cache
ﬁles. Reardon et al. [32] observed that the Android
phone’s normal usage created 10,000 such ﬁles a day;
if each ﬁle triggers an erase block erasure, then this
solution causes unacceptable wear on the dedicated
segment of the ﬂash memory used for ﬁle headers.
Their proposal encrypts the entire ﬁle before writ-
ing it to the storage medium with a single key and
without mention of the creation or storage of IVs.
(See Section 4.3 for more analysis on per-ﬁle versus
per-data-node encryption.) Our solution diﬀers from
theirs by batching deletions and purging based on an
interval, by considering the eﬀect on wear levelling,
by allowing ﬁne-grained deletions of overwritten and
truncated data, and by being fully implemented.

7 Conclusions

DNEFS and its instance UBIFSec are the ﬁrst fea-
sible solution for eﬃcient secure deletion for ﬂash
memory operating within ﬂash memory’s speciﬁca-
tion. It provides guaranteed secure deletion against
a computationally-bounded peek-a-boo attacker by
encrypting each data node with a diﬀerent key
and storing the keys together on the ﬂash storage
medium. The erase blocks containing the keys are
logically updated to remove old keys, replacing them
with fresh random data that can be used as keys for
new data. It is ﬁne-grained in that parts of ﬁles that
are overwritten are also securely deleted.

We have implemented UBIFSec and analyzed it
experimentally to ensure that it is eﬃcient, requiring
a small evenly-levelled increase in ﬂash memory wear
and little additional computation time. UBIFSec
was easily added to UBIFS, where cryptographic op-
erations are added seamlessly to UBIFS’s read/write
data path, and changes to key state are handled by
UBIFS’s existing index of data nodes.

Acknowledgments

This work was partially supported by the Zurich In-
formation Security Center. It represents the views
of the authors. We would like to thank our anony-
mous reviews for their many helpful comments and
Artem Bityutskiy for his help integrating UBIFSec
into UBIFS.

References

[1] Ban, A. Flash ﬁle system. US Patent, no. 5404485, 1995.
[2] Bauer, S., and Priyantha, N. B. Secure Data Dele-
tion for Linux File Systems. Usenix Security Symposium
(2001), 153–164

[3] Boneh, D., and Lipton, R. J. A revocable backup
system.
the 6th conference on
USENIX Security Symposium, Focusing on Applications
of Cryptography - Volume 6 (Berkeley, CA, USA, 1996),
USENIX Association, pp. 91–96.

In Proceedings of

[4] Charles Manning. How YAFFS Works. 2010.
[5] Chow, J., Pfaff, B., Garfinkel, T., and Rosenblum,
M. Shredding Your Garbage: Reducing Data Lifetime
through Secure Deallocation. In Proceedings of the 14th
conference on USENIX Security Symposium - Volume
14 (Berkeley, CA, USA, 2005), SSYM’05, USENIX As-
sociation.

[6] Chung, T.-S., Park, D.-J., Park, S., Lee, D.-H., Lee,
S.-W., and Song, H.-J. A survey of Flash Translation
Layer. Journal of Systems Architecture 55, 5-6 (2009),
332–343.

[7] Cormen, T., Leiserson, C., and Rivest, R. Introduc-

tion to Algorithms. McGraw Hill, 1998.

[8] Gal, E., and Toledo, S. Algorithms and Data Struc-
tures for Flash Memories. ACM Computing Surveys 37
(2005), 138–163.

[9] Garfinkel, S., and Shelat, A. Remembrance of Data
Passed: A Study of Disk Sanitization Practices. IEEE
Security & Privacy (January 2003), 17–27.

[10] Gleixner, T., Haverkamp, F., and Bityutskiy, A.

UBI - Unsorted Block Images. 2006.
[11] Google, Inc. Google Nexus Phone.
[12] Grupp, L. M., Caulfield, A. M., Coburn, J., Swan-
son, S., Yaakobi, E., Siegel, P. H., and Wolf, J. K.
Characterizing ﬂash memory: anomalies, observations,
and applications.
In Proceedings of the 42nd Annual
IEEE/ACM International Symposium on Microarchitec-
ture (New York, NY, USA, 2009), MICRO 42, ACM,
pp. 24–33.

[13] Halderman, J. A., Schoen, S. D., Heninger, N.,
Clarkson, W., Paul, W., Calandrino, J. A., Feld-
man, A. J., Appelbaum, J., and Felten, E. W. Lest we
remember: cold-boot attacks on encryption keys. Com-
munications of the ACM 52 (May 2009), 91–98.

[14] Hunter, A. A Brief Introduction to the Design of

UBIFS. 2008.

[15] Intel Corporation. Understanding the Flash Transla-

tion Layer (FTL) Speciﬁcation. 1998.

[16] Intel Corporation. Intel Solid-State Drive Optimizer.

2009.

[17] Joukov, N., Papaxenopoulos, H., and Zadok, E. Se-
cure Deletion Myths, Issues, and Solutions. ACM Work-
shop on Storage Security and Survivability (2006), 61–
66.

[18] Lee, B., Son, K., Won, D., and Kim, S. Secure Data
Deletion for USB Flash Memory. Journal of Information
Science and Engineering 27 (2011), 933–952.

[19] Lee, J., Yi, S., Heo, J., and Park, H. An Eﬃcient
Secure Deletion Scheme for Flash File Systems. Journal
of Information Science and Engineering (2010), 27–38.

[20] Menezes, A. J., van Oorschot, P. C., and Vanstone,
S. A. Handbook of Applied Cryptography. CRC Press,
2001.

[21] Micron, Inc. Design and Use Considerations for NAND

Flash Memory Introduction. 2006.

[22] Micron Technology, Inc. Technical Note: Design and

Use Considerations for NAND Flash Memory. 2006.

[23] Memory Technology Devices (MTD): Subsystem for

Linux. 2008.

[24] National Industrial Security Program Operating Manual.

July 1997.

[25] NSA/CSS Storage Device Declassiﬁcation Manual.

November 2000.

[26] Open NAND Flash Interface. Open NAND Flash In-

terface Speciﬁcation, version 3.0. 2011.

[27] Perlman, R. The Ephemerizer: Making Data Disap-

pear. Tech. rep., Mountain View, CA, USA, 2005.

[28] Peterson, Z., Burns, R., and Herring, J. Secure
Deletion for a Versioning File System. USENIX Con-
ference on File and Storage Technologies (2005).

[29] Plumb, C. shred(1) - Linux man page.
[30] P¨opper, C., Basin, D., Capkun, S., and Cremers,
C. Keeping Data Secret under Full Compromise using
Porter Devices. In Computer Security Applications Con-
ference (2010), pp. 241–250.

[31] Provos, N. Encrypting virtual memory. In Proceedings
of the 9th USENIX Security Symposium (2000), pp. 35–
44.

[32] Reardon, J., Marforio, C., Capkun, S., and Basin,
D. Secure Deletion on Log-structured File Systems. 7th
ACM Symposium on Information, Computer and Com-
munications Security (2012).

[33] Rivest, R. L. All-Or-Nothing Encryption and The Pack-
age Transform.
In Fast Software Encryption Confer-
ence (1997), Springer Lecture Notes in Computer Sci-
ence, pp. 210–218.

[34] Rosenblum, M., and Ousterhout, J. K. The Design
and Implementation of a Log-Structured File System.
ACM Transactions on Computer Systems 10 (1992), 1–
15.

[35] Sun, K., Choi, J., Lee, D., and Noh, S. Models and De-
sign of an Adaptive Hybrid Scheme for Secure Deletion
of Data in Consumer Electronics. IEEE Transactions on
Consumer Electronics 54 (2008), 100–104.

[36] Swanson, S., and Wei, M. SAFE: Fast, Veriﬁable San-

itization for SSDs. October 2010.

[37] UBIFSec Patch.
[38] Voris, J., Saxena, N., and Halevi, T. Accelerome-
ters and randomness: perfect together. In Proceedings
of the fourth ACM conference on Wireless network se-
curity (New York, NY, USA, 2011), WiSec ’11, ACM,
pp. 115–126.

[39] Wei, M., Grupp, L. M., Spada, F. M., and Swanson,
S. Reliably Erasing Data from Flash-Based Solid State
Drives.
In Proceedings of the 9th USENIX conference
on File and Storage Technologies (Berkeley, CA, USA,
2011), pp. 105–117.

[40] Woodhouse, D. JFFS: The Journalling Flash File Sys-

tem. In Ottawa Linux Symposium (2001).

16

