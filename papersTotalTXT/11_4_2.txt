No More Gotos: Decompilation Using Pattern-Independent Control-Flow Structuring

and Semantics-Preserving Transformations

Khaled Yakdan∗, Sebastian Eschweiler†, Elmar Gerhards-Padilla†, Matthew Smith∗

∗University of Bonn, Germany
{yakdan, smith}@cs.uni-bonn.de
†Fraunhofer FKIE, Germany

{sebastian.eschweiler, elmar.gerhards-padilla}@fkie.fraunhofer.de

Abstract—Decompilation is important for many security appli-
cations; it facilitates the tedious task of manual malware reverse
engineering and enables the use of source-based security tools on
binary code. This includes tools to ﬁnd vulnerabilities, discover
bugs, and perform taint tracking. Recovering high-level control
constructs is essential for decompilation in order to produce
structured code that is suitable for human analysts and source-
based program analysis techniques. State-of-the-art decompilers
rely on structural analysis, a pattern-matching approach over
the control ﬂow graph,
to recover control constructs from
binary code. Whenever no match is found, they generate goto
statements and thus produce unstructured decompiled output.
Those statements are problematic because they make decompiled
code harder to understand and less suitable for program analysis.
the ﬁrst decompiler
to offer a goto-free output. DREAM uses a novel pattern-
independent control-ﬂow structuring algorithm that can recover
all control constructs in binary programs and produce structured
decompiled code without any goto statement. We also present
semantics-preserving transformations that can transform unstruc-
tured control ﬂow graphs into structured graphs. We demonstrate
the correctness of our algorithms and show that we outperform
both the leading industry and academic decompilers: Hex-Rays
and Phoenix. We use the GNU coreutils suite of utilities as a
benchmark. Apart from reducing the number of goto statements
to zero, DREAM also produced more compact code (less lines of
code) for 72.7% of decompiled functions compared to Hex-Rays
and 98.8% compared to Phoenix. We also present a comparison
of Hex-Rays and DREAM when decompiling three samples from
Cridex, ZeusP2P, and SpyEye malware families.

In this paper, we present DREAM,

I.

INTRODUCTION

Malicious software (malware) is one of the most serious
threats to the Internet security today. The level of sophistication
employed by current malware continues to evolve signiﬁcantly.
For example, modern botnets use advanced cryptography, com-
plex communication and protocols to make reverse engineering
harder. These security measures employed by malware authors
are seriously hampering the efforts by computer security
researchers and law enforcement [4, 32] to understand and
take down botnets and other types of malware. Developing

Permission to freely reproduce all or part of this paper for noncommercial
purposes is granted provided that copies bear this notice and the full citation
on the ﬁrst page. Reproduction for commercial purposes is strictly prohibited
without the prior written consent of the Internet Society, the ﬁrst-named author
(for reproduction of an entire paper only), and the author’s employer if the
paper was prepared within the scope of employment.
NDSS ’15, 8-11 February 2015, San Diego, CA, USA
Copyright 2015 Internet Society, ISBN 1-891562-38-X
http://dx.doi.org/10.14722/ndss.2015.23185

effective countermeasures and mitigation strategies requires a
thorough understanding of functionality and actions performed
by the malware. Although many automated malware analysis
techniques have been developed, security analysts often have
to resort to manual reverse engineering, which is difﬁcult and
time-consuming. Decompilers that can reliably generate high-
level code are very important tools in the ﬁght against malware:
they speed up the reverse engineering process by enabling
malware analysts to reason about the high-level form of code
instead of its low-level assembly form.

Decompilation is not only beneﬁcial for manual analy-
sis, but also enables the application of a wealth of source-
based security techniques in cases where only binary code
is available. This includes techniques to discover bugs [5],
apply taint tracking [10], or ﬁnd vulnerabilities such as RICH
[7], KINT [38], Chucky [42], Dowser [24], and the property
graph approach [41]. These techniques beneﬁt from the high-
level abstractions available in source code and therefore are
faster and more efﬁcient than their binary-based counterparts.
For example, the average runtime overhead for the source-
based taint tracking system developed by Chang et al. [10] is
0.65% for server programs and 12.93% for compute-bound
applications, whereas the overhead of Minemu, the fastest
binary-based taint tracker, is between 150% and 300% [6].

One of the essential steps in decompilation is control-ﬂow
structuring, which is a process that recovers the high-level
control constructs (e.g., if-then-else or while loops) from
the program’s control ﬂow graph (CFG) and thus plays a vital
role in creating code which is readable by humans. State-of-
the-art decompilers such as Hex-Rays [22] and Phoenix [33]
employ structural analysis [31, 34] (§II-A3) for this step. At a
high level, structural analysis is a pattern-matching approach
that tries to ﬁnd high-level control constructs by matching
regions in the CFG against a predeﬁned set of region schemas.
When no match is found, structural analysis must use goto
statements to encode the control ﬂow inside the region. As a
result, it is very common for the decompiled code to contain
many goto statements. For instance, the de facto industry stan-
dard decompiler Hex-Rays (version v2.0.0.140605) produces
1,571 goto statements for a peer-to-peer Zeus sample (MD5
hash 49305d949fd7a2ac778407ae42c4d2ba) that consists of
997 nontrivial functions (functions with more than one basic
block). The decompiled malware code consists of 49,514 lines
of code. Thus, on average it contains one goto statement for
each 32 lines of code. This high number of goto statements
makes the decompiled code less suitable for both manual
and automated program analyses. Structured code is easier to
understand [16] and helps scale program analysis [31]. The

1
2
3
4
5
6
7
8

int foo(){

int i = 0;
while(i < MAX){

print(i);
i = i + 1;

}
return i;

}

DECL

SEQ

WHILE

RETURN

int

=

<

SEQ

i

i

0

i

MAX

CALL

=

print

ARG

i

+

int i = 0

c: i < MAX

c

print(i)
i = i + 1

¬c

i

i

1

return i

(a) Exemplary code sample

(b) Abstract Syntax Tree

(c) Control Flow Graph

Fig. 1: Code representations.

collapsed to one node of corresponding type. If no match is
found, goto statements are inserted to represent the control
ﬂow. In the literature, acyclic and cyclic subgraphs for which
no match is found are called proper and improper intervals,
respectively. For instance, Figure 2 shows the progression of
structural analysis on a simple example from left to right.
In the initial (leftmost) graph nodes n1 and c2 match the
shape of a while loop. Therefore, the region is collapsed
into one node that is labeled as a while region. The new
node is then reduced with node c1 into an if-then region
and ﬁnally the resulting graph is reduced to a sequence. This
series of reductions are used to represent the control ﬂow as
if (c1){while (¬c2){n1}} ; n2

B. Problem Deﬁnition

Given a program P in CFG form, the problem of control-
ﬂow structuring is to recover high-level, structured control con-
structs such as loops, if-then and switch constructs from
the graph representation. An algorithm that solves the control-
ﬂow structuring problem is a program transformation function
fP that returns, for a program’s control ﬂow graph PCFG, a
semantically equivalent abstract syntax tree PAST. Whenever
fP cannot ﬁnd a high-level structured control construct it
will resort to using goto statements. In the context of this
paper, we denote code that does not use goto statements as
structured code. The control-ﬂow of P can be represented in
several ways, i.e., several correct ASTs may exist. In its general
form structural analysis can and usually does contain goto
statements to represent the control ﬂow. Our goal is to achieve
fully structured code, i.e., code without any goto. For this, we
restrict the solution space to structured solutions. That is, all
nodes n ∈ PAST representing control constructs must belong
to the set of structured constructs shown in Table I. The table
does not contain for loops since these are not needed at this
stage of the process. for loops are recovered during our post-
structuring optimization step to enhance readability (§VI).

C. Running Example

As an example illustrating a sample control ﬂow graph and
running throughout this paper, we consider the CFG shown in
Figure 3. In this graph, code nodes are denoted by ni where i
is an integer. Code nodes are represented in white. Condition
nodes are represented in blue and labeled with the condition
tested at that node. The example contains three regions that we

TABLE I: AST nodes that represent high-level control con-
structs

AST Node

Seq [ni]i∈1..k

Cond [c, nt, nf ]

Loop [τ, c, nb]

Switch [v,C, nd]

Description

Sequence of nodes [n1, . . . , nk] executed in
order. Sequences can also be represented as
Seq [n1, . . . , nk].
If construct with a condition c, a true branch nt
and a false branch nf . It may have only one
branch.
Loop of type τ ∈ {τwhile, τdowhile, τendless}
with continuation condition c and body nb.
Switch construct consisting of a variable v, a
list of cases C = [(V1, n1) , . . . , (Vk, nk)], and
a default node nd. Each case (Vi, ni) represents
a node ni that is executed when v ∈ Vi

use to illustrate different parts of our structuring algorithm. R1
represents a loop that contains a break statement resulting in
an exit from the middle of the loop to the successor node.
R2 is a proper interval (also called abnormal selection path).
In this region, the subgraph headed at b1 cannot be structured
as an if-then-else region due to an abnormal exit caused
by the edge (b2, n6). Similarly, the subgraph with the head
at b2 cannot be structured as if-then-else region due to
an abnormal entry caused by the edge (n4, n5). Due to this,
structural analysis represents at least one edge in this region
as a goto statement. The third region, R3, represents a loop
with an unstructured condition, i.e., it cannot be structured by
structural analysis. These three regions where chosen such that
the difﬁculty for traditional structuring algorithms increases
from R1 to R3. The right hand side of Figure 5 shows how
the structuring algorithm of Hex-Rays structures this CFG.
For comparison, the left hand side shows how the algorithms
developed over the course of this paper structure the CFG.
As can be seen for the three regions, the traditional approach
produces goto statements and thus impacts readability. Even
in this toy example a non-negligible amount of work needs to
be invested to extract the semantics of region R3. In contrast,
using our approach,
the entire region is represented by a
single while loop with a single clear and understandable
continuation condition.

3

is justiﬁed by the fact that any slice node n has a simple path
to the sink node. The path represented by dfsStack and the
currently explored edge e is simple if the target node of e is
not in dfsStack.

We extend Algorithm 1 to calculate the graph slice from
a given node to a set of sink nodes. For this purpose, we
ﬁrst create a virtual sink node nv, add edges from the sink
set to nv, compute SG (ns, nv), and ﬁnally remove nv and
its incoming edges. Figure 6 shows the computed graph slice
between nodes d1 and n9 in our running example. The slice
shows that n9 is reached from d1 if and only if the condition
(d1 ∧ ¬d3) ∨ (¬d1 ∧ ¬d2) is satisﬁed.

2) Deriving and Simplifying Conditions: After having com-
puted the slice SG (ns, ne), the reaching conditions for all slice
nodes can be computed by one traversal over the nodes in
their topological order. This guarantees that all predecessors
of a node n are handled before n. To compute the reaching
condition of node n, we need the reaching conditions of its
direct predecessors and the tags of incoming edges from these
nodes. Speciﬁcally, we compute the reaching conditions using
the formula:

(cid:95)

(cr (ns, v) ∧ τ (v, n))

cr (ns, n) =

v∈Preds(n)

where Preds (n) returns the immediate predecessors of node
n and τ (v, n) is the tag assigned to edge (v, n). Then, we
simplify the logical expressions.

B. Structuring Acyclic Regions

The key idea behind our algorithm is that any directed
acyclic graph has at least one topological ordering deﬁned by
its reverse postordering [14, p. 614]. That is, we can order
its nodes linearly such that for any directed edge (u, v), u
comes before v in the ordering. Our approach to structuring
acyclic region proceeds as follows. First, we compute reaching
conditions from the region header h to every node n in the
region. Next, we construct the initial AST as sequence of
code nodes in topological order associated with corresponding
reaching conditions,
it represents the control ﬂow in-
side the region as if (cr (h, n1)){n1} ; . . . ; if (cr (h, nk)){nk}.
the initial AST is not optimal. For example,
Obviously,
nodes with complementary conditions are represented as two
if-then constructs if (c){nt} if (¬c){nf} and not as one
if-then-else construct if (c){nt} else{nf}. Therefore, in
the second phase, we iteratively reﬁne the initial AST to ﬁnd
a concise high-level representation of control ﬂow inside the
region.

i.e.,

1) Abstract Syntax Tree Reﬁnement: We apply three re-
ﬁnement steps to AST sequence nodes. First, we check if
there exist subsets of nodes that can be represented using
if-then-else. We denote this step by condition-based re-
ﬁnement since it reasons about the logical expressions rep-
resenting nodes’ reaching conditions. Second, we search for
nodes that can be represented by switch constructs. Here,
we also look at
the checks (comparisons) represented by
each logical variable. Hence, we denote it by condition-aware
reﬁnement. Third, we additionally use the reachability relations
among nodes to represent them as cascading if-else con-
structs. The third step is called reachability-based reﬁnement.

6

At a high level, our reﬁnement steps iterate over the
children of each sequence node V and choose a subset Vc ∈ V
that satisﬁes a speciﬁc criterion. Then, we construct a new
compound AST node vc that represents control ﬂow inside
Vc and replaces it in a way that preserves the topological
order of V . That is, vc is placed after all nodes reaching it
and before all nodes reached from it. Note that we deﬁne
reachability between two AST nodes in terms of corresponding
basic blocks in the CFG, i.e., let u, v be two AST nodes, u
reaches v if u contains a basic block that reaches a basic block
contained in v.

Condition-based Reﬁnement. Here, we use the observation
that nodes belonging to the true branch of an if construct with
condition c is executed (reached) if and only if c is satisﬁed.
That is, the reaching condition of corresponding node(s) is an
AND expression of the form c∧ R. Similarly, nodes belonging
to the false branch have reaching conditions of the form ¬c∧R.
This reﬁnement step chooses a condition c and divides children
nodes into three groups:
true-branch candidates Vc, false-
branch candidates V¬c, and remaining nodes. If the true-branch
and false-branch candidates contain more than two nodes, i.e.,
|Vc| + |V¬c| ≥ 2, we create a condition node vc for c with
children {Vc, V¬c} whose conditions are replaced by terms R.
Obviously, the second term of logical AND expressions (c or
¬c) is implied by the conditional node.

The conditions that we use in this reﬁnement are chosen
as follows: we ﬁrst check for pairs of code nodes (ni, nj)
that satisfy cr (h, ni) = ¬cr (h, nj) and group according
to cr (h, ni). These conditions correspond to if-then-else
constructs, and thus are given priority. When no such pairs
can be found, we traverse all nodes in topological order
(including conditional nodes) and check if nodes can be
structured by the reaching condition of the currently visited
node. Intuitively, this traversal mimics the nesting order by
visiting the topmost nodes ﬁrst. Clustering according to the
corresponding conditions allows to structure inner nodes by
removing common factors from logical expressions. Therefore,
we iteratively repeat this step on all newly created sequence
nodes to ﬁnd further nodes with complementing conditions.

In our running example, when the algorithm structures the
acyclic region headed at node b1 (region R2), it computes
the initial AST as shown in Figure 7. Condition nodes are
represented by white nodes with up to two outgoing edges
that represent when the condition is satisﬁed (black arrowhead)
or not (white arrowhead). Sequence nodes are depicted by
blue nodes. Their children are ordered from left to right in
topological order. Leaf nodes (rectangles) are the basic blocks.
The algorithm performs a condition-based reﬁnement wrt.
condition b1 ∧ b2 since nodes n5 and n6 have complementary
conditions. This results in three clusters Vb1∧b2 = {n6},
V¬(b1∧b2) = {n5}, and Vr = {n4} and leads to creating a
condition node. At this point, no further condition-based re-
ﬁnement is possible. Cifuentes proposed a method to structure
compound conditions by deﬁning four patterns that describe
the shape of subgraphs resulting from short circuit evaluation
of compound conditions [11]. Obviously, this method fails if
no match to these patterns is found.

Condition-aware Reﬁnement. This step checks if the child
nodes, or a subset of them, can be structured as a switch

reﬁnement step. The algorithm iteratively extends the current
set of loop nodes by looking for successor nodes that have all
their immediate predecessors in the loop and are dominated
by the header node. When a successor node is identiﬁed as
loop node, its immediate successors that are not currently
loop nodes are added to the set of successor nodes. The
algorithm stops when the set of successor nodes contains
at most one node, i.e., the ﬁnal unique loop successor is
identiﬁed, or when the previous iteration did not ﬁnd new
successor nodes. If the loop still has multiple successors after
reﬁnement, we select from them the successor of the loop
node with smallest post-order as the loop ﬁnal successor. The
remaining successors are classiﬁed as abnormal exit nodes.
We then transform the region into a single-successor region
as will be described in Section V-B. For instance, when
structuring region R1 in our running example (Figure 3), the
algorithm identiﬁes the following initial loop and successor
nodes Nloop = {c1, n1, c2, n3, c3}, Nsucc = {n2, n9}. Next,
node n2 is added to the set of loop nodes since all its prede-
cessors are loop nodes. This results in a unique loop node and
the ﬁnal sets Nloop = {c1, n1, c2, n3, c3, n2}, Nsucc = {n9}.

Algorithm 2: Loop Successor Reﬁnement

Input

: Initial sets of loop nodes Nloop and successor
nodes Nsucc; loop header nh

Output: Reﬁned Nloop and Nsucc
1 Nnew ← Nsucc;
2 while |Nsucc| > 1 ∧ Nnew (cid:54)= ∅ do
Nnew ← ∅;
forall the n ∈ Nsucc do

3
4
5
6
7
8

if preds(n) ⊆ Nloop then
Nloop ← Nloop ∪ {n};
Nsucc ← Nsucc \ {n};
Nnew ← Nnew ∪
{u : u ∈ [succs(n) \ Nloop] ∧ dom(nh,u)};

end

end
Nsucc ← Nsucc ∪ Nnew

9
10
11
12 end

Phoenix [33] employs a similar approach to deﬁne loop
membership. The key difference to our approach is that
Phoenix assumes that the loop successor is either the imme-
diate successor of the header or latching node. For example,
in case of endless loops with multiple break statements or
loops with unstructured continuation condition (e.g., region
R3), the simple assumption that loop successor is directly
reached from loop header or latching nodes fails. In these
cases Phoenix generates an endless loop and represents exits
using goto statements. In contrast, our successor reﬁnement
technique described above does not suffer from this problem
and generates structured code without needing to use goto
statements.

3) Loop Type and Condition: In order to identify loop type
and condition, we ﬁrst represent each edge to the successor
node as a break statement and compute the AST of the
loop body after reﬁnement nb. Note that the loop body is
an acyclic region that we structure as explained in §IV-B.
Next, we represent the loop as endless loop with the computed

8

while (1)
if (c1)
n1
else
. . .
if (¬c3)
break

CONDTOSEQ→

while (1)

while (c1)

n1
. . .
if (¬c3)
break

DOWHILE→

do

while (c1)

n1

. . .

while (c3)

Fig. 9: Example of loop type inference of region R1.

body’s AST, i.e., n(cid:96) = Loop [τendless,−, nb]. Our assumption is
justiﬁed since all exits from the loop are represented by break
statements. Finally, we infer the loop type and continuation
condition by reasoning about the structure of loop n(cid:96).
Inference rules. We specify loop structuring rules as inference
rules of the form:

P1 P2

. . . Pn

C

The top of the inference rule bar contains the premises
P1, P2, . . . , Pn. If all premises are satisﬁed,
then we can
conclude the statement below the bar C. Figure 8 presents our
loop structuring rules. The ﬁrst premise in our rules describes
the input loop structure, i.e., loop type and body structure.
The remaining premises describe additional properties of loop
body. The conclusion is described as a transformation rule of

the form n (cid:59) ´n. Inference rules provide a formal compact
notation for single-step inference and implicitly specify an
inference algorithm by recursively applying rules on premises
until a ﬁxed point is reached. We denote by Br a break
statement, and by Bc
r a condition node that represents the
r = Cond [c, Seq [Br] ,−]. We
statement if (c){break}, i.e., Bc
represent by n ⇓ Br the fact that a break statement is attached
to each exit from the control construct represented by node n.

The operator(cid:80) returns the list of statements in a given node.

In our running example, computing the initial loop structure
for region R1 results in the ﬁrst (leftmost) code in Figure 9.
The loop body consists of an if statement with break state-
ments only in its false branch. This matches the CONDTOSEQ
rule, which transforms the loop body into a sequence of a
while loop and the false branch of the if statement. The rule
states that in this case the true branch of the if statement (n1)
is continuously executed as long as the condition c1 is satisﬁed.
Then, control ﬂows to the false branch. This is repeated until
the execution reaches a break statement. The resulting loop
body is a sequence that ends with a conditional break B¬c3
that matches the DOWHILE rule. The second transformation
results in the third (rightmost) loop structure. At this point the
inference algorithm reaches a ﬁxed point and terminates.

r

To give an intuition of the unstructured code produced by
structural analysis when a region in the CFG does not match its
predeﬁned region schemas, we consider the region R3 in our
running example. Computing the body’s AST of the loop in re-
gion R3 and assuming an endless loop results in the loop repre-
sented as while (1){if ((¬d1 ∧ ¬d2) ∨ (d1 ∧ ¬d3)){break;} . . .}.
The loop’s body starts with a conditional break and
hence is
structured according to the WHILE rule into
while ((d1 ∧ d3) ∨ (¬d1 ∧ d2)){. . .}. We wrote a small function
that produces the same CFG as the region R3 and decompiled
it with DREAM and Hex-Rays. Figure 11 shows that our

nk = Bc

r

DOWHILE

NESTEDDOWHILE

(cid:104)

n(cid:96) = Loop

n(cid:96) (cid:59) Loop

n(cid:96) = Loop

n1 = Bc

τendless,−, Seq [ni]i∈1..k(cid:105)

τendless,−, Seq [ni]i∈1..k(cid:105)
(cid:104)
(cid:104)
τdowhile,¬c, Seq [ni]i∈1..k−1(cid:105)
(cid:104)
τwhile,¬c, Seq [ni]i∈2..k(cid:105)
τendless,−, Seq [ni]i∈1..k(cid:105) ∀i ∈ 1..k − 1 : Br /∈(cid:80) [ni] nk = Cond [c, nt,−]
(cid:104)
n(cid:96) (cid:59) Loop
τdowhile,¬c, Seq [ni]i∈1..k−1(cid:105)
n(cid:96) (cid:59) Loop

τendless,−, Seq

n(cid:96) = Loop

(cid:21)(cid:35)

WHILE

(cid:34)

Loop

, nt

r

(cid:104)

(cid:105)

n(cid:96) = Loop

(cid:104)
(cid:104)

nk = ´nk ⇓ Br

(cid:20)
τendless,−, Seq [ni]i∈1..k(cid:105)
n(cid:96) (cid:59) Seq
(cid:3)(cid:105)
(cid:104)
τendless,−, Seq(cid:2)Loop [τwhile, c, nt] , nf
(cid:3)(cid:105)
τendless,−, Seq(cid:2)Loop [τwhile,¬c, nf ] , nt

(cid:104)
(cid:104)
n(cid:96) (cid:59) Loop
(cid:104)
τendless,−, Cond [c, nt, nf ]
n(cid:96) (cid:59) Loop
Fig. 8: Loop structuring rules. The input to the rules is a loop node n(cid:96).

(cid:105) Br /∈(cid:80) [nt] Br ∈(cid:80) [nf ]
(cid:105) Br ∈(cid:80) [nt] Br /∈(cid:80) [nf ]

τendless,−, Cond [c, nt, nf ]

n1, . . . , nk−1, ´nk

LOOPTOSEQ

CONDTOSEQNEG

CONDTOSEQ

n(cid:96) = Loop

n(cid:96) = Loop

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

1
2
3
4
5
6
7
8
9
10

signed int __cdecl loop(signed int a1)
{

signed int v2; // [sp+1Ch] [bp-Ch]@1

v2 = 0;
while ( a1 > 1 ){

if ( v2 > 10 )

goto LABEL_7;

LABEL_6:

printf("inside_loop");
++v2;
--a1;

}
if ( v2 <= 100 )

goto LABEL_6;

LABEL_7:

printf("loop_terminated");
return v2;

}

Fig. 10: Decompiled code generated by Hex-Rays.

int loop(int a){

int b = 0;
while((a <= 1 && b <= 100)||(a > 1 && b <= 10)){

printf("inside_loop");
++b;
--a;

}
printf("loop_terminated");
return b;

}

Fig. 11: Decompiled code generated by DREAM.

D. Side Effects

Our structuring algorithm may result in the same condition
appearing multiple times in the computed AST. For example,
structuring region R2 in the running example leads to the AST
shown in Figure 7 where condition b1 is tested twice. If the
variables tested by condition b1 are modiﬁed in block n4,
the second check of b1 in the AST would not be the same
as the ﬁrst check. As a result, the code represented by the
computed AST would not be semantically equivalent to the
CFG representation.

To guarantee the semantics-preserving property of our
algorithm, we ﬁrst check if any condition is used multiple
times in the computed AST. If this is the case, we check if any
of the variables used in the test is changed on an execution
path between any two uses. This includes if the variable is
assigned a new value, used in a call expression, or used in
reference expression (its address is read). If a possible change
is detected, we insert a Boolean variable to store the initial
value of the condition. All subsequent uses of the condition
are replaced by the inserted Boolean variable.

E. Summary

In this section, we have discussed our approach to creating
an AST for single-entry and single-successor CFG regions.
The above algorithm can structure every CFG except cyclic
regions with multiple entries and/or multiple successors. The
following section discusses how we handle these problematic
regions.

V. SEMANTICS-PRESERVING CONTROL-FLOW

TRANSFORMATIONS

approach correctly found the loop type and continuation con-
dition. In comparison, Hex-Rays produced unstructured code
with two goto statements as shown in Figure 10; one goto
statement jumps outside the loop and the other one jumps back
in the loop.

In this section, we describe our method to transform
cyclic regions into semantically equivalent single-entry single-
successor regions. As the only type of regions that cannot be
structured by our pattern-independent structuring algorithm are
cyclic regions with multiple entries or multiple successors, we

9

We implement several transformations that ﬁnd simpler
forms for certain control constructs. For instance, we trans-
form if statements that assign different values to the same
variable into a ternary operator. That
is, code such as
if (c){x = vt} else{x = vf} is transformed into the equivalent
form x = c ? vt : vf . Also, we identify while loops that can
be represented as for loops. for loop candidates are while
loops that have a variable x used both in their continuation
condition and the last statement in their body. We then check
if a single deﬁnition of x reaches the loop entry that is only
used in the loop body. We transform the loop into a for loop
if the variables used in the deﬁnition are not used on the way
from the deﬁnition to the loop entry. These checks allow us
to identify for loops even if their initialization statements are
not directly before the loop.

Several functions such as strcpy, strlen, or strcmp
are often inlined by compilers. That is, a function call is
replaced by the called function body. Having several duplicates
of the same function results in larger code and is detrimental
to manual analysis. DREAM recognizes and outlines several
functions. That is, it replaces the corresponding code by the
equivalent function call.

For the third optimization, we leverage API calls to assign
meaningful names to variables. API functions have known
signatures including the types and names of parameters. If
a variable is used in an API call, we give it the name of
corresponding parameter if that name is not already used.

VII. EVALUATION

In this section, we describe the results of the experiments
we have performed to evaluate DREAM. We base our evalua-
tion on the technique used to evaluate Phoenix by Schwartz et
al. [33]. This evaluation used the GNU coreutils to evaluate
the quality of the decompilation results. We compared our
results with Phoenix [33] and Hex-Rays [22]. We included
Hex-Rays because it is the leading commercial decompiler and
the de facto industry standard. We tested the latest version of
Hex-Rays at the time of writing, which is v2.0.0.140605. We
picked Phoenix because it is the most recent and advanced
academic decompiler. We did not include dcc [11], DISC [28],
REC [1], and Boomerang [17] in our evaluation. The reason
is that these projects are either no longer actively maintained
(e.g., Boomerang) or do not support x86 (e.g., dcc). However,
most
they are outperformed by Phoenix. The
implementation of Phoenix is not publicly available yet. How-
ever, the authors kindly agreed to share both the coreutils
binaries used in their experiments and the raw decompiled
source code produced by Phoenix to enable us to compute our
metrics and compare our results with theirs. We very much
appreciate this good scientiﬁc practice. This way, we could
ensure that all three decompilers are tested on the same binary
code base. We also had the raw source code produced by all
three decompilers as well, so we can compare them fairly. In
addition to the GNU coreutils benchmark we also evaluated
our approach using real-world malware samples. Speciﬁcally,
we decompiled and analyzed ZeusP2P, SpyEye, Cridex. For
this part of our evaluation we could only compare our approach
to Hex-Rays since Phoenix is not yet released.

importantly,

A. Metrics

We evaluate our approach with respect to the following

quantitative metrics.

• Correctness. Correctness measures the functional equiv-
alence between the decompiled output and the input code.
More speciﬁcally, two functions are semantically equiv-
alent if they follow the same behavior and produce the
same results when they are executed using the same set
of parameters. Correctness is a crucial criterion to ensure
that the decompiled output is a faithful representation of
the corresponding binary code.
• Structuredness. Structuredness measures the ability of
a decompiler to recover high-level control ﬂow structure
and produce structured decompiled code. Structuredness
is measured by the number of generated goto statements
in the output. Structured code is easier to understand [16]
and helps scale program analysis [31]. For this reason, it is
desired to have as few goto statements in the decompiled
code as possible. These statements indicate the failure to
ﬁnd a better representation of control ﬂow.
• Compactness. For compactness we perform two mea-
surements: ﬁrst, we measure the total lines of code gen-
erated by each decompiler. This gives a global picture on
the compactness of decompiled output. Second, we count
for how many functions each decompiler generated the
fewest lines of code compared to the others. If multiple
decompilers generate the same (minimal) number of lines
of code, that is counted towards the total of each of them.

B. Experiment Setup & Results

To evaluate our algorithm on the mentioned metrics, we

conducted two experiments.

1) Correctness Experiment: We evaluated the correctness
of our algorithm on the GNU coreutils 8.22 suite of
utilities. coreutils consist of a collection of mature pro-
grams and come with a suite of high-coverage tests. We
followed a similar approach to that proposed in [33] where
the coreutils tests were used to measure correctness. Also,
since the coreutils source code contains goto statements,
this means that both parts of our algorithm are invoked;
the pattern-independent structuring part and the semantics-
preserving transformations part. Our goal is to evaluate the
control-ﬂow structuring component. For this, we computed
the CFG for each function in the coreutils source code
and provided it as input to our algorithm. Then, we replaced
the original functions with the generated algorithm output,
compiled the restructured coreutils source code, and ﬁnally
executed the tests. We used joern [41] to compute the CFGs.
Joern is a state-of-the-art platform for analysis of C/C++ code.
It generates code property graphs, a novel graph representation
of code that combines three classic code representations;
ASTs, CFGs, and Program Dependence Graphs (PDG). Code
property graphs are stored in a Neo4J graph database. More-
over, a thin python interface for joern and a set of useful
utility traversals are provided to ease interfacing with the graph
database. We iterated over all parsed functions in the database
and extracted the CFGs. We then transformed statements in
the CFG nodes into DREAM’s intermediate representation. The
extracted graph representation was then provided to our struc-
turing algorithm. Under the assumption of correct parsing, we

11

Considered Functions F

Functions after preprocessor
Functions correctly parsed by joern
Functions passed tests after structuring

|F|
1,738
1,530
1,530

Number of gotos

219
129
0

TABLE II: Correctness results.

can attribute the failure of any test on the restructured functions
to the structuring algorithm. To make the evaluation tougher,
we used the source ﬁles produced by the C-preprocessor, since
depending on the operating system and installed software,
some functions or parts of functions may be removed by the
preprocessor before passing them to the compiler. That in turn
would lead to potential structuring errors to go unnoticed if
the corresponding function is removed by the preprocessor.
We got the preprocessed ﬁles by passing the --save-temps
to CFLAGS in the conﬁgure script. The preprocessed source
code contains 219 goto statements.

2) Correctness Results: Table II shows statistics about the
functions included in our correctness experiments. The pre-
processed coreutils source code contains 1,738 functions.
We encountered parsing errors for 208 functions. We excluded
these functions from our tests. The 1,530 correctly parsed
functions were fed to our structuring algorithm. Next, we
replaced the original functions in coreutils by the structured
code produced by our algorithm. The new version of the
source code passed all coreutils tests. This shows that
our algorithm correctly recovered control-ﬂow abstractions
from the input CFGs. More importantly, goto statements in
the original source code are transformed into semantically
equivalent structured forms.

The original Phoenix evaluation shows that their control-
ﬂow structuring algorithm is correct. Thus, both tools correctly
structure the input CFG.

3) Structuredness and Compactness Experiment: We tested
and compareed DREAM to Phoenix and Hex-Rays. In this
experiment we used the same GNU coreutils 8.17 binaries
used in Phoenix evaluation. Structuredness is measured by the
number of goto statements in code. These statements indicate
that the structuring algorithm was unable to ﬁnd a structured
representation of the control ﬂow. Therefore, structuredness is
inversely proportional to the number of goto statements in
the decompiled output. To measure compactness, we followed
a straightforward approach. We used David A. Wheeler’s
SLOCCount utility to measure the lines of code in each
decompiled function. To ensure fair comparison, the Phoenix
evaluation only considered functions that were decompiled
by both Phoenix and Hex-Rays. We extend this principle to
only consider functions that were decompiled by all the three
decompilers. If this was not done, a decompiler that failed to
decompile functions would have an unfair advantage. Beyond
that, we extend the evaluation performed by Schwartz et al.
[33] in several ways.

• Duplicate functions. In the original Phoenix evaluation
all functions were considered, i.e., including duplicate
functions. It is common to have duplicate functions as

the result of the same library function being statically
linked to several binaries, i.e., its code is copied into
the binary. Depending on the duplicate functions this can
skew the results. Thus, we wrote a small IDAPython script
that extracts the assembly listings of all functions and
then computed the SHA-512 hash for the resulting ﬁles.
We found that of the 14,747 functions contained in the
coreutils binaries, only 3,141 functions are unique, i.e.,
78.7% of the functions are duplicates. For better com-
parability, we report the results both on the ﬁltered and
unﬁltered function lists. However, for future comparisons
we would argue that ﬁltering duplicate functions before
comparison avoids skewing the results based on the same
code being included multiple times.
• Also in the original Phoenix evaluation only recompilable
functions were considered in the goto test. In the context
of coreutils, this meant that only 39% of the unique
functions decompiled by Phoenix were considered in the
goto experiment. We extend these tests to consider the
intersection of all functions produced by the decompilers,
since even non-recompilable functions are valuable and
important to look at, especially for malware and security
analysis. For instance, the property graph approach [41]
to ﬁnd vulnerabilities in source code does not assume that
the input source code is compilable. Also, understanding
the functionality of a sample is the main goal of manual
malware analysis. Hence, the quality of all decompiled
code is highly relevant and thus included in our evalua-
tion. For completeness, we also present the results based
on the functions used in the original evaluation done by
Schwartz et al.

4) Structuredness & Compactness Results: Table III sum-
marizes the results of our second experiment. For the sake of
completeness, we report our results in two settings. First, we
consider all functions without ﬁltering duplicates as was done
in the original Phoenix evaluation. We report our results for the
functions considered in the original Phoenix evaluation (i.e.,
only recompilable functions) (T1) and for the intersection of
all functions decompiled by the three decompilers (T2). In the
second setting we only consider unique functions and again
report the results only for the functions used in the original
Phoenix study (T3) and for all functions (T4). In the table |F|
denotes the number of functions considered. The following
three columns report on the metrics deﬁned above. First, the
number of goto statements in the functions is presented. This
is the main contribution of our paper. While both state-of-
the-art decompilers produced thousands of goto statements
for the full list of functions, DREAM produced none. We
believe this is a major step forward for decompilation. Next,
we present total lines of code generated by each decompiler in
the four settings. DREAM generated more compact code overall
than Phoenix and Hex-Rays. When considering all unique
functions, DREAM’s decompiled output consists of 107k lines
of code in comparison to 164k LoC in Phoenix output and
135k LoC produced by Hex-Rays. Finally, the percentage of
functions for which a given decompiler generated the most
compact function is depicted. In the most relevant test setting
T4, DREAM produced the minimum lines of code for 75.2% of
the functions. For 31.3% of the functions, Hex-Rays generated
the most compact code. Phoenix achieved the best compactness
in 0.7% of the cases. Note that the three percentages exceed

12

Considered Functions F

p ∩ F r

h

h

p ∩ F r

coreutils functions with duplicates
T1 : F r
T2 : Fd ∩ Fp ∩ Fh
coreutils functions without duplicates
T3 : F r
T4 : Fd ∩ Fp ∩ Fh
Malware Samples
ZeusP2P
SpyEye
Cridex

|F|

8,676
10,983

785
1,821

1,021
442
167

Number of goto Statements

Lines of Code

Compact Functions

DREAM Phoenix Hex-Rays

DREAM Phoenix Hex-Rays

DREAM Phoenix Hex-Rays

0
0

0
0

0
0
0

40

4,505

31

4,231

N/A
N/A
N/A

47

3,166

28

2,949

1,571
446
144

93k
196k

15k
107k

42k
24k
7k

243k
422k

30k
164k

N/A
N/A
N/A

120k
264k

18k
135k

53k
28k
9k

81.3%
81%

74.9%
75.2%

82.9%
69.9%
84.8%

0.3%
0.2%

1.1%
0.7%

N/A
N/A
N/A

32.1%
30.4%

36.2%
31.3%

14.5%
25.7%
12.3%

TABLE III: Structuredness and compactness results. For the coreutiles benchmark, we denote by Fx the set of functions
decompiled by compiler x. F r
x is the set of recompilable functions decompiled by compiler x. d represents DREAM, p represents
Phoenix, and h represents Hex-Rays.

100% due to the fact that multiple decompilers could generate
the same minimal number of lines of code. In a one on one
comparison between DREAM and Phoenix, DREAM scored
98.8% for the compactness of the decompiled functions. In
a one on one comparison with Hex-Rays, DREAM produced
more compact code for 72.7% of decompiled functions.

5) Malware Analysis: For our malware analysis, we picked
three malware samples from three families: ZeusP2P, Cridex,
and SpyEye. The results for the malware samples shown
in Table III are similarly clear. DREAM produces goto-free
and compact code. As can be seen in the Zeus sample,
Hex-Rays produces 1,571 goto statements. These statements
make analyzing these pieces of malware very time-consuming
and difﬁcult. While further studies are needed to evaluate if
compactness is always an advantage, the total elimination of
goto statements from the decompiled code is a major step
forward and has already been of great beneﬁt to us in our
work analyzing malware samples.

Due to space constraints, we cannot present a com-
the decompiled malware source code in this
parison of
this reason, we have created a supplemen-
paper. For
tal document which can be accessed under
the follow-
ing URL: https://net.cs.uni-bonn.de/ﬁleadmin/ag/martini/Staff/
yakdan/code_snippets_ndss_2015.pdf. Here we present listings
of selected malware functions so that the reader can get a
personal impression on the readability improvements offered
by DREAM compared to Hex-Rays.

VIII. RELATED WORK

There has been much work done in the ﬁeld of de-
compilation and abstraction recovery from binary code. In
this section, we review related work and place DREAM in
the context of existing approaches. We start by reviewing
control-ﬂow structuring algorithms. Next, we discuss work in
decompilation, binary code extraction and analysis. Finally,
techniques to recover type abstractions from binary code are
discussed.

Control-ﬂow structuring. There exist two main approaches
used by modern decompilers to recover control-ﬂow structure

from the CFG representation, namely interval analysis and
structural analysis. Originally, these techniques were devel-
oped to assist data ﬂow analysis in optimizing compilers.
Interval analysis [3, 13] deconstructs the CFG into nested
regions called intervals. The nesting structure of these regions
helps to speed up data-ﬂow analysis. Structural analysis [34] is
a reﬁned form of interval analysis that is developed to enable
the syntax-directed method of data-ﬂow analysis designed for
ASTs to be applicable on low-level intermediate code. These
algorithms are also used in the context of decompilation to
recover high-level control constructs from the CFG.

to vanilla structural analysis. The goal

Prior work on control-ﬂow structuring proposed several
enhancement
is to
recover more control structure and minimize the number
of goto statements in the decompiled code. Engel et. al.
[18] extended structural analysis to handle C-speciﬁc control
statements. They proposed a Single Entry Single Successor
(SESS) analysis as an extension to structural analysis to handle
the case of statements that exist before break and continue
statements in the loop body.

These approaches share a common property; they rely on a
predeﬁned set of region patterns to structure the CFG. For this
reason, they cannot structure arbitrary graphs without using
goto statements. Our approach is fundamentally different in
that it does not rely on any patterns.

Another related line of research lies in the area of elimi-
nating goto statements at the source code level such as [19]
and [39]. These approaches deﬁne transformations at the AST
level to replace goto statements by equivalent constructs. In
some cases, several transformations are necessary to remove a
single goto statement. These approaches increase the code
size and miss opportunities to ﬁnd more concise forms to
represent the control-ﬂow. Moreover, they may insert unneces-
sary Boolean variables. For example, these approaches cannot
ﬁnd the concise form found by DREAM for region R3 in our
running example. These algorithms do not solve the control-
ﬂow structuring problem as deﬁned in Section II-B.
Decompilers. Cifuentes laid the foundations for modern de-
compilers. In her PhD thesis [11], she presented several tech-
niques to decompile binary code into a high-level language.

13

These techniques were implemented in dcc, a decompiler for
Intel 80286/DOS to C. The structuring algorithm in dcc [12]
is based on interval analysis. She also presented four region
patterns to structure regions resulted from the short-circuit
evaluation of compound conditional expressions, e.g., x ∨ y.
Van Emmerik proposed to use the Static Single Assignment
(SSA) form for decompilation in his PhD thesis [17]. His
work demonstrates the advantages of the SSA form for several
data ﬂow components of decompilers, such as expression
propagation, identifying function signatures, and eliminating
dead code. His approach is implemented in Boomerang, an
open-source decompiler. Boomerang’s structuring algorithm is
based on parenthesis theory [35]. Although faster than interval
analysis, it recovers less structure.

Chang et. el. [9] demonstrated the possibility of applying
source-level tools to assembly code using decompilation. For
this goal, they proposed a modular decompilation architecture.
Their architecture consists of a series of decompilers connected
by intermediate languages. For their applications, no control-
ﬂow structuring is performed.

Hex-Rays is the de facto industry standard decompiler. It
is built as plugin for the Interactive Disassembler Pro (IDA).
Hex-Rays is closed source, and thus little is known about its
inner workings. It uses structural analysis [22]. As noted by
Schwartz et el. in [33], Hex-Rays seems to use an improved
version of vanilla structural analyses.

Yakdan et al. [40] developed REcompile, a decompiler
that employs interval analysis to recover control structure. The
authors also proposed node splitting to reduce the number of
goto statements. Here, nodes are split into several copies.
While this reduces the amount of goto statements, it increases
the size of decompiled output.

Phoenix is the state-of-the-art academic decompiler [33].
It
is built on top of the CMU Binary Analysis Platform
(BAP) [8]. BAP lifts sequential x86 assembly instructions
into an intermediate language called BIL. It also uses TIE
[29] to recover types from binary code. Phoenix enhances
structural analysis by employing two techniques: ﬁrst, iterative
reﬁnement chooses an edge and represents it using a goto
statement when the algorithm cannot make further progress.
This allows the algorithm to ﬁnd more structure. Second,
semantics-preserving ensures correct control structure recov-
ery. The authors proposed correctness as an important metric
to measure the performance of a decompiler.

The key property that all structuring algorithms presented
above share is the reliance on pattern matching, i.e, they use
a predeﬁned set of region schemas that are matched against
regions in the CFG. This is a key issue that prevents these
algorithms from structuring arbitrary CFGs. This leads to
unstructured decompiled output with goto statements. Our
algorithm does not rely on such patterns and is therefore able to
produce well-structured code without a single goto statement.

Binary code extraction. Correctly extracting binary code
is essential for correct decompilation. Research in this ﬁeld
is indispensable for decompilation. Kruegel et al. presented
a method [27] to disassemble x86 obfuscated code. Jakstab
[26] is a static analysis framework for binaries that follows
the paradigm of iterative disassembly. That is, it interleaves

multiple disassembly rounds with data-ﬂow analysis to achieve
accurate and complete CFG extraction. Zeng et el. presented
trace-oriented programming (TOP) [43] to reconstruct pro-
gram source code from execution traces. The executed instruc-
tions are translated into a high-level program representation
using C with templates and inlined assembly. TOP relies on
dynamic analysis and is therefore able to cope with obfuscated
binaries. With the goal of achieving high coverage, an ofﬂine
combination component combines multiple runs of the binary.
BitBlaze [37] is a binary analysis platform. The CMU Binary
Analysis Platform (BAP) [8] is successor to the binary analysis
techniques developed for Vine in the BitBlaze project.

Type recovery. Reconstructing type abstractions from binary
code is important for decompilation to produce correct and
high-quality code. This includes both elementary and complex
types. Several prominent approaches have been developed
in this ﬁeld including Howard [36], REWARDS [30], TIE
[29], and [23]. Other work [15, 20, 21, 25] focused on C++
speciﬁc issues, such as recovering C++ objects, reconstructing
class hierarchy, and resolving indirect calls resulting from
virtual inheritance. Since our work focuses on the control ﬂow
structuring we do not make a contribution to type recovery but
we based our type recovery on TIE [29].

IX. CONCLUSION

In this paper we presented the ﬁrst control-ﬂow struc-
turing algorithm that
is capable of recovering all control
structure and thus does not generate any goto statements. Our
novel algorithm combines two techniques: pattern-independent
structuring and semantics-preserving transformations. The key
property of our approach is that
it does not rely on any
patterns (region schemas). We implemented these techniques
in our DREAM decompiler and evaluated the correctness
of our control-ﬂow structuring algorithm. We also evaluated
our approach against the de facto industry standard decom-
piler, Hex-Rays, and the state-of-the-art academic decompiler,
Phoenix. Our evaluation shows that DREAM outperforms both
decompilers; it produced more compact code and recovered
the control structure of all the functions in the test without any
goto statements. We also decompiled and analyzed a number
of real-world malware samples and compared the results to
Hex-Rays. Again, DREAM performed very well, producing
goto-free and compact code compared to Hex-Rays, which
had one goto for every 32 lines of code. This represents
a signiﬁcant step forward for decompilation and malware
analysis. In future work, we will further examine the quality
of the code produced by DREAM speciﬁcally concerning the
compactness. Our experience based on the malware samples
we analyzed during the course of this paper suggests that more
compact code is better for human understanding. However, it
is conceivable that in some cases less compact code is easier
to understand. This will require further research and potential
optimization of the post-processing step.

ACKNOWLEDGEMENTS

We are grateful to Fabian Yamaguchi, the author of joern,
who was very helpful and created several patches to improve
the parsing of the coreutils. We sincerely thank Edward
J. Schwartz for sharing the Phoenix experiments results. We

14

would also like to thank the anonymous reviewers for their
valuable feedback.

REFERENCES

[1] REC Studio 4 - Reverse Engineering Compiler. http://www.backerstreet.

com/rec/rec.htm. Page checked 7/20/2014.

[2] The IDA Pro disassembler and debuger. http://www.hex-rays.com/

idapro/.

[3] F. E. Allen, “Control Flow Analysis,” in Proceedings of ACM Sympo-

sium on Compiler Optimization, 1970.

[4] D. Andriesse, C. Rossow, B. Stone-Gross, D. Plohmann, and H. Bos,
“Highly Resilient Peer-to-Peer Botnets Are Here: An Analysis of
Gameover Zeus,” in Proceedings of the 8th IEEE International Confer-
ence on Malicious and Unwanted Software (MALWARE), 2013.

[5] A. Bessey, K. Block, B. Chelf, A. Chou, B. Fulton, S. Hallem, C. Henri-
Gros, A. Kamsky, S. McPeak, and D. Engler, “A Few Billion Lines of
Code Later: Using Static Analysis to Find Bugs in the Real World,”
Communications of the ACM, vol. 53, no. 2, pp. 66–75, Feb. 2010.

[6] E. Bosman, A. Slowinska, and H. Bos, “Minemu: The World’s Fastest
Taint Tracker,” in Proceedings of the 14th International Conference on
Recent Advances in Intrusion Detection (RAID), 2011.

[7] D. Brumley, T. Chiueh, R. Johnson, H. Lin, and D. Song, “RICH:
Automatically Protecting Against Integer-Based Vulnerabilities,” in
Proceedings of
the 14th Network and Distributed System Security
Symposium (NDSS), 2007.

[8] D. Brumley, I. Jager, T. Avgerinos, and E. J. Schwartz, “BAP: A Binary
Analysis Platform,” in Proceedings of the 23rd International Conference
on Computer Aided Veriﬁcation (CAV), 2011.

[9] B.-Y. E. Chang, M. Harren, and G. C. Necula, “Analysis of Low-
level Code Using Cooperating Decompilers,” in Proceedings of the 13th
International Conference on Static Analysis (SAS), 2006.

[10] W. Chang, B. Streiff, and C. Lin, “Efﬁcient and Extensible Security
Enforcement Using Dynamic Data Flow Analysis,” in Proceedings of
the 15th ACM Conference on Computer and Communications Security
(CCS), 2008.

[11] C. Cifuentes, “Reverse Compilation Techniques,” Ph.D. dissertation,

Queensland University of Technology, 1994.

[12] ——, “Structuring Decompiled Graphs,” in Proceedings of the 6th

International Conference on Compiler Construction (CC), 1996.
J. Cocke, “Global Common Subexpression Elimination,” in Proceedings
of the ACM Symposium on Compiler Optimization, 1970.

[13]

[14] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction

to Algorithms, 3rd ed. The MIT Press, 2009.

[15] D. Dewey and J. T. Gifﬁn, “Static detection of C++ vtable escape
vulnerabilities in binary code,” in Proceedings of the 19th Network and
Distributed System Security Symposium (NDSS), 2012.

[16] E. W. Dijkstra, “Letters to the Editor: Go to Statement Considered
Harmful,” Communications of the ACM, vol. 11, no. 3, pp. 147–148,
Mar. 1968.

[17] M. J. V. Emmerik, “Static Single Assignment for Decompilation,” Ph.D.

dissertation, University of Queensland, 2007.

[18] F. Engel, R. Leupers, G. Ascheid, M. Ferger, and M. Beemster,
“Enhanced Structural Analysis for C Code Reconstruction from IR
Code,” in Proceedings of the 14th International Workshop on Software
and Compilers for Embedded Systems (SCOPES), 2011.

[19] A. Erosa and L. J. Hendren, “Taming Control Flow: A Structured
Approach to Eliminating Goto Statements,” in Proceedings of 1994
IEEE International Conference on Computer Languages, 1994.

[20] A. Fokin, E. Derevenetc, A. Chernov, and K. Troshina, “SmartDec:
Approaching C++ Decompilation,” in Proceedings of the 2011 18th
Working Conference on Reverse Engineering (WCRE), 2011.

[21] A. Fokin, K. Troshina, and A. Chernov, “Reconstruction of Class
Hierarchies for Decompilation of C++ Programs,” in Proceedings of the
14th European Conference on Software Maintenance and Reengineering
(CSMR), 2010.
I. Guilfanov, “Decompilers and Beyond,” in Black Hat, USA, 2008.

[22]

15

[23]

[24]

I. Haller, A. Slowinska, and H. Bos, “MemPick: High-Level Data
Structure Detection in C/C++ Binaries,” in Proceedings of the 20th
Working Conference on Reverse Engineering (WCRE), 2013.
I. Haller, A. Slowinska, M. Neugschwandtner, and H. Bos, “Dowsing
for Overﬂows: A Guided Fuzzer to Find Buffer Boundary Violations,”
in Proceedings of the 22nd USENIX Security Symposium, 2013.

[25] W. Jin, C. Cohen, J. Gennari, C. Hines, S. Chaki, A. Gurﬁnkel,
J. Havrilla, and P. Narasimhan, “Recovering C++ Objects From Binaries
Using Inter-Procedural Data-Flow Analysis,” in Proceedings of ACM
SIGPLAN on Program Protection and Reverse Engineering Workshop
(PPREW), 2014.
J. Kinder and H. Veith, “Jakstab: A Static Analysis Platform for
Binaries,” in Proceedings of
the 20th International Conference on
Computer Aided Veriﬁcation (CAV), 2008.

[26]

[27] C. Kruegel, W. Robertson, F. Valeur, and G. Vigna, “Static Disassembly
of Obfuscated Binaries,” in Proceedings of the 13th Conference on
USENIX Security Symposium, 2004.

[28] S. Kumar. DISC: Decompiler for TurboC. http://www.debugmode.com/

[29]

dcompile/disc.htm. Page checked 7/20/2014.
J. Lee, T. Avgerinos, and D. Brumley, “TIE: Principled Reverse En-
gineering of Types in Binary Programs,” in Proceedings of the 18th
Network and Distributed System Security Symposium (NDSS), 2011.

[30] Z. Lin, X. Zhang, and D. Xu, “Automatic Reverse Engineering of Data
Structures from Binary Execution,” in Proceedings of the 17th Annual
Network and Distributed System Security Symposium (NDSS), 2010.

[31] S. S. Muchnick, Advanced Compiler Design and Implementation. San

Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1997.

[32] C. Rossow, D. Andriesse, T. Werner, B. Stone-Gross, D. Plohmann,
C. J. Dietrich, and H. Bos, “P2PWNED: Modeling and Evaluating the
Resilience of Peer-to-Peer Botnets,” in Proceedings of the 34th IEEE
Symposium on Security and Privacy (S&P), 2013.

[33] E. J. Schwartz, J. Lee, M. Woo, and D. Brumley, “Native x86 Decom-
pilation using Semantics-Preserving Structural Analysis and Iterative
Control-Flow Structuring,” in Proceedings of the 22nd USENIX Security
Symposium, 2013.

[34] M. Sharir, “Structural Analysis: A New Approach to Flow Analysis
in Optimizing Compilers,” Computer Languages, vol. 5, no. 3-4, pp.
141–153, Jan. 1980.

[35] D. Simon, “Structuring Assembly Programs,” Honours thesis, Univer-

sity of Queensland, 1997.

[36] A. Slowinska, T. Stancescu, and H. Bos, “Howard: A Dynamic Excava-
tor for Reverse Engineering Data Structures,” in Proceedings of the 18th
Annual Network and Distributed System Security Symposium (NDSS),
2011.

[37] D. Song, D. Brumley, H. Yin, J. Caballero, I. Jager, M. G. Kang,
Z. Liang, J. Newsome, P. Poosankam, and P. Saxena, “BitBlaze: A New
Approach to Computer Security via Binary Analysis,” in Proceedings
of the 4th International Conference on Information Systems Security
(ICISS), 2008.

[38] X. Wang, H. Chen, Z. Jia, N. Zeldovich, and M. F. Kaashoek, “Improv-
ing Integer Security for Systems with KINT,” in Proceedings of the 10th
USENIX Conference on Operating Systems Design and Implementation
(OSDI), 2012.

[39] M. H. Williams and G. Chen, “Restructuring Pascal Programs Contain-

ing Goto Statements,” The Computer Journal, 1985.

[40] K. Yakdan, S. Eschweiler, and E. Gerhards-Padilla, “REcompile: A De-
compilation Framework for Static Analysis of Binaries,” in Proceedings
of the 8th IEEE International Conference on Malicious and Unwanted
Software (MALWARE), 2013.

[41] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck, “Modeling and
Discovering Vulnerabilities with Code Property Graphs,” in Proceedings
of the 35th IEEE Symposium on Security and Privacy (S&P), 2014.

[42] F. Yamaguchi, C. Wressnegger, H. Gascon, and K. Rieck, “Chucky:
Exposing Missing Checks in Source Code for Vulnerability Discov-
ery,” in Proceedings of the 20th ACM Conference on Computer and
Communications Security (CCS), 2013.
J. Zeng, Y. Fu, K. A. Miller, Z. Lin, X. Zhang, and D. Xu, “Obfuscation
Resilient Binary Code Reuse Through Trace-oriented Programming,” in
Proceedings of the 20th ACM Conference on Computer and Communi-
cations Security (CCS), 2013.

[43]

