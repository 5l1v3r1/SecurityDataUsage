ACTIDS: An Active Strategy For Detecting And Localizing

Network Attacks

Eitan Menahem, Yuval Elovici and

Nir Amar

Deutsche and Telekom Laboratories and
Information Science Engineering Dept.,

Ben-Gurion University

Be’er Sheva, 84105, Israel

{eitanme,amarn,elovici}@bgu.ac.il

Gabi Nakibly

National EW Research & Simulation Center
Rafael – Advanced Defense Systems Ltd.

Haifa, Israel

gabin@rafael.co.il

ABSTRACT
In this work we investigate a new approach for detecting at-
tacks which aim to degrade the network’s Quality of Service
(QoS). To this end, a new network-based intrusion detec-
tion system (NIDS) is proposed. Most contemporary NIDSs
take a passive approach by solely monitoring the network’s
production traﬃc. This paper explores a complementary
approach in which distributed agents actively send out pe-
riodic probes. The probes are continuously monitored to
detect anomalous behavior of the network. The proposed
approach takes away much of the variability of the network’s
production traﬃc that makes it so diﬃcult to classify. This
enables the NIDS to detect more subtle attacks which would
not be detected using the passive approach alone. Further-
more, the active probing approach allows the NIDS to be
eﬀectively trained using only examples of the network’s nor-
mal states, hence enabling an eﬀective detection of zero-
day attacks. Using realistic experiments, we show that an
NIDS which also leverages the active approach is consider-
ably more eﬀective in detecting attacks which aim to degrade
the network’s QoS when compared to an NIDS which relies
solely on the passive approach.

Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: security
and protection; I.2.6 [Learning]: Concept and parameters
Learning

Keywords
Anomaly detection, Active Probing, One-Class Learning

1.

INTRODUCTION

Network intrusion detection systems (NIDS) are key in
the security architecture of many organizations. A typical
NIDS inspects traﬃc ﬂowing into, out of, or inside the tar-

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
AISec’13, November 4, 2013, Berlin, Germany.
Copyright 2013 ACM 978-1-4503-2488-5/13/11 ...$15.00.
http://dx.doi.org/10.1145/2517312.2517323

get network while attempting to isolate a malicious activity.
Broadly speaking, an NIDS tries to identify two types of
malicious activities. The ﬁrst type includes attacks carried
over network traﬃc that target end nodes. One such exam-
ple is a worm that aims to infect and take control over end
nodes. Another example is a reconnaissance activity that
scans active IP addresses or opens ports at some end nodes.
The second type of attack targets the network itself. At-
tacks of this type include exhaustion of a link’s bandwidth
by overwhelming it with traﬃc as well as unauthorized mod-
iﬁcation of a network’s routing process. The ultimate goal of
such attacks is to degrade the QoS provided by the network
to its users.

Traditionally, NIDSs are broadly classiﬁed based on the
style of detection they use. Some systems rely on a pre-
cise description of the malicious activity, i.e., knowledge-
based (signature-based) detection. Other systems rely on
statistical modeling of the network’s normal state and re-
gard signiﬁcant deviations from this state as attacks, i.e.,
anomaly-based detection.

The current paper addresses the problem of detecting QoS
degrading attacks which target the network itself, and fur-
ther focuses on attack classes which have not yet been seen
before, i.e., zero-day attacks. In order detect new attacks
classes, an IDS should not rely on the description of known
attacks (e.g., DNS poisoning or OSPF attacks), their speciﬁc
features (e.g., increased number of failed TCP connections)
or even on monitoring known attacks surfaces (e.g., DNS
cache or routing table), since a new attack class might ex-
ploit the vulnerabilities of previously unexplored attack sur-
faces, which might have signiﬁcantly diﬀerent features from
those of previous attacks. Consequently, the knowledge-
based IDS approach is not suitable for the task at hand.
Hence, in the rest of the paper we focus on the anomaly-
based detection approach.

Most anomaly-based NIDS take a passive approach. They
rely exclusively on monitoring the network’s production traf-
ﬁc and extracting the relevant features that indicate the pro-
gression of an attack. However, experience has shown that
the immense variability of network traﬃc is a major stum-
bling block of the NIDS [40]. Such variability is demon-
strated in many of the network’s traﬃc features and con-
sequently makes them very diﬃcult to predict over short
time scales (seconds to hours) and furthermore presents dif-
ﬁculties in detecting anomalies generated by the network
attacks.
In addition, a passive NIDS needs to process all

55the production traﬃc ﬂowing through it. It has been shown
that anomaly-based NIDS are unable to run at line rate in
the network core [23]. In order to mitigate this problem, an
NIDS may be deployed closer to the network edge where its
visibility of the entire network traﬃc is reduced. Alterna-
tively, packet ﬂow sampling may be used, however, that can
further degrade the detection accuracy [8].

Our work takes a somewhat diﬀerent approach and aims
to complement the pitfalls of the passive approach. We pro-
pose to produce artiﬁcial probing traﬃc exchanged between
various agents over the network. General QoS features are
then extracted from the probing traﬃc, rather than from the
production traﬃc. Lastly, anomalies are identiﬁed based on
those features. The rationale of this approach is as follows.
First, the probing traﬃc is much more predictable than the
production traﬃc and therefore an abrupt change in one of
the features gleaned from that traﬃc is most likely sourced
at an anomaly of the network rather than a legitimate shift
of traﬃc. This approach takes away much of the variabil-
ity of network traﬃc which makes it so diﬃcult to predict.
Secondly, the probing traﬃc is treated by the network as
regular production traﬃc and therefore probing traﬃc will
be aﬀected by network attacks as much as the production
traﬃc is. Thus, the detection potential is not diminished.

Note that a fundamental property to this approach is that
it detects anomalies produced by the attack eﬀects on the
network, such as changes in traﬃc delays or packet losses.
This allows us to detect any attack, including zero-day at-
tacks, as long as it aﬀects the network’s QoS parameters. In
particular, we do not attempt to monitor the by-products
eﬀects of speciﬁc types of attacks, such as the increased
number of failed TCP connections, increased traﬃc volume,
shifts in port or IP distribution or changes to routing ta-
bles and DNS caches. This will allow us to identify a wide
range of attacks which target the network, regardless of their
nature or the techniques they employ. Examples of such at-
tacks include but are not limited to: 1) subversion of the
routing process in the network; 2) poisoning of basic net-
work services, such as the DNS; and 3) overloading of the
network’s links or routers. Overall, our NIDS architecture
aims to detect every attack that has some adverse eﬀect on
the service provided to users by the network.

Another key feature of our work is the use of one-class
learning. Our NIDS is trained only on a normal network
state. This means that network attack records and speciﬁ-
cations are not required, further creating substantial opera-
tional beneﬁts. The key assumption of this strategy is that
it is possible to obtain a complete picture of almost all nor-
mal states in the network and thereby reliably infer that any
other state is anomalous. This assumption is known as the
Closed World assumption [46]. In general, the assumption is
believed to be impractical for most real-life problems since
it is impossible to cover all normal states. Nonetheless, our
work focuses on the network‘s eﬀects and the QoS it pro-
vides to its users. This QoS is generally regarded as stable
and predictable to the extent that many network operators
have commercial obligations to it under service level agree-
ments (SLAs). Therefore, our working assumption is that
we can cover all normal network states with a high degree of
conﬁdence. Furthermore, it has been shown [4] that a devia-
tion of network performance for operational purposes can be
successfully detected using anomaly detection techniques.

1.1 Applicability

The NIDS architecture proposed in this work aims to iden-
tify and locate attacks that have an adverse eﬀect on the
network and service provided to its users. That is, our aim
is not to ﬁnd attacks on the network’s end hosts that do not
aﬀect the network itself, such as the silent spread of a worm
in the network, a slow port scan or a buﬀer-overﬂow that
aﬀects only the victim end host. Therefore, it is important
to note that we do not aim to propose an alternative to ex-
isting detection approaches, but rather to complement them
where they are less eﬀective.

Our assumption is that the attacker’s goal is to do one
of the following: (a) inhibit or reduce the service provided
by the network to its users (e.g., DoS attacks) or (b) mod-
ify services provided by the network (e.g., routing process
or other basic services such as DNS). As mentioned above,
we aim to detect all such attacks regardless of the speciﬁc
techniques they employ or the protocol they use over the IP
protocol.

Notice that abnormal benign events, such as ﬂash crowd
or Byzantine faults (which might occur due to unexpected
network failures or topology changes) should be detected by
the NIDS if presenting a signiﬁcant change from the net-
work’s normal state. We argue that these benign events
should be addressed by the network operator and should
not be counted as false alarms of the NIDS.

The active-NIDS has full knowledge of the paths its probes
travel during training time; therefore, when an anomaly oc-
curs, it can localize the aﬀected network devices by iden-
tifying the probes with anomalous features. The anomaly
localization feature provides the security specialist and the
network operator with valuable information. Although this
does not close the gap between anomaly detection output
and operational meaning, it is a step closer to this goal.

Our NIDS architecture identiﬁes the location of the at-
tack’s eﬀect, not the attack’s traﬃc or its source. The ratio-
nale of this approach is based on the fact that the detection
of the true source of a real-world attack demands elaborate
forensic work conducted by a human expert. The ﬁndings
or alerts of any NIDS are only the starting point to detect-
ing the true source of the attack. This is especially true in
the case of the common multi-stage attacks [12]. The pro-
posed NIDS strategy is focused on intra-domain network set-
tings, such as enterprise or ISP networks. In such a setting,
the network operator has the ability to deploy the proposed
distributed architecture. In addition, the operator has full
knowledge of the network topology and the routes taken be-
tween every pair of end nodes. This knowledge is essential
in order to deploy our proposed architecture eﬃciently.
1.2 One-Class Learning

One-class learning is a special case of machine learning
whose main goal is to diﬀerentiate examples of the class of
interest from all other examples. The one-class classiﬁer is
trained from a training-set containing only the instances of
that class. In network security, one-class classiﬁers most fre-
quently train on the normal state of the network, i.e., they
model the network’s normal state. There are two main fea-
tures that make one-class classiﬁers attractive for network
security. First, they do not require any labeled attack in-
stances, which are very diﬃcult and costly to attain and
second, they can identify zero-day attacks, which are often
the attacker’s weapon of choice.

561.3 One-Class Ensemble Learning

One of the contributions of this work is the application of
meta learning-based ensemble learning to NIDS. The main
idea behind this ensemble methodology is to weigh several
individual classiﬁers (ensemble members) and to combine
them via another classiﬁer (combiner) to obtain a classiﬁer
that outperforms the ensemble members. Theoretically, an
ensemble can beneﬁt from any independent base-classiﬁer
that performs even slightly better than a random one (see
the Condorcet Jury Theorem and [36]).
Indeed, previous
studies in supervised ensemble learning (e.g., [28]) show that
combining classiﬁcation models can produce a better classi-
ﬁer in terms of prediction accuracy.

In contrast to the research on supervised learning, progress
in the theory of combining one-class classiﬁers has been lim-
ited. According to [13], up until 2008, this research ﬁeld
was relatively new and had not been thoroughly explored.
In particular, in the setup of diverse ensemble members, only
two combining methods were considered for one-class prob-
lems: the ﬁx-rule [42, 18] and meta-learning [29] ensembles.
Meta-learning is a process of learning from basic classi-
ﬁers (ensemble members); the inputs of the meta-learner are
the outputs of the ensemble-member classiﬁers. The goal of
meta-learning ensembles is to train a meta-model (meta-
classiﬁer) that will combine the ensemble members’ predic-
tions into a single prediction.
In order to create such en-
sembles, both the ensemble members and the meta-classiﬁer
need to be trained. Since the meta-classiﬁer training re-
quires already trained ensemble members, these must be
trained ﬁrst. The ensemble members are then used to pro-
duce outputs (classiﬁcations) from which the meta-level dataset
(meta-dataset) is created. This dataset will be used for
training the meta-classiﬁer. The basic building blocks of
meta-learning are the meta-features, which are measured
properties of the ensemble members, e.g., an ensemble mem-
bers’ predictions. A vector of meta-features comprises a
meta-instance, i.e., meta-instance ≡< f meta
>. A
collection of meta-instances comprises the meta-training-set
upon which the meta-classiﬁer is trained.
Paper Outline
The rest of this paper is organized as follows.
In Section
2 we present related work. In Section 3, we introduce the
proposed NIDS method. We then discuss the experimen-
tal setup and the network attacks used in the evaluation in
Section 4 and present the results in Section 5. Next, in sec-
tion 6, we discuss the possibility of computing an optimal
probing scheme by using general DEC-POMDP algorithms.
Finally, we close with conclusions in Section 7.

; :::; f meta

1

k

2. RELATED WORKS

In the past two decades, many anomaly-based NIDS schemes

have been proposed. The common trend among these sys-
tems is that they model the network’s normal state by an-
alyzing aspects of the production traﬃc, e.g., network ﬂows
[24, 34, 14], packet payload information [45, 44, 16, 15], net-
work event analysis [39, 33, 6] or management information
base (MIB) aggregated information [2, 17, 32].

By depending solely on the production traﬃc (or its ag-
gregations), of which they have no control, these NIDS take
a passive approach and should hence be labeled “passive-
NIDS.” The passive approach has many advantages like high

data availability and simplicity. However, many drawbacks
also exist. Sommer et al. [40] list ﬁve attributes of the net-
work security domain that usually prohibit (passive) anomaly-
based NIDS from being implemented or adopted in real net-
works. They argue that detecting attacks is a very diﬀerent
and more diﬃcult task than the classic task of machine-
learning. Their list of ﬁve attributes is as follows: First,
“real-life” errors bear a very high cost in reality. Second,
usually only a very limited training data is available. Third,
there is a diﬀerence between the anomaly detector output
and the operational meaning. Fourth, it is diﬃcult to train
a sound model of the network’s normal state because of the
huge variance in the training data. Fifth, machine-learning
NIDS cannot be evaluated in conditions suﬃciently close to
reality and therefore their benchmark results do not reﬂect
their eventual performance on real networks.

In contrast to passive-NIDS, the active-NIDS approach,
by which the NIDS relies on self-traﬃc incurred by probe
sending, can overcome some of the aforementioned deﬁcien-
cies. A fundamental property of the active-NIDS is that dur-
ing the normal network operation, its probe features, such
as hop count and average round trip time, are characterized
by very low variability. This means that probes’ features are
very predictable, thus can be modeled easily and eﬀectively
by training classiﬁers on limited size datasets, as we show in
Section 5. While classiﬁcation errors are not easy to avoid
completely, maintaining the very costly false alarms at zero,
while at the same time detecting every signiﬁcant attack, is
indeed possible, but at the cost of a detection time delay.
In this study we demonstrate such a technique and measure
its detection time delay (we refer to this as Time-to-Detect,
denoted T tD).

A related branch of research is active sensors network
(ASN) [26, 9, 22] in which autonomous sensors cooperate in
a decentralize manner to achieve a common goal. A central
problem in this context is distributed information fusion.
This is similar to our context in which several reports from
diﬀerent parts of the network should be fused into a single
report on an attack occurrence. However, there are two key
diﬀerences between our context and the ASN context. The
ﬁrst is the in the ASN case there is no global knowledge on
the topology, while in our case we deal with networks (e.g.
ISP or enterprise networks) having known topologies. The
second diﬀerence is that in ASN there is no central facility
to coordinate the cooperation eﬀort, while in our case we as-
sume there is a central detection server. The latter diﬀerence
is manifested in the fact that in ASN information fusion is
usually distributed among the sensors while in our work the
central server is responsible for this. In addition, in ASN it
is usually assumed [26] that the likelihood function of a sen-
sor’s measurement value given the true value is known. In
our context, this likelihood function is the probability that
a sensor will report an attack given that indeed an attack
occurs.
In our work, however, we only deal with positive
learning, i.e., having with no attack data, where we do not
require having measurements of actual attacks, and hence,
there is no need to obtain such likelihood function. We note
that other problems related to ASN, beyond information fu-
sion, such as scheduling of sensor activation, formation of
communication network and data aggregation is of no im-
portance in our context since in our work all sensors report
directly to the central server and no resource constraints are
imposed on the sensors.

572.1 Active Probing based NIDS

Up until now, the active probing approach has been prac-
ticed mainly for network monitoring, fault localization and
network diagnosis, e.g., [5, 11, 38]. Our survey found only
two related studies. The ﬁrst, by Barbhuiya et al.
[3],
presents a genuine NIDS, whereas the research by Barford et
al. [4] is actually related to network QoS assurance, though
is quite easy to translate for network-security purposes.

The active-NIDS studied by Barbhuiya et al. [3] sends
address resolution protocol (ARP) probes to detect ARP
spooﬁng attacks. The authors devised a formal model of
the protocol state transitions with which they later detect
anomalies. The main drawbacks of their method are that it
is protocol-speciﬁc, namely, it is not suﬃciently general to
detect other classes of attacks and it protects the hosts only
in the local network.

Barford et al.

[4] also take the active probing approach
but with the intentions of identifying network QoS anomalies
and localizing anomalous links. Their ultimate goal is to
detect and localize QoS anomalies that might occur on any
existing path between any two measurement nodes. To do
so, they must cover the entire network by a probing scheme.
To avoid producing and analyzing O(N 2) probes in each
time unit, they limit the monitoring task to only k selected
(with probability) paths in each time unit; since they assume
that the anomalies are persistent, they are certain to detect
them after ﬁnite time periods. Furthermore, they perform
the anomaly detection and localization task in two stages;
ﬁrst an anomaly detection stage, followed by the anomaly
localization stage.

While this approach works for QoS assurance, it is not
entirely suitable for the network security domain, since net-
work security attacks that cause link failures are intentional,
malicious, and might be limited in time, while in operational
networks, link failures tend to occur sporadically and are
persistent.

Moreover, the two-phase detection and localization method
has been criticized [35] as producing sub-optimal anomaly
detection and localization performance. An additional limi-
tation of Barford et al.’s method is that they put no bound
on the length of their probes, and by not doing so, they
are usually required to send elevated numbers of probes to
localize an anomalous link, in contrast to a short probing
scheme, such as the one described in the present study.
3. THE PROPOSED METHOD

The proposed method, ACtive Network Intrusion Detec-
tion Strategy (ACTIDS), is mainly characterized by four
features: (1) dispatching of probe packets to generate net-
work traﬃc, from which features are extracted; (2) hier-
archical anomaly detection architecture, i.e., the network
is partitioned into smaller disjointed sub-networks (a single
autonomous anomaly classiﬁer (local anomaly classiﬁer) is
linked with each network sub-network); (3) anomaly classi-
ﬁers are trained on examples of the normal network state
(i.e., one-class) so attack examples are not required during
training; and (4) in combining the predictions of the local
anomaly classiﬁers, a state-of-the-art meta-learning ensem-
ble is used.
3.1 Architecture

ACTIDS includes three main components: Agents, Sen-
sors, and a Central Detection Server (CDS). The hierarchy

is as follows. Each Sensor gathers information from multi-
ple Agents, which are the elementary parts, and the CDS
combines information from multiple Sensors. These three
components are described in depth below.

Agent - lightweight software responsible for transmitting
probe packets and extracting QoS statistics from incoming
probe packets. The Agents are installed at selected hosts.
The probe packets are standard IP packets which carry spe-
cial protocol values in order to be distinguished from other
packets at the network layer. Agents are linked to at least
one Sensor.

Sensor - an anomaly detector overseeing a limited-size
connected network section. During the setup phase (see 3.2),
the network is partitioned into disjointed sections, each of
which is assigned to a single Sensor. The Sensor, as can be
seen in Figure 1, is made up of an Instance Creator module
and a machine-learning anomaly classiﬁer and is bound to
Agents installed at its network section. The Instance Cre-
ator module continuously aggregates data from the bound
Agents, and once every pre-deﬁned time period, it produces
an instance that is then classiﬁed by the anomaly classiﬁer.
The classiﬁcation or anomaly score is a numerical value in
the range of [0,1] which represents the likelihood of local
anomaly and is outputted to the CDS, where it is combined
with other Sensors’ classiﬁcations.

Figure 1: The Sensor’s conceptual architecture

Central Detection Server (CDS) - produces the global
anomaly score, which indicates the security condition of the
entire network. The CDS plays a role in all three states
of the proposed NIDS. In the setup phase, the CDS has
two functions. First, it determines the network partition,
according to the partition algorithm; each partition holds
a single Sensor. The CDS’s second function during setup
is to determine the probing scheme. During the training
phase, the CDS’s anomaly classiﬁer is trained. Lastly, in the
detection phase, the CDS produce a global anomaly score by
combining anomaly scores produced by the Sensors.

ACTIDS has a three stage life cycle: setup, training, and
prediction. All the aforementioned stages are fully auto-
matic and require very limited user intervention. We now
describe each of these stages.
3.2 Setup Phase

In the setup stage two pre-processes of ACTIDS are exe-
cuted; the ﬁrst determines the Sensors’ partition while the
second determines the probing scheme.
3.2.1 Local Anomaly Sensors
During the setup phase, the network is logically parti-
tioned into sub-networks. A local anomaly detector (Sen-

58Algorithm 1: Partition the network into disjointed Sen-
sors
Input: N etworkT opology : network’s topology
Input: max routers : the maximal number of routers

in each Sensor

Output: SensorsMap : a list of Sensors
Routers ←ParseTopology(N etworkT opology)
SensorsM ap ← ∅
foreach r in Router do

Degree ← CalculateDegree(r)

Sort(Routers) by Degree;
while |Routers| > 0 do

currentSensor ←createEmptySensor ()
kernel ←deleteFirst (Routers);
Add Kernel To currentSensor
while |currentSensor| ≤ max routers do
nr ←DeleteNeighbor (kernel,Routers)
Add nr To currentSensor

Add currentSensor To SensorsM ap

2. To reduce both Probe traﬃc and learning complexity,

use as few Probes as possible.

3. To enable accurate detection of the anomaly source,

Probe-Paths should be short, i.e., 4 or 6 edges.

These guidelines can be translated into an optimization
problem in which we aim to cover all the network’s edges
with a minimal set of Probe-Paths over a bounded number
of edges. A special case of this problem, in which no bound
on the Probe-Paths’ length exists, has been shown to be an
instance of the NP-hard Minimum Set Cover problem [5].
However, covering all the network’s edges is very likely to
generate too much Probe traﬃc and increase the learning
dimensionality, thus making the learning task too complex.
Our probing scheme uses only on the order of O(|v|) Probe-
Paths while attempting to cover most of the network devices.
This is because each router sends, at most, a single Probe,
whose path length is bound by the pathLength constant, as
presented in Algorithm 2.

Such a probing scheme will generate suﬃcient information
to detect attacks whose eﬀects reach beyond the boundaries
of a single Sensor. As the network attack aﬀects a greater
section of the network, and is therefore regarded as more
severe from the perspective of the defender, the probability
of the eﬀect being captured by at least one Probe increases.
The DeleteNighbor(r,Routers) routine extracts the neigh-
bor router of k with the highest rank and deletes it from
router group Routers.
3.3 Training Phase

In this stage all the machine-learning models, i.e., the lo-
cal anomaly detectors attached to the Sensors and the global
anomaly classiﬁer in the CDS, are trained. At the end of this
stage, the proposed NIDS is ready for the detection phase.
The training process comprises three distinct tasks: produc-
ing the training datasets, training the Sensors’ classiﬁers,
and training the global anomaly classiﬁer. We now discuss
each of these tasks.

Figure 2: Example of two probes, sent across two
Sensors, which then send their corresponding local
anomaly score to the CDS

sor) is attached to each sub-network. Algorithm 1 estab-
lishes a set of disjoint Sensors, each of which contains up to
max number of routers routers.

Definition 1. (Probe). Probe is a special media that is

sent between two Agents.

The Probes are used for extracting crude network fea-
tures upon which anomalies are detected. Probes make a
roundtrip, starting at the source Agent, traveling to the
destination Agent at one of the adjacent Sensors, and then
returning to the source. The roundtrip is necessary in order
to avoid router time synchronization, which may be diﬃ-
cult to attain in real-life networks. A Probe can be made of
any type of media which is appropriate for sending over the
network, e.g., IP packet, ICMP packet or any application
packet of choice. The source and destination hosts relate to
diﬀerent Sensors.

Definition 2. (Probe-Path). Probe-Path is a set of con-
nected edges (v; e) which follow the shortest path between the
Probe’s source host and its destination host and back to the
source host.

Definition 3. (Probe re-sending). Let Tprs denote the
constant time interval between subsequent Probe re-sending.
3.2.2 Probe Dispatching Mechanism
The Probing Mechanism produces a list of Probes. Later,
during the training and detection phases (the second and
third stages of the proposed NIDS’s life cycle, respectively),
this Probe list is sent to enable Agents to measure statistics
regarding the network’s QoS. During the training and de-
tecting stages, the Probes are re-sent once every ﬁxed (con-
ﬁgurable) period of time. A procedure for an automatic
generation of a probing scheme is presented in Algorithm 2.
We call this algorithm “the probing algorithm.” The prob-
ing algorithm is the second and ﬁnal algorithm related to
the setup phase. The inputs for this algorithm are the Sen-
sors’ partition, a list of routers, and the Probe length, i.e.,
probesLength. The probing scheme algorithm follows three
guidelines:

1. To increase the likelihood of detecting anomalies, Probe-

Paths should cover as many of the network’s edges as
possible.

59Algorithm 2: Probe-Scheme Generation
Input: RoutersM ap, SensorsM ap, probesLength
Output: list of Probes
pl ← probesLength
P robeList ← ∅
foreach r in RouterM ap do

← GetSensor

(r,SensorsM ap)

srcS
Candidates ←RoutersAtDistance (r,pl)
foreach c in Candidates do
destS ←GetSensor (c,SensorsM ap)
if Not(srcS==destS) AND
ExistProbeBetween(srcS,destS)=false then

src ← r
dest ← c
P robeList+=New Probe(src,dest)
end foreach

Definition 4. (Classiﬁcation Time). Let Tcl denote the
constant time interval between behavior classiﬁcations of sub-
sequent networks.

Prior to training the machine-learning-based classiﬁers, a set
of appropriate training data is produced. This is achieved in
several stages. First, the NIDS begins sending Probes across
the network according to the pre-acquired Probe-Scheme.
The Probe traﬃc is added to the network’s normal chaotic
traﬃc. As the Probes are re-sent once every Tprs during
the entire training phase, they incur the traﬃc from which
the Agents collect raw statistics. When an Agent receives
a Probe, it extracts three raw features; hop count, travel
time, and num lost probes and delivers them to the Sensor
to which it is attached. The Sensor aggregates the received
statistics until, once per Tcl (Tcl >> Tprs), it computes the
average and the variance of the data accumulated for the
ﬁrst two raw features, computes the lost probes percentage,
and constructs an instance. The instance is then added to
the Sensor’s private training-set. Next, the statistics at the
Sensors are nulliﬁed and the process repeats until the end
of the training phase, by which time all the training-sets
contain the same number of instances. Table 1 presents the
Sensors’ training-set structure.

Next ,when the Sensors’ training-sets generation is done,
the Sensors’ classiﬁers are trained, each on its own private
training-set. Lastly , ACTIDS uses the TUPSO ensemble
scheme [29] to train the CDS’s classiﬁer, which is responsible
for combining the Sensors’ local anomaly score during the
prediction phase.

Probe: R8 −→ R2

Time
Unit
1
2
...

Avg.
Delay
a1;1
a2;1
...

Var.
Delay
a1;2
a2;2
...

Avg. Hop
Count
a1;3
a2;3
...

Var. Hop
Count
a1;4
a2;4
...

...

...............

% Lost
Probes

a1;5
a2;5
...

...............
...............
...............

Table 1: The Sensor’s training-set structure. at;i
denotes the value of feature i at time unit t

Let P m

t =< pm

t;1; :::; pm

t;n > denote the vector containing
the Sensors’ predictions, pm
n for time unit t, where n
is the number of Sensors. TUPSO generates a special train-
set for the CDS’s combiner classiﬁer by applying k dedicated

1 ; :::; pm

Time Unit f1 (P m

1
2
...

t ) f2 (P m

t ) f3 (P m
ma1;1 ma1;2 ma1;3
ma2;1 ma2;2 ma2;3

t ) ... f6 (P m
t )
... ma1;6
... ma2;6
...

...

...

...

...

f7 (P m
t )
ma1;7
ma2;7

...

Table 2: The training-set structure of the CDS’s
combiner
aggregate functions (described in [29]), f (·), to P m. Each
such aggregation represents a single feature (i.e., column) in
the combiner’s train-set.

TUPSO’s training process starts with training the en-
semble members, followed by training the meta-classiﬁer,
i.e., the CDS’s anomaly classiﬁer. The ensemble members
and the meta-classiﬁer are trained by an inner k-fold cross-
validation training process. First, the ensemble members’
training-sets are partitioned into k splits. Next, in each
cross-validation fold, each ensemble member is trained us-
ing its own k − 1 splits. Afterwards, each trained ensemble
member classiﬁes its remaining split to produce the meta-
instances. The meta-instances in each fold are added to a
meta-dataset. After k folds, the meta-dataset contains the
same number of instances as the original dataset. Lastly,
each ensemble member is re-trained on all the instances
in its training-set and the meta-classiﬁer is trained on the
meta-dataset. Table 2 illustrates the general structure of
the CDS’s training-set. mat;i denotes the value of feature i
at time unit t.
3.4 Prediction Phase

The prediction phase represents the “on-line” state of the
proposed NIDS. To generate the special traﬃc required by
the proposed NIDS, the Agents continuously send Probes
once every Tprs, according to the Probe-Scheme. Upon re-
ceiving the Probes, the Agents extract the same raw features
as they did in the training phase. Each Agent adds the fea-
tures it has extracted to the attached Sensor, where they
are accumulated. Once every Tcl, each Sensor averages the
accumulated values and generates a local instance. These
instances are then classiﬁed by the Sensors’ classiﬁers and
their statistics are reset. Lastly, the predictions are com-
bined by TUPSO, which outputs the ensemble prediction
for time t, denoted as EPt.
NIDS Output
The NIDS’s ensemble output for time t, EPt, is capable of
classifying a snapshot of the network’s behavior as it reﬂects
the combined anomaly scores of the local Sensors. How-
ever, in situations where the NIDS Sensors are very sensi-
tive, they may incur an erratic global prediction and thus in-
duce false alarms (i.e., normal network behavior mistakenly
classiﬁed as anomalous). To reﬁne its classiﬁcation quality,
the presented NIDS uses the exponentially weighted mov-
ing average technique (EWMA) which produces smoother
and conservative anomaly scores and, as a result reduces,
the potential for false alarms. This, as we show in Section
5, is one of the factors responsible for incurring extremely
low false-positive rates while at the same time allowing for
a high detection rate. In a nutshell, the EWMA technique
accumulates anomalous behavior “evidence” over time and,
where there is suﬃcient evidence, will output an “anomaly”
classiﬁcation. The NIDS output, N AS ∈ [0; 1], is computed
as follows:

N ASt = N ASt(cid:0)1 + (cid:24) ∗ ∆(t; p)

60where ∆(t; p) is the diﬀerence between the weighted average
of two time-series of the same length, p=2. The ﬁrst series
begins at t − p and the second at t − p=2:

∑ p

(cid:0)1

2

i=0 ( 1
(cid:0)i

p
2

∆(t; p) =

∑ p
∗ (EPt(cid:0) p
i=1 ( 1
i )

2

2 +i

− EPt(cid:0)p+i))

Variable t is a discrete time point with granularity tcl,
and p is the sum length of the time-series taken into con-
sideration; (cid:24) ∈ R>0 is the amplitude assigned to changes in
the network behavior; N ASt represents the likelihood of a
network-wide anomaly in time t.

4. EXPERIMENTAL SETUP

In this section, we specify the conditions in which the
proposed NIDS was investigated. First, we illustrate the
network simulation used for executing the NIDS. Then, we
describe the machine-learning algorithms that participated
in the evaluation. Lastly, we present the performance met-
rics needed to measure the eﬀectiveness of the NIDS.

The NIDS’s machine-learning modules were implemented
within the WEKA machine-learning framework [46]. To
evaluate the performance of the proposed NIDS, we used
the OMNeT++ [43] simulation framework, which allows a
network to be simulated with full IP stacks. We used real
router-level ISP topologies, as mapped by the RocketFuel
project [41]. Table 3 lists the ISP topologies used, which in-
clude between ten and several hundred routers. The traﬃc
between the routers was generated based on the well-known
gravity model [21]. In this model each router is randomly
given a weight, and the average traﬃc volume between two
routers is proportional to the product of their weights. The
actual pattern of the generated traﬃc was based on the “self-
similar” model, which is widely believed to be the model that
best ﬁts Internet traﬃc [25].

Topology Name
AS1755
Ebone (Europe)
AS3257 Tiscali (Europe)
AS3356
Level3 (U.S.)
AS4755 VSNL (India)

Routers
163
276
624
11

Links
300
400
5,300
12

direct traﬃc through a narrow link or lengthen its route in
order to increase the delay and the packet-loss the traﬃc
experiences. The second way to harm the QoS of a network
is to induce extra production traﬃc that would normally
not have been generated. One example of this is a reﬂected
SYN attack in which the attacker sends many spoofed SYN
packets to various end hosts who respond with SYN-ACK
packets. In this example, the extra traﬃc is induced by the
attacker itself (SYN packets), as well as the victim end hosts
(SYN-ACK packets). By inducing a high volume of traﬃc
an attacker may exhaust the bandwidth of one or more links
in the network.

We evaluated our NIDS only against attack scenarios of
the ﬁrst type, as it is considered more subtle and harder to
detect. There are various ways for an attacker to implement
such attacks. For example, he may advertise false routing
advertisements in order to change the routing tables, he may
poison a DNS cache in order to change the destination IP
address of the traﬃc; or he may launch any application-
speciﬁc impersonation attack in order to divert traﬃc to
a false end host. From the network’s point of view, and
consequently from our NIDS’s point of view, all such attacks
have a similar eﬀect, namely, traﬃc diversion.

To evaluate our NIDS’s eﬀectiveness against traﬃc diver-
sion attacks we executed two OPSF attacks, i.e., partially
disconnecting and link-weight distortion attacks, and two
DNS attack variants, i.e., DNS cache poisoning and author-
itative DNS server poisoning 1. The description of these four
attack variants is given in [27]. These attacks are currently
the most ﬂexible and powerful way to achieve traﬃc diver-
sion. Please note that ACTIDS is not a protocol-speciﬁc
solution, i.e., it is not limited to the abovementioned proto-
cols.
4.2 Datasets

In each experiment related to the above mentioned attack
scenarios, two datasets were generated: one training-set and
one evaluation set. The training-set contained instances re-
lated to normal network behavior only, whereas the evalua-
tion set contained instances related to normal and attacked
periods (33.3% labeled “normal” and the remaining 66.67%
labeled “attack”).

Table 3: Properties of the autonomous systems used
in the evaluation. Source: RocketFuel

4.3 Classiﬁers

The simulation was executed for 4500 time units (T U );
the simulation was run for 1020 T U before the NIDS was
executed to ensure the OSPF converged, then it began to
send probes at a rate of 25 per T U for the duration of the
experiment, which was set to 3500 T U . The classiﬁcation
period, tcl, was deﬁned as 1 T U so that overall, the datasets
(training- and test-sets) contained 3480 instances. Since the
Sensors contained diﬀering numbers of data sources (i.e.,
number of Probes), their datasets varied by the number of
features. Throughout the evaluation, we used max routers =
4 and probesLength = 4 and a classiﬁcation threshold Θ =
0:75 over N AS, which was suﬃciently high so as to produce
zero false positives while maximizing the true-positive rate.
4.1 Attack Scenarios

In general, there are two ways for an attacker to harm the
QoS of a network. The ﬁrst way is to change the routes the
normal production traﬃc takes. This way, the attacker may

For evaluation purposes, we made use of three one-class al-
gorithms: OC-PGA [20], 1-SVM [37], and ADIFA (Attribute
DIstribution Function Approximation) [30]. We selected
these base-classiﬁers because they represent the prominent
families of one-class classiﬁers: density (OC-PGA) and bound-
ary (OC-SVM). The OC-PGA algorithm is an adaptation
for one-class learning from a well-known unsupervised al-
gorithm [19] and the ADIFA algorithm is a univariate one-
class anomaly detection algorithm based on meta-learning.
In training the meta-classiﬁer (the classiﬁer of the global
anomaly detector), we made use of ADIFA because it per-
formed considerably better than the other learning algo-
rithms.
4.4 Performance Metrics

In evaluating the NIDS, the following performance metrics
were used: classiﬁcation error rate, F-Score, area under the
1See: www.researchgate.net/publication/250612169

61ROC curve (AUC), and Time-to-Detect. These performance
metrics provide suﬃcient information to assess the ﬁtness of
the proposed NIDS for detecting network anomalies origi-
nated by network attacks. Figure 3 depicts a network at-
tack that begins at “attack start” time and ends after twelve
Tcl time-units. Also shown is the corresponding predicted
attack score produced by the NIDS (i.e., N AS).

Figure 3: A network attack surface and the cor-
responding predicted global network anomaly score
over time

Time-to-Detect (TtD) is a temporal metric that measures
the time units (TU) from the commencement of a network
attack to detection by the classiﬁer (the NIDS). Usually
there is a trade-oﬀ between TtD and false-positive rate (FPR).
A low TtD (a good measure) typically comes at the price of
a greater FPR, since the classiﬁer is less able to distinguish
between noise signals, which are erratic, and attack signals,
which tend to be more stable.

The Aread under the ROC curve (AUC) measures the ef-
fectiveness of the classiﬁer. The ROC curve is a graph pro-
duced by plotting the true positive rate T P R (a.k.a recall)
versus the false positive rate (T P R = tp=(tp + f n); F P R =
f p=(f p + tn)). The AUC value of the best possible classiﬁer
will be equal to unity. This would imply that it is possible to
conﬁgure the classiﬁer so that it will have 0% false positive
and 100% true positive classiﬁcations. The worst possible
binary classiﬁer (obtained by ﬂipping a coin for example) has
AUC of 0.5. AUC is considered as an objective performance
metric as it does not depend on the speciﬁc discrimination
threshold used by the classiﬁer.

5. EXPERIMENTAL RESULTS

In this section we present the empirical results, obtained
by the proposed NIDS in the four of the above mentioned
attack scenarios.

5.1 Probes Trafﬁc Volume

The probes are the most important tool the proposed
NIDS uses for identifying network anomalies. Unfortunately,
they inevitably add some traﬃc to the network. By doing
so, they might aﬀect the NIDS measurements, or even, if a
signiﬁcant volume of probe traﬃc is produced, can reduce
the network QoS. Therefore, it is imperative to quantify the
probes’ eﬀect on the network. Fortunately, since the probes
dispatching scheme, probes packet size, and the re-sending
frequency remain constant during the entire on-line phase,
computing the augmented network traﬃc volume is straight-
forward.

Table 4 summarizes the properties of the NIDS probes
for the used network topologies. For calculating the probes

traﬃc the following were assumed. A probe packet size is
exactly 64 bytes, each probe is sent 25 times per second,
and the bandwidth of every network link is 100Mb/s. The
results indicate that the probes traﬃc volume is insigniﬁcant
in terms of its capability to harm the network QoS.

Topology
1755
3257
3356
4755

#Probes
(Probes/Sec)

Probe Traﬃc

(Bytes/Sec) % of network’s bandwidth

125
75

1,375
350

8,000
4,800
88,000
22,400

0.004%
0.005%
0.007%
0.006%

Table 4: Properties of the Probes traﬃc

5.2 Comparison with Passive Features

A basic assumption made in this study is that Probes’
traﬃc features can yield a superior predictive model com-
pared to passive-oriented feature sources. In this experiment
we compared the Probes’ features with two passive traﬃc
features sources, i.e., link-based features and router-based
features, with the intention of determining whether the pri-
mary assumption holds. In addition, we tested whether a
mixture of feature sources can improve the NIDS classi-
ﬁers’ performance. As link-based features, #bytes/sec and
#packet/sec features were extracted for each link, and as
router-based features, three management-information-base
(MIB) features were extracted for each router:
#packets forwarded, #local deliveries and #unroutable packets.

The NIDS was evaluated with both the partially discon-
necting and the DNS cache poisoning attacks on the AS3356
and AS4755 network topology, respectively. We used only
the ADIFA algorithm for training the Sensor classiﬁers.

Figure 4 presents a stacked plot of four aggregate features
(each data-point is an average of 25 raw values), extracted by
the proposed NIDS during a simulation run, in which a DNS
cache poisoning attack was executed after 925 seconds. The
upper most graph shows the #packets forwarded feature,
as measured in router R28, the second graph from the top
shows the #local deliveries feature of the R13 ↔ R19 link,
the third graph shows the R28 → R13 probe’s round trip
average travel time,

and the graph at the bottom shows the variance of the
travel time of the same probe. The data was captured at
a simulation run, where a DNS cache poisoning attack was
executed using the AS4755 topology. The gray graphs repre-
sent the raw data, the black bold graphs are the trend lines
(using the moving average), and the red lines plot the three
standard deviations range, computed from the raw value,
over a historical time period of 60 seconds.

The graphs in Figure 4 lead to two conclusions. First,
prior to the attack, both the MIB and Link features are much
less stable compared to the two probe features. Second,
the mean and variance of the probes raw data are much
more aﬀected by the attack, compared to both the MIB and
Link raw features; while the average and variance of the
raw MIB values increased due to the attack by 27% and
40% respectively, the average and variance of the probe’s
raw data (travel time variance) were increased by 426% and
1,680% respectively.

Continuing the comparison between the active and passive
features, the next logical step is to study the classiﬁcation
performance of the proposed NIDS when using the diﬀerent

62Figure
disconnecting attack over the AS3566 topology

ROC graphs

the partially-

for

5:

These results are highly signiﬁcant because they suggest
that probe-based NIDS can successfully detect zero-day at-
tacks, without needing to monitor the attack’s traﬃc or the
attack objective, such as the OSPF routing table or the DNS
cache. Interestingly, in both attack classes, the combination
of probe features with either link or MIB features resulted
with an inferior classiﬁcation model, as long as zero FPR
was required.

The following examines some additional aspects of the
proposed NIDS. This examination is arranged in four sec-
tions, each of which deals with a diﬀerent attack class.
5.3 Detecting Partially Disconnecting Attacks
In this experiment, ACTIDS is evaluated on attacks that
logically disconnect half the victim(s) interfaces so that traf-
ﬁc can still pass or be routed through them on some inter-
faces. Hence the victim(s) are only partially “disconnected”.
This experiment has two goals: ﬁrst, to study the ability
of the presented NIDS to detect network-based attacks with
global eﬀects; second, to determine which machine-learning
algorithm is best suited for detecting the abovementioned
attacks. Table 6 summarizes the attack scenarios and the
attributes of their corresponding datasets.

Attacker! Victim

Attack Scenarios
Topology
AS3257 R1 ! R3, R1 ! R6
AS1755 R2 ! R11, R4 ! R13, R9 !
AS4755 R14 ! R13, R2 ! R11, R2 !

R15

AS3356

R16
R75 ! R78, R75 ! R79
R77 ! R20, R8 ! R76

ACTIDS datasets

Sensors Features Avg.

4
4

15

54

12.5
22.5

12.06

10.1

Table 6: Partially Disconnecting attack scenarios

The results in Table 7 show that ADIFA algorithm achieve
a detection rate greater than 98% and that all three algo-
rithms had no false-positive classiﬁcations: Their only clas-
siﬁcation errors were made during the attack period (false
negative) when the NIDS was waiting to obtain “suﬃcient”
global anomaly evidence.

Comparison of the two presented output functions reveals
that EP (ensemble prediction output) is the most respon-
sive measure for detecting attacks; however, it comes at the
price of a higher FPR. In contrast, N AS has longer TtD
than EP , but generates fewer false-positive classiﬁcations,
which is usually a more important property for real NIDS.

Figure 4: Comparison between “passive” network
features and “active” (probe) features

feature-set combinations. To this end, four scenarios of the
partially-disconnecting attack were executed on the AS3356
topology, of which description is given in Table 6.

The results in Table 5 show that when trained using probe-
based features, ACTIDS performed better compared to when
trained on link-based or MIB-based features. A zero FPR
was achieved only by using the probe-based features, while
the other feature sources produced a considerably higher
FPR.

In order to expand the generality of the above results,
we plot ROC graphs by measuring the classiﬁcation TPR
and FPR of the proposed NIDS, for a range of classiﬁcation
thresholds. Figure 5 contains ROC plots for the partially-
disconnecting attack, and Figure 7 contains ROC graphs for
the DNS cache poisoning attack.

These ROC plots show that only the probe-feature group
was able to produce a signiﬁcant TPR (0.85 and 0.86 for
the partially disconnecting and the DNS cache poisoning
attacks, respectively) while, at the same time, inﬂicting no
false positives at all.

ric
t
e
M

FPR

TPR

r
e
t
u
o
R
tim
Vic
R78
R79
R20
R69
R76
Total
R78
R79
R20
R69
R76
Total

s
k

Lin

s
B
I
M

s
e
b
o
r
P
0.000 0.000 0.000
0.000 0.096 0.153
0.000 0.118 0.124
0.000 0.167 0.205
0.000 0.175 0.141
0.000 0.111 0.124
0.990 0.968 0.722
0.989 0.664 0.989
0.885 0.998 0.990
0.329 0.787 0.342
0.989 0.993 0.990
0.836 0.882 0.806

s
k

Lin

s
B
I
M
&
s
k

s
B
I
M
&
e
b
o
r
P

s
e
r
u
t
a
Fe
All

Lin
&
e
b
o
r
P
0.000 0.000 0.000 0.000
0.086 0.143 0.146 0.138
0.103 0.135 0.126 0.153
0.156 0.185 0.224 0.223
0.060 0.132 0.150 0.146
0.081 0.119 0.129 0.132
0.988 0.989 0.901 0.988
0.989 0.988 0.988 0.986
0.994 0.990 0.990 0.990
0.723 0.346 0.606 0.591
0.991 0.990 0.988 0.989
0.937 0.861 0.894 0.909

Table 5: A comparison between ACTIDS features
sources, based on TPR and FPR metrics

63AS Algorithm TPR

FPR % Error F-Score

TtD

3257

1755

4755

3356

Total

Total

1-SVM
OC-PGA
ADIFA
1-SVM
OC-PGA
ADIFA
1-SVM
OC-PGA
ADIFA
1-SVM
OC-PGA
ADIFA
1-SVM
OC-PGE
ADIFA

EP
N AS

0.981
0.980
0.982
0.981
0.980
0.982
0.981
0.681
0.982
0.981
0.793
0.982
0.981
0.847
0.982

0.878
0.934

0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000

0.007
0.000

0.014
0.014
0.013
0.014
0.014
0.013
0.014
0.229
0.013
0.014
0.149
0.013
0.014
0.110
0.013

0.090
0.047

0.990
0.990
0.991
0.990
0.990
0.991
0.990
0.710
0.991
0.990
0.856
0.991
0.990
0.877
0.991

0.886
0.951

49.000
50.500
46.500
49.000
51.000
46.333
49.000
49.667
46.333
48.667
48.333
47.000
48.909
49.818
46.474

28.810
48.492

Table 7: Result table for the partially disconnecting
attack class
5.4 Detecting Link Weight Distortion Attacks
Continuing further with the experimentation, we evaluate
the proposed NIDS on the ‘Link Weight Distortion’ attack
over three diﬀerent scenarios.
In the ﬁrst two scenarios a
single link weight is aﬀected, whereas in the third, two links
are simultaneously aﬀected. Speciﬁcally, the implemented
attack increased the target link’s weight (cost) by one orders
of magnitude, to insure a global anomaly. The experiment
was performed on the largest network topology, i.e., AS3356.

Aﬀected Link Algorithm TPR FPR %Error F-Score TtD
R50 $ R79

1-SVM
0.000
0.000
0.000
OC-PGA 0.913
0.985 0.000
ADIFA
0.000
1-SVM
0.000
0.000
OC-PGA 0.000
ADIFA
0.468
0.000

0.667
0.062
0.011
0.667
0.667
0.379

0.0
0.955
0.992
0.0
0.0
0.637

n/a
33.0
18.0
n/a
n/a
136.0

R79 $ R80

R50 $
R79R79 $ R80

1-SVM
0.000
OC-PGA 0.990
ADIFA

0.000
0.000
0.995 0.000

0.667
0.667
0.003

0.0
0.995
0.998

n/a
25.0
13.0

Router R28 was used as the adversary, whereas the victims
role was given to routers R1, R2, R3, R8, and R9.

Figure 6: AUC results for the proposed NIDS over
diﬀerent DNS cache poisoning percentage

The results in Figure 6 indicates that when the cache poi-
soning attack intensity was below 40 percent, none of the
feature sets provided suﬃcient and robust information for
intrusion detection. A change of trends started from an in-
tensity around 40 to 50 percent. From this point and on,
the Probe features incurred a detection AUC rate of above
0.95, which was greater than that of both the Link and MIB
feature sets. It is important to note that until the attack
reached the critical value of around 40 percents, the network
QoS remained unharmed, as no evidence of increased delays,
jitter or packet loss could be observed, either automatically
or manually, from the extracted raw network data.

Table 8: Result table for the Link-Weight Distortion
attack class

The results in Table 8 show that the Link Weight Distor-
tion attack was, in general, harder to detect, when compared
to the partially disconnecting attack. Consequently, when
the proposed NIDS used the OC-PGA classiﬁer, it did not
detect one of the three attacks, and with the 1-SVM classiﬁer
it missed the attacks of all three scenarios. In contrast, when
it used the ADIFA classiﬁers it detected all the attacks, and
presented the best classiﬁcation performance among the ex-
amined classiﬁers, with the shortest time-to-detect and the
highest true positive detection rate.
5.5 Detecting DNS Cache Poisoning Attack

In this section we study the detection performance of the
proposed NIDS on a simulated eﬀect of the DNS cache poi-
soning. Speciﬁcally, we examine the NIDS sensitivity using
an increasing attack intensity, implemented by a series of ex-
periments, each simulates an increased percentage of DNS
poisoned entries at the victim routers. Five DNSs (routers)
were subjected attacked during the experiment series to in-
sure a substantial eﬀect on the network’s QoS, even when
only 10 percents of the DNS cache entries are poisoned. The
experiment series was performed using the AS4755 topology.

Figure 7: ROC graphs for the DNS cache poisoning
attack over the AS4755 topology

6. DISCUSSION

In this section we present a general method for choosing
an optimal probing scheme. More speciﬁcally, we give an in-
tuitive method for representing our active NIDS as a special
DEC-POMDP. This will allow to use general DEC-POMDP
algorithms to generate optimal policies, i.e., optimal prob-
ing schemes, which will take into consideration probes costs
and classiﬁcation rewards.
6.1 Adaptive Probing Strategy using

DEC-POMDP

The active approach taken by the proposed NIDS requires
sending periodic probes, upon which anomalous network be-
havior can be detected. In section 3.2.2 we presented an al-
gorithm, which, given a network topology, computes a static

64probing scheme. In a real-life NIDS implementation, how-
ever, a static probing scheme will not be suﬃciently eﬀec-
tive over time, since it ignores important operational factors,
such as topology changes, network traﬃc load, probing costs,
classiﬁcation errors cost, and even the computed network’s
current anomaly-level. An active-NIDS that considers such
factors may require a ﬂexible probing scheme.

There are many ways to produce a ﬂexible probing scheme,
but since that ACTIDS is a special case of a decentral-
ized partially-observable Markovian decision process (DEC-
POMDP), it is natural to use planning algorithms (e.g., [31],
[1] and [10]) to generate an optimal dynamic probing scheme.
The decentralized partially observable Markov decision
processes (DEC-POMDP), a special case of MDP, is deﬁned
by a set of states, a ﬁnite set of actions to choose from,
ﬁnite sets of observations, an immediate reward function,
observation probabilities matrix, and a probabilistic transi-
tion matrix [7]. The goal of all MDP problems is to derive
a policy, which is a mapping from states to actions, which
represents the best actions to take for each state, for a given
horizon length. A partially-observable MDP (POMDP) is
very similar to MDP, however, in POMDP one cannot di-
rectly observe the current state of the process. Instead, it
is possible to produce observations, which provide a hint
about current state. Since the observations may be proba-
bilistic, an observation model should be used to indicate the
probability of each observation for each state in the model.
The Sensors in ACTIDS can be seen as a cooperative
group of agents that operate in a stochastic environment. In
this setup, the agents‘ task is to correctly “guess” their local
state, in a timely manner. There are only two local-states
for each agenti: Si;0 and Si; 1, which represents the “normal”
and “anomalous” states of agenti’s sub-network. Each agent
can decide, based on previous observations, which probes to
send next, while taking into consideration probes coverage,
probe cost, and a detection reward matrix (contains the re-
ward/cost of TP, FP, TN and FN classiﬁcation). In addition,
there are two global states - S0 and S1, which represents
the network-wide “normal” and “anomalous” states, respec-
tively. The global probing scheme, i.e., the global policy,
is the summation of probes, selected to be sent during the
next time interval. Notice that the above-mentioned setup
is a unique variation of DEC-POMDP, in which the actions
taken by the cooperative group of agents does not aﬀect the
states in the process. Instead, there are one or more hidden
“players” (e.g., an adversary or a natural phenomenon), that
have the ability to controls the states. Therefore, there is no
probabilistic transition matrix for this special DEC-POMDP
problem. We leave the formal deﬁnition and implementation
of this DEC-POMDP problem as a future work.

7. SUMMARY AND CONCLUSIONS

In this paper we have presented a new active strategy for
detecting network attacks, called ACTIDS. It detects net-
work attacks by means of self-produced short-range probes,
hierarchical anomaly detection and one-class ensemble learn-
ing. We examined the proposed NIDS on 4 network topolo-
gies over 35 attack scenarios. The smallest network con-
tained 9 routers, whereas the largest contained 82 routers.
The experiments spanned ﬁve dimensions: attack class, fea-
tures groups, topology, learning algorithm, and the NIDS
output function. Our results show that the active approach
can perform better than the passive approach in detecting

attacks that harm the QoS of the network. Experiments
on the same platform, the active Probes features produced
a considerably better classiﬁcation performance, compared
to the passive features.
In particular, the Probe features
incurred a zero false-detection rate (FPR), while achieving
a true-detection rate (recall) of 85%. In order to produce
such a high recall value, the NIDS, when trained on passive-
related features, suﬀered from more than 10% false-detection
classiﬁcations.

The time taken for ACTIDS to detect a network attack
was, on average, 46.8 seconds. This is an excellent result
bearing in mind the zero-false-positives constraint and that
network attacks, within the scope of this paper, are expected
to last longer than an order of minutes.

8. REFERENCES
[1] Amato, C., Bernstein, D. S., and Zilberstein, S.

Optimizing memory-bounded controllers for
decentralized pomdps. arXiv preprint arXiv:1206.5258
(2012).

[2] Bao, C.-M. Intrusion detection based on one-class

svm and snmp mib data. In IAS (2009), pp. 346–349.

[3] Barbhuiya, F. A., Biswas, S., and Nandi, S. An

active des based ids for arp spooﬁng. In SMC (2011),
pp. 2743–2748.

[4] Barford, P., Duffield, N., Ron, A., and

Sommers, J. Network performance anomaly detection
and localization. In INFOCOM 2009, IEEE (april
2009), pp. 1377 –1385.

[5] Bejerano, Y., and Rastogi, R. Robust monitoring

of link delays and faults in ip networks. IEEE/ACM
Trans. Netw. 14, 5 (2006), 1092–1103.

[6] Benali, F., Bennani, N., Gianini, G., and

Cimato, S. A distributed and privacy-preserving
method for network intrusion detection. In OTM
Conferences (2) (2010), pp. 861–875.

[7] Bernstein, D. S., Givan, R., Immerman, N., and

Zilberstein, S. The complexity of decentralized
control of markov decision processes. Math. Oper. Res.
27, 4 (2002), 819–840.

[8] Brauckhoff, D., Tellenbach, B., Wagner, A.,

May, M., and Lakhina, A. Impact of packet
sampling on anomaly detection metrics. In Proceedings
of ACM SIGCOMM on Internet measurement (2006),
pp. 159–164.

[9] Carle, J., and Simplot-Ryl, D. Energy-eﬃcient

area monitoring for sensor networks. Computer 37, 2
(2004), 40–46.

[10] Chades, I., Scherrer, B., and Charpillet, F. A
heuristic approach for solving decentralized-pomdp:
Assessment on the pursuit problem. In Proceedings of
the 2002 ACM symposium on Applied computing
(2002), ACM, pp. 57–62.

[11] Cheng, L., Qiu, X., Meng, L., Qiao, Y., and

Boutaba, R. Eﬃcient active probing for fault
diagnosis in large scale and noisy networks. In
INFOCOM (2010), pp. 2169–2177.

[12] Clark, D. D., and Landau, S. Untangling

attribution. In Workshop on Deterring Cyberattacks:
Informing Strategies and Developing Options for U.S.
Policy (2010).

65[13] Giacinto, G., Perdisci, R., Rio, M. D., and Roli,

F. Intrusion detection in computer networks by a
modular ensemble of one-class classiﬁers. Information
Fusion 9, 1 (2008), 69–82.

[14] Gupta, K. K., Nath, B., and Ramamohanarao,
K. Layered approach using conditional random ﬁelds
for intrusion detection. IEEE Trans. Dependable Sec.
Comput. 7, 1 (2010), 35–49.

[15] Hubballi, N., Biswas, S., and Nandi, S. Layered

higher order n-grams for hardening payload based
anomaly intrusion detection. In ARES (2010),
pp. 321–326.

[16] Hwang, K., Cai, M., Chen, Y., and Qin, M.

Hybrid intrusion detection with weighted signature
generation over anomalous internet episodes. IEEE
Trans. Dependable Sec. Comput. 4, 1 (2007), 41–55.

[17] Jou, Y. F., Gong, F., Sargor, J. G., Wu, X., Wu,

S. F., Chang, H. C., and Wang, F. Design and
implementation of a scalable intrusion detection
system for the protection of network infrastructure. In
In DARPA Information Survivability Conference and
Exposition (2000), pp. 25–27.

[18] Juszczak, P., and Duin, R. P. W. Combining

one-class classiﬁers to classify missing data. In
Multiple Classiﬁer Systems (2004), pp. 92–101.

[19] Knorr, E. M., and Ng, R. T. A uniﬁed notion of
outliers: Properties and computation. In In Proc. of
the International Conference on Knowledge Discovery
and Data Mining (1997), AAAI Press, pp. 219–222.

[20] Kontorovich, A., Hendler, D., and Menahem, E.

Metric anomaly detection via asymmetric risk
minimization. In SIMBAD (2011), pp. 17–30.

[21] Kowalski, J. P., and Warfield, B. Modelling

traﬃc demand between nodes in a telecommunications
network. In in ATNAC (1995).

[22] Kreucher, C., Kastella, K., and III, A. O. H.

Sensor management using an active sensing approach.
Signal Processing 85, 3 (2005), 607 – 624.

[23] Lakhina, A., Crovella, M., and Diot, C. Mining

anomalies using traﬃc feature distributions. In
Proceedings of SIGCOMM (2005), pp. 217–228.

[24] Lee, W., Stolfo, S. J., Chan, P. K., Eskin, E.,
Fan, W., Miller, M., Hershkop, S., and Zhang,
J. Real time data mining-based intrusion detection,
2001.

[25] Leland, W., Taqqu, M., Willinger, W., and
Wilson, D. On the self-similar nature of ethernet
traﬃc (extended version). Networking, IEEE/ACM
Transactions on 2, 1 (feb 1994), 1 –15.

[26] Makarenko, A., and Durrant-whyte, H.

Decentralized data fusion and control in active sensor
networks. In In Proceedings of the Seventh
International Conference on Information Fusion
(2004).

[27] Menahem, E., Nakibly, G., and Elovici, Y.

Degrading the Network’s Quality of Service via Traﬃc
Diversion Attacks. Tech. rep., 12 2012.

[28] Menahem, E., Rokach, L., and Elovici, Y. Troika
- an improved stacking schema for classiﬁcation tasks.
Inf. Sci. 179, 24 (2009), 4097–4122.

[29] Menahem, E., Rokach, L., and Elovici, Y.

Combining one-class classiﬁers via meta-learning. In -
to appear in CIKM‘13 (2013).

[30] Menahem, E., Schclar, A., Rokach, L., and

Elovici, Y. Securing your transactions: Detecting
anomalous patterns in xml documents. CoRR
abs/1209.1797 (2012).

[31] Nair, R., Tambe, M., Yokoo, M., Pynadath,
D. V., and Marsella, S. Taming decentralized
pomdps: Towards eﬃcient policy computation for
multiagent settings. In IJCAI (2003), pp. 705–711.

[32] Nassar, M., State, R., and Festor, O. A

framework for monitoring sip enterprise networks. In
NSS (2010), pp. 1–8.

[33] Olivain, J., and Goubault-Larrecq, J. The
orchids intrusion detection tool. In CAV (2005),
pp. 286–290.

[34] Salem, O., Vaton, S., and Gravey, A. A scalable,
eﬃcient and informative approach for anomaly-based
intrusion detection systems: theory and practice. Int.
Journal of Network Management 20, 5 (2010),
271–293.

[35] Salhi, E., Lahoud, S., and Cousin, B. Joint

optimization of monitor location and network anomaly
detection. In LCN (2010), pp. 204–207.

[36] Schapire, R. E. The strength of weak learnability.

Machine Learning 5 (1990), 197–227.

[37] Sch(cid:127)olkopf, B., Platt, J. C., Shawe-taylor, J.,
Smola, A. J., and Williamson, R. C. Estimating
the support of a high-dimensional distribution, 1999.

[38] Singh, V., Schulzrinne, H., and Miao, K. Dyswis:
An architecture for automated diagnosis of networks.
In Network Operations and Management Symposium,
2008. NOMS 2008. IEEE (2008), IEEE, pp. 851–854.
[39] Sommer, R. Bro: An open source network intrusion

detection system. In DFN-Arbeitstagung ¨uber
Kommunikationsnetze (2003), pp. 273–288.

[40] Sommer, R., and Paxson, V. Outside the closed

world: On using machine learning for network
intrusion detection. In Security and Privacy (SP),
2010 IEEE Symposium on (may 2010), pp. 305 –316.
[41] Spring, N. T., Mahajan, R., and Wetherall, D.

Measuring isp topologies with rocketfuel. In
SIGCOMM (2002), pp. 133–145.

[42] Tax, D. M., and Duin, R. P. Combining one-class

classiﬁers. In in Proc. Multiple Classiﬁer Systems,
2001 (2001), Springer Verlag, pp. 299–308.

[43] Varga, A., and Hornig, R. An overview of the
omnet++ simulation environment. In SimuTools
(2008), p. 60.

[44] Wang, H., Zhang, D., and Shin, K. G.

Change-point monitoring for the detection of dos
attacks. IEEE Trans. Dependable Sec. Comput. 1, 4
(2004), 193–208.

[45] Wang, K., and Stolfo, S. J. Anomalous

payload-based network intrusion detection. In RAID
(2004), pp. 203–222.

[46] Witten, I. H., and Frank, E. Data Mining:

Practical machine learning tools and techniques, 2nd
edition ed. Morgan Kaufmann, San Francisco, 2005.

66