Trustworthy Whole-System Provenance  

for the Linux Kernel

Adam Bates, Dave (Jing) Tian, and Kevin R.B. Butler, University of Florida;  

Thomas Moyer, MIT Lincoln Laboratory

https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/bates

This paper is included in the Proceedings of the 24th USENIX Security SymposiumAugust 12–14, 2015 • Washington, D.C.ISBN 978-1-931971-232Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXTrustworthy Whole-System Provenance for the Linux Kernel

Adam Bates, Dave (Jing) Tian,

Kevin R.B. Butler
University of Florida

Thomas Moyer

MIT Lincoln Laboratory

{adammbates,daveti,butler}@ufl.edu

thomas.moyer@ll.mit.edu

Abstract

In a provenance-aware system, mechanisms gather
and report metadata that describes the history of each ob-
ject being processed on the system, allowing users to un-
derstand how data objects came to exist in their present
state. However, while past work has demonstrated the
usefulness of provenance, less attention has been given
to securing provenance-aware systems. Provenance it-
self is a ripe attack vector, and its authenticity and in-
tegrity must be guaranteed before it can be put to use.

We present Linux Provenance Modules

(LPM),
the ﬁrst general framework for the development of
provenance-aware systems. We demonstrate that LPM
creates a trusted provenance-aware execution environ-
ment, collecting complete whole-system provenance
while imposing as little as 2.7% performance overhead
on normal system operation. LPM introduces new mech-
anisms for secure provenance layering and authenticated
communication between provenance-aware hosts, and
also interoperates with existing mechanisms to provide
strong security assurances. To demonstrate the poten-
tial uses of LPM, we design a Provenance-Based Data
Loss Prevention (PB-DLP) system. We implement PB-
DLP as a ﬁle transfer application that blocks the trans-
mission of ﬁles derived from sensitive ancestors while
imposing just tens of milliseconds overhead. LPM is the
ﬁrst step towards widespread deployment of trustworthy
provenance-aware applications.

1

Introduction

A provenance-aware system automatically gathers and
reports metadata that describes the history of each ob-
ject being processed on the system. This allows users to
track, and understand, how a piece of data came to ex-
ist in its current state. The application of provenance

The Lincoln Laboratory portion of this work was sponsored by the
Assistant Secretary of Defense for Research & Engineering under Air
Force Contract #FA8721-05-C-0002. Opinions, interpretations, con-
clusions and recommendations are those of the author and are not nec-
essarily endorsed by the United States Government.

is presently of enormous interest in a variety of dis-
parate communities including scientiﬁc data processing,
databases, software development, and storage [43, 53].
Provenance has also been demonstrated to be of great
value to security by identifying malicious activity in data
centers [5, 27, 56, 65, 66], improving Mandatory Access
Control (MAC) labels [45, 46, 47], and assuring regula-
tory compliance [3].

Unfortunately, most provenance collection mecha-
nisms in the literature exist as fully-trusted user space
applications [28, 27, 41, 56]. Even kernel-based prove-
nance mechanisms [43, 48] and sketches for trusted
provenance architectures [40, 42] fall short of providing
a provenance-aware system for malicious environments.
The problem of whether or not to trust provenance is fur-
ther exacerbated in distributed environments, or in lay-
ered provenance systems, due to the lack of a mechanism
to verify the authenticity and integrity of provenance col-
lected from different sources.

In this work, we present Linux Provenance Modules
(LPM), the ﬁrst generalized framework for secure prove-
nance collection on the Linux operating system. Mod-
ules capture whole-system provenance, a detailed record
of processes, IPC mechanisms, network activity, and
even the kernel itself; this capture is invisible to the ap-
plications for which provenance is being collected. LPM
introduces a gateway that permits the upgrading of low
integrity workﬂow provenance from user space. LPM
also facilitates secure distributed provenance through an
authenticated, tamper-evident channel for the transmis-
sion of provenance metadata between hosts. LPM inter-
operates with existing security mechanisms to establish a
hardware-based root of trust to protect system integrity.
trustworthy whole-system
provenance, we demonstrate the power of our approach
by presenting a scheme for Provenance-Based Data Loss
Prevention (PB-DLP). PB-DLP allows administrators to
reason about the propagation of sensitive data and control
its further dissemination through an expressive policy
system, offering dramatically stronger assurances than
existing enterprise solutions, while imposing just mil-

Achieving the goal of

USENIX Association  

24th USENIX Security Symposium  319

Figure 1: A provenance graph showing the attack footprint of a malicious binary. Edges encode relationships that ﬂow
backwards into the history of system execution, and writing to an object creates a second node with an incremented
version number. Here, we see that the binary has rewritten /etc/rc.local, likely in an attempt to gain persistence
after a system reboot.

liseconds of overhead on ﬁle transmission. To our knowl-
edge, this work is the ﬁrst to apply provenance to DLP.
Our contributions can thus be summarized as follows:

• Introduce Linux Provenance Modules (LPM).
LPM facilitates secure provenance collection at the
kernel layer, supports attested disclosure at the ap-
plication layer, provides an authenticated channel
for network transmission, and is compatible with
the W3C Provenance (PROV) Model [59]. In eval-
uation, we demonstrate that provenance collection
imposes as little as 2.7% performance overhead.

• Demonstrate secure deployment.

Leveraging
LPM and existing security mechanisms, we create
a trusted provenance-aware execution environment
for Linux. Through porting Hi-Fi [48] and provid-
ing support for SPADE [29], we demonstrate the
relative ease with which LPM can be used to secure
existing provenance collection mechanisms. We
show that, in realistic malicious environments, ours
is the ﬁrst proposed system to offer secure prove-
nance collection.

• Introduce Provenance-Based Data Loss Preven-
tion (PB-DLP). We present a new paradigm for
the prevention of data leakage that searches object
provenance to identify and prevent the spread of
sensitive data. PB-DLP is impervious to attempts
to launder data through intermediary ﬁles and IPC.
We implement PB-DLP as a ﬁle transfer applica-
tion, and demonstrate its ability to query object an-
cestries in just tens of milliseconds.

2 Background

Data provenance, sometimes called lineage, describes
the actions taken on a data object from its creation up
to the present. Provenance can be used to answer a va-
riety of historical questions about the data it describes.
Such questions include, but are not limited to, “What
processes and datasets were used to generate this data?"

and “In what environment was the data produced?" Con-
versely, provenance can also answer questions about the
successors of a piece of data, such as “What objects on
the system were derived from this object?" Although po-
tential applications for such information are nearly lim-
itless, past proposals have conceptualized provenance in
different ways, indicating that a one-size-ﬁts-all solution
to provenance collection is unlikely to meet the needs of
all of these audiences. We review these past proposals
for provenance-aware systems in Section 8.

The commonly accepted representation for data prove-
nance is a directed acyclic graph (DAG). In this work, we
use the W3C PROV-DM speciﬁcation [59] because it is
pervasive and facilitates the exchange of provenance be-
tween deployments. An example PROV-DM graph of a
malicious binary is shown in Figure 1. This graph de-
scribes an attack in which a binary running with root
privilege reads several sensitive system ﬁles, then ed-
its those ﬁles in an attempt to gain persistent access to
the host. Edges encode relationships between nodes,
pointing backwards into the history of system execution.
Writing to an object triggers the creation of a second ob-
ject node with an incremented version number. This par-
ticular provenance graph could serve as a valuable foren-
sics tool, allowing system administrators to better under-
stand the nature of a network intrusion.

2.1 Data Loss Prevention

Data Loss Prevention (DLP) is enterprise software that
seeks to minimize the leakage of sensitive data by moni-
toring and controlling information ﬂow in large, complex
organizations [1].1 In addition to the desire to control in-
tellectual property, another motivator for DLP systems
is demonstrating regulatory compliance for personally-
identiﬁable information (PII),2 as well as directives such

1 Our overview of data loss prevention is based on review of pub-
licly available product descriptions for software developed by Bit9,
CDW, Cisco, McAfee, Symantec, and Titus.

2 See NIST SP 800-122

320  24th USENIX Security Symposium 

USENIX Association

2

UsedUsedUsedUsedUsedWasControlledByWasGeneratedByWasGeneratedByWasGeneratedByWasGeneratedByWasGeneratedBy/etc/rc.local:0/bin/ps:0/var/spool/cron/root:0/etc/passwd:0/etc/shadow:0Malicious Binary/etc/rc.local:1/bin/ps:1/var/spool/cron/root:1/etc/passwd:1/etc/shadow:1rootas PCI,3 HIPAA,4 SOX.5 or E.U. Data Protection.6 As
encryption can be used to protect data at rest from unau-
thorized access, the true DLP challenge involves prevent-
ing leakage at the hands of authorized users, both mali-
cious and well-meaning agents. This latter group is a
surprisingly big problem in the ﬁght to control an organi-
zation’s intellectual property; a 2013 study conducted by
the Ponemon Institute found that over half of companies’
employees admitted to emailing intellectual property to
their personal email accounts, with 41 percent admitting
to doing so on a weekly basis [2].
It is therefore im-
portant for a DLP system to be able to exhaustively ex-
plain which pieces of data are sensitive, where that data
has propagated to within the organization, and where it
is (and is not) permitted to ﬂow.

As DLP systems are proprietary and are marketed so
as to abstract away the complex details of their inter-
nal workings, we cannot offer a complete explanation of
their core features. However, some of the mechanisms
in such systems are known. Many DLP products use a
regular expression-based approach to identify sensitive
data, operating similarly to a general-purpose version of
Cornell’s Spider7. For example, in PCI compliance,3
DLP might attempt to identify credit card numbers in
outbound emails by searching for 16 digit numbers that
pass a Mod-10 validation check [39]. Other DLP systems
use a label-based approach to identify sensitive data, tag-
ging document metadata with security labels. The Titus
system accomplishes this by having company employees
manually annotate the documents that they create;8 plug-
ins for applications (e.g., Microsoft Ofﬁce) then prevent
the document from being transmitted to or opened by
other employees that lack the necessary clearance. In ei-
ther approach, DLP software is difﬁcult to conﬁgure and
prone to failure, offering marginal utility at great price.

3 Linux Provenance Modules

To serve as the foundation for secure provenance-aware
systems, we present Linux Provenance Modules (LPM).
We provide a working deﬁnition for the provenance our
system will collect in §3.1. In §3.2 we consider the ca-
pabilities and aims of a provenance-aware adversary, and
identify security and design goals in §3.3. The LPM de-
sign is presented in §3.4, and in §3.5 we demonstrate its
secure deployment. An expanded description of our sys-
tem is available in our technical report [8].

3 See https://www.pcisecuritystandards.org
4 See http://www.hhs.gov/ocr/privacy
5 Short for the Sarbanes-Oxley Act, U.S. Public Law No. 107-20
6 See EU Directive 95/46/EC
7 See http://www2.cit.cornell.edu/security/tools
8 See http://www.titus.com

3.1 Deﬁning Whole-System Provenance

In the design of LPM, we adopt a model for whole-
system provenance9 that is broad enough to accom-
modate the needs of a variety of existing provenance
projects. To arrive at a deﬁnition, we inspect four
past proposals that collect broadly scoped provenance:
SPADE [29], LineageFS [53], PASS [43], and Hi-Fi [48].
SPADE provenance is structured around primitive oper-
ations of system activities with data inputs and outputs.
It instruments ﬁle and process system calls, and asso-
ciates each call to a process ID (PID), user identiﬁer, and
network address. LineageFS uses a similar deﬁnition,
associating process IDs with the ﬁle descriptors that the
process reads and writes. PASS associates a process’s
output with references to all input ﬁles and the command
line and process environment of the process; it also ap-
pends out-of-band knowledge such as OS and hardware
descriptions, and random number generator seeds, if pro-
vided.
In each of these systems, networking and IPC
activity is primarily reﬂected in the provenance record
through manipulation of the underlying ﬁle descriptors.
Hi-Fi takes an even broader approach to provenance,
treating non-persistent objects such as memory, IPC, and
network packets as principal objects.

We observe that, in all instances, provenance-aware
systems are exclusively concerned with operations on
controlled data types, which are identiﬁed by Zhang et
al.
as ﬁles, inodes, superblocks, socket buffers, IPC
messages, IPC message queue, semaphores, and shared
memory [64]. Because controlled data types represent a
super set of the objects tracked by system layer prove-
nance mechanisms, we deﬁne whole-system provenance
as a complete description of agents (users, groups) con-
trolling activities (processes) interacting with controlled
data types during system execution.

3.2 Threat Model & Assumptions

We consider an adversary that has gained remote access
to a provenance-aware host or network. Once inside the
system, the attacker may attempt to remove provenance
records, insert spurious information into those records,
or ﬁnd gaps in the provenance monitor’s ability to record
information ﬂows. A network attacker may also attempt
to forge or strip provenance from data in transit. Be-
cause captured provenance can be put to use in other ap-
plications, the adversary’s goal may even be to target the
provenance monitor itself. The implications and meth-
ods of such an attack are domain-speciﬁc. For example:

9This term is coined in [48], but not explicitly deﬁned. We demon-
strate the concrete requirements of a collection mechanism for whole-
system provenance in this work.

USENIX Association  

24th USENIX Security Symposium  321

3

• Scientiﬁc Computing: An adversary may wish to ma-
nipulate provenance in order to commit fraud, or to in-
ject uncertainty into records to trigger a “Climategate”-
like controversy [50].

• Access Control: When used to mediate access deci-
sions [7, 45, 46, 47], an attacker could tamper with
provenance in order to gain unauthorized access, or to
perform a denial-of-service attack on other users by ar-
tiﬁcially escalating the security level of data objects.

• Networks: Provenance metadata can also be associ-
ated with packets in order to better understand network
events in distributed systems [5, 65, 66]. Coordinating
multiple compromised hosts, an attacker may attempt
to send unauthenticated messages to avoid provenance
generation and to perform data exﬁltration.

We deﬁne a provenance trusted computing base (TCB)
to be the kernel mechanisms, provenance recorder, and
storage back-ends responsible for the collection and
management of provenance. Provenance-aware appli-
cations are not considered part of the TCB.

We make the following assumption with regards to the
TCB. In Linux, kernel modules have unrestricted access
to kernel memory, meaning that there is no mechanism
for protecting LPM from the rest of the kernel. The ker-
nel code is therefore trusted; we assume that the stock
kernel will not seek to tamper with the TCB. However,
we do consider the possibility that the kernel could be
compromised after installation through its interactions
with user space applications. To facilitate host attestation
in distributed environments, we also assume access to
a Public Key Infrastructure (PKI) for provenance-aware
hosts to publish their public signing keys.

3.3 System Goals
We set out to provide the following security assurances
in the design of of our system-layer provenance collec-
tion mechanism. McDaniel et al.
liken the needs of a
secure provenance monitor [42] to the reference monitor
guarantees laid out by Anderson [4]: complete media-
tion, tamperproofness, and veriﬁability. We deﬁne these
guarantees as follows:

G1 Complete. Complete mediation for provenance has
been discussed elsewhere in the literature in terms
of assuring completeness [32]: that the provenance
record be gapless in its description of system activ-
ity. To facilitate this, LPM must be able to observe
all information ﬂows that pass through controlled
data types.

G2 Tamperproof. As many provenance use cases in-
volve enhancing system security, LPM will be an

Prov. Aware 
Applications

IMA

TPM

user space
GZip

SQL

Neo4j

SNAP
kernel space

Provenance
Recorder

Relay
Buffer

Prov. Module

Prov. Hooks

NF Hooks

System Provenance
Integrity Measurements

Workﬂow Provenance

Figure 2: Diagram of the LPM Framework. Kernel
hooks report provenance to a recorder in userspace,
which uses one of several storage back-ends.
The
recorder is also responsible for evaluating the integrity
of workﬂow provenance prior to storing it.

adversarial target. The TCB must therefore be im-
pervious to disabling or manipulation by processes
in user space.

G3 Veriﬁable. The functionality of LPM must be
veriﬁably correct. Additionally, local and remote
users should be able to attest whether the host with
which they are communicating is running the se-
cured provenance-aware kernel.

Through surveying past work in provenance-aware
systems, we identify the following additional goals to
support whole-system provenance:

G4 Authenticated Channel.

In distributed environ-
ments, provenance-aware systems must provide a
means of assuring authenticity and integrity of
provenance as it is communicated over open net-
works [7, 42, 48, 65]. While we do not seek to
provide a complete distributed provenance solution
in LPM, we do wish to provide the required build-
ing blocks within the host for such a system to ex-
ist. LPM must therefore be able to monitor ev-
ery network message that is sent or received by the
host, and reliably explain these messages to other
provenance-aware hosts in the network.

G5 Attested Disclosure. Layered provenance, where
additional metadata is disclosed from higher opera-
tional layers, is a desirable feature in provenance-
aware systems, as applications are able to report
workﬂow semantics that are invisible to the oper-
ating system [44]. LPM must provide a gateway for

322  24th USENIX Security Symposium 

USENIX Association

4

upgrading low integrity user space disclosures be-
fore logging them in the high integrity provenance
record. This is consistent with the Clark-Wilson In-
tegrity model for upgrading or discarding low in-
tegrity inputs [17].

In order to bootstrap trust in our system, we have im-
plemented LPM as a parallel framework to Linux Secu-
rity Modules (LSM) [60, 61]. Building on these results,
we show in Section 4 that this approach allows LPM to
inherit the formal assurances that have been veriﬁed for
the LSM architecture.

3.4 Design & Implementation
An overview of the LPM architecture is shown in Fig-
ure 2. The LPM patch places a set of provenance hooks
around the kernel; a provenance module then registers
to control these hooks, and also registers several Netﬁl-
ter hooks; the module then observes system events and
transmits information via a relay buffer to a provenance
recorder in user space that interfaces with a datastore.
The recorder also accepts disclosed provenance from ap-
plications after verifying their correctness using the In-
tegrity Measurements Architecture (IMA) [52].

In designing LPM, we ﬁrst considered using an exper-
imental patch to the LSM framework that allows “stack-
ing” of LSM modules 10. However, at this time, no stan-
dard exists for handling when modules make conﬂict-
ing decisions, creating the potential unpredicted behav-
ior. We also felt that dedicated provenance hooks were
necessary; by collecting provenance after LSM autho-
rization routines, we ensure that the provenance history
is an accurate description of authorized system events. If
provenance collection occurred during authorization, as
would be the case with stacked LSMs, it would not be
possible to provide this property.

3.4.1 Provenance Hooks

The LPM patch introduces a set of hook functions in the
Linux kernel. These hooks behave similarly to the LSM
framework’s security hooks in that they facilitate mod-
ularity, and default to taking no action unless a module
is enabled. Each provenance hook is placed directly be-
neath a corresponding security hook. The return value of
the security hook is checked prior to calling the prove-
nance hook, thus assuring that the requested activity has
been authorized prior to provenance capture; we consider
the implications of this design in Section 4. A workﬂow
for the hook architecture is depicted in Figure 3. The
LPM patch places over 170 provenance hooks, one for
each of the LSM authorization hooks. In addition to the

10See https://lwn.net/Articles/518345/

user space

kernel space

Text Editor

open System Call

Look Up Inode

Error Checks

DAC Checks

LSM Module
Examine context.
Does request pass policy?
Grant or deny.

LPM Module
Examine context.
Collect provenance.
If successful, grant.

LSM Hook

"Authorized?"
Yes or No

LPM Hook

"Prov collected?"

Yes or No

Access Inode

Figure 3: Hook Architecture for the open system call.
Provenance is collected after DAC and LSM checks, en-
suring that it accurately reﬂects system activity. LPM
will only deny the operation if it fails to generate prove-
nance for the event.

hooks that correspond to existing security hooks, we also
support Pohly et al.’s Hi-Fi [48] hook that is necessary to
preserve Lamport timestamps on network messages [38].

3.4.2 Netﬁlter Hooks
LPM uses Netﬁlter hooks to implement a cryptographic
message commitment protocol.
In Hi-Fi, provenance-
aware hosts communicated by embedding a provenance
sequence number in the IP options ﬁeld [49] of each out-
bound packet [48]. This approach allowed Hi-Fi to com-
municate as normal with hosts that were not provenance-
aware, but unfortunately was not secure against a net-
work adversary. In LPM, provenance sequence numbers
are replaced with Digital Signature Algorithm (DSA)
signatures, which are space-efﬁcient enough to embed in
the IP Options ﬁeld. We have implemented full DSA
support in the Linux kernel by creating signing rou-
tines to use with the existing DSA veriﬁcation func-
tion. DSA signing and veriﬁcation occurs in the NetFil-
ter inet_local_out and inet_local_in hooks.
In inet_local_out, LPM signs over the immutable
ﬁelds of the IP header, as well as the IP payload.
In
inet_local_in, LPM checks for the presence of a
signature, then veriﬁes the signature against a conﬁg-
urable list of public keys. If the signature fails, the packet
is dropped before it reaches the recipient application,
thus ensuring that there are no breaks in the continuity of
the provenance log. The key store for provenance-aware
hosts is obtained by a PKI and transmitted to the ker-
nel during the boot process by writing to securityfs.
LPM registers the Netﬁlter hooks with the highest prior-

USENIX Association  

24th USENIX Security Symposium  323

5

ity levels, such that signing occurs just before transmis-
sion (i.e., after all other IPTables operations), and sig-
nature veriﬁcation occurs just after the packet enters the
interface (i.e., before all other IPTables operations).

3.4.3 Provenance Modules
Here, we introduce two of our own provenance modules
(Provmon, SPADE), as well as brieﬂy mention the work
of our peers (UPTEMPO):

• Provmon. Provmon is an extended port of the Hi-Fi
security module [48]. The original Hi-Fi code base
was 1,566 lines of code, requiring 723 lines to be mod-
iﬁed in the transition. Our extensions introduced 728
additional lines of code. The process of porting did
not affect the module’s functionality, although we have
subsequently extended the Hi-Fi protocol to capture
additional lineage information:
File Versioning. The original Hi-Fi protocol did not
track version information for ﬁles, leading to uncer-
tainty as to the exact contents of a ﬁle at the time it
was read. Accurately recovering this information in
user space was not possible due to race conditions be-
tween kernel events. Because versioning is necessary
to break cycles in provenance graphs [43], we have
added a version ﬁeld to the provenance context for in-
odes, which is incremented on each write.
Network Context. Hi-Fi omitted remote host address
information for network events, reasoning that source
information could be forged by a dishonest agent in the
network. These human-interpretable data points were
replaced with an assigned random identiﬁer for each
packet. We found, however, that these identiﬁers could
not be interpreted without remote address information,
and incorporated the recording of remote IP addresses
and ports into Provmon.

• SPADE. The SPADE system is an increasingly pop-
ular option for provenance auditing, but collecting
provenance in user space limits SPADE’s expressive-
ness and creates the potential for incomplete prove-
nance.
To address this limitation, we have cre-
ated a mechanism that reports LPM provenance into
SPADE’s Domain-Speciﬁc Language pipe [29]. This
permits the collection of whole-system provenance
while simultaneously leveraging SPADE’s existing
storage, remote query, and visualization utilities.

• Using Provenance to Expedite MAC Policies (UP-
TEMPO). Using LPM as a collection mechanism,
Moyer et al.
investigate provenance analysis as a
means of administrating Mandatory Access Control
(MAC) policies [54]. UPTEMPO ﬁrst observes system
execution in a sterile environment, aggregating LPM

provenance in a centralized data store. It then recov-
ers the implicit information ﬂow policy through min-
ing the provenance store to generate a MAC policy for
the distributed system, decreasing both administrator
effort and the potential for misconﬁguration.

3.4.4 Provenance Recorders
LPM provides modular support for different storage
through provenance recorders. To prevent an inﬁnite
provenance loop, recorders are ﬂagged as provenance-
opaque [48] using the security.provenance ex-
tended attribute, which is checked by LPM before creat-
ing a new event. Each recorder was designed to be as ag-
nostic to the active LPM as possible, making them easy
to adapt to new modules.

We currently provide provenance recorders that of-
fer backend storage for Gzip, PostGreSQL, Neo4j, and
SNAP. Commentary on our PostGreSQL and Neo4j re-
porters can be found in our technical report [8]. We make
use of the Gzip and SNAP recorders during our evalua-
tion in Section 6.

The Gzip recorder incurs low storage overheads and
fast insertion speeds. On our test bed, we observed
this recorder processing up to 400,000 events per sec-
ond from the Provmon provenance stream. However, be-
cause the provenance is not stored in an easily queried
form, this back-end is best suited for environments where
queries are an ofﬂine process.

To create graph storage that was efﬁcient enough for
LPM, we used the SNAP graphing library11 to design
a recorder that maintains an in-memory graph database
that is fully compliant with the W3C PROV-DM Model
[59]. We have observed insertion speeds of over 150,000
events per second using the SNAP recorder, and highly
efﬁcient querying as well. This recorder is further evalu-
ated in Section 6.

3.4.5 Workﬂow Provenance
To support layered provenance while preserving our se-
curity goals, we require a means of evaluating the in-
tegrity of user space provenance disclosures. To ac-
complish this, we extend the LPM Provenance Recorder
to use the Linux Integrity Measurement Architecture
(IMA) [35, 52]. IMA computes a cryptographic hash of
each binary before execution, extends the measurement
into a TPM Platform Control Register (PCR), and stores
the measurement in kernel memory. This set of measure-
ments can be used by the Recorder to make a decision
about the integrity of the a Provenance-Aware Applica-
tion (PAA) prior to accepting the disclosed provenance.
When a PAA wishes to disclose provenance, it opens a

11See http://snap.stanford.edu

324  24th USENIX Security Symposium 

USENIX Association

6

Figure 4: A provenance graph of image conversion.
Here, workﬂow provenance (WasDerivedFrom) encodes
a relationship that more accurately identiﬁes the output
ﬁles’ dependencies compared to only using kernel layer
observations (Used, WasGeneratedBy).

new UNIX domain socket to send the provenance data
to the Provenance Recorder. The Recorder uses its own
UNIX domain socket to recover the process’s pid, then
uses the /proc ﬁlesystem to ﬁnd the full path of the bi-
nary, then uses this information to look up the PAA in
the IMA measurement list. The disclosed provenance is
recorded only if the signature of PAA matches a known-
good cryptographic hash.

As a demonstration of this functionality, we created
a provenance-aware version of the popular ImageMag-
ick utility 12. ImageMagick contains a batch conversion
tool for image reformatting, mogrify. Shown in Fig-
ure 4, mogrify reads and writes multiple ﬁles during
execution, leading to an overtainting problem – at the
kernel layer, LPM is forced to conservatively assume
that all outputs were derived from all inputs, creating
false dependencies in the provenance record. To address
this, we extended the Provmon protocol to support a
new message, provmsg_imagemagick_convert,
which links an input ﬁle directly to its output ﬁle. When
the recorder receives this message, it ﬁrst checks the list
of IMA measurements to conﬁrm that ImageMagick is
in a good state. If successful, it then annotates the exist-
ing provenance graph, connecting the appropriate input
and output objects with WasDerivedFrom relationships.
Our instrumentation of ImageMagick demonstrates that
LPM supports layered provenance at no additional cost
over other provenance-aware systems [29, 43], and does
so in a manner that provides assurance of the integrity of
the provenance log.

3.5 Deployment

We now demonstrate how we used LPM in the deploy-
ment of a secure provenance-aware system. Additional
background on the security technologies use in our de-
ployment can be found in our technical report [8].

12See http://www.imagemagick.org

3.5.1 Platform Integrity

We conﬁgured LPM to run on a physical machine with
a Trusted Platform Module (TPM). The TPM provides a
root of trust that allows for a measured boot of the sys-
tem. The TPM also provides the basis for remote attes-
tations to prove that LPM was in a known hardware and
software conﬁguration. The BIOS’s core root of trust for
measurement (CRTM) bootstraps a series of code mea-
surements prior to the execution of each platform com-
ponent. Once booted, the kernel then measures the code
for user space components (e.g., provenance recorder)
before launching them, through the use of the Linux In-
tegrity Measurement Architecture (IMA)[52]. The result
is then extended into TPM PCRs, which forms a veriﬁ-
able chain of trust that shows the integrity of the system
via a digital signature over the measurements. A remote
veriﬁer can use this chain to determine the current state
of the system using TPM attestation.

We conﬁgured the system with Intel’s Trusted Boot
(tboot),13 which provides a secure boot mechanism, pre-
venting system from booting into the environment where
critical components (e.g., the BIOS, boot loader and
the kernel) are modiﬁed.
Intel tboot relies on the In-
tel TXT14 to provide a secure execution environment.
15 Additionally, we compiled support for IMA into the
provenance-aware kernel, which is necessary in order for
the LPM Recorder to be able to measure the integrity of
provenance-aware applications.

3.5.2 Runtime Integrity

After booting into the provenance-aware kernel, the run-
time integrity of the TCB (deﬁned in §3.2) must also be
assured. To protect the runtime integrity of the kernel,
we deploy a Mandatory Access Control (MAC) policy,
as implemented by Linux Security Modules. On our pro-
totype deployments, we enabled SELinux’s MLS policy,
the security of which was formally modeled by Hicks et
al.
[33]. Reﬁning the SELinux policy to prevent Ac-
cess Vector Cache (AVC) denials on LPM components
required minimal effort; the only denial we encountered
was when using the PostgreSQL recorder, which was
quickly remedied with the audit2allow tool. Pre-
serving the integrity of LPM’s user space components,
such as the provenance recorder, was as simple as creat-
ing a new policy module. We created a policy module to
protect the LPM recorder and storage back-end using the
sepolicy utility. Uncompiled, the policy module was
only 135 lines.

13 See http://sf.net/projects/tboot
14 See https: //www.kernel.org/doc/Documentation/intel_txt.txt
15For virtual environments, similar functionality can be provided on
Xen via TPM sealing and the virtual TPM (vTPM), which is bound to
the physical TPM of the host system.

USENIX Association  

24th USENIX Security Symposium  325

7

UsedUsedWasDerivedFromWasGeneratedByWasDerivedFromWasGeneratedBya.pngb.pngmogrify -format jpg *.pnga.jpgb.jpg4 Security

In this section, we demonstrate that our system meets
all of the required security goals for trustworthy whole-
system provenance. In this analysis, we consider an LPM
deployment on a physical machine that was enabled with
the Provmon module and has been conﬁgured to the con-
ditions described in Section 3.5.
Complete (G1). We deﬁned whole-system provenance
as a complete description of agents (users, groups) con-
trolling activities (processes) interacting with controlled
data types during system execution (§ 3.1). LPM at-
tempts to track these system objects through the place-
ment of provenance hooks (§3.4.1), which directly fol-
low each LSM authorization hook. The LSM’s complete
mediation property has been formally veriﬁed [20, 64];
in other words, there is an authorization hook prior to
every security-sensitive operation. Because every inter-
action with a controlled data type is considered security-
sensitive, we know that a provenance hook resides on
all control paths to the provenance-sensitive operations.
LPM is therefore capable of collecting complete prove-
nance on the host.

It is important to note that, as a consequence of plac-
ing provenance hooks beneath authorization hooks, LPM
is unable to record failed access attempts. However, in-
serting the provenance layer beneath the security layer
ensures accuracy of the provenance record. Moreover,
failed authorizations are a different kind of metadata than
provenance because they do not describe processed data;
this information is better handled at the security layer,
e.g., by the SELinux Access Vector Cache (AVC) Log.
Tamperproof (G2). The runtime integrity of the LPM
trusted computing base is assured via the SELinux MLS
policy, and we have written a policy module that protects
the LPM user space components (§3.5.2). Therefore, the
only way to disable LPM would be to reboot the sys-
tem into a different kernel; this action can be disallowed
through secure boot techniques,13 and is detectable by
remote hosts via TPM attestation (§3.5.1).
Veriﬁable (G3). While we have not conducted an in-
dependent formal veriﬁcation of LPM, our argument for
its correctness is as follows. A provenance hook follows
each LSM authorization hook in the kernel. The correct-
ness of LSM hook placement has been veriﬁed through
both static and dynamic analysis techniques [20, 25, 34].
Because an authorization hook exists on the path of ev-
ery sensitive operation to controlled data types, and LPM
introduces a provenance hook behind each authorization
hook, LPM inherits LSM’s formal assurance of complete
mediation over controlled data types. This is sufﬁcient
to ensure that LPM can collect the provenance of every
sensitive operation on controlled data types in the kernel
(i.e., whole-system provenance).

Authenticated Channel (G4). Through use of Net-
ﬁlter hooks [57], LPM embeds a DSA signature in ev-
ery outbound network packet. Signing occurs immedi-
ately prior to transmission, and veriﬁcation occurs im-
mediately after reception, making it impossible for an
adversary-controlled application running in user space
to interfere. For both transmission and reception, the
signature is invisible to user space.
Signatures are
removed from the packets before delivery, and LPM
feigns ignorance that the options ﬁeld has been set if
get_options is called. Hence, LPM can enforce that
all applications participate in the commitment protocol.
Prior to implementing our own message commitment
protocol in the kernel, we investigated a variety of ex-
isting secure protocols. The integrity and authenticity of
provenance identiﬁers could also be protected via IPsec
[36], SSL tunneling,16 or other forms of encapsulation
[5, 65]. We elected to move forward with our approach
because 1) it ensures the monitoring of all all processes
and network events, including non-IP packets, 2) it does
not change the number of packets sent or received, en-
suring that our provenance mechanism is minimally in-
vasive to the rest of the Linux network stack, and 3)
it preserves compatibility with non-LPM hosts. An al-
ternative to DSA signing would be HMAC [9], which
offers better performance but requires pairwise keying
and sacriﬁces the non-repudiation policy; BLS, which
approaches the theoretical maximum security parame-
ter per byte of signature [12]; or online/ofﬂine signature
schemes [15, 23, 26, 55].

Authenticated Disclosures (G5). We make use
of IMA to protect
the channel between LPM and
provenance-aware applications wishing to disclose
provenance.
IMA is able to prove to the provenance
recorder that the application was unmodiﬁed at the time
it was loaded into memory, at which point the recorder
can accept the provenance disclosure into the ofﬁcial
record. If the application is known to be correct (e.g.,
through formal veriﬁcation), this is sufﬁcient to estab-
lish the runtime integrity of the application. However, if
the application is compromised after execution, this ap-
proach is unable to protect against provenance forgery.

A separate consideration for all of the above security
properties are Denial of Service (DoS) attacks. DoS at-
tacks on LPM do not break its security properties. If an
attacker launches a resource exhaustion attack in order
to prevent provenance from being collected, all kernel
operations will be disallowed and the host will cease to
function. If a network attacker tampers with a packet’s
provenance identiﬁer, the packet will not be delivered to
the recipient application.
In all cases, the provenance
record remains an accurate reﬂection of system events.

http://docs.oracle.com/cd/E23823_01/html/816-5175/kssl-

16See
5.html

8

326  24th USENIX Security Symposium 

USENIX Association

Locations = [ ]
for each s in a,FindSuccessors(a) do

Algorithm 1 Summarizes a’s propagation through the system.
Require: a is an entity
1: procedure REPORT(a)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11: end procedure

Locations.Add(< s.remote_ip,s.port >)

else if s.type is Network Packet then

Locations.Add(< s.disk,s.directory >)

end for
return Locations

if s.type is File then

end if

(cid:31) Assigns an empty list.

5 LPM Application: Provenance-Based

Data Loss Prevention

To further demonstrate the power of LPM, we now in-
troduce a set of mechanisms for Provenance-Based Data
Loss Prevention (PB-DLP) that offer dramatically sim-
pliﬁed administration and improved enforcement over
existing DLP systems. A provenance-based approach is
a novel and effective means of handling data loss preven-
tion; to our knowledge, we are the ﬁrst in the literature
to do so. The advantage of our approach when compared
to existing systems is that LPM-based provenance-aware
systems already perform system-wide capture of infor-
mation ﬂows between kernel objects. Data loss preven-
tion in such a system therefore becomes a matter of pre-
venting all derivations of a sensitive source entity, e.g., a
Payment Card Industry (PCI) database, from being writ-
ten to a monitored destination entity (e.g., a network in-
terface).

We begin by deﬁning a policy format for PB-DLP. In-

dividual rules take the form

< Srcs = [src1,src2, . . . ,src n],dst >

where Srcs is a list of entities representing persistent
data objects, and dst is a single entity representing either
a persistent data object such as a ﬁle or interface or an ab-
stract entity such as a remote host. The goal for PB-DLP
is as follows – an entity e1 with ancestors A is written
to entity e2 if and only if A (cid:31)⊇ Srcs for all rules in the
rule set where e2 = dst. The reason that sources are ex-
pressed as sets is that, at times, the union of information
is more sensitive than its individual components. For ex-
ample, sharing a person’s last name or birthdate may be
permissible, while sharing the last name and birthdate is
restricted as PII.2

Below, we deﬁne the functions that realize this goal.
First, we deﬁne two provenance-based functions as the
basis for a DLP monitoring phase, which allows admin-
istrators to learn more about the propagation of sensitive
data on their systems. Then, we deﬁne mechanisms for a
DLP enforcement phase.

if d = rule.dst then

for each rule in Rules do

A = FindAncestors(e)
NumSrcs = length(rule.Srcs)
for each src in rule.Srcs do

Algorithm 2 Mediates request to write e to d given Rules.
Require: e,d are entities
Require: Rules is a PB-DLP policy
1: procedure PROVWRITE(e,d,Rules)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end procedure

end if
end for
if NumSrcs = 0 then

end for
return PB-DLP_PERMIT

if src in A then
NumSrcs−−

return PB-DLP_DENY

end if

end if

(cid:31) A ⊇ Srcs, deny.

(cid:31) A (cid:31)⊇ Srcs, permit.

5.1 Monitoring Phase
The goal of monitoring is to allow administrators to rea-
son about how sensitive data is stored and put to use on
their systems. The end product of the monitor phase is a
set of rules (a policy) that restrict the permissible ﬂows
for sensitive data sources. Monitoring is an ongoing pro-
cess in DLP, where administrators attempt to iteratively
improve protection against data leakage. The ﬁrst step is
to identify the data that needs protection. Identifying the
source of such information is often quite simple; for ex-
ample, a database of PCI or PII data. However, reliably
ﬁnding data objects that were derived from this source is
extraordinarily complicated using existing solutions, but
is simple now with LPM. To begin, we deﬁne a helper
function for system monitoring:

1. FindSuccessors(Entity): This function performs a
provenance graph traversal to obtain the list of data
objects derived from Entity.

FindSuccessors can then be used as the basis for a

function that summarizes the spread of sensitive data:

2. Report(Entity): List the locations that a target object
and its successors have propagated. This function is
deﬁned in Algorithm 1.

The information provided by Report is similar to the
data found in the Symantec DLP Dashboard [1], and
could be used as the backbone of a PB-DLP user inter-
face. Administrators can use this information to write a
PB-DLP policy or revise an existing one.

5.2 Enforcement Phase
Possessing a PB-DLP policy, the goal of the enforcement
phase is to prevent entities that were derived from sensi-
tive sources from being written to restricted locations. To

USENIX Association  

24th USENIX Security Symposium  327

9

do so, we need to inspect the object’s provenance to dis-
cover the entities from which it was derived. We deﬁne
the following helper function:

3. FindAncestors(Entity): This function performs a
provenance graph traversal to obtain the list of data
objects used in the creation of Entity.

FindAncestors can be then used as the basis for a func-

tion that prevents the spread of sensitive data:

4. ProvWrite(Entity, Destination, Rules): Write the tar-
get entity to the destination if and only if it is valid to
the provided rule set, as deﬁned in Algorithm 2.

5.3 File Transfer Application
In many enterprise networks that are isolated from the
Internet via ﬁrewalls and proxies, it is desirable to share
ﬁles with external users. File transfer services are one
way to achieve this, and provide a single entry/exit point
to the enterprise network where ﬁles being transferred
can be examined before being released.17
In the case
of incoming ﬁles, scans can check for known malware,
and in some cases, check for other types of malicious
behavior from unknown malware.

We implemented PB-DLP as a ﬁle transfer applica-
tion for provenance-aware systems using LPM’s Prov-
mon module. The application interfaced with LPM’s
SNAP recorder using a custom API. Before permitting
a ﬁle to be transmitted to a remote host, the application
ran a query that traversed WasDerivedFrom edges to re-
turn a list of the ﬁle’s ancestors, permitting the transfer
only if the ﬁle was not derived from a restricted source.
PB-DLP allows internal users to share data, while ensur-
ing that sensitive data is not exﬁltrated in the process.

Because provenance graphs continue to grow indeﬁ-
nitely over time, in practice the bottleneck of this appli-
cation is the speed of provenance querying. We evaluate
the performance of PB-DLP queries in Section 6.3.

5.4 PB-DLP Analysis
Below, we select two open source systems that approx-
imate label based and regular expression (regex) based
DLP solutions, and compare their beneﬁts to PB-DLP.

5.4.1 Label-Based DLP

The SELinux MLS policy [31] provides information ﬂow
security through a label-based approach, and could be
used to approximate a DLP solution without relying on

commercial products. Proprietary label-based DLP sys-
tems rely on manual annotations provided by users, re-
quiring them to provide correct labeling based on their
knowledge of data content. Using SELinux as an exem-
plar labeling system is therefore an extremely conserva-
tive approach to analysis.

Within an MLS system, each subject, and object, is
assigned a classiﬁcation level, and categories, or com-
partments. Consider an example system, with classi-
ﬁcation levels, {A,B} with A dominating B, and com-
partments {α,β}. We can model our policy as a lat-
tice, where each node in the lattice is a tuple of the
form {< level >,{compartments}}. Once the policy
is deﬁned, it is possible to enforce the simple and *-
properties. If a user has access to data with classiﬁcation
level A, and compartment α, he cannot read anything in
compartment {β} (no read-up). Furthermore, when data
is accessed in A,{α}, the user cannot write anything to
B,{α} (no write-down).
In order to use SELinux’s MLS enforcement as a DLP
solution, the administrator conﬁgures the policy to en-
force the constraint that no data of speciﬁed types can
be sent over the network. However, this is difﬁcult in
practice. Consider an example system that processes PII.
The users of the system may need to access information,
such as last names, and send these to the payroll depart-
ment to ensure that each employee receives a paycheck.
Separately, the user may need to send a list of birthdays
to another user in the department to coordinate birthday
celebrations for each month. Either of these activities are
acceptable (Figure 5, Decision Condition 2). However,
it is common practice for organizations to have stricter
sharing policies for data that contains multiple forms of
PII, so while either of these identiﬁers could be transmit-
ted in isolation, the two pieces of information combined
could not be shared (Figure 5, Decision Condition 3).

The MLS policy cannot easily handle this type of data
fusion. In order to provide support for correctly label-
ing fused data, an administrator would need to deﬁne the
power set of all compartments within the MLS policy.
In the example above, the administrator would deﬁne the
following compartments: {}, {α}, {β}, {α,β}. In the
default conﬁguration SELinux supports 256 unique cate-
gories, meaning an SELinux DLP policy could only sup-
port eight types of data. Furthermore, the MLS policy
does not support deﬁning multiple categories within a
single sensitivity level18. This implies that the MLS pol-
icy cannot support having a security level for A,{α} and
for A,{α,β}. Instead, the most restrictive labeling must
be deﬁned to protect the data on the system. In contrast,
PB-DLP can support an arbitrary number of data fusions.

17 Two examples of vendors that provide this capability are FireEye

(http://www.ﬁreeye.com) and Accellion (http://www.accellion.com/)

18See

the

deﬁnition

of

level

statements

at http://

selinuxproject.org/page/MLSStatements

328  24th USENIX Security Symposium 

USENIX Association

10

1

Training_Data:0

Birth_Dates:0

SSNs:0

2

Used
Used

join Birth_Dates SSNs > PII_Data

WasGeneratedBy

3

PII_Data:0

Used

gzip PII_Data

WasGeneratedBy

4

PII_Data.gz:0

Figure 5: A provenance graph of PII data objects that are ﬁrst fused and then transformed. The numbers mark
DLP decision conditions. Objects marked by green circles should not be restricted, while red octagons should be
restricted. Label-Based DLP correctly handles data resembling PII (1,2) and data transformations (4), but struggles
with data fusions (3). Regex-Based DLP correctly identiﬁes data fusions (3), but is prone to incorrect handling of data
resembling PII (1) and fails to identify data transformations (4). PB-DLP correctly handles all conditions.

5.4.2 Regex-Based DLP

The majority of DLP software relies on pattern matching
techniques to identify sensitive data. While enterprise so-
lutions offer greater sophistication and customizability,
their fundamental approach resembles that of Cornell’s
Spider 7, a forensics tools for identifying sensitive per-
sonal data (e.g., credit card or social security numbers).
Because it is open source, we make use of Spider as an
exemplar application for regex-based DLP.

for

set of

identifying

PII,
social

potential
a

is pre-conﬁgured with a

Regex approaches are prone to false positives.
regular
Spider
e.g.,
expressions
se-
(\d{3}-\d{2}-\d{4}) identiﬁes
curity number. However,
it is common practice for
developers to generate and distribute training datasets
to aid in software testing (Figure 5, Decision Condition
1). Spider is oblivious to information ﬂows, instead
searching for content
that bears structural similarity
to PII, and therefore would be unable to distinguish
between true PII and training data. PB-DLP tracks the
propagation of data from its source onwards, and could
trivially differentiate between true PII and training sets.
Regex approaches are also prone to false negatives.
Even after the most trivial data transformations, PII and
PCI data is no longer identiﬁable to the Spider system
(Figure 5, Decision Condition 4), permitting its exﬁltra-
tion. To demonstrate, we generated a ﬁle full of ran-
dom valid social security numbers that Spider was able to
identify. We then ran gzip on the ﬁle and stored it in a
second ﬁle. Spider was unable to identify the second ﬁle,
but PB-DLP correctly identiﬁed both ﬁles as PII since the
gzip output was derived from a sensitive input.

6 Evaluation

We now evaluate the performance of LPM. Our bench-
marks were run on a bare metal server machine with 12
GB memory and 2 Intel Xeon quad core CPUs. The Red
Hat 2.6.32 kernel was compiled and installed under 3 dif-
ferent conﬁgurations: all provenance disabled (Vanilla),

Vanilla

LPM

Provmon

0.14 (0%)
0.21 (0%)
1.6 (2%)

0.14
0.21
1.57
2.75
0.25
1.37
380
873
2990

0.14 (0%)
0.32 (52%)
2.8 (78%)
3.91 (42%)
0.25 (0%)
1.39 (1%)
401 (6%)
911 (4%)
3113 (4%)

Test Type
Process tests, times in µseconds (smaller is better)
null call
null I/O
stat
open/close ﬁle
signal install
signal handle
fork process
exec process
shell process
File and memory latencies in µseconds (smaller is better)
ﬁle create (0k)
ﬁle delete (0k)
ﬁle create (10k)
ﬁle delete (10k)
mmap latency
protect fault
page fault
100 fd select

11.2 (-3%)
8.12 (-5%)
21.6 (-8%)
12 (-4%)
1053 (-1%)
0.3 (-6%)
0.016 (0%)
1.53 (0%)

15.8 (37%)
11.8 (39%)
28.8 (23%)
14.7 (18%)
1120 (5%)
0.346 (8%)
0.016 (0%)
1.53 (0%)

2.42 (-12%)
0.25 (0%)
1.29 (-6%)
396 (4%)
879 (1%)
3000 (0%)

11.5
8.51
23.4
12.5
1062
0.32
0.016
1.53

Table 1: LMBench measurements for LPM kernels. All
times are in microseconds. Percent overhead for modi-
ﬁed conﬁgurations are shown in parenthesis.

LPM scaffolding installed but without an enabled mod-
ule (LPM), and LPM installed with the Provmon module
enabled (Provmon).

6.1 Collection Performance
We used LMBench to microbenchmark LPM’s impact
on system calls as well as ﬁle and memory latencies.
Table 1 shows the averaged results over 10 trials for
each kernel, with a percent overhead calculation against
Vanilla. For most measures, the performance differ-
ences between LPM and Vanilla are negligible. Com-
paring Vanilla to Provmon, there are several measures
in which overhead is noteworthy: stat, open/close, ﬁle
creation and deletion. Each of these benchmarks in-
volve LMBench manipulating a temporary ﬁle that re-
sides in /usr/tmp/lat_fs/. Because an absolute
path is provided, before each system call occurs LM-
Bench ﬁrst traverses the path to the ﬁle, resulting in
the creation of 3 different provenance events in Prov-
mon’s inode_permission hook, each of which is
transmitted to user space via the kernel relay. While

USENIX Association  

24th USENIX Security Symposium  329

11

Figure 6: Growth of provenance
storage overheads during kernel
compilation.

Figure 7: Performance of ancestry
queries for objects created during
kernel compilation.

Figure 8: LPM network overhead
can be reduced with batch signature
schemes.

the overheads seem large for these operations, the log-
ging of these three events only imposes approximately
1.5 microseconds per traversal. Moreover, the over-
head for opening and closing is signiﬁcantly higher than
the overhead than reads and writes (Null I/O); thus, the
higher open/close costs are likely to be amortized over
the course of regular system use. A provenance module
could further reduce this overhead by maintaining state
about past events within the kernel, then blocking the
creation of redundant provenance records.

Test
Kernel Compile
Postmark
Blast

Vanilla
598 sec
25 sec
376 sec

Provmon
614 sec
27 sec
390 sec

Overhead

2.7%
7.5%
4.8%

Table 2: Benchmarking Results. Our provenance module
imposed just 2.7% overhead on kernel compilation.

To gain a more practical sense of the costs of LPM, we
also performed multiple benchmark tests that represented
realistic system workloads. Each trial was repeated 5
times to ensure consistency. The results are summarized
in Table 2. For the kernel compile test, we recompiled
the kernel source (in a ﬁxed conﬁguration) while booted
into each of the kernels. Each compilation occurred on
16 threads. The LPM scaffolding (without an enabled
module) is not included, because in both tests it imposed
less than 1% overhead. In spite of seemingly high over-
heads for ﬁle I/O, Provmon imposes just 2.7% overhead
on kernel compilation, or 16 seconds. The Postmark test
simulates the operation of an email server. It was con-
ﬁgured to run 15,000 transactions with ﬁle sizes rang-
ing from 4 KB to 1 MB in 10 subdirectories, with up to
1,500 simultaneous transactions. The Provmon module
imposed just 7.5% overhead on this task. To estimate
LPM’s overhead for scientiﬁc applications, we ran the
BLAST benchmarks 19, which simulates biological se-
quence workloads obtained from analysis of hundreds of
thousands of jobs from the National Institutes of Health.

19See

http://ﬁehnlab.ucdavis.edu/staff/kind/Collector/Benchmark/

Blast_Benchmark

For kernel compile and postmark, Provmon outper-
forms the PASS system, which exacted 15.6% and 11.5%
overheads on kernel compilation and postmark, respec-
tively [43]. Provmon introduces comparable kernel com-
pilation overhead to Hi-Fi (2.8%) [48]. It is difﬁcult to
compare our Blast results to SPADE and PASS, as both
used a custom workload instead of a publicly available
benchmark. SPADE reports an 11.5% overhead on a
large workload [29], while PASS reports just an 0.7%
overhead. Taken as a whole, though, LPM collection ei-
ther meets or exceeds the performance of past systems,
while providing additional security assurances.

6.2 Storage Overhead
A major challenge to automated provenance collection is
the storage overhead incurred. We plotted the growth of
provenance storage using different recorders during the
kernel compilation benchmark, shown in Figure 6. LPM
generated 3.7 GB of raw provenance. This required only
450 MB of storage with the Gzip recorder, but prove-
nance cannot be efﬁciently queried in this format. The
SNAP recorder builds an in-memory provenance graph.
We approximated the storage overhead through polling
the virtual memory consumed by the recorder process in
the /proc ﬁlesystem. The SNAP graph required 1.6 GB
storage; the reduction from the raw provenance stream
is due to the fact that redundant events did not lead to
the creation of new graph components. In contrast, the
PASS system generates 1.3 GB of storage overhead dur-
ing kernel compilation while collecting less information
(e.g., shared memory). LPM’s storage overheads are thus
comparable to other provenance-aware systems.

6.3 Query Performance (PB-DLP)
We evaluated query performance using our exemplar PB-
DLP application and LPM’s SNAP recorder. The prove-
nance graph that was populated using the routine from
the kernel compile benchmark. This yielded a raw prove-
nance stream of 3.7 GB, which was translated by the
recorder into a graph of 6,513,398 nodes and 6,754,059

330  24th USENIX Security Symposium 

USENIX Association

12

 0 1 2 3 4 5 0 2 4 6 8 10Storage Cost (GBytes)Time (Minutes)Raw Prov. StreamGZip RecorderSNAP Recorder 0.95 0.96 0.97 0.98 0.99 1 0 5 10 15 20 25Cumulative DensityResponse Time (Milliseconds) 0 1000 2000 3000 4000 5000Iperf PerformanceThroughput (Mbps)VanillaLPMProvmonBatch Sigedges. We were then able to use the graph to issue an-
cestry queries, in which the subgraphs were traversed to
ﬁnd the data object ancestors of a given ﬁle. Because
we did not want ephemeral objects with limited ances-
tries to skew our results, we only considered the results
of objects with more than 50 ancestors.

In the worst case, which was a node that had 17,696
ancestors, the query returned in just 21 milliseconds.
Effectively, we were able to query object ancestries at
line speed for network activity. We are conﬁdent that
this approach can scale to large databases through pre-
computation of expensive operations at data ingest, mak-
ing it a promising strategy for provenance-aware dis-
tributed systems; however, we note that these results are
highly dependent on the size of the graph. Our test graph,
while large, would inevitably be dwarfed by the size of
the provenance on long-lived systems. Fortunately, there
are also a variety of techniques for reducing these costs.
Bates et al. show that the results from provenance graph
traversals can be extensively cached when using a ﬁxed
security policy, which would allow querying to amortize
to a small constant cost [7]. LPM could also be extended
to support deduplication [63, 62] and policy-based prun-
ing [6, 13], both of which could further improve perfor-
mance by reducing the size of provenance graphs.

6.4 Message Commitment Protocol
Under each kernel conﬁguration, we performed iperf
benchmarking to discover LPM’s impact on TCP
throughput. iperf was launched in both client and server
mode over localhost.
The client was launched
with 16 threads, two per each CPU. Our results can be
found in Figure 8. The Vanilla kernel reached 4490
Mbps. While the LPM framework imposed negligible
cost compared to the vanilla kernel (4480 Mbps), Prov-
mon’s DSA-based message commitment protocol re-
duced throughput by an order of magnitude (482 Mbps).
Through use of printk instrumentation, we found that
the average overhead per packet signature was 1.2 ms.

This result is not surprising when compared to IPsec
performance. IPsec’s Authentication Header (AH) mode
uses an HMAC-based approach to provide similar guar-
antees as our protocol. AH has been shown to reduce
throughput by as much as half [16]. An HMAC approach
is a viable alternative to establish integrity and data ori-
gin authenticity and would also ﬁt into the options ﬁeld,
but would require the negotiation of IPsec security as-
sociations. Our message commitment protocol has the
beneﬁt of being fully interoperable with other hosts, and
does not require a negotiation phase before communi-
cation occurs. Another option for increasing through-
put would be to employ CPU instruction extensions [30]
and security co-processor [19] to accelerate the speed of

DSA. Yet another approach to reducing our impact on
network performance would be to employ a batch signa-
ture scheme [11]. We tested this by transmitting a sig-
nature over every 10 packets during TCP sessions, and
found that throughput increased by 3.3 times to approx-
imately 1600 Mbps. Due to the fact that this overhead
may not be suitable for some environments, Provmon
can be conﬁgured to use Hi-Fi identiﬁers [48], which are
vulnerable to network attack but impose negligible over-
head. LPM’s impact on network performance is speciﬁc
to the particular module, and can be tailored to meet the
needs of the system.

7 Discussion

Without the aid of provenance-aware applications, LPM
will struggle to accurately track dependencies through
workﬂow layer abstractions. The most obvious example
of such an abstraction is the copy/paste buffer in window-
ing applications like Xorg. This is a known side channel
for kernel layer security mechanisms, one that has been
addressed by the Trusted Solaris project [10], Trusted X
[21, 22], the SELinux-aware X window system [37], Se-
cureView 20, and General Dynamics’ TVE 21. Without
provenance-aware windowing, LPM will conservatively
assume that all ﬁles opened for reading are dependencies
of the paste buffer, leading to false dependencies. LPM
is also unable to observe system side channels, such as
timing channels or L2 cache measurements [51], a limi-
tation shared by many other security solutions [18].

Although we have not presented a secure distributed
provenance-aware system in this work, LPM provides
the foundation for the creation of such a system. In the
presented modules, provenance is stored locally by the
host and retrieved on an as-needed basis from other hosts.
This raises availability concerns as hosts inevitably begin
to fail. Availability could be improved with minimal per-
formance and storage overheads through Gehani et al.’s
approach of duplicating provenance at k neighbors with
a limited graph depth d [27, 28].

Finally, LPM does not address the matter of prove-
nance conﬁdentiality; this is an important challenge that
is explored elsewhere in the literature [14, 46]. LPM’s
Recorders provide interfaces that can be used to intro-
duce an access control layer onto the provenance store.

8 Related Work

While myriad provenance-aware systems have been pro-
posed in the literature, the majority disclose provenance
within an application [32, 41, 65] or workﬂow [24, 58]. It

20See http://www.ainfosec.com/secureview
21See http://gdc4s.com/tve.html

USENIX Association  

24th USENIX Security Symposium  331

13

is difﬁcult or impossible to obtain complete provenance
in this manner. This is because systems events that occur
outside of the application, but still effect its execution,
will not appear in the provenance record.

The alternative to disclosed systems are automatic
provenance-aware systems, which collect provenance
transparently within the operating system. Gehani et
al.’s SPADE is a multi-platform system for eScience and
grid computing audiences, with an emphasis on low la-
tency and availability in distributed environments [29].
SPADE’s provenance reporters make use of familiar ap-
plication layer utilities to generate provenance, such as
polling ps for process information and lsof for net-
work information. This gives rise to the possibility of in-
complete provenance due to race conditions. The PASS
project collects the provenance of system calls at the vir-
tual ﬁlesystem (VFS) layer. PASSv1 provides base func-
tions for provenance collection that observe processes’
ﬁle I/O activity [43]. Because these basic functions are
manually placed around the kernel, there is no clear
way to extend PASSv1 to support additional collection
hooks; we address this limitation in the modular design
of LPM. PASSv2 introduces a Disclosed Provenance API
for tighter integration between provenance collected at
different layers of abstraction, e.g., at the application
layer [44]. PASSv2 assumes that disclosing processes
are benign, while LPM provides a secure disclosure
mechanism for attesting the correctness of provenance-
aware applications. Both SPADE and PASS are designed
for benign environments, making no attempt to protect
their collection mechanisms from an adversary.

Previous work has considered the security of prove-
nance under relaxed threat models relative to LPM’s. In
SProv, Hasan et al. introduce provenance chains, cryp-
tographic constructs that prevent the insertion or dele-
tion of provenance inside of a series of events [32].
SProv effectively demonstrates the authentication prop-
erties of this primitive, but is not intended to serve as a
secure provenance-aware system; attackers can still ap-
pend false records to the end of the chain, delete the
whole chain, or disable the library altogether. Zhou et
al. consider provenance corruption an inevitability, and
show that provenance can detect some malicious hosts in
distributed environments provided that a critical mass of
correct hosts still exist [65]. They later strengthen these
assurances through use of provenance-aware software-
deﬁned networking [5]. These systems consider only
network events, and are unable to speak to the internal
state of hosts. Lyle and Martin sketch the design for
a secure provenance monitor based on trusted comput-
ing [40]. However, they conceptualize provenance as
a TPM-aided proof of code execution, overlooking in-
terprocess communication and other system activity that
could inform execution results, and therefore offer infor-

mation that is too coarse-grained to meet the needs of
some applications. Moreover, to the best of our knowl-
edge their system is unimplemented.

The most promising model to date for secure prove-
nance collection is Pohly et al.’s Hi-Fi system [48]. Hi-Fi
is a Linux Security Module (LSM) that collects whole-
system provenance that details the actions of processes,
IPC mechanisms, and even the kernel itself (which does
not exclusively use system calls). Hi-Fi attempts to pro-
vide a provenance reference monitor [42], but remains
vulnerable to the provenance-aware adversary that we
describe in Section 3.2. Enabling Hi-Fi blocks the in-
stallation of other LSM’s, such as SELinux or Tomoyo,
or requires a third party patch to permit module stack-
ing. This blocks the installation of MAC policy on the
host, preventing runtime integrity assurances. Hi-Fi is
also vulnerable to adversaries in the network, who can
strip the provenance identiﬁers from packets in transit,
resulting in irrecoverable provenance. Unlike LPM, Hi-
Fi does not attempt to provide layered provenance ser-
vices, and therefore does not consider the integrity and
authenticity of provenance-aware applications.

Provenance collection is a form of information ﬂow
monitoring that is related, but fundamentally distinct,
from past areas of study. Due to space constraints, our
discussion of Information Flow Control (IFC) systems
has been relegated to our technical report [8].

9 Conclusion

In this work, we have presented LPM, a platform for
the creation of trusted provenance-aware execution en-
vironments. Our system imposes as little as 2.7% per-
formance overhead on normal system operation, and can
respond to queries about data object ancestry in tens of
milliseconds. We have used LPM as the foundation of
a provenance-based data loss prevention system that can
scan ﬁle transmissions to detect the presence of sensitive
ancestors in just tenths of a second. The Linux Prove-
nance Module Framework is an exciting step forward for
both provenance- and security-conscious communities.

Acknowledgements

We would like to thank Rob Cunningham, Alin Do-
bra, Will Enck, Jun Li, Al Malony, Patrick McDaniel,
Daniela Oliveira, Nabil Schear, Micah Sherr, and Patrick
Traynor for their valuable comments and insight, as well
as Devin Pohly for his sustained assistance in working
with Hi-Fi, and Mugdha Kumar for her help developing
LPM SPADE support. This work was supported in part
by the US National Science Foundation under grant num-
bers CNS-1118046, CNS-1254198, and CNS-1445983.

332  24th USENIX Security Symposium 

USENIX Association

14

Availability

The LPM code base, including all user space utilities and
patches for both Red Hat and the mainline Linux kernels,
is available at http://linuxprovenance.org.

References
[1] Symantec Data Loss Prevention Customer Brochure. http://

www.symantec.com/data-loss-prevention.

[2] What’s Yours

is Mine:

Intellectual Property at Risk.

Your
symantec.com/mktginfo/whitepaper/WP_
WhatsYoursIsMine-HowEmployeesarePutting
YourIntellectualPropertyatRisk_dai211501_
cta69167.pdf.

How Employees

are Putting
https://www4.

[3] R. Aldeco-Pérez and L. Moreau. Provenance-based Auditing of
Private Data Use. In Proceedings of the 2008 International Con-
ference on Visions of Computer Science, VoCS’08, Sept. 2008.

[4] J. P. Anderson. Computer Security Technology Planning Study.
Technical Report ESD-TR-73-51, Air Force Electronic Systems
Division, 1972.

[5] A. Bates, K. Butler, A. Haeberlen, M. Sherr, and W. Zhou. Let
SDN Be Your Eyes: Secure Forensics in Data Center Networks.
In NDSS Workshop on Security of Emerging Network Technolo-
gies, SENT, Feb. 2014.

[6] A. Bates, K. R. B. Butler, and T. Moyer. Take Only What You
Need: Leveraging Mandatory Access Control Policy to Reduce
Provenance Storage Costs. In 7th Workshop on the Theory and
Practice of Provenance, TaPP’15, July 2015.

[7] A. Bates, B. Mood, M. Valafar, and K. Butler. Towards Secure
Provenance-based Access Control in Cloud Environments.
In
Proceedings of the 3rd ACM Conference on Data and Applica-
tion Security and Privacy, CODASPY ’13, pages 277–284, New
York, NY, USA, 2013. ACM.

[8] A. Bates, D. Tian, K. R. B. Butler, and T. Moyer. Linux Prove-
nance Modules: Trustworthy Whole-System Provenance for the
Linux Kernel. Technical Report REP-2015-578, University of
Florida CISE Dept, 2015.

[9] M. Bellare, R. Canetti, and H. Krawczyk. Keyed Hash Functions
and Message Authentication. In Proceedings of Crypto’96, vol-
ume 1109 of LNCS, pages 1–15, 1996.

[10] M. Bellis, S. Lofthouse, H. Grifﬁn, and D. Kucukreisoglu.

Trusted Solaris 8 4/01 Security Target. 2003.

[11] A. Bittau, D. Boneh, M. Hamburg, M. Handley, D. Mazieres,
Cryptographic protection of TCP Streams
https://tools.ietf.org/html/

and Q. Slack.
(tcpcrypt).
draft-bittau-tcp-crypt-01.

[12] D. Boneh, B. Lynn, and H. Shacham. Short Signatures from the
Weil Pairing. In C. Boyd, editor, Advances in Cryptology – ASI-
ACRYPT 2001, volume 2248 of Lecture Notes in Computer Sci-
ence, pages 514–532. Springer Berlin Heidelberg, 2001.

[13] U. Braun, S. L. Garﬁnkel, D. A. Holland, K.-K. Muniswamy-
Reddy, and M. I. Seltzer. Issues in Automatic Provenance Col-
lection. In International Provenance and Annotation Workshop,
pages 171–183, 2006.

[14] U. Braun and A. Shinnar. A Security Model for Provenance.
Technical Report TR-04-06, Harvard University Computer Sci-
ence Group, 2006.

[15] D. Catalano, M. Di Raimondo, D. Fiore, and R. Gennaro. Off-
line/On-line Signatures: Theoretical Aspects and Experimental
Results. In PKC’08: Proceedings of the Practice and theory in
public key cryptography, 11th international conference on Pub-
lic key cryptography, pages 101–120, Berlin, Heidelberg, 2008.
Springer-Verlag.

[16] S. Chaitanya, K. Butler, A. Sivasubramaniam, P. McDaniel, and
M. Vilayannur. Design, Implementation and Evaluation of Secu-
rity in iSCSI-based Network Storage Systems. In Proceedings of
the Second ACM Workshop on Storage Security and Survivability,
StorageSS ’06, pages 17–28, New York, NY, USA, 2006. ACM.
[17] D. D. Clark and D. R. Wilson. A Comparison of Commercial and
In IEEE S&P, Oakland,

Military Computer Security Policies.
CA, USA, Apr. 1987.

[18] D. Cock, Q. Ge, T. Murray, and G. Heiser. The Last Mile: An
Empirical Study of Some Timing Channels on seL4.
In ACM
Conference on Computer and Communications Security, pages
570–581, Scottsdale, AZ, USA, nov 2014.

[19] J. Dyer, M. Lindemann, R. Perez, R. Sailer, L. van Doorn, and
S. Smith. Building the IBM 4758 Secure Coprocessor. Computer,
34(10):57–66, Oct 2001.

[20] A. Edwards, T. Jaeger, and X. Zhang. Runtime Veriﬁcation of
Authorization Hook Placement for the Linux Security Modules
Framework. In Proceedings of the 9th ACM Conference on Com-
puter and Communications Security, CCS’02, 2002.

[21] J. Epstein and J. Picciotto. Trusting X: Issues in Building Trusted
X Window Systems -or- Whatâ ˘A ´Zs Not Trusted About X. In Pro-
ceedings of the 14th Annual National Computer Security Confer-
ence, 1991.

[22] J. Epstein and M. Shugerman. A Trusted X Window System
Server for Trusted Mach. In USENIX MACH Symposium, pages
141–156, 1990.

[23] S. Even, O. Goldreich, and S. Micali. On-line/off-line Digital
Signatures. In Proceedings on Advances in cryptology, CRYPTO
’89, pages 263–275, New York, NY, USA, 1989. Springer-Verlag
New York, Inc.

[24] I. T. Foster, J.-S. Vöckler, M. Wilde, and Y. Zhao. Chimera: AVir-
tual Data System for Representing, Querying, and Automating
Data Derivation. In Proceedings of the 14th Conference on Sci-
entiﬁc and Statistical Database Management, SSDBM’02, July
2002.

[25] V. Ganapathy, T. Jaeger, and S. Jha. Automatic placement of
authorization hooks in the linux security modules framework. In
Proceedings of the 12th ACM Conference on Computer and Com-
munications Security, CCS ’05, pages 330–339, New York, NY,
USA, 2005. ACM.

[26] C.-z. Gao and Z.-a. Yao. A Further Improved Online/Ofﬂine Sig-

nature Scheme. Fundam. Inf., 91:523–532, August 2009.

[27] A. Gehani, B. Baig, S. Mahmood, D. Tariq, and F. Zaffar. Fine-
grained Tracking of Grid Infections.
In Proceedings of the
11th IEEE/ACM International Conference on Grid Computing,
GRID’10, Oct 2010.

[28] A. Gehani and U. Lindqvist. Bonsai: Balanced Lineage Authen-
tication. In Proceedings of the 23rd Annual Computer Security
Applications Conference, ACSAC’07, Dec 2007.

[29] A. Gehani and D. Tariq. SPADE: Support for Provenance Audit-
ing in Distributed Environments. In Proceedings of the 13th In-
ternational Middleware Conference, Middleware ’12, Dec 2012.
[30] S. Gueron and V. Krasnov. Speed Up Big-Number Multiplication
Using Single Instruction Multiple Data (SIMD) Architectures,
June 7 2012. US Patent App. 13/491,141.

USENIX Association  

24th USENIX Security Symposium  333

15

[31] C. Hanson. SELinux and MLS: Putting The Pieces Together. In

In Proceedings of the 2nd Annual SELinux Symposium, 2006.

[32] R. Hasan, R. Sion, and M. Winslett. The Case of the Fake Pi-
casso: Preventing History Forgery with Secure Provenance. In
Proceedings of the 7th USENIX Conference on File and Storage
Technologies, FAST’09, San Francisco, CA, USA, Feb. 2009.

[33] B. Hicks, S. Rueda, L. St.Clair, T. Jaeger, and P. McDaniel.
A Logical Speciﬁcation and Analysis for SELinux MLS Policy.
ACM Trans. Inf. Syst. Secur., 13(3):26:1–26:31, July 2010.

[34] T. Jaeger, A. Edwards, and X. Zhang. Consistency Analysis of
Authorization Hook Placement in the Linux Security Modules
Framework. ACM Trans. Inf. Syst. Secur., 7(2):175–205, May
2004.

[35] T. Jaeger, R. Sailer, and U. Shankar. PRIMA: Policy-reduced
Integrity Measurement Architecture. In Proceedings of the 11th
ACM Symposium on Access Control Models and Technologies,
SACMAT ’06, pages 19–28, New York, NY, USA, 2006. ACM.
[36] S. Kent and R. Atkinson. RFC 2406: IP Encapsulating Security

Payload (ESP). 1998.

[37] D. Kilpatrick, W. Salamon, and C. Vance. Securing the X Win-

dow System with SELinux. Technical report, Jan. 2003.

[38] L. Lamport. Time, Clocks, and the Ordering of Events in a Dis-

tributed System. Commun. ACM, 21(7):558–565, July 1978.

[39] H. Luhn. Computer for Verifying Numbers, Aug. 23 1960. US

Patent 2,950,048.

[40] J. Lyle and A. Martin. Trusted Computing and Provenance: Bet-
In 2nd Workshop on the Theory and Practice of

ter Together.
Provenance, TaPP’10, Feb. 2010.

[41] P. Macko and M. Seltzer. A General-purpose Provenance Li-
In 4th Workshop on the Theory and Practice of Prove-

brary.
nance, TaPP’12, June 2012.

[42] P. McDaniel, K. Butler, S. McLaughlin, R. Sion, E. Zadok, and
M. Winslett. Towards a Secure and Efﬁcient System for End-to-
End Provenance. In Proceedings of the 2nd conference on The-
ory and practice of provenance, San Jose, CA, USA, Feb. 2010.
USENIX Association.

[43] K. Muniswamy-Reddy, D. A. Holland, U. Braun, and M. Seltzer.
Provenance-Aware Storage Systems. In Proceedings of the 2006
USENIX Annual Technical Conference, 2006.

[44] K.-K. Muniswamy-Reddy, U. Braun, D. A. Holland, P. Macko,
D. Maclean, D. Margo, M. Seltzer, and R. Smogor. Layering in
Provenance Systems. In Proceedings of the 2009 Conference on
USENIX Annual Technical Conference, ATC’09, June 2009.

[45] D. Nguyen, J. Park, and R. Sandhu. Dependency Path Patterns
As the Foundation of Access Control in Provenance-aware Sys-
tems. In Proceedings of the 4th USENIX Conference on Theory
and Practice of Provenance, TaPP’12, pages 4–4, Berkeley, CA,
USA, 2012. USENIX Association.

[46] Q. Ni, S. Xu, E. Bertino, R. Sandhu, and W. Han. An Access
In Secure

Control Language for a General Provenance Model.
Data Management, Aug. 2009.

[47] J. Park, D. Nguyen, and R. Sandhu. A Provenance-Based Access
Control Model. In Proceedings of the 10th Annual International
Conference on Privacy, Security and Trust (PST), pages 137–144,
2012.

[48] D. Pohly, S. McLaughlin, P. McDaniel, and K. Butler. Hi-Fi: Col-
lecting High-Fidelity Whole-System Provenance. In Proceedings
of the 2012 Annual Computer Security Applications Conference,
ACSAC ’12, Orlando, FL, USA, 2012.

[49] J. Postel. RFC 791: Internet protocol. 1981.

[50] A. C. Revkin. Hacked E-mail is New Fodder for Climate Dispute.

New York Times, 20, 2009.

[51] T. Ristenpart, E. Tromer, H. Shacham, and S. Savage. Hey, You,
Get Off of My Cloud: Exploring Information Leakage in Third-
Party Compute Clouds.
In Proceedings of the 16th ACM Con-
ference on Computer and Communications Security (CCS’09),
pages 199–212, Chicago, IL, USA, Oct. 2009. ACM.

[52] R. Sailer, X. Zhang, T. Jaeger, and L. van Doorn. Design and
Implementation of a TCG-based Integrity Measurement Archi-
tecture.
In SSYM’04: Proceedings of the 13th conference on
USENIX Security Symposium, pages 16–16, Berkeley, CA, USA,
2004. USENIX Association.
P. Cao.

Lineage

System.

File

[53] C.

Sar

and

http://crypto.stanford.edu/~cao/lineage.html.

[54] J. Seibert, G. Baah, J. Diewald, and R. Cunningham. Using
Provenance To Expedite MAC Policies (UPTEMPO) (Previously
Known as IPDAM). Technical Report USTC-PM-015, MIT Lin-
coln Laboratory, October 2014.

[55] A. Shamir and Y. Tauman.

Improved Online/Ofﬂine Signa-
In J. Kilian, editor, Advances in Cryptology —
ture Schemes.
CRYPTO 2001, volume 2139 of Lecture Notes in Computer Sci-
ence, pages 355–367. Springer Berlin / Heidelberg, 2001.

[56] D. Tariq, B. Baig, A. Gehani, S. Mahmood, R. Tahir, A. Aqil, and
F. Zaffar. Identifying the Provenance of Correlated Anomalies. In
Proceedings of the 2011 ACM Symposium on Applied Computing,
SAC ’11, Mar. 2011.

[57] The Netﬁlter Core Team. The Netﬁlter Project: Packet Mangling

for Linux 2.4. http://www.netﬁlter.org/, 1999.

[58] J. Widom. Trio: A System for Integrated Management of Data,
Accuracy, and Lineage. Technical Report 2004-40, Stanford In-
foLab, Aug. 2004.

[59] World Wide Web Consortium. PROV-Overview: An Overview
of the PROV Family of Documents. http://www.w3.org/
TR/prov-overview/, 2013.

[60] C. Wright, C. Cowan, J. Morris, S. Smalley, and G. Kroah-
Hartman. Linux Security Module Framework. In Ottawa Linux
Symposium, page 604, 2002.

[61] C. Wright, C. Cowan, S. Smalley, J. Morris, and G. Kroah-
Hartman. Linux security modules: General security support for
the linux kernel.
In Proceedings of the 11th USENIX Security
Symposium, pages 17–31, Berkeley, CA, USA, 2002. USENIX
Association.

[62] Y. Xie, K.-K. Muniswamy-Reddy, D. Feng, Y. Li, and D. D. E.
Long. Evaluation of a Hybrid Approach for Efﬁcient Provenance
Storage. Trans. Storage, 9(4):14:1–14:29, Nov. 2013.

[63] Y. Xie, K.-K. Muniswamy-Reddy, D. D. E. Long, A. Amer,
D. Feng, and Z. Tan. Compressing Provenance Graphs, June
2011.

[64] X. Zhang, A. Edwards, and T. Jaeger. Using CQUAL for Static
In Proceedings of

Analysis of Authorization Hook Placement.
the 11th USENIX Security Symposium, 2002.

[65] W. Zhou, Q. Fei, A. Narayan, A. Haeberlen, B. T. Loo, and
In Proceedings of
M. Sherr.
the 23rd ACM Symposium on Operating Systems Principles,
SOSP’11, Oct. 2011.

Secure Network Provenance.

[66] W. Zhou, M. Sherr, T. Tao, X. Li, B. T. Loo, and Y. Mao. Efﬁcient
Querying and Maintenance of Network Provenance at Internet-
Scale. In ACM SIGMOD International Conference on Manage-
ment of Data (SIGMOD), June 2010.

334  24th USENIX Security Symposium 

USENIX Association

16

